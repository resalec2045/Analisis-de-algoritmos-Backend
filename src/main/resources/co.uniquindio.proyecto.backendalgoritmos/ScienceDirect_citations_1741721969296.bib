@article{SHYJA2023100465,
title = {Link quality and energy efficient optimal simplified cluster based routing scheme to enhance lifetime for wireless body area networks},
journal = {Nano Communication Networks},
volume = {37},
pages = {100465},
year = {2023},
issn = {1878-7789},
doi = {https://doi.org/10.1016/j.nancom.2023.100465},
url = {https://www.sciencedirect.com/science/article/pii/S1878778923000315},
author = {V. Irine Shyja and G. Ranganathan and V. Bindhu},
keywords = {WBAN, Clustering, Cluster head, Multipath routing scheme, Link quality},
abstract = {Monitoring of patient’s health in the medical industry can be enabled using wireless body area networks (WBANs), which are already used for various purposes, including assisting in human safety. It is imperative to use better power management strategies since the body sensors are small and the battery cannot hold a charge for a long time. Due to the vast amounts of information generated by medical sensors, resource-constrained networks face a significant challenge when guaranteeing the specified quality of service (QoS). Moreover, the WBAN regularly meets the primary hassle of QoS degradation because of congestion WBAN structure can easily compromise heterogeneous and complex networks. Either inappropriate data collection or using energy effectively to transmit medical data without the expense of travel and length has become an important one. To address this issue, the present research work ‘Link Quality and Energy Efficient Optimal Clustering-Multipath (LEOC-MP)’ scheme tries to explore an answer. The main goals of the LEOC-MP (Optimal Link Quality and Energy Efficient Optimal Clustering-Multipath) system are to guarantee node-to-node link quality, lengthen network life, and compute high-performing cluster heads to guarantee reliable multi path data transfer. This work was executed in three phases. First, an optimal simplified clustering technique for data collection from body sensors using an improved pelican optimization (ICO) algorithm is introduced. Next, multiple design constraints for node rank computation, energy efficiency, link quality, path loss, distance, and delay are used. Besides, an Auto-Regressive Probabilistic Neural Network (AR-PNN) is introduced to optimize those design constraints and compute the cluster head (CH) of each cluster. Multipath firing is then performed using a moderated puffer-fish optimization (MPO) algorithm that finds the closest optimal and shortest node to transmit optimal drug data. The work is simulated using an NS-3 environment, and the results are obtained. The outcome of this work is analyzed with existing methodologies, and the results prove that the present work consistently outperforms the existing methodologies.}
}
@article{SCHAEFER2024108796,
title = {GPT-4 as a biomedical simulator},
journal = {Computers in Biology and Medicine},
volume = {178},
pages = {108796},
year = {2024},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2024.108796},
url = {https://www.sciencedirect.com/science/article/pii/S0010482524008813},
author = {Moritz Schaefer and Stephan Reichl and Rob {ter Horst} and Adele M. Nicolas and Thomas Krausgruber and Francesco Piras and Peter Stepper and Christoph Bock and Matthias Samwald},
keywords = {Biomedical simulation, Large language models, GPT-4, Computational biology, Artificial intelligence},
abstract = {Background
Computational simulation of biological processes can be a valuable tool for accelerating biomedical research, but usually requires extensive domain knowledge and manual adaptation. Large language models (LLMs) such as GPT-4 have proven surprisingly successful for a wide range of tasks. This study provides proof-of-concept for the use of GPT-4 as a versatile simulator of biological systems.
Methods
We introduce SimulateGPT, a proof-of-concept for knowledge-driven simulation across levels of biological organization through structured prompting of GPT-4. We benchmarked our approach against direct GPT-4 inference in blinded qualitative evaluations by domain experts in four scenarios and in two quantitative scenarios with experimental ground truth. The qualitative scenarios included mouse experiments with known outcomes and treatment decision support in sepsis. The quantitative scenarios included prediction of gene essentiality in cancer cells and progression-free survival in cancer patients.
Results
In qualitative experiments, biomedical scientists rated SimulateGPT's predictions favorably over direct GPT-4 inference. In quantitative experiments, SimulateGPT substantially improved classification accuracy for predicting the essentiality of individual genes and increased correlation coefficients and precision in the regression task of predicting progression-free survival.
Conclusion
This proof-of-concept study suggests that LLMs may enable a new class of biomedical simulators. Such text-based simulations appear well suited for modeling and understanding complex living systems that are difficult to describe with physics-based first-principles simulations, but for which extensive knowledge is available as written text. Finally, we propose several directions for further development of LLM-based biomedical simulators, including augmentation through web search retrieval, integrated mathematical modeling, and fine-tuning on experimental data.}
}
@incollection{2007229,
title = {Chapter 6 - Computational Methods for Optimal Filtering of Stochastic Signals},
editor = {A. Torokhti and P. Howlett},
series = {Mathematics in Science and Engineering},
publisher = {Elsevier},
volume = {212},
pages = {229-290},
year = {2007},
booktitle = {Computational Methods for Modelling of Nonlinear Systems},
issn = {0076-5392},
doi = {https://doi.org/10.1016/S0076-5392(07)80049-X},
url = {https://www.sciencedirect.com/science/article/pii/S007653920780049X}
}
@incollection{LUND202435,
title = {3 - Choice Awareness strategies},
editor = {Henrik Lund},
booktitle = {Renewable Energy Systems (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
pages = {35-50},
year = {2024},
isbn = {978-0-443-14137-9},
doi = {https://doi.org/10.1016/B978-0-443-14137-9.00003-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443141379000036},
author = {Henrik Lund},
keywords = {Alternatives assessment, Choice Awareness, Feasibility studies, Institutional barriers, Market barriers, Path dependency, Public regulation, Radical technological change, Renewable energy systems, Technical alternatives},
abstract = {This chapter introduces strategies to raise the awareness of how to implement smart energy systems in fully decarbonized societies using the Choice Awareness theory. Choice Awareness is based on the understanding that existing institutional perceptions and organizational interests will often seek to eliminate certain choices from the political decision-making process, when the introduction of radical technological change is discussed. The counterstrategy is to raise public awareness that alternatives do exist and that it is possible to make a choice. Counterstrategies may involve the design of technical alternatives, feasibility studies based on institutional economic thinking, and the design of public regulation measures seen in the light of conflicting interests as well as changes in the democratic decision-making infrastructure. Each of the strategies is elaborated on in this chapter.}
}
@article{ZHANG2010S238,
title = {Biomimetic Construction of Category Mental Imagery Based on Recognition Mechanism of Visual Cortex of Human Brain},
journal = {Journal of Bionic Engineering},
volume = {7},
pages = {S238-S244},
year = {2010},
issn = {1672-6529},
doi = {https://doi.org/10.1016/S1672-6529(09)60241-9},
url = {https://www.sciencedirect.com/science/article/pii/S1672652909602419},
author = {Xianghe Zhang and Dan Wang and Luquan Ren and Pingping Liu},
keywords = {multi-dimensional space biomimetic informatics, artificial intelligence, cognitive science, mental imagery, visual cortex, object recognition},
abstract = {The principle of homology-continuity in Multi-Dimensional Biomimetic Informatics Space is applied to construct the identifying mechanism of category of deep representation of mental imagery. The model of each cerebral region involved in recognizing is established respectively and a feedforward method for establishing category mental imagery is proposed. First, the model of feature acquisition is developed based on Hubel-Wiesel model, and Gaussian function is used to simulate the simple cell receptive field to satisfy the specific function of visual cortex. Second, multiple input aggregation operation is employed to simulate the feature output of complex cells to get the invariance representation in feature space. Then, imagery basis is extracted by unsupervised learning algorithm based on the primary feature and category mental imagery is obtained by building Radial Basis Function (RBF) network. Finally, the system model is tested by training set and test set composed of real images. Experimental results show that the proposed method can establish valid deep representation of these samples, based on which the biomimetic construction of category mental imagery can be achieved. This method provides a new idea for solving imagery problem and studying imagery thinking.}
}
@article{GILBERT2018278,
title = {Decoding intentions of self and others from fMRI activity patterns},
journal = {NeuroImage},
volume = {172},
pages = {278-290},
year = {2018},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2017.12.090},
url = {https://www.sciencedirect.com/science/article/pii/S105381191731114X},
author = {Sam J. Gilbert and Hoki Fung},
keywords = {Intentions, MVPA, fMRI},
abstract = {Previous studies using multi-voxel pattern analysis have decoded the content of participants' delayed intentions from patterns of fMRI data. Here we investigate whether this technique can be used to decode not only participants' own intentions, but also their representation of the intentions held by other people. In other words: if Sam is thinking about Hoki, can we decode the content of Hoki's intention by scanning Sam's brain? We additionally distinguished two components of intentions: action-plans versus goals, and included novel control analyses that allowed us to distinguish intending an outcome from simply expecting it to occur or simulating its consequences. Regions of frontal, parietal, and occipital cortex contained patterns from which it was possible to decode intentions of both self and other. Furthermore, crossclasification between self and other was possible, suggesting overlap between the two. Control analyses suggested that these results reflected visuo-spatial processes by which intentions were generated in our paradigm, rather than anything special about intentions per se. There was no evidence for any representation of intentions as mental states distinct from visuospatial processes involved in generating their content and/or simulating their outcomes. These findings suggest that the brain activity patterns decoded in intention-decoding fMRI studies may reflect domain-general processes rather than being intention-specific.}
}
@article{LAW2023112555,
title = {Frontopolar cortex represents complex features and decision value during choice between environments},
journal = {Cell Reports},
volume = {42},
number = {6},
pages = {112555},
year = {2023},
issn = {2211-1247},
doi = {https://doi.org/10.1016/j.celrep.2023.112555},
url = {https://www.sciencedirect.com/science/article/pii/S2211124723005661},
author = {Chun-Kit Law and Nils Kolling and Chetwyn C.H. Chan and Bolton K.H. Chau},
keywords = {frontopolar cortex, ventromedial prefrontal cortex, decision making, environment choice, convolutional neural network, CNN},
abstract = {Summary
Important decisions often involve choosing between complex environments that define future item encounters. Despite its importance for adaptive behavior and distinct computational challenges, decision-making research primarily focuses on item choice, ignoring environment choice altogether. Here we contrast previously studied item choice in ventromedial prefrontal cortex with lateral frontopolar cortex (FPl) linked to environment choice. Furthermore, we propose a mechanism for how FPl decomposes and represents complex environments during decision making. Specifically, we trained a choice-optimized, brain-naive convolutional neural network (CNN) and compared predicted CNN activation with actual FPl activity. We showed that the high-dimensional FPl activity decomposes environment features to represent the complexity of an environment to make such choice possible. Moreover, FPl functionally connects with posterior cingulate cortex for guiding environment choice. Further probing FPl’s computation revealed a parallel processing mechanism in extracting multiple environment features.}
}
@article{SCHUMNY1993319,
title = {Nanosystems - molecular machinery, manufacturing, and computation: by K. Eric Drexler. John Wiley & Sons, Chichester, England, 1992. ISBN 0-471-57518-6. 556 pages. Illustrated, Appendices, Glossary with detailed explanations, 337 references, extended Index.},
journal = {Computer Standards & Interfaces},
volume = {15},
number = {2},
pages = {319-320},
year = {1993},
note = {Object-Oriented Reference Models},
issn = {0920-5489},
doi = {https://doi.org/10.1016/0920-5489(93)90019-N},
url = {https://www.sciencedirect.com/science/article/pii/092054899390019N},
author = {Harald Schumny}
}
@article{MEY1988743,
title = {Computation and the soul},
journal = {Journal of Pragmatics},
volume = {12},
number = {5},
pages = {743-789},
year = {1988},
issn = {0378-2166},
doi = {https://doi.org/10.1016/0378-2166(88)90056-2},
url = {https://www.sciencedirect.com/science/article/pii/0378216688900562},
author = {Jacob L. Mey and Mary Talbot},
abstract = {This article is both a review of Sperber and Wilson's Relevance and a broader critical discussion of linguistic pragmatics, from which Relevance has arisen. Four separate sections focus on Communication, Assumptions, the Metaphor of the Black Box and the Principle of Relevance itself. The authors conclude that the reductionist model of the human mind as an information-processing device, developed by Sperber and Wilson, is irredeemably asocial and therefore relevant to neither communication nor cognition.}
}
@article{AKPOLAT2025,
title = {Enhancing operational reliability for high penetration of green hydrogen production in energy islands: A power-to-X case study},
journal = {International Journal of Hydrogen Energy},
year = {2025},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2025.02.131},
url = {https://www.sciencedirect.com/science/article/pii/S0360319925006962},
author = {Alper Nabi Akpolat},
keywords = {Distributed energy resources, Energy islands, Green hydrogen production, Power electronic converters, Machine learning, Power-to-X, Reliability},
abstract = {The production, storage, and conversion of hydrogen into energy, as well as its use in areas such as green ammonia production for agriculture or the catalysis of natural gas, are of significant interest due to their stable structure and source diversity. For this purpose, energy islands (EIs) have been established near consumption points, and renewable energy (RE) obtained from photovoltaic (PV) panels and wind turbines (WTs) is used for green hydrogen production (GHP). In these EIs, hydrogen production from renewable sources shows significant growth, contributing to the power-to-X (P2X) system in terms of storage and flexibility. GHP through renewables will likely become a prominent solution for EIs within the next ten years. One of the bottlenecks here is not to reflect the adverse effects of the variable nature of renewables while transferring sustainable energy. Herein, to enhance operation sustainability, stability, and reliability of renewable-based distributed energy resources (DERs), machine learning (ML)-based techniques can be neat and auxiliary solutions. Power electronic converters (PECs) have such a duty as being the backbone of transferring energy in renewables. The crucial matter is here to keep the inputs and outputs of the converters as stable as possible. In this context, this paper outlines an ML approach to reduce the computational burden and enhance reliability. Therefore, this paper proposes the utilization of an EI to strengthen the stability and reliability of the general scheme. This work is a preliminary attempt and effective solution to establish these EIs, including GHP, considering feasibility criteria and improving reliability. Furthermore, this study examines the essential components, design criteria, challenges, and future issues for system establishment. It aims to facilitate the work of researchers in this field and further enhance the development of EIs.}
}
@article{GAULD2024116255,
title = {Exploring the interplay of clinical reasoning and artificial intelligence in psychiatry: Current insights and future directions},
journal = {Psychiatry Research},
volume = {342},
pages = {116255},
year = {2024},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2024.116255},
url = {https://www.sciencedirect.com/science/article/pii/S0165178124005407},
author = {Christophe Gauld and Vincent P. Martin and Hugo Bottemanne and Pierre Fourneret and Jean-Arthur Micoulaud-Franchi and Guillaume Dumas},
keywords = {Artificial intelligence, Statistical prediction, Psychiatry, Computational sciences, Explainability, Deep learning},
abstract = {For many years, it has been widely accepted in the psychiatric field that clinical practice cannot be reduced to finely tuned statistical prediction systems utilizing diverse clinical data. Clinicians are recognized for their unique and irreplaceable roles. In this brief historical overview, viewed through the lens of artificial intelligence (AI), we propose that comprehending the reasoning behind AI can enhance our understanding of clinical reasoning. Our objective is to systematically identify the factors that shape clinical reasoning in medicine, based on six factors that were historically considered beyond the reach of statistical methods: open-endedness, unanalyzed stimulus-equivalences, empty cells, theory mediation, insufficient time, and highly configured functions. Nevertheless, a pertinent consideration in the age of AI is whether these once-considered insurmountable specific factors of clinicians are now subject to scrutiny or not. Through example in AI, we demonstrate that a deeper understanding of these factors not only sheds light on clinical decision-making and its heuristic processes but also underscores the significance of collaboration between AI experts and healthcare professionals. This comparison between AI and clinical reasoning contributes to a better grasp of the current challenges AI faces in the realm of clinical medicine.}
}
@article{KEMPF2023103747,
title = {Point pattern and spatial analyses using archaeological and environmental data – A case study from the Neolithic Carpathian Basin},
journal = {Journal of Archaeological Science: Reports},
volume = {47},
pages = {103747},
year = {2023},
issn = {2352-409X},
doi = {https://doi.org/10.1016/j.jasrep.2022.103747},
url = {https://www.sciencedirect.com/science/article/pii/S2352409X22004102},
author = {Michael Kempf and Gerrit Günther},
keywords = {Environmental archaeology, Quantitative archaeology, Computational methods, Spatial analysis, R, Point pattern analysis (PPA)},
abstract = {Computational methods recently gained momentum in archaeological science, particularly affecting large site distribution samples and environmental explanatory parameters. However, quantitative and environmental archaeology are still considered to be limited to a small number of experts and thus less ready to use in general research. Here, we present a case study that integrates computational methods and environmental data into archaeological spatial analyses using Point Pattern Analysis (PPA). We introduce a basic approach to model, visualise, and interpret archaeological site distributions as functions of explanatory covariates in a regional setting of the Neolithic period in the Carpathian Basin. The integration of environmental and socio-cultural variables in a multicomponent analysis allows to distinguish site location parameters and preferences across different chronological periods. Using the code to this article and open-access spatial data, the workflow can be adapted to different regional contexts and chronological periods, making it particularly suitable for spatial pattern comparison.}
}
@article{ERDMANN202242,
title = {A generative framework for the study of delusions},
journal = {Schizophrenia Research},
volume = {245},
pages = {42-49},
year = {2022},
note = {Computational Approaches to Understanding Psychosis},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2020.11.048},
url = {https://www.sciencedirect.com/science/article/pii/S0920996420306277},
author = {Tore Erdmann and Christoph Mathys},
keywords = {Delusion, Dirichlet process, Hierarchical predictive coding, Auxiliary hypothesis, Epistemic trust},
abstract = {Despite the ubiquity of delusional information processing in psychopathology and everyday life, formal characterizations of such inferences are lacking. In this article, we propose a generative framework that entails a computational mechanism which, when implemented in a virtual agent and given new information, generates belief updates (i.e., inferences about the hidden causes of the information) that resemble those seen in individuals with delusions. We introduce a particular form of Dirichlet process mixture model with a sampling-based Bayesian inference algorithm. This procedure, depending on the setting of a single parameter, preferentially generates highly precise (i.e. over-fitting) explanations, which are compartmentalized and thus can co-exist despite being inconsistent with each other. Especially in ambiguous situations, this can provide the seed for delusional ideation. Further, we show by simulation how the excessive generation of such over-precise explanations leads to new information being integrated in a way that does not lead to a revision of established beliefs. In all configurations, whether delusional or not, the inference generated by our algorithm corresponds to Bayesian inference. Furthermore, the algorithm is fully compatible with hierarchical predictive coding. By virtue of these properties, the proposed model provides a basis for the empirical study and a step toward the characterization of the aberrant inferential processes underlying delusions.}
}
@article{TONKS2021102036,
title = {How situational competence beliefs and task value relate to inference strategies and comprehension during reading},
journal = {Learning and Individual Differences},
volume = {90},
pages = {102036},
year = {2021},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2021.102036},
url = {https://www.sciencedirect.com/science/article/pii/S104160802100073X},
author = {Stephen M. Tonks and Joseph P. Magliano and John Schwartz and Ryan D. Kopatich},
keywords = {Reading motivation, Inference generation, Reading comprehension, College students},
abstract = {In two studies, we explored the associations among situational reading-related competence beliefs and task value, inference strategies, comprehension during reading, and foundational skills in college age students. In Study 1, 93 participants from a community college completed assessments of comprehension and two types of inference strategies (elaboration and bridging), each immediately followed by a survey of their competence beliefs and task value regarding the task. Results showed that competence beliefs and task value related positively to reading comprehension. In addition, task value was positively associated with both elaborating and bridging inferences, and competence beliefs correlated positively with bridging inferences. In Study 2, we investigated these associations further in a group of 418 students studying at three different colleges. Participants completed the same assessments for competence beliefs, task value, and inference strategies, as well as assessments of comprehension and foundational reading skills. Study 2 analyses revealed that foundational reading skills were a strong predictor of both types of inferencing and also comprehension. Further, when controlling for foundational reading skills, task value predicted elaboration and bridging inferences, whereas competence beliefs did not predict inferencing, but were trending as a predictor of comprehension. Finally, we created a path model to explore mediational effects, and found that task value positively predicted comprehension performance through increased elaborations while thinking aloud.}
}
@article{WORTMANN2017173,
title = {Differentiating parametric design: Digital workflows in contemporary architecture and construction},
journal = {Design Studies},
volume = {52},
pages = {173-197},
year = {2017},
note = {Parametric Design Thinking},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2017.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X17300352},
author = {Thomas Wortmann and Bige Tunçer},
keywords = {parametric design, design automation, architectural design, software design, parametric master model},
abstract = {This paper examines Parametric Design (PD) in contemporary architectural practice. It considers three case studies: The Future of Us pavilion, the Louvre Abu Dhabi and the Morpheus Hotel. The case studies illustrate how, compared to non-parametrically and older parametrically designed projects, PD is employed to generate, document and fabricate designs with a greater level of detail and differentiation, often at the level of individual building components. We argue that such differentiation cannot be achieved with conventional Building Information Modelling and without customizing existing software. We compare the case studies' PD approaches (objected-oriented programming, functional programming, visual programming and distributed visual programming) and decomposition, algorithms and data structures as crucial factors for the practical viability of complex parametric models and as key aspects of PD thinking.}
}
@article{CHRISTENSEN2016125,
title = {Towards a formal assessment of design literacy: Analyzing K-12 students' stance towards inquiry},
journal = {Design Studies},
volume = {46},
pages = {125-151},
year = {2016},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2016.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X1530140X},
author = {Kasper Skov Christensen and Mikkel Hjorth and Ole Sejer Iversen and Paulo Blikstein},
keywords = {design education, design research, reflective practices, evaluation},
abstract = {We present a tool for quantitative assessment of K-12 students' stance towards inquiry as an important part of students' development of design literacy. On a basis of design thinking literature, we position designerly stance towards inquiry as a prerequisite for engaging with wicked problems. The Design Literacy (DeL) assessment tool contains design of a qualitative survey question, a coding scheme for assessing aspects of a designerly stance towards inquiry, and a description of how, we have validated the results through a large-scale survey administration in K-12 education. Our DeL tool is meant to provide educators, leaders, and policy makers with strong arguments for introducing design literacy in K-12 schools, which, we posit, function within in an age of measurement.}
}
@article{TABACHNECKSCHIJF1997305,
title = {CaMeRa: A computational model of multiple representations},
journal = {Cognitive Science},
volume = {21},
number = {3},
pages = {305-350},
year = {1997},
note = {Advances in analogy research: Integration of theory and data from the cognitive, computational, and neural sciences},
issn = {0364-0213},
doi = {https://doi.org/10.1016/S0364-0213(99)80026-3},
url = {https://www.sciencedirect.com/science/article/pii/S0364021399800263},
author = {Hermina J.M. Tabachneck-Schijf and Anthony M. Leonardo and Herbert A. Simon},
abstract = {This research aims to clarify, by constructing and testing a computer simulation, the use of multiple representations in problem solving, focusing on their role in visual reasoning. The model is motivated by extensive experimental evidence in the literature for the features it incorporates, but this article focuses on the system's structure. We illustrate the model's behavior by simulating the cognitive and perceptual processes of an economics expert as he teaches some well-learned economics principles while drawing a graph on a blackboard. Data in the experimental literature and concurrent verbal protocols were used to guide construction of a linked production system and parallel network, CaMeRa (Computation with Multiple Representations), that employs a “Mind's Eye” representation for pictorial information, consisting of a bitmap and associated node-link structures. Propositional list structures are used to represent verbal information and reasoning. Small individual pieces from the different representations are linked on a sequential and temporary basis to form a reasoning and inferencing chain, using visually encoded information recalled to the Mind's Eye from long-term memory and from cues recognized on an external display. CaMeRa, like the expert, uses the diagrammatic and verbal representations to complement one another, thus exploiting the unique advantages of each.}
}
@incollection{ZHANG2021440,
title = {The Use and Value of Geographic Information Systems in Transportation Modeling},
editor = {Roger Vickerman},
booktitle = {International Encyclopedia of Transportation},
publisher = {Elsevier},
address = {Oxford},
pages = {440-447},
year = {2021},
isbn = {978-0-08-102672-4},
doi = {https://doi.org/10.1016/B978-0-08-102671-7.10364-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780081026717103641},
author = {Ming Zhang},
keywords = {Accessibility modeling, Big-data, Four-step models, Geographic information systems (GIS), Land use-transportation integrated (LUTI) models, New mobility, Relational database, Topological data structure, Traffic simulation, Transportation modeling, Visualization},
abstract = {Geographic information systems (GIS) and transportation modeling have been developing in their respective fields separately. However, the geographic focus of GIS and the geographic nature of transportation make the use of GIS in transportation modeling both a logical choice and a motivation for modeling enhancement. This chapter first explains how the topological data structure underlying the design of vector GIS fits well the needs of data handling for transportation modeling. It then discusses the use and value of GIS in transportation modeling in three areas: (1) GIS for the four-step transportation models and for specific transportation modeling interests such as accessibility modeling and land use-transportation integrated (LUTI) modeling; (2) GIS visualization capabilities for presenting transportation modeling output and for facilitating visual thinking and knowledge construction; and (3) the potential of and challenges to GIS in transportation modeling amid rapid development of big-data technologies and new mobility.}
}
@incollection{KIRWAN202047,
title = {Chapter 3 - Strategies, planning, and design},
editor = {Christopher Kirwan and Fu Zhiyong},
booktitle = {Smart Cities and Artificial Intelligence},
publisher = {Elsevier},
pages = {47-67},
year = {2020},
isbn = {978-0-12-817024-3},
doi = {https://doi.org/10.1016/B978-0-12-817024-3.00003-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128170243000039},
author = {Christopher Kirwan and Fu Zhiyong},
keywords = {Citizen Engagement, Co-design, Data Visualization, Design Thinking, Generative Design, Information Architecture, Living Lab, Metadesign, Real-time Data, Simulation, Transdisciplinary Methods, User Experience},
abstract = {Traditional planning and design methodologies can now be augmented by new innovative tools and processes enabled by AI and smart technologies. These facilitate a more open-ended, multi-dimensional approach that incorporates diverse stakeholders to shape the potential of a collective intelligent operating system — one that best reflects the inherent nature of each unique city and urban condition. The design of smart cities must incorporate and adapt a combination of universal standards and localized policies through global civil society organizations and public-private-people partnerships established to serve the user and citizen as participants of the living city. New methods including co-design, co-creation, citizen participation and user experience (UX) feedback foster inclusive cities. Living labs and innovation hubs provide opportunities and spaces to prototype such initiatives. Transdisciplinary approaches are needed more than ever to expand our scope of inclusion to all life forms, including the rights of animals and nature as stakeholders. By applying a new combination of human and AI-enabled methods such as design thinking, machine learning and generative design, cities can now augment and improve their current state seamlessly, integrating technologies and management as an autopoietic smart city operating system.}
}
@article{RODRIGUEZMENDEZ2024103804,
title = {UK net-zero policy design – from optimisation to robustness},
journal = {Environmental Science & Policy},
volume = {158},
pages = {103804},
year = {2024},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2024.103804},
url = {https://www.sciencedirect.com/science/article/pii/S1462901124001382},
author = {Quirina {Rodriguez Mendez} and Mark Workman and Geoff Darch},
keywords = {Robust Decision Making, Deep Uncertainty, Greenhouse Gas Removal, Climate modelling, United Kingdom Net-Zero target},
abstract = {The need to deal with the deep uncertainty and system complexity associated to Net-Zero pathways, especially those relying on emergent greenhouse gas removal (GGR) technologies, has resulted in a growing body of literature on alternative decision-support approaches. Exploratory modelling, and specifically Robust Decision Making (RDM), are potential approaches capable of addressing these challenges: by exploring a wide range of conceivable futures, they explicitly embrace deep uncertainties while seeking to reduce system vulnerabilities. However, though RDM methods have been well documented, there is little insight as to how such approach might be integrated into Net-Zero policy design processes. By means of a workshop (n=17) and interviews (n=13) with the UK climate policy and energy modelling communities, this contribution provides insights into the role and potential of RDM in explicitly dealing with the deep uncertainties that pervade in the establishment of a 60–100 MtCO2 UK GGR sector within three decades. The consultation process revealed that there is an appetite from the decision-making and analytical communities in integrating exploratory modelling concepts into UK policy design processes. It is recommended that to bridge the gap between theoretical RDM constructs and their broader adoption, the analytical process should include a broader set of disciplines and expertise. Specifically for the modelling community, this work suggests that in-use computational models should be adapted, rather than new tools developed. Key challenges also arise from the time and resources required, suggesting small scale place-based pilots could promote the acceptability and foster the adoption of the RDM methodology.}
}
@article{FESTA2024,
title = {Incidence of circular refurbishment measures on indoor air quality and comfort conditions in two real buildings: Experimental and numerical analysis},
journal = {Energy and Built Environment},
year = {2024},
issn = {2666-1233},
doi = {https://doi.org/10.1016/j.enbenv.2024.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S2666123324000394},
author = {Valentino Festa and Silvia Ruggiero and Sara Riccardi and Margarita- Niki Assimakopoulos and Dimitra Papadaki},
keywords = {Energy building refurbishment, Experimental campaign, Indoor air quality, Thermal comfort, Computational fluid dynamics analysis},
abstract = {The application of Circular Economy to construction sector is a key to attain carbon neutrality, since it is responsible of 40 % of natural resource consumption. In this frame the importance of an efficient building refurbishment process throughout recycled material and renewable energy is fundamental. From an overview about building refurbishment emerges the need to investigate aspects related to Indoor Environmental Quality and the comparison between in-field measurements with output of dynamic simulation models. The present study aims to fill these two gaps by means an energy renovation of two real buildings in Greece. The work develops within the European project “Drive 0″, born to promote deep environmentally friendly retrofitting by means of circular renovation concepts. The methodological approach involves on-site monitoring of a series of parameters describing the energy, microclimate environmental and air quality, before and after the energy requalification. In addition, a numerical model developed in Building Energy Simulation program is calibrated and a Computational Fluid Dynamics is developed. From the in-field measurements emerges that, on one hand, the refurbishment of heating system shows a great improvement of indoor thermal conditions, with Total Volatile Organic Compounds concentration that sometimes exceed 3.0 mg/m3; on the other hand an integrated thermal insulation reduces infiltrations and changes the envelope behaviour, with a global energy saving of 30 % during winter and autumn periods. Another result of the study shows that a numerical model developed in Building Energy Simulation program and calibrated on energy consumption can greatly fit the local thermal comfort distribution of the occupant zone and predict the indoor air quality, if it outputs are used as input data in a Computational Fluid Dynamics study. These results can be beneficial to decision makers and designers for evaluating emitters positioning, opening design and mechanical ventilation strategies, aimed at reducing energy costs.}
}
@article{ZHANG2024101412,
title = {Predicting the Mathematics Literacy of Resilient Students from High‐performing Economies: A Machine Learning Approach},
journal = {Studies in Educational Evaluation},
volume = {83},
pages = {101412},
year = {2024},
issn = {0191-491X},
doi = {https://doi.org/10.1016/j.stueduc.2024.101412},
url = {https://www.sciencedirect.com/science/article/pii/S0191491X24000919},
author = {Yimei Zhang and Maria Cutumisu},
keywords = {Academic resilience, Machine learning, Mathematics literacy, Cultural differences},
abstract = {Mathematics is a crucial yet challenging subject for all students. Therefore, it is important to understand the role of academic resilience in mathematics, which enables students to overcome academic challenges. This study applied two machine learning algorithms, Lasso Regression (LR) and Random Forest (RF), to predict the mathematics literacy of resilient students from high-performing economies across cultures in PISA 2022. The findings indicated both RF and LR performed better in Western cultures than in Eastern cultures. Furthermore, in Eastern cultures, mathematics self-efficacy for 21st-century skills played an important role in predicting resilient students’ mathematics literacy, followed by self-efficacy towards mathematics, and mathematics anxiety. In Western cultures, self-efficacy towards mathematics was the predominant predictor, followed by mathematics self-efficacy for 21st-century skills. Theoretically, this study identifies key factors in predicting resilient students’ mathematics literacy across cultures. Methodologically, it is the first to apply ML in exploring resilient students’ mathematics literacy. Practically, it guides educators interested in developing interventions to improve resilient students’ mathematics literacy.}
}
@article{BRADLEY2016277,
title = {Pilot Testing the Debriefing for Meaningful Learning Evaluation Scale},
journal = {Clinical Simulation in Nursing},
volume = {12},
number = {7},
pages = {277-280},
year = {2016},
issn = {1876-1399},
doi = {https://doi.org/10.1016/j.ecns.2016.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S1876139916000104},
author = {Cynthia Sherraden Bradley and Kristina Thomas Dreifuerst},
keywords = {DML, debriefing, effective briefing, debriefing evaluation, measurement},
abstract = {Background
Debriefing for Meaningful Learning (DML), an evidence-based debriefing method, promotes thinking like a nurse through reflective learning. Despite widespread adoption of DML, little is known about how well it is implemented. To assess the effectiveness of DML implementation, an evaluative rubric was developed and tested.
Sample
Three debriefers who had been trained to use DML at least 1 year previously, submitted five recorded debriefings each for evaluation.
Methods
Three raters who were experts in DML scored each of the 15 recorded debriefing session using DML Evaluation Scale (DMLES). Observable behaviors were scored with binary options. These raters also assessed the items in the DMLES for content validity.
Results
Cronbach's alpha, intraclass correlation coefficients, and Content Validity Index scores were calculated to determine reliability and validity.
Conclusion
Use of DMLES could support quality improvement, teacher preparation, and faculty development. Future testing is warranted to investigate the relationship between DML implementation and clinical reasoning.}
}
@article{PATON200263,
title = {Process, structure and context in relation to integrative biology},
journal = {Biosystems},
volume = {64},
number = {1},
pages = {63-72},
year = {2002},
issn = {0303-2647},
doi = {https://doi.org/10.1016/S0303-2647(01)00176-9},
url = {https://www.sciencedirect.com/science/article/pii/S0303264701001769},
author = {Ray Paton},
keywords = {Ecology, Proteins, Category theory, Modelling, Function, Liver},
abstract = {This paper seeks to provide some integrative tools of thought regarding biological function related to ideas of process, structure, and context. The incorporation of linguistic and mathematical thinking is discussed within the context of managing thinking about natural systems as described by Robert Rosen. Examples from ecology, protein networks, and liver function are introduced to illustrate key ideas. It is hoped that these tools of thought, and the further work needed to mobilise such ideas, will continue to address a number of issues raised and pursued by Michael Conrad, such as the seed-germination model and vertical information processing.}
}
@article{DELIMA2024107089,
title = {Integrating artificial intelligence and wing geometric morphometry to automate mosquito classification},
journal = {Acta Tropica},
volume = {249},
pages = {107089},
year = {2024},
issn = {0001-706X},
doi = {https://doi.org/10.1016/j.actatropica.2023.107089},
url = {https://www.sciencedirect.com/science/article/pii/S0001706X23002760},
author = {Vinicio Rodrigues {de Lima} and Mauro César Cafundó {de Morais} and Karin Kirchgatter},
keywords = {Mosquito-borne diseases, Species identification, Integrative approach},
abstract = {Mosquitoes (Diptera: Culicidae) comprise over 3500 global species, primarily in tropical regions, where the females act as disease vectors. Thus, identifying medically significant species is vital. In this context, Wing Geometric Morphometry (WGM) emerges as a precise and accessible method, excelling in species differentiation through mathematical approaches. Computational technologies and Artificial Intelligence (AI) promise to overcome WGM challenges, supporting mosquito identification. AI explores computers' thinking capacity, originating in the 1950s. Machine Learning (ML) arose in the 1980s as a subfield of AI, and deep Learning (DL) characterizes ML's subcategory, featuring hierarchical data processing layers. DL relies on data volume and layer adjustments. Over the past decade, AI demonstrated potential in mosquito identification. Various studies employed optical sensors, and Convolutional Neural Networks (CNNs) for mosquito identification, achieving average accuracy rates between 84 % and 93 %. Furthermore, larval Aedes identification reached accuracy rates of 92 % to 94 % using CNNs. DL models such as ResNet50 and VGG16 achieved up to 95 % accuracy in mosquito identification. Applying CNNs to georeference mosquito photos showed promising results. AI algorithms automated landmark detection in various insects' wings with repeatability rates exceeding 90 %. Companies have developed wing landmark detection algorithms, marking significant advancements in the field. In this review, we discuss how AI and WGM are being combined to identify mosquito species, offering benefits in monitoring and controlling mosquito populations.}
}
@article{FORREST19901,
title = {Emergent computation: Self-organizing, collective, and cooperative phenomena in natural and artificial computing networks: Introduction to the proceedings of the ninth annual CNLS conference},
journal = {Physica D: Nonlinear Phenomena},
volume = {42},
number = {1},
pages = {1-11},
year = {1990},
issn = {0167-2789},
doi = {https://doi.org/10.1016/0167-2789(90)90063-U},
url = {https://www.sciencedirect.com/science/article/pii/016727899090063U},
author = {Stephanie Forrest}
}
@article{SEYMOUR2020117212,
title = {Hierarchical models of pain: Inference, information-seeking, and adaptive control.},
journal = {NeuroImage},
volume = {222},
pages = {117212},
year = {2020},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2020.117212},
url = {https://www.sciencedirect.com/science/article/pii/S1053811920306984},
author = {Ben Seymour and Flavia Mancini},
keywords = {Pain, Nociception, Information theory, Reinforcement learning, Optimal control, Predictive coding, Epistemic value, Free energy principle, Endogenous modulation},
abstract = {Computational models of pain consider how the brain processes nociceptive information and allow mapping neural circuits and networks to cognition and behaviour. To date, they have generally have assumed two largely independent processes: perceptual inference, typically modelled as an approximate Bayesian process, and action control, typically modelled as a reinforcement learning process. However, inference and control are intertwined in complex ways, challenging the clarity of this distinction. Here, we consider how they may comprise a parallel hierarchical architecture that combines inference, information-seeking, and adaptive value-based control. This sheds light on the complex neural architecture of the pain system, and takes us closer to understanding from where pain ’arises’ in the brain.}
}
@article{PIQUEIRA2016271,
title = {A comparison of LMC and SDL complexity measures on binomial distributions},
journal = {Physica A: Statistical Mechanics and its Applications},
volume = {444},
pages = {271-275},
year = {2016},
issn = {0378-4371},
doi = {https://doi.org/10.1016/j.physa.2015.10.040},
url = {https://www.sciencedirect.com/science/article/pii/S0378437115008882},
author = {José Roberto C. Piqueira},
keywords = {Binomial, Complexity, Information, Measure, Probability},
abstract = {The concept of complexity has been widely discussed in the last forty years, with a lot of thinking contributions coming from all areas of the human knowledge, including Philosophy, Linguistics, History, Biology, Physics, Chemistry and many others, with mathematicians trying to give a rigorous view of it. In this sense, thermodynamics meets information theory and, by using the entropy definition, López-Ruiz, Mancini and Calbet proposed a definition for complexity that is referred as LMC measure. Shiner, Davison and Landsberg, by slightly changing the LMC definition, proposed the SDL measure and the both, LMC and SDL, are satisfactory to measure complexity for a lot of problems. Here, SDL and LMC measures are applied to the case of a binomial probability distribution, trying to clarify how the length of the data set implies complexity and how the success probability of the repeated trials determines how complex the whole set is.}
}
@article{SAMPSON20052095,
title = {Comments on: “Pore network simulation of fluid inbibition into paper during coating: II. Characterization of paper's morphology and computation of its effective permeability tensor” by Ghassemzadeh and Sahimi [Chemical Engineering Science 59(2004) 2265–2280]},
journal = {Chemical Engineering Science},
volume = {60},
number = {7},
pages = {2095},
year = {2005},
issn = {0009-2509},
doi = {https://doi.org/10.1016/j.ces.2004.12.005},
url = {https://www.sciencedirect.com/science/article/pii/S0009250904009327},
author = {W.W. Sampson and C.T.J. Dodson}
}
@article{BORSTLER2023111592,
title = {Investigating acceptance behavior in software engineering—Theoretical perspectives},
journal = {Journal of Systems and Software},
volume = {198},
pages = {111592},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111592},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222002680},
author = {Jürgen Börstler and Nauman bin Ali and Martin Svensson and Kai Petersen},
keywords = {Acceptance behavior, Dual process theory, Technology acceptance, Theory, TAM, UTAUT, TPB},
abstract = {Background:
Software engineering research aims to establish software development practice on a scientific basis. However, the evidence of the efficacy of technology is insufficient to ensure its uptake in industry. In the absence of a theoretical frame of reference, we mainly rely on best practices and expert judgment from industry-academia collaboration and software process improvement research to improve the acceptance of the proposed technology.
Objective:
To identify acceptance models and theories and discuss their applicability in the research of acceptance behavior related to software development.
Method:
We analyzed literature reviews within an interdisciplinary team to identify models and theories relevant to software engineering research. We further discuss acceptance behavior from the human information processing perspective of automatic and affect-driven processes (“fast” system 1 thinking) and rational and rule-governed processes (“slow” system 2 thinking).
Results:
We identified 30 potentially relevant models and theories. Several of them have been used in researching acceptance behavior in contexts related to software development, but few have been validated in such contexts. They use constructs that capture aspects of (automatic) system 1 and (rational) system 2 oriented processes. However, their operationalizations focus on system 2 oriented processes indicating a rational view of behavior, thus overlooking important psychological processes underpinning behavior.
Conclusions:
Software engineering research may use acceptance behavior models and theories more extensively to understand and predict practice adoption in the industry. Such theoretical foundations will help improve the impact of software engineering research. However, more consideration should be given to their validation, overlap, construct operationalization, and employed data collection mechanisms when using these models and theories.}
}
@article{LEMOEL2020110,
title = {Towards a multi-level understanding in insect navigation},
journal = {Current Opinion in Insect Science},
volume = {42},
pages = {110-117},
year = {2020},
note = {Neuroscience * Biomechanics of Insect Flight and Bio-inspired engineering},
issn = {2214-5745},
doi = {https://doi.org/10.1016/j.cois.2020.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S2214574520301310},
author = {Florent {Le Moël} and Antoine Wystrach},
abstract = {To understand the brain is to understand behaviour. However, understanding behaviour itself requires consideration of sensory information, body movements and the animal’s ecology. Therefore, understanding the link between neurons and behaviour is a multi-level problem, which can be achieved when considering Marr’s three levels of understanding: behaviour, computation, and neural implementation. Rather than establishing direct links between neurons and behaviour, the matter boils down to understanding two transitions: the link between neurons and brain computation on one hand, and the link between brain computations and behaviour on the other hand. The field of insect navigation illustrates well the power of such two-sided endeavour. We provide here examples revealing that each transition requires its own approach with its own intrinsic difficulties, and show how modelling can help us reach the desired multi-level understanding.}
}
@article{RAMOS202335,
title = {An institutional modernization project in chemical engineering education in Brazil: Developing broader competencies for societal challenges},
journal = {Education for Chemical Engineers},
volume = {44},
pages = {35-44},
year = {2023},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2023.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S1749772823000167},
author = {Bruno Ramos and Moisés Teles dos Santos and Ardson S. Vianna and Luiz Kulay},
keywords = {Process safety, Chemical reaction engineering, Education, Active-based learning},
abstract = {Contemporary societal challenges put in evidence the need to improve the hard and soft skills of chemical engineering students. To promote a more student-centered approach, active-based learning, and improved assessment strategies, the Brazilian government approved the so-called New National Curriculum Guidelines (NCG) for engineering courses. To comply with those guidelines, the Department of Chemical Engineering of the Polytechnic School of the University of São Paulo (USP) is currently developing an educational modernization process sponsored by the Fulbright Commission in Brazil, called Special Program for Modernization of Undergraduate Education (PMG). The project is based on three pillars of modernization: content (what), form (how), and infrastructure (where). This paper describes initiatives in each of those pillars: content and format changes in Chemical Reaction Engineering and Process Safety courses and the creation of new spaces for a student-centered approach (an innovative classroom layout and a makerspace). By gathering two concrete classroom experiences guided by a broader institutional educational policies (the PMG project and the NCG), this paper highlights that slight changes can lead to great improvements in the learning process, leading to more engagement in the development of hard skills while favoring improvements in soft skills, such as communication, team-based work, and critical thinking.}
}
@article{SHRYANE2020112806,
title = {Is cognitive behavioural therapy effective for individuals experiencing thought disorder?},
journal = {Psychiatry Research},
volume = {285},
pages = {112806},
year = {2020},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2020.112806},
url = {https://www.sciencedirect.com/science/article/pii/S0165178119302793},
author = {Nick Shryane and Richard Drake and Anthony P. Morrison and Jasper Palmier-Claus},
keywords = {Psychosis, Cognitive behavioural therapy, Thought disorder, Randomized Controlled Trial},
abstract = {Various clinical guidelines recommend cognitive behavioural therapy (CBT) to treat psychosis without reference to patients’ thought disorder. However, there is a risk that disorganized thinking hampers CBT. We tested the prediction that thought disorder would interfere with the effectiveness of CBT for hallucinations and delusions, compared to treatment as usual and supportive counselling, in secondary data from two large, single blind randomised controlled trials. We fitted latent growth curve models separately for the development of frequency and distress of symptoms. CBT was significantly more successful than counselling in reducing delusional frequency in the short term and hallucinatory distress at any point, even in those with relatively high thought disorder. We found little evidence that clinicians should restrict CBT in this subgroup of patients. Nevertheless, the findings highlight the importance of effective initial treatment of thought disorder in maximising the benefit of CBT for psychosis, particularly for reducing distress from hallucinations.}
}
@article{TALL1999223,
title = {What Is the Object of the Encapsulation of a Process?},
journal = {The Journal of Mathematical Behavior},
volume = {18},
number = {2},
pages = {223-241},
year = {1999},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(99)00029-2},
url = {https://www.sciencedirect.com/science/article/pii/S0732312399000292},
author = {David Tall and Michael Thomas and Gary Davis and Eddie Gray and Adrian Simpson},
abstract = {Several theories have been proposed to describe the transition from process to object in mathematical thinking. Yet, what is the nature of this “object” produced by the “encapsulation” of a process? Here, we outline the development of some of the theories (including Piaget, Dienes, Davis, Greeno, Dubinsky, Sfard, Gray, and Tall) and consider the nature of the mental objects (apparently) produced through encapsulation and their role in the wider development of mathematical thinking. Does the same developmental route occur in geometry as in arithmetic and algebra? Is the same development used in axiomatic mathematics? What is the role played by imagery?}
}
@article{HUANG200870,
title = {Investigating the cognitive behavior of generating idea sketches through neural network systems},
journal = {Design Studies},
volume = {29},
number = {1},
pages = {70-92},
year = {2008},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2007.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X07000750},
author = {Yinghsiu Huang},
keywords = {drawings, computer supported design, visual reasoning, neural network},
abstract = {Design can be regarded as a seeing–moving–seeing process, where designers repeatedly see and generate ideas that are based on what they have done. The crucial point of design thinking is how designers recognize ambiguous shapes from sketches and then transfer them into different shapes. This study attempts to conduct cognitive experiments to elucidate the sketching process and to simulate two types of sketching behavior used by neural network systems. When exhibiting the first type of sketching behavior, designers are able to transform their original sketches to satisfy requirements. Simulating this type of visual cognitive behavior by neural networks could help computers modify shapes to meet design requirements, as human designers do. When demonstrating the second type of sketching behavior, designers are able to see an ambiguous shape as different complete shapes so as to associate divergent design ideas. Another set of neural networks investigated in this study could also associate different shapes by adjusting the TSL and produce different idea sketches from the same shape.}
}
@article{BUTLER2021170,
title = {Expert performance and crowd wisdom: Evidence from English Premier League predictions},
journal = {European Journal of Operational Research},
volume = {288},
number = {1},
pages = {170-182},
year = {2021},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2020.05.034},
url = {https://www.sciencedirect.com/science/article/pii/S037722172030480X},
author = {David Butler and Robert Butler and John Eakins},
keywords = {OR in sports, Prediction, Experts},
abstract = {This paper analyses the forecasting accuracy of experts vis-à-vis laypeople over three seasons of English Premier League matches. We find that former professional football players have superior forecasting ability when compared to laypeople. The results give partial support to the view that a crowd forecast offers the greatest precision. Pundits generate a positive return while both the crowd and laypeople generate losses. As the prediction of multiple score outcomes represents a computationally difficult task, both groups display forecasting biases including a preference toward specific score forecasts. The results are relevant for those concerned with gambling behaviour if the forecasting strategies adopted here generalise to match betting markets.}
}
@article{GARG2024101391,
title = {Molecular Mechanics Demonstrate S-COMT as promising therapeutic receptor when analyzed with secondary plant metabolites},
journal = {Journal of the Indian Chemical Society},
volume = {101},
number = {11},
pages = {101391},
year = {2024},
issn = {0019-4522},
doi = {https://doi.org/10.1016/j.jics.2024.101391},
url = {https://www.sciencedirect.com/science/article/pii/S0019452224002711},
author = {Deepanshu Garg and Aarya Vashishth and Maharsh Jayadeep Jayawant and Virupaksha A. Bastikar},
keywords = {S-COMT receptor, Depression, Plant secondary metabolites, Molecular docking, Molecular dynamic simulation},
abstract = {Major depressive disorder (MDD) and other psychiatric conditions are debilitating illnesses affecting millions globally. Catechol-O-methyltransferase (COMT), an enzyme that regulates dopamine and norepinephrine breakdown in the brain, has emerged as a potential therapeutic target for these disorders. This study explores the inhibitory potential of plant secondary metabolites against S-COMT using computational techniques. COMT exists in two isoforms: membrane-bound COMT (MB-COMT), primarily found in brain neurons, and soluble COMT (S-COMT), present in peripheral tissues. S-COMT, particularly in the prefrontal cortex, is crucial for regulating neurotransmitters and maintaining cognitive function. Studies suggest S-COMT variants might be linked to the development of depression, schizophrenia, and other psychiatric disorders. Current COMT inhibitors often suffer from limitations, necessitating the exploration of novel therapeutic strategies. This study employed in-silico methods to investigate plant secondary metabolites as potential S-COMT inhibitors. Here, we describe the S-COMT protein structure retrieval and validation, followed by molecular docking simulations to identify plant compounds with the strongest binding affinity to the receptor's active site. Key amino acid residues involved in these interactions were also analyzed. Furthermore, molecular dynamics simulations were conducted to assess the stability of the top-scoring protein-ligand complexes over a 100-ns timeframe. The results explored the stability of ligand binding within the active site and its impact on the overall conformation of the S-COMT receptor. Our findings highlight promising therapeutic potential for these plant-derived compounds. Further in vitro and in vivo studies are warranted to validate their efficacy and safety for potential clinical applications in treating S-COMT-related disorders.
Subjects
Bioinformatics and Computational Biology, Proteomics, Neurogenerative Diseases.}
}
@article{ARORA1990131,
title = {Computational design optimization: A review and future directions},
journal = {Structural Safety},
volume = {7},
number = {2},
pages = {131-148},
year = {1990},
issn = {0167-4730},
doi = {https://doi.org/10.1016/0167-4730(90)90063-U},
url = {https://www.sciencedirect.com/science/article/pii/016747309090063U},
author = {Jasbir S. Arora},
keywords = {optimization methods, nonlinear problems, review, computational aspects, engineering design},
abstract = {A mathematical model for design optimization of engineering systems is defined. Computational algorithms to treat the model are reviewed and their features are discussed. The attributes of a good algorithm are given. Sequential quadratic programming algorithms that generate and use the approximate Hessian of the Lagrange function to calculate the search direction are the most recent methods. They are the most reliable methods among the available ones. Several other computational aspects, such as robust implementation of algorithms, use of a knowledge base, interactive use of optimization, and use of a database and database management system, are discussed. Recent developments in the field and future directions are presented.}
}
@article{MU2025109748,
title = {Adaptive model-agnostic meta-learning network for cross-machine fault diagnosis with limited samples},
journal = {Engineering Applications of Artificial Intelligence},
volume = {141},
pages = {109748},
year = {2025},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2024.109748},
url = {https://www.sciencedirect.com/science/article/pii/S0952197624019079},
author = {Mingzhe Mu and Hongkai Jiang and Xin Wang and Yutong Dong},
keywords = {Fault diagnosis, Rotating machine, Adaptive model-agnostic meta-learning network, Cross-machine, Limited samples},
abstract = {Deep learning-based methods have been extensively studied in rotating machinery defect diagnosis. However, training an accurate and robust diagnostic model is still a challenge under severe domain bias and limited samples. For this reason, a new adaptive model-agnostic meta-learning (AMAML) is proposed for cross-machine fault diagnosis with limited samples. First, a novel adaptive feature encode network is built, incorporating lightweight spatial-bilateral channel attention. This enables the network to extract critical fault information in multiple dimensions adaptively within limited samples, which improves the learning efficiency of generalized diagnostic knowledge. Then, an adaptive loss computation (ALC) method is devised, which inventively realizes the interaction between loss computation and model performance. The underfitting and overfitting dilemmas under few-shot conditions are tackled by ALC. Finally, an adaptive meta-optimization strategy is proposed for dynamically adapting the update strategy of the base learner, so that the model is always optimized in the direction of strong generalizability while obtaining high performance. Six cross-machine diagnosis tasks are conducted to verify the effectiveness of AMAML. The average diagnostic accuracy of the AMAML under the 5-shot setting reached 97.42%. Experiments confirm that AMAML is superior to other prevailing methods and is potentially promising for engineering applications.}
}
@article{LIU2022189,
title = {Granular cabin: An efficient solution to neighborhood learning in big data},
journal = {Information Sciences},
volume = {583},
pages = {189-201},
year = {2022},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2021.11.034},
url = {https://www.sciencedirect.com/science/article/pii/S0020025521011543},
author = {Keyu Liu and Tianrui Li and Xibei Yang and Xin Yang and Dun Liu and Pengfei Zhang and Jie Wang},
keywords = {Computational efficiency, Granular computing, Neighborhood learning, Neighborhood rough set},
abstract = {Neighborhood Learning (NL) is a paradigm covering theories and techniques of neighborhood, which facilitates data organization, representation and generalization. While delivering impressive performances across various fields such as granular computing, cluster analysis, NL is argued to be computationally demanding, thereby limiting its utility and applicability. In this study, a simple and generic scheme named granular cabin is proposed for drastically speeding up the algorithmic implementation of NL. Specifically, this scheme is deployed to Neighborhood Rough Set (NRS) which is a typical NL methodology. And three major applications of NRS are concerned including approximation computation, neighborhood classification and feature selection. Extensive experiments demonstrate that NRS methodology enhanced by granular cabin consumes much less time. This study offers a promising solution that ensures the great potential of NL in big data.}
}
@article{CORDA2021100834,
title = {The secret of planets’ perihelion between Newton and Einstein},
journal = {Physics of the Dark Universe},
volume = {32},
pages = {100834},
year = {2021},
issn = {2212-6864},
doi = {https://doi.org/10.1016/j.dark.2021.100834},
url = {https://www.sciencedirect.com/science/article/pii/S2212686421000650},
author = {Christian Corda},
abstract = {Three different approaches show that, contrary to a longstanding conviction older than 160 years, the advance of Mercury’s perihelion can be achieved in Newtonian gravity with a very high precision by correctly analyzing the situation without neglecting Mercury’s mass. General relativity remains more precise than Newtonian physics, but Newtonian framework is more powerful than researchers and astronomers were thinking till now, at least for the case of Mercury. The Newtonian formula of the advance of planets’ perihelion breaks down for the other planets. The predicted Newtonian result is indeed too large for Venus and Earth. Therefore, it is also shown that corrections due to gravitational and rotational time dilation, in an intermediate framework which analyzes gravity between Newton and Einstein, solve the problem. By adding such corrections, a result consistent with the one of general relativity is indeed obtained. Thus, the most important results of this paper are two: (i) It is not correct that Newtonian theory cannot predict the anomalous rate of precession of the perihelion of planets’ orbit. The real problem is instead that a pure Newtonian prediction is too large. (ii) Perihelion’s precession can be achieved with the same precision of general relativity by extending Newtonian gravity through the inclusion of gravitational and rotational time dilation effects. This second result is in agreement with a couple of recent and interesting papers of Hansen, Hartong and Obers. Differently from such papers, in the present work the importance of rotational time dilation is also highlighted. Finally, it is important to stress that a better understanding of gravitational effects in an intermediate framework between Newtonian theory and general relativity, which is one of the goals of this paper, could, in principle, be crucial for a subsequent better understanding of the famous Dark Matter and Dark Energy problems.}
}
@article{MURRAY2019928,
title = {Center Finding in E. coli and the Role of Mathematical Modeling: Past, Present and Future},
journal = {Journal of Molecular Biology},
volume = {431},
number = {5},
pages = {928-938},
year = {2019},
issn = {0022-2836},
doi = {https://doi.org/10.1016/j.jmb.2019.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S0022283619300269},
author = {Seán M. Murray and Martin Howard},
keywords = {bacterial cell division positioning, plasmid segregation, MinCDE system, ParABS system, mathematical modeling},
abstract = {We review the key role played by mathematical modeling in elucidating two center-finding patterning systems in Escherichia coli: midcell division positioning by the MinCDE system and DNA partitioning by the ParABS system. We focus particularly on how, despite much experimental effort, these systems were simply too complex to unravel by experiments alone, and instead required key injections of quantitative, mathematical thinking. We conclude the review by analyzing the frequency of modeling approaches in microbiology over time. We find that while such methods are increasing in popularity, they are still probably heavily under-utilized for optimal progress on complex biological questions.}
}
@incollection{POULSEN201543,
title = {Chapter 3 - Better Concurrency and SIMD on HBM},
editor = {James Reinders and Jim Jeffers},
booktitle = {High Performance Parallelism Pearls},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {43-67},
year = {2015},
isbn = {978-0-12-802118-7},
doi = {https://doi.org/10.1016/B978-0-12-802118-7.00003-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128021187000030},
author = {Jacob Weismann Poulsen and Per Berg and Karthik Raman},
keywords = {HIROMB-BOOS-Model, Danish Meteorological Institute, Operational ocean models, Parallelization, Validation, Verification, OpenMP, MPI, Intel® VTune™ Amplifier, Performance Monitoring Unit, Vectorization, Nesting, Scaling, Cache layout},
abstract = {This chapter describes some work that is being performed at the Danish Meteorological Institute for optimization of a 3D ocean circulation model code with roots back to the 1990s and which is known as the HIROMB-BOOS-Model. The optimization of this large code is instructive agreeing with the authors’ strong belief that the best performance only comes with a focus on architecting for it starting with appropriate data structures. The thinking process and techniques used in this chapter have wide applicability: focus on data locality and then apply threading and vectorization techniques. This way of thinking should be on the mind of every programmer working to design a high-performance application.}
}
@article{PAILLARD2025101182,
title = {GREEN: A lightweight architecture using learnable wavelets and Riemannian geometry for biomarker exploration with EEG signals},
journal = {Patterns},
pages = {101182},
year = {2025},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2025.101182},
url = {https://www.sciencedirect.com/science/article/pii/S2666389925000303},
author = {Joseph Paillard and Jörg F. Hipp and Denis A. Engemann},
keywords = {electroencephalography, EEG, biomarkers, deep learning, wavelets, Riemannian geometry, brain-computer interface, BCI},
abstract = {Summary
Spectral analysis using wavelets is widely used for identifying biomarkers in EEG signals. Recently, Riemannian geometry has provided an effective mathematical framework for predicting biomedical outcomes from multichannel electroencephalography (EEG) recordings while showing concord with neuroscientific domain knowledge. However, these methods rely on handcrafted rules and sequential optimization. In contrast, deep learning (DL) offers end-to-end trainable models achieving state-of-the-art performance on various prediction tasks but lacks interpretability and interoperability with established neuroscience concepts. We introduce Gabor Riemann EEGNet (GREEN), a lightweight neural network that integrates wavelet transforms and Riemannian geometry for processing raw EEG data. Benchmarking on six prediction tasks across four datasets with over 5,000 participants, GREEN outperformed non-deep state-of-the-art models and performed favorably against large DL models while using orders-of-magnitude fewer parameters. Computational experiments showed that GREEN facilitates learning sparse representations without compromising performance. By integrating domain knowledge, GREEN combines a desirable complexity-performance trade-off with interpretable representations.}
}
@article{GORDON2018273,
title = {Healthier Choices in School Cafeterias: A Systematic Review of Cafeteria Interventions},
journal = {The Journal of Pediatrics},
volume = {203},
pages = {273-279.e2},
year = {2018},
issn = {0022-3476},
doi = {https://doi.org/10.1016/j.jpeds.2018.07.031},
url = {https://www.sciencedirect.com/science/article/pii/S0022347618309363},
author = {Katelyn Gordon and Linda Dynan and Robert Siegel},
keywords = {school cafeteria, behavioral economics, childhood obesity, food selection},
abstract = {Objective
To describe school cafeteria interventions in terms of a behavioral economics scheme and to assess which system is more likely to be effective in improving food selection or consumption.
Study design
With this systematic review, we categorize cafeteria interventions using the behavioral economics theory of Kahneman into system 1 (fast and intuitive thinking) and system 2 (slow and cognitively demanding) or mixed (having elements of system 1 and system 2). Pertinent studies were identified from review of the literature of interventions performed in school and cafeteria settings in children grades K-12 within the past 5 years (2012-2017) at time of search.
Results
In all, 48 of 978 studies met inclusion criteria. By defining success as a 30% improvement in a desired outcome or statistically significant reduction in body mass index, 89% of system 1, 67% of mixed (had both system 1 and 2 elements), and only 33% of system 2 interventions were successful.
Conclusions
This review found successful system 1 type school cafeteria interventions to be more common than system 2 type interventions and system 2 type interventions are less effective than system 1.}
}
@article{SCHULTZ2010174,
title = {Models and methods in motion: Declining the dogma dance},
journal = {Futures},
volume = {42},
number = {2},
pages = {174-176},
year = {2010},
note = {Epistemological pluralism in futures studies},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2009.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0016328709001736},
author = {Wendy Schultz},
abstract = {I take a communicative pragmatist and realist approach to futures studies. This implies a sensitivity to understanding what the audience can absorb and using futures methods effectively to create spaces for new futures. While Wilber's work affords us with new insights to engage with methodology, is not the only path. Indeed, it is intellectual bigotry to demand that everyone master the tools one personally deems most appropriate. Critical conversations about futures must remain open, where post-modernist and integral thinking widen our horizons, they are welcomed, where they straitjacket our thoughts, they are not.}
}
@article{WHEELER2020192,
title = {Ideology and predictive processing: coordination, bias, and polarization in socially constrained error minimization},
journal = {Current Opinion in Behavioral Sciences},
volume = {34},
pages = {192-198},
year = {2020},
note = {Political Ideologies},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2020.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352154620300632},
author = {Nathan E Wheeler and Suraiya Allidina and Elizabeth U Long and Stephen P Schneider and Ingrid J Haas and William A Cunningham},
abstract = {Recent models of cognition suggest that the brain may implement predictive processing, in which top-down expectations constrain incoming sensory data. In this perspective, expectations are updated (error minimization) only if sensory data sufficiently deviate from these expectations (prediction error). Although originally applied to perception, predictive processing is thought to generally characterize cognitive architecture, including the social cognitive processes involved in ideological thinking. Scaling up these simple computational principles to the social sphere outlines a path by which group members may adopt shared ideologies and beliefs to predict behavior and cooperate with each other. Because ideological judgments are of specific interest to others in our political groups, we may increasingly regulate each other’s thinking, sharing the process of error minimization. In this paper, we outline how this process of shared error minimization may lead to shared ideologies and beliefs that allow group members to predict and cooperate with each other, and how, as a consequence, political polarization and extremism may result.}
}
@article{PACE2023105433,
title = {Exploring future research and innovation directions for a sustainable blue economy},
journal = {Marine Policy},
volume = {148},
pages = {105433},
year = {2023},
issn = {0308-597X},
doi = {https://doi.org/10.1016/j.marpol.2022.105433},
url = {https://www.sciencedirect.com/science/article/pii/S0308597X22004808},
author = {Lisa A. Pace and Ozcan Saritas and Alan Deidun},
keywords = {Foresight, Blue economy, Interdisciplinary science, Marine science, Sustainable development, Stakeholder participation},
abstract = {The blue economy integrates commercial, research and innovation activities across diverse industrial sectors. Achieving a sustainable blue economy requires unlocking the potential of science and innovation to develop innovative ocean sustainability solutions. This study explores the role of foresight in co-creating alternative, preferred futures for a sustainable blue economy looking towards 2030 and in establishing an interdisciplinary dialogue about research and innovation opportunities to achieve these futures. To this end, a foresight exercise is conducted with marine scientists and researchers in 6 countries in Europe. The exercise is designed in three stages: scanning, scenario-building and strategic orientation, and uses a combination of foresight methods to encourage creative thinking and exploration. The scenarios developed in the study describe alternative future worlds built on the establishment of self-sustaining communities and engaged societies; the diffusion of digitalisation and growth of blue biotechnologies; booming ecosystem services and open and collaborative research infrastructures that impact different sectors of the blue economy. A portfolio of research and innovation areas is developed that aims to inspire new research directions in four domains: (i) integrated ocean management tools; (ii) closed loop, circular polyculture systems; (iii) co-creation of innovation and transdisciplinary research; and (iv) open access and collaborative databases supporting ecosystem services. The study highlights the role of foresight in bridging across disciplinary perspectives and industry sectors. Foresight can be used to complement Decision-Support Systems and other quantitative approaches for research agenda-setting and for decision-making on policies addressing sustainability in the marine sciences. The process contributes to futures skills-building at institutional level and helps establish a futures mindset for strategic planning.}
}
@article{MCGILL2021113697,
title = {Evaluation of public health interventions from a complex systems perspective: A research methods review},
journal = {Social Science & Medicine},
volume = {272},
pages = {113697},
year = {2021},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2021.113697},
url = {https://www.sciencedirect.com/science/article/pii/S0277953621000290},
author = {Elizabeth McGill and Vanessa Er and Tarra Penney and Matt Egan and Martin White and Petra Meier and Margaret Whitehead and Karen Lock and Rachel {Anderson de Cuevas} and Richard Smith and Natalie Savona and Harry Rutter and Dalya Marks and Frank {de Vocht} and Steven Cummins and Jennie Popay and Mark Petticrew},
keywords = {Systems thinking, Complexity science, Evaluation methodologies, Public health, Practice},
abstract = {Introduction
Applying a complex systems perspective to public health evaluation may increase the relevance and strength of evidence to improve health and reduce health inequalities. In this review of methods, we aimed to: (i) classify and describe different complex systems methods in evaluation applied to public health; and (ii) examine the kinds of evaluative evidence generated by these different methods.
Methods
We adapted critical review methods to identify evaluations of public health interventions that used systems methods. We conducted expert consultation, searched electronic databases (Scopus, MEDLINE, Web of Science), and followed citations of relevant systematic reviews. Evaluations were included if they self-identified as using systems- or complexity-informed methods and if they evaluated existing or hypothetical public health interventions. Case studies were selected to illustrate different types of complex systems evaluation.
Findings
Seventy-four unique studies met our inclusion criteria. A framework was developed to map the included studies onto different stages of the evaluation process, which parallels the planning, delivery, assessment, and further delivery phases of the interventions they seek to inform; these stages include: 1) theorising; 2) prediction (simulation); 3) process evaluation; 4) impact evaluation; and 5) further prediction (simulation). Within this framework, we broadly categorised methodological approaches as mapping, modelling, network analysis and ‘system framing’ (the application of a complex systems perspective to a range of study designs). Studies frequently applied more than one type of systems method.
Conclusions
A range of complex systems methods can be utilised, adapted, or combined to produce different types of evaluative evidence. Further methodological innovation in systems evaluation may generate stronger evidence to improve health and reduce health inequalities in our complex world.}
}
@article{KLIGER20217,
title = {Dynamic Archeology or Distant Reading: Literary Study Between Two Formalisms},
journal = {Russian Literature},
volume = {122-123},
pages = {7-28},
year = {2021},
note = {Digital Humanities and Russian and East European Studies},
issn = {0304-3479},
doi = {https://doi.org/10.1016/j.ruslit.2021.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0304347921000429},
author = {Ilya Kliger},
keywords = {Computational Literary Studies, Distant Reading, Literary Form, Russian Formalism, OPOIAZ},
abstract = {Scholars working within computational literary studies often invoke Russian Formalism as a methodologically like-minded school of thought and a repository of useful insights, which can at last be tested with the help of recently developed digital techniques. Yet the two formalisms diverge starkly when it comes to three of their most fundamental categories of analysis: first, in their respective conceptions of literary form itself; next, in their notions of history and of what it means to tell the history of form; and finally, in the ways in which they construe the relationship between literature and society as a whole, or, in other words, in their corresponding sociologies of literary form. This paper, then, is a contribution to creating the conditions for the possibility of a genuine exchange between the two formalisms here at issue by focusing, first and foremost, on what divides them.}
}
@article{CAI2024118870,
title = {An efficient Meta-VSW method for ship behaviors recognition and application},
journal = {Ocean Engineering},
volume = {311},
pages = {118870},
year = {2024},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2024.118870},
url = {https://www.sciencedirect.com/science/article/pii/S002980182402208X},
author = {Zhiyuan Cai and Qidong Fan and Lecheng Li and Long Yu and Congbo Li},
keywords = {Ship behavior recognition, Unsupervised algorithm, Massive unknown data, Meta-trajectory, Operational efficiency, Fuel consumption},
abstract = {Ship behaviors refer to the operational process such as sailing, entering into port/departure, etc., which indicate by their position, speed, and so on. The collected big data normally have been treated by unsupervised Machine Learning methods. However, the process is time consuming and lacks consideration of time continuity. From the unknown data to recognize and recur the ship behaviors is still a complex problem. Hence, this study proposes a universal Meta-trajectory Variable Sliding Window (Meta-VSW) method to provide an efficient and high-fidelity solution. In this method, the ship data were connected into the smallest units by the meta-trajectory coding, and combines with variable sliding windows to achieve fast, continuous and accurate recognition of ship behaviors. Taking an inland-water ship and a marine transport ship as examples, the validity of the method was fulfilled and compared with two commonly used algorithms, Affinity Propagation (AP) and Density-Based Spatial Clustering of Applications with Noise (DBSCAN). It has the fastest computational speed and can effectively classify the behaviors of massive unknown data from different ships. And it has good performance in capturing behavior boundaries, with the recognition accuracy up to 0.9. Then, the method was applied to analyze the operational effectively and fuel consumption.}
}
@article{BECKER201979,
title = {Two results on slime mold computations},
journal = {Theoretical Computer Science},
volume = {773},
pages = {79-106},
year = {2019},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2018.08.027},
url = {https://www.sciencedirect.com/science/article/pii/S0304397518305590},
author = {Ruben Becker and Vincenzo Bonifaci and Andreas Karrenbauer and Pavel Kolev and Kurt Mehlhorn},
keywords = {, Dynamical systems, Linear programming, Optimization, Approximation algorithms},
abstract = {We present two results on slime mold computations. In wet-lab experiments by Nakagaki et al. (2000) [1] the slime mold Physarum polycephalum demonstrated its ability to solve shortest path problems. Biologists proposed a mathematical model, a system of differential equations, for the slime's adaption process (Tero et al., 2007) [3]. It was shown that the process convergences to the shortest path (Bonifaci et al., 2012) [5] for all graphs. We show that the dynamics actually converges for a much wider class of problems, namely undirected linear programs with a non-negative cost vector. Combinatorial optimization researchers took the dynamics describing slime behavior as an inspiration for an optimization method and showed that its discretization can ε-approximately solve linear programs with positive cost vector (Straszak and Vishnoi, 2016) [14]. Their analysis requires a feasible starting point, a step size depending linearly on ε, and a number of steps with quartic dependence on opt/(εΦ), where Φ is the difference between the smallest cost of a non-optimal basic feasible solution and the optimal cost (opt). We give a refined analysis showing that the dynamics initialized with any strongly dominating point converges to the set of optimal solutions. Moreover, we strengthen the convergence rate bounds and prove that the step size is independent of ε, and the number of steps depends logarithmically on 1/ε and quadratically on opt/Φ.}
}
@incollection{KAMESWARI202581,
title = {Chapter 4 - Future trends and research challenges in digital twins},
editor = {Sailesh Iyer and Anand Nayyar and Anand Paul and Mohd Naved},
booktitle = {Digital Twins for Smart Cities and Villages},
publisher = {Elsevier},
pages = {81-101},
year = {2025},
isbn = {978-0-443-28884-5},
doi = {https://doi.org/10.1016/B978-0-443-28884-5.00004-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044328884500004X},
author = {Y. Lalitha Kameswari and B. {Omkar Lakshmi Jagan} and Thayyaba Khatoon Mohammed and Shady H.E. {Abdel Aleem}},
keywords = {Artificial intelligence, Digital twins, Integration, Internet of Things, Machine learning, Urban planning},
abstract = {With applications ranging from manufacturing and healthcare to urban planning and energy, digital twin technology has become a ground-breaking idea. In order to provide light on the potential breakthroughs and obstacles that lie ahead, this study examines future trends and research challenges in the field of digital twins. Several significant themes are anticipated to influence the development of digital twins as they continue to change. Digital twins will be better able to learn from complex situations in real time because to the combination of artificial intelligence (AI) and machine learning (ML). Digital twins will be given the ability to foresee, improve, and react to dynamic changes as a result of this AI-driven evolution, which will ultimately result in more effective and resilient systems. Another important development is the idea of federated digital twins, in which various interconnected digital twin instances work together to represent a larger, interconnected system. By integrating the strengths of several digital twins, this method makes it easier to model and analyze very complex and interconnected systems, such as smart cities or multimodal transportation networks. It is also projected that digital twins would spread into the Internet of Things (IoT) space. A closer connection between the real and virtual worlds will be made possible by the seamless integration of sensors, actuators, and data streams with digital twin platforms. This pattern will open the door to fresh perspectives and opportunities for improvement. To fully realize the potential of digital twins, a number of scientific challenges must be overcome. As the integration of real-time data from physical systems raises worries about unauthorized access and potential vulnerabilities, data privacy and security continue to be of the utmost importance. Additionally, sophisticated methods for data assimilation, model validation, and uncertainty quantification are needed to create accurate and trustworthy digital twin models. Another urgent issue is interoperability. The creation of standardized interfaces and protocols is essential to facilitate seamless integration and data sharing as digital twins proliferate across many sectors and domains. Furthermore, novel approaches to distributed computing and high-performance simulation are necessary to meet the scalability and computational requirements of large-scale digital twin ecosystems. In-depth analysis of these trends and problems is provided in this chapter, along with suggestions for future research areas and solutions. The field of digital twins is set for a paradigm-shifting impact on how we build, function, and interact with the physical world by tackling these issues and exploiting new trends.}
}
@article{ZALL2024101285,
title = {Towards emotion-aware intelligent agents by utilizing knowledge graphs of experiences},
journal = {Cognitive Systems Research},
volume = {88},
pages = {101285},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101285},
url = {https://www.sciencedirect.com/science/article/pii/S1389041724000792},
author = {Raziyeh Zall and Mohammad Reza Kangavari},
keywords = {Affective computing, Intelligent agent, Cognitive architecture, Appraisal theory, Emotional mental state},
abstract = {Because of the increasing presence of intelligent agents in various aspects of human social life, social skills play a vital role in ensuring these systems exhibit acceptable and realistic behavior in social communication. The importance of emotional intelligence in social capabilities is noteworthy, so incorporating emotions into the behaviors of intelligent agents is essential. Therefore, some computational models of emotions have been presented to develop intelligent agents that exhibit emotional human-like behaviors. However, most current computational models of emotions neglect the dynamic learning of the affective meaning of events based on agents’ experiences. Such models evaluate the events in the environment according to emotional aspects without considering the context of the situations. Also, these models capture the emotional states of agents by using predefined rules determined according to psychological theories. Therefore, they disregard the data-driven methods that can obtain the relationships between appraisal variables and emotions based on natural human data with fewer assumptions on the nature of such relationships. To address these issues, we proposed a novel and unified affective-cognitive framework (EIAEC) to facilitate the development of emotion-aware intelligent agents. EIAEC uses appraisal theories to acquire the emotional states of the agent in various situations. This paper contains four main contributions: 1- We have designed an efficient episodic memory that uses events and their conditional contexts to store and retrieve knowledge and experiences. This memory facilitates emotional expressions and decision-making adapted to the situations of the agent. 2- A novel method has been proposed that learns context-dependent affective values associated with events by using the agent’s experiences in various contexts. Subsequently, we acquired appraisal variables using the elements and related meta-data in episodic memory. 3- We have proposed a new data-driven method that maps appraisal variables to emotional states. 4- Moreover, a method has been developed to update the activation values regarding actions by using the emotional states of the agent. This method models the influence of emotions on the agent’s decision-making. Finally, we simulate a driving scenarios in our proposed framework to manifest the generated emotions in different situations and conditions. Moreover, we show how the proposed method learns the affective meaning of events and actions used in appraisal computing.}
}
@article{GARBEY1990293,
title = {Massively parallel computation of conservation laws},
journal = {Parallel Computing},
volume = {16},
number = {2},
pages = {293-304},
year = {1990},
issn = {0167-8191},
doi = {https://doi.org/10.1016/0167-8191(90)90067-J},
url = {https://www.sciencedirect.com/science/article/pii/016781919090067J},
author = {Marc Garbey and David Levine},
keywords = {Cellular automata, Partial differential equations, Method of characteristics, Parallel algorithms, Conservation laws},
abstract = {We present a new method for computing solutions of conservation laws based on the use of cellular automata with the method of characteristics. The method exploits the high degree of parallelism available with cellular automata and retains important features of the method of characteristics. It yields high numerical accuracy and extends naturally to adaptive meshes and domain decomposition methods for perturbed conservation laws. We describe the method and its implementation for a Dirichlet problem with a single conservation law for the one-dimensional case. Numerical results for the one-dimensional law with the classical Burgers nonlinearity or the Buckley-Leverett equation show good numerical accuracy outside the neighborhood of the shocks. The error in the area of the shocks is of the order of the mesh size. The algorithm is well suited for execution on both massively parallel computers and vector machines. We present timing results for an Alliant FX/8, Connection Machine Model 2, and CRAY X-MP.}
}
@incollection{MAGGIONI2010255,
title = {Knowledge Domains and Domain Learning},
editor = {Penelope Peterson and Eva Baker and Barry McGaw},
booktitle = {International Encyclopedia of Education (Third Edition)},
publisher = {Elsevier},
edition = {Third Edition},
address = {Oxford},
pages = {255-264},
year = {2010},
isbn = {978-0-08-044894-7},
doi = {https://doi.org/10.1016/B978-0-08-044894-7.00483-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780080448947004838},
author = {L. Maggioni and P.A. Alexander},
keywords = {Discipline, Domain, History, Knowledge, Learning, Mathematics, Reading, Science, Writing},
abstract = {The roots of current disciplines and domains of study reach well back in history. An exploration of their development shows that these areas of knowledge have not only reflected cultural changes, but have also influenced societies, especially through formal educational systems. Besides being characterized by their focus on a particular part of the world, disciplines are also distinguished by a specific way of thinking about their respective domains of study. Psychological research has identified several features of these pathways to knowledge (e.g., reading, writing, history, mathematics, and science) that generally define the landscape of academic practice.}
}
@article{OMRAN2022114806,
title = {Valorization of agro-industrial biowaste to green nanomaterials for wastewater treatment: Approaching green chemistry and circular economy principles},
journal = {Journal of Environmental Management},
volume = {311},
pages = {114806},
year = {2022},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2022.114806},
url = {https://www.sciencedirect.com/science/article/pii/S0301479722003796},
author = {Basma A. Omran and Kwang-Hyun Baek},
keywords = {Green synthesis, Zero-cost, Nanomaterials, Wastewater treatment, Sustainability},
abstract = {Water pollution is one of the most critical issues worldwide and is a priority in all scientific agendas. Green nanotechnology presents a plethora of promising avenues for wastewater treatment. This review discusses the current trends in the valorization of zero-cost, biodegradable, and readily available agro-industrial biowaste to produce green bio-nanocatalysts and bio-nanosorbents for wastewater treatment. The promising roles of green bio-nanocatalysts and bio-nanosorbents in removing organic and inorganic water contaminants are discussed. The potent antimicrobial activity of bio-derived nanodisinfectants against water-borne pathogenic microbes is reviewed. The bioactive molecules involved in the chelation and tailoring of green synthesized nanomaterials are highlighted along with the mechanisms involved. Furthermore, this review emphasizes how the valorization of agro-industrial biowaste to green nanomaterials for wastewater treatment adheres to the fundamental principles of green chemistry, circular economy, nexus thinking, and zero-waste manufacturing. The potential economic, environmental, and health impacts of valorizing agro-industrial biowaste to green nanomaterials are highlighted. The challenges and future outlooks for the management of agro-industrial biowaste and safe application of green nanomaterials for wastewater treatment are summarized.}
}
@article{MANZOLLI2022112211,
title = {A review of electric bus vehicles research topics – Methods and trends},
journal = {Renewable and Sustainable Energy Reviews},
volume = {159},
pages = {112211},
year = {2022},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2022.112211},
url = {https://www.sciencedirect.com/science/article/pii/S1364032122001344},
author = {Jônatas Augusto Manzolli and João Pedro Trovão and Carlos Henggeler Antunes},
keywords = {Electric bus, Electric mobility, Research gaps, Sustainability, Fleet operation, Energy management},
abstract = {The transportation sector accounts for a significant share of greenhouse gas emissions. Hence, the electrification of this sector is a crucial contributor to the mitigation of global warming. Recent studies suggest that electric vehicles will be economically paired with internal combustion engine vehicles in the near future. However, relying on private vehicle decarbonization only cannot deliver comprehensive space management efficiency solutions in urban environments. Therefore, it is essential to invest in the technological development and deployment of electric buses for public transportation, directly enhancing the quality of life in large cities. From this perspective, this review examines a wide range of scientific literature on electric bus research using science mapping methods and content analysis to support critical thinking unveiling the main research streams, methods, and gaps of the field. The analysis indicates that future research on electric buses will be mainly devoted to sustainability (encompassing economic, environmental and quality of service dimensions), energy management strategies, and fleet operation.}
}
@article{KUGURAKOVA2016217,
title = {Neurobiological Plausibility as Part of Criteria for Highly Realistic Cognitive Architectures},
journal = {Procedia Computer Science},
volume = {88},
pages = {217-223},
year = {2016},
note = {7th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2016, held July 16 to July 19, 2016 in New York City, NY, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.07.428},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916316842},
author = {Vlada Kugurakova and Maxim Talanov and Denis Ivanov},
keywords = {Lövheim cube, cognitive architectures, neurobiological realism},
abstract = {In this paper we analyze neurobiologically inspired approaches to implement emotions in computational systems. We propose the criteria for realistic cognitive architectures and analyze current architectures using aforementioned criteria. The analysis indicated several interesting architectures H-CogAff, BICA that inspired us to start the development of our own based on biological realistic approaches.}
}
@article{KAKOOEE20241466,
title = {Impact of Pavlovian Approach Bias on Bidirectional Planning in Spatial Navigation Tasks},
journal = {Procedia Computer Science},
volume = {246},
pages = {1466-1478},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.593},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924026449},
author = {Reza Kakooee and Mohammad TH Beheshti and Mehdi Keramati},
keywords = {Reinforcement Learning, Computational Modeling, Bidirectional Planning, Decision-Making, Pavlovian Bias},
abstract = {Bidirectional planning refers to a form of goal-directed decision-making process that combines forward and backward planning. Forward planning expands decision trees from the current state towards simulated futures, while backward planning starts the tree expansion from specific goal points in the opposite direction. Previous research has highlighted the impact of Pavlovian approach bias on behavior, showing that animals move towards appetitive outcomes regardless of the appropriateness of such behavior for achieving those outcomes. However, it remains unexplored whether the Pavlovian approach influences behavior by biasing backward planning. This research introduces a spatial navigation task to investigate the involvement of backward planning in humans’ action-selection process and to determine whether the Pavlovian approach biases behavior through backward planning. The results reveal the behavioral signature of backward planning in humans and show that Pavlovian approach bias can influence both forward and backward planning, leading to decisions that are not necessarily instrumentally more efficient. Additionally, we developed a bidirectional planning algorithm based on reinforcement learning to simulate the participants’ decisions. The simulation results suggest that the observed behavioral patterns can be parsimoniously explained by assuming that the Pavlovian approach bias acts as a pruning mechanism when expanding decision trees in both forward and backward directions.}
}
@article{BENTON2023105626,
title = {Associative learning or Bayesian inference? Revisiting backwards blocking reasoning in adults},
journal = {Cognition},
volume = {241},
pages = {105626},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2023.105626},
url = {https://www.sciencedirect.com/science/article/pii/S0010027723002603},
author = {Deon T. Benton and David H. Rakison},
keywords = {Causal reasoning, Causal mechanisms, Computational models, Analytical models, Associative learning, Bayesian inference},
abstract = {Causal reasoning is a fundamental cognitive ability that enables humans to learn about the complex interactions in the world around them. However, the cognitive mechanisms that underpin causal reasoning are not well understood. For instance, there is debate over whether Bayesian inference or associative learning best captures causal reasoning in human adults. The two experiments and computational models reported here were designed to examine whether adults engage in one form of causal inference called backwards blocking reasoning, whether the presence of potential distractors affects performance, and how adults' ratings align with the predictions of different computational models. The results revealed that adults engaged in backwards blocking reasoning regardless of whether distractor objects are present and that their causal judgements supported the predictions of a Bayesian model but not the predictions of two different associative learning models. Implications of these results are discussed.}
}
@article{RUTER2000519,
title = {Analysis, finite element computation and error estimation in transversely isotropic nearly incompressible finite elasticity},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {190},
number = {5},
pages = {519-541},
year = {2000},
issn = {0045-7825},
doi = {https://doi.org/10.1016/S0045-7825(99)00286-8},
url = {https://www.sciencedirect.com/science/article/pii/S0045782599002868},
author = {Marcus Rüter and Erwin Stein},
abstract = {In this paper we present constitutive models for nearly incompressible, transversely isotropic materials in finite hyperelasticity, particularly for reinforced rubber-like materials, which are of essential engineering interest. The theory is developed using a convected curvilinear coordinate system based on a mixed two-field displacement–pressure energy functional. Furthermore, an a posteriori error estimator without multiplicative constants is derived for non-linear anisotropic problems, which measures the discretization error in the first Piola–Kirchhoff stresses in the L2-norm by solving local Neumann problems with equilibrated tractions. Illustrative numerical examples demonstrate the anisotropic material behaviour of reinforced materials and the efficiency of using adaptive finite element methods.}
}
@article{MCCOWN201233,
title = {Farmers use intuition to reinvent analytic decision support for managing seasonal climatic variability},
journal = {Agricultural Systems},
volume = {106},
number = {1},
pages = {33-45},
year = {2012},
issn = {0308-521X},
doi = {https://doi.org/10.1016/j.agsy.2011.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0308521X11001557},
author = {R.L. McCown and P.S. Carberry and N.P. Dalgliesh and M.A. Foale and Z. Hochman},
keywords = {Decision support, Simulation, Information system, Cognitive system, Intuition, Climatic risk},
abstract = {The FARMSCAPE Information System emerged in a long-running research program aimed at making simulation models useful to Australian farmers in managing climatic variability. This paper is about how well it has worked. This is reported in relation to two standards: (1) the value to thinking and action expressed by farmers and their consultants, (2) correspondence with theory about learning and judgement in uncertain external environments. The former utilises recorded narrative interviews with participants over many years. The latter uses a cognitive framework drawn from theory of judgment and decision making featuring the relationship between intuition and analysis (McCown, 2011). The cognitive theory framework makes sense of several evaluation surprises. The first was high enthusiasm by largely-intuitive farmers for an analytic approach to soil water in conjunction with a newly-appreciated “bucket” metaphor for water balance. The second surprise was the virtual absence of soil water measurement 10years later. This had been replaced by various intuitive estimates, calibrated to maintain a heuristic relationship with regard to the “bucket” as a resource. Farmers and their advisers were facilitated in using simulation for thought experiments and planning under climatic uncertainty. Benchmarking enabled problem solving in documented conditions. Scenario analysis using historical climate records supported thought experiments by providing probability distributions that were valued for shaping expectations as a “history of the future”. In retrospective evaluation interviews, researchers were surprised to find that yield forecasting and tactical decision making, anticipated to be analyses that were both site- and season-specific forecasts, had served farmers as “management gaming” simulations to aid formulating action rules for such conditions, thus reducing the need for an on-going decision-aiding service. Equipped with their soil monitoring techniques and with their heuristic rules, farmers still reserved a place for simulation “when you’ve got a planting situation out of the ordinary.”}
}
@article{DENHAM2022105526,
title = {Visualization and modeling of forest fire propagation in Patagonia},
journal = {Environmental Modelling & Software},
volume = {158},
pages = {105526},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105526},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222002262},
author = {Mónica M. Denham and Sigfrido Waidelich and Karina Laneri},
keywords = {Simulation, Modeling, Forest fire behavior, High-performance computing, GPGPU},
abstract = {Fire propagation is a big concern all over the world. Visualization is a valuable tool to test possible different scenarios for fire spread, specially for designing strategies for fire control, mitigation and management. We present a parallel High-Performance Computing (HPC) forest fire simulator with an interactive and intuitive user interface that offers several functionalities to the user. The visualization interface allows to choose the propagation model of preference, the scenario of interest, as well as numerous simulation features including firebreaks and ignition points. We show some of the outputs for two different mathematical models for fire spreading. The simulator was developed with an open source philosophy in the framework of Faster Than Real Time (FTRT) applications thinking on its possible use in the field during a forest fire propagation. It can be run in Linux (Ubuntu) and Windows Operating Systems and for portability purposes the simulator was also implemented on a NVIDIA Jetson Nano.}
}
@article{ZMIGROD202034,
title = {The role of cognitive rigidity in political ideologies: theory, evidence, and future directions},
journal = {Current Opinion in Behavioral Sciences},
volume = {34},
pages = {34-39},
year = {2020},
note = {Political Ideologies},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2019.10.016},
url = {https://www.sciencedirect.com/science/article/pii/S2352154619301147},
author = {Leor Zmigrod},
abstract = {A contentious debate in political psychology has centred on the role of cognitive rigidity in shaping individuals’ political ideologies and worldviews. Early theories in the 1950s posited that strict ideological doctrines may tend to attract individuals with dispositions towards mental rigidity. This question has persisted: Does psychological rigidity foster a tendency towards ideological extremism? This review evaluates the empirical landscape with respect to the rigidity-of-the-extreme and the rigidity-of-the-right hypotheses and offers conceptual and methodological recommendations for future research avenues. The evidence suggests that cognitive rigidity is linked to ideological extremism, partisanship, and dogmatism across political and non-political ideologies. Advances in the measurement of ideological extremity and cognitive rigidity will facilitate further elucidation regarding how exactly the two hypotheses may be reconciled and why they have been historically placed in a potentially false competition. This synthesis suggests that a scientifically rigorous understanding of the cognitive roots of ideological thinking may be essential for developing effective antidotes to intolerance and intergroup hostility.}
}
@article{BULLEY20203457,
title = {Children Devise and Selectively Use Tools to Offload Cognition},
journal = {Current Biology},
volume = {30},
number = {17},
pages = {3457-3464.e3},
year = {2020},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2020.06.035},
url = {https://www.sciencedirect.com/science/article/pii/S0960982220308514},
author = {Adam Bulley and Thomas McCarthy and Sam J. Gilbert and Thomas Suddendorf and Jonathan Redshaw},
keywords = {cognitive artifacts, cognitive offloading, cognitive development, extended mind, metacognition},
abstract = {Summary
From maps sketched in sand to supercomputing software, humans ubiquitously enhance cognitive performance by creating and using artifacts that bear mental load [1, 2, 3, 4, 5]. This extension of information processing into the environment has taken center stage in debates about the nature of cognition in humans and other animals [6, 7, 8, 9]. How does the human mind acquire such strategies? In two experiments, we investigated the developmental origins of cognitive offloading in 150 children aged between 4 and 11 years. We created a memory task in which children were required to recall the location of hidden targets. In one experiment, participants were provided with a pre-specified cognitive offloading opportunity: an option to mark the target locations with tokens during the hiding period. Even 4-year-old children quickly adopted this external strategy and, in line with a metacognitive account, children across ages offloaded more often when the task was more difficult. In a second experiment, we provided children with the means to devise their own cognitive offloading strategy. Very few younger children spontaneously devised a solution, but by ages 10 and 11, nearly all did so. In a follow-up test phase, a simple prompt greatly increased the rate at which the younger children devised an offloading strategy. These findings suggest that sensitivity to the difficulties of thinking arises early in development and improves throughout the early school years, with children learning to modify the world around them to compensate for their cognitive limits.}
}
@article{LI2025102888,
title = {From assistance to reliance: Development and validation of the large language model dependence scale},
journal = {International Journal of Information Management},
volume = {83},
pages = {102888},
year = {2025},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2025.102888},
url = {https://www.sciencedirect.com/science/article/pii/S0268401225000209},
author = {Zewei Li and Zheng Zhang and Mingwei Wang and Qi Wu},
keywords = {Factor model of LLMs dependence, Large language models, Functional dependence, Existential dependence, Alleviating interventions},
abstract = {With the rapid advancement of large language models (LLMs), the phenomenon of LLMs dependence has emerged and garnered significant attention. However, previous scales have been insufficient to measure the extent of individuals' dependence on LLMs. The current study aims to utilize the human-computer trust model and addiction theory to develop and validate the LLMs dependence scale (LDS) and to report its psychometric properties. An exploratory structural investigation of LLMs dependence was conducted with a sample of 421 LLMs users (Sample 1), which included items analysis, exploratory factor analysis, and network analysis. Additionally, a formal test was performed with a separate sample of 1030 LLMs users (Sample 2), with the data undergoing structural validation through confirmatory factor analysis, validity testing, and reliability testing. To mitigate the potential negative impacts of LLMs dependence, we employed the NodeIdentifyR algorithm for computational simulation interventions to identify critical intervention nodes within the LLMs dependence network. The results indicated that the LDS (18 items) exhibited a bifactor structure of functional dependence and existential dependence. The confirmatory factor model demonstrated a good fit and the LDS also showed good criterion-related validity. Subsequent simulated results of alleviating interventions suggested that users' existential dependence was primarily driven by their dependence on LLMs to handle tedious and boring tasks, while functional dependence was more influenced by users' belief in the omnipotence of LLMs. In summary, the factor structure of the LDS is clear, and its reliability and validity indices meet psychometric standards, making it an effective tool for measuring LLMs dependence.}
}
@article{ZHANG2022101060,
title = {The neural encoding of productive phonological alternation in speech production: Evidence from Mandarin Tone 3 sandhi},
journal = {Journal of Neurolinguistics},
volume = {62},
pages = {101060},
year = {2022},
issn = {0911-6044},
doi = {https://doi.org/10.1016/j.jneuroling.2022.101060},
url = {https://www.sciencedirect.com/science/article/pii/S0911604422000045},
author = {Jie Zhang and Caicai Zhang and Stephen Politzer-Ahles and Ziyi Pan and Xunan Huang and Chang Wang and Gang Peng and Yuyu Zeng},
keywords = {Tone sandhi, Mandarin Chinese, Speech production, Event-related potentials, Phonological alternation, Word frequency},
abstract = {The understanding of alternation is a key goal in phonological research. But little is known about how phonological alternations are implemented in speech production. The current study tested the hypothesis that the production of words that undergo a highly productive alternation, Mandarin Tone 3 sandhi, is supported by a computation mechanism, which predicts that this alternation is subserved by neural activity in a time-window associated with post-lexical phonological and phonetic encoding regardless of word frequency. ERPs were recorded while participants sub-vocally produced high- and low-frequency disyllabic words that do or do not require sandhi. Sandhi words elicited more positive ERPs than non-sandhi words over left anterior channels around 336–520 ms after participants saw the cue instructing them to initiate sub-vocal production, but this effect was not significantly modulated by word frequency. These findings are consistent with predictions of the computation mechanism and have implications for current psycholinguistic models of speech production. (150 words)}
}
@article{WANG2022269,
title = {Intelligent Attack Analysis for IRS Communications with Incomplete Information},
journal = {Procedia Computer Science},
volume = {202},
pages = {269-276},
year = {2022},
note = {International Conference on Identification, Information and Knowledge in the internet of Things, 2021},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.04.035},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922005695},
author = {Han Wang and Tianlin Zhu and Dapeng Li and Rui Jiang and Xiaoming Wang and Youyun Xu},
keywords = {IRS system, monitoring, attacking tactic, greedy, robust attack},
abstract = {Intelligent Reflection Surface (IRS) will be widely used in future communication system construction to reduce construction costs and improve coverage. However, IRS systems are generally equipped with controllers to receive wireless signal instructions, this increases the vulnerability of future communications. In this paper, we present an attack tactic to provide a way of thinking for the future defense deployment. At the beginning, the hacker cannot know the whole communication system, then it continuously attacks the IRS system, monitor the communication system, and sequentially learns new information about the system in each attacking round in order to attack more effectively in the next round. A two-layer optimal mathematical model is presented to describe the BS and the hacker’s decision. And the two-layer optimization which is difficult to solve is transformed into a single layer linear optimization by using equivalent transformation and dual transformation. A series of mathematical experiments are used to test different scenarios applicable to different monitoring style, and verify that the tactic proposed in this paper can effectively interfere with the system.}
}
@article{SAFARZYNSKA20121011,
title = {Evolutionary theorizing and modeling of sustainability transitions},
journal = {Research Policy},
volume = {41},
number = {6},
pages = {1011-1024},
year = {2012},
note = {Special Section on Sustainability Transitions},
issn = {0048-7333},
doi = {https://doi.org/10.1016/j.respol.2011.10.014},
url = {https://www.sciencedirect.com/science/article/pii/S0048733312000595},
author = {Karolina Safarzyńska and Koen Frenken and Jeroen C.J.M. {van den Bergh}},
keywords = {Coevolution, Evolutionary economics, Group selection, Lock-in, Niche, Regime, Social learning, Transition, Transition management},
abstract = {This paper argues that evolutionary thinking and modeling can contribute to the emerging research on sustainability transitions and their management. Evolutionary theory provides a range of concepts and mechanisms that are useful in making existing theorizing about transitions more precise and complete. In particular, we will discuss how the multi-level, multi-phase, co-evolutionary, and social learning dynamics underlying transitions can be addressed in evolutionary models. In addition, evolutionary theorizing offers suggestions for extending current theoretical frameworks of transitions. Group selection provides a good example. We review the small set of formal evolutionary models of sustainability transitions, and show that existing formal evolutionary models of technological, social and institutional change can provide useful inputs to transition research and management.}
}
@article{BUEHLER20081101,
title = {Theoretical and computational hierarchical nanomechanics of protein materials: Deformation and fracture},
journal = {Progress in Materials Science},
volume = {53},
number = {8},
pages = {1101-1241},
year = {2008},
issn = {0079-6425},
doi = {https://doi.org/10.1016/j.pmatsci.2008.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0079642508000510},
author = {Markus J. Buehler and Sinan Keten and Theodor Ackbarow},
abstract = {Proteins constitute the building blocks of biological materials such as tendon, bone, skin, spider silk or cells. An important trait of these materials is that they display highly characteristic hierarchical structures, across multiple scales, from nano to macro. Protein materials are intriguing examples of materials that balance multiple tasks, representing some of the most sustainable material solutions that integrate structure and function. Here we review progress in understanding the deformation and fracture mechanisms of hierarchical protein materials by using a materials science approach to develop structure-process-property relations, an effort defined as materiomics. Deformation processes begin with an erratic motion of individual atoms around flaws or defects that quickly evolve into formation of macroscopic fractures as chemical bonds rupture rapidly, eventually compromising the integrity of the structure or the biological system leading to failure. The combination of large-scale atomistic simulation, multi-scale modeling methods, theoretical analyses combined with experimental validation provides a powerful approach in studying deformation and failure phenomena in protein materials. Here we review studies focused on the molecular origin of deformation and fracture processes of three types of protein materials. The review includes studies of collagen – Nature’s super-glue; beta-sheet rich protein structures as found in spider silk – a natural fiber that can reach the strength of a steel cable; as well as intermediate filaments – a class of alpha-helix based structural proteins responsible for the mechanical integrity of eukaryotic cells. The article concludes with a discussion of the significance of universally found structural patterns such as the staggered collagen fibril architecture or the alpha-helical protein motif.}
}
@article{KOKIS200226,
title = {Heuristic and analytic processing: Age trends and associations with cognitive ability and cognitive styles},
journal = {Journal of Experimental Child Psychology},
volume = {83},
number = {1},
pages = {26-52},
year = {2002},
issn = {0022-0965},
doi = {https://doi.org/10.1016/S0022-0965(02)00121-2},
url = {https://www.sciencedirect.com/science/article/pii/S0022096502001212},
author = {Judite V. Kokis and Robyn Macpherson and Maggie E. Toplak and Richard F. West and Keith E. Stanovich},
abstract = {Developmental and individual differences in the tendency to favor analytic responses over heuristic responses were examined in children of two different ages (10- and 11-year-olds versus 13-year-olds), and of widely varying cognitive ability. Three tasks were examined that all required analytic processing to override heuristic processing: inductive reasoning, deductive reasoning under conditions of belief bias, and probabilistic reasoning. Significant increases in analytic responding with development were observed on the first two tasks. Cognitive ability was associated with analytic responding on all three tasks. Cognitive style measures such as actively open-minded thinking and need for cognition explained variance in analytic responding on the tasks after variance shared with cognitive ability had been controlled. The implications for dual-process theories of cognition and cognitive development are discussed.}
}
@article{RUCH2024115840,
title = {Alterations in performance and discriminating power of the death/suicide implicit association test across the lifespan},
journal = {Psychiatry Research},
volume = {335},
pages = {115840},
year = {2024},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2024.115840},
url = {https://www.sciencedirect.com/science/article/pii/S0165178124001252},
author = {Donna A. Ruch and Jeffrey A. Bridge and Jaclyn Tissue and Sean P. Madden and Hanga Galfavy and Marianne Gorlyn and Arielle H. Sheftall and Katalin Szanto and John G. Keilp},
keywords = {Death/suicide implicit association, Suicide, Suicide risk assessment, Suicide prevention/early detection},
abstract = {The Death/Suicide Implicit Association Test (d/s-IAT) has differentiated individuals with prior and prospective suicide attempts in previous studies, however, age effects on test results remains to be explored. A three-site study compared performance on the d/s-IAT among participants aged 16–80 years with depression and prior suicide attempt (n = 82), with depression and no attempts (n = 80), and healthy controls (n = 86). Outcome measures included the standard difference (D) score, median reaction times, and error rates. Higher D scores represent a stronger association between death/suicide and self, while lower scores represent a stronger association between life and self. The D scores differed significantly among groups overall. Participants with depression exhibited higher scores compared to healthy controls, but there was no difference between participants with and without prior suicide attempts(F[2,242]=8.76, p<.001). Response times for participants with prior attempts differed significantly from other groups, with no significant differences in error rates. The D score was significantly affected by age (β =-0.007, t = 3.65, p<.001), with slowing of response times in older ages. Results suggest reaction time d/s-IAT D scores may not distinguish implicit thinking about suicide as response times slow with age, but slowed response times may be sensitive to suicide risk potentially indicating basic information processing deficits.}
}
@article{WANG2024111131,
title = {Three-way clustering: Foundations, survey and challenges},
journal = {Applied Soft Computing},
volume = {151},
pages = {111131},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.111131},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623011493},
author = {Pingxin Wang and Xibei Yang and Weiping Ding and Jianming Zhan and Yiyu Yao},
keywords = {Cluster analysis, Two-way clustering, Three-way decision, Three-way clustering},
abstract = {Clustering, as an unsupervised data mining technique, allows us to classify similar objects into the same cluster according to certain criteria. It helps us identify patterns between objects, reveal the associations between objects, and discover hidden structures. Traditional two-way clustering (2W clustering) algorithms represent one cluster by one set and only two types of relationships are considered between a sample and a cluster, namely, belonging to and not belonging to. Two-way decision is not always feasible especially in situations that are characterized by uncertainty and lack of information. Guided by the principle of three-way decision (3WD) as thinking in threes, three-way clustering (3W clustering) addresses the information uncertainty problem using core and the fringe regions to character a cluster. The universe is split into three sections by these two sets, which capture three kinds of relationships between objects and a cluster, namely, belonging to, partially belonging to, and not belonging-to. Compared with 2W clustering methods, 3W clustering incorporates the fringe region to describe the uncertain relationship between objects and clusters, which provides more information about the clustering structure. This survey points out the historical developments of three-way clustering and makes an overview of the achievements in the field of three-way clustering. In addition, to reap a clearer grasp of the development and research significance of three-way clustering, we divide the existing three-way clustering approaches into two categories and present the bibliometric analysis of related approaches. Finally, we point out some challenges and future research topics in three-way clustering. It is hoped that this review can serve as a reference and provide convenience for scholars and practitioners in the field of three-way clustering.}
}
@article{CARVALHO2016169,
title = {Origins and evolution of enactive cognitive science: Toward an enactive cognitive architecture},
journal = {Biologically Inspired Cognitive Architectures},
volume = {16},
pages = {169-178},
year = {2016},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2015.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X15000535},
author = {Leonardo Lana de Carvalho and Denis James Pereira and Sophia Andrade Coelho},
keywords = {Cognitive science, Enaction, Complex systems, Cognitive architecture},
abstract = {This paper presents a historical perspective on the origin of the enactive approach to cognitive science, starting chronologically from cybernetics, with the aim of clarifying its main concepts, such as enaction, autopoiesis, structural coupling and natural drift; thus showing their influences in computational approaches and models of cognitive architecture. Works of renowned authors, as well as some of their main commentators, were addressed to report the development of enactive approach. We indicate that the enactive approach transcends its original context within biology, and at a second moment within connectionism, changes the understanding of the relationships so far established between the body and the environment, and the ideas of conceptual relationships between the mind and the body. The influence on computational theories is of great importance, leading to new artificial intelligence systems as well as the proposition of complex, autopoietic and alive machines. Finally, the article stresses the importance of the enactive approach in the design of agents, understanding that previous approaches have very different cognitive architectures and that a prototypical model of enactive cognitive architecture is one of the largest challenges today.}
}
@article{GUPTA19941,
title = {On the principles of fuzzy neural networks},
journal = {Fuzzy Sets and Systems},
volume = {61},
number = {1},
pages = {1-18},
year = {1994},
issn = {0165-0114},
doi = {https://doi.org/10.1016/0165-0114(94)90279-8},
url = {https://www.sciencedirect.com/science/article/pii/0165011494902798},
author = {M.M. Gupta and D.H. Rao},
keywords = {Fuzzy logic, neural networks, fuzzy neural networks, confluence operation, synpatic and somatic operations},
abstract = {Over the last decade or so, significant advances have been made in two distinct technological areas: fuzzy logic and computational neutral networks. The theory of fuzzy logic provides a mathematical framework to capture the uncertainties associated with human cognitive processes, such as thinking and reasoning. Also, it provides a mathematical morphology to emulate certain perceptual and linguistic attributes associated with human cognition. On the other hand, the computational neural network paradigms have evolved in the process of understanding the incredible learning and adaptive features of neuronal mechanisms inherent in certain biological species. Computational neural networks replicate, on a small scale, some of the computational operations observed in biological learning and adaptation. The integration of these two fields, fuzzy logic and neural networks; has given birth to an emerging technological field — the fuzzy neural networks. The fuzzy neural networks have the potential to capture the benefits of the two fascinating fields, fuzzy logic and neural networks, into a single capsule. The intent of this tutorial paper is to describe the basic notions of biological and computational neuronal morphologies, and to describe the principles and architectures of fuzzy neural networks. Towards this goal, we develop a fuzzy neural architecture based upon the notion of T-norm and T-conorm connectives. An error-based learning scheme is described for this neural structure.}
}
@article{MEHRYAR2024109812,
title = {AI and climate resilience governance},
journal = {iScience},
volume = {27},
number = {6},
pages = {109812},
year = {2024},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2024.109812},
url = {https://www.sciencedirect.com/science/article/pii/S2589004224010344},
author = {Sara Mehryar and Vahid Yazdanpanah and Jeffrey Tong},
keywords = {Natural sciences, Earth sciences, Environmental science, Environmental policy, Social sciences},
abstract = {Summary
While artificial intelligence (AI) offers promising solutions to address climate change impacts, it also raises many application limitations and challenges. A risk governance perspective is used to analyze the role of AI in supporting decision-making for climate adaptation, spanning risk assessment, policy analysis, and implementation. This comprehensive review combines expert insights and systematic literature review. The study’s findings indicate a large emphasis on applying AI to climate “risk assessments,” particularly regarding hazard and exposure assessment, but a lack of innovative approaches and tools to evaluate resilience and vulnerability as well as prioritization and implementation process, all of which involve subjective, qualitative, and context-specific elements. Additionally, the study points out challenges such as difficulty of simulating complex long-term changes, and evolving policies and human behavior, reliance on data quality and computational resources, and the need for improved interpretability of results as areas requiring further development.}
}
@article{DECOUGNY1994157,
title = {Load balancing for the parallel adaptive solution of partial differential equations},
journal = {Applied Numerical Mathematics},
volume = {16},
number = {1},
pages = {157-182},
year = {1994},
issn = {0168-9274},
doi = {https://doi.org/10.1016/0168-9274(94)00039-5},
url = {https://www.sciencedirect.com/science/article/pii/0168927494000395},
author = {H.L. deCougny and K.D. Devine and J.E. Flaherty and R.M. Loy and C. Özturan and M.S. Shephard},
abstract = {An adaptive technique for a partial differential system automatically adjusts a computational mesh or varies the order of a numerical procedure with a goal of obtaining a solution satisfying prescribed accuracy criteria in an optimal fashion. Processor load imbalances will, therefore, be introduced at adaptive enrichment steps during the course of a parallel computation. We develop and describe three procedures for retaining and restoring load balance that have low unit cost and are appropriate for use in an adaptive solution environment. Tiling balances load by using local optimality criteria within overlapping processor neighborhoods. Elemental data are migrated between processors within the same neighborhoods to restore balance. Tiling is restricted to uniform two-dimensional meshes and provides limited control of communications volume by priority-based element selection criteria. These shortcomings can potentially be overcome by creating a dynamic partition graph connecting processors and their neighboring regions. After coloring the edges of the graph, elemental data are iteratively transferred between processors by pairwise exchange to permit a more global migration. Octree decomposition of a spatial domain is a successful three-dimensional mesh generation strategy. The octree structure facilities a rapid load balancing procedure by performing tree traversals that (i) appraise subtree costs and (ii) partition spatial regions accordingly. Computational results are reported for two- and three-dimensional problems using nCUBE/2 hypercube, MasPar MP-2, and Thinking Machines CM-5 computers.}
}
@article{VANCOUVER20081,
title = {Integrating self-regulation theories of work motivation into a dynamic process theory},
journal = {Human Resource Management Review},
volume = {18},
number = {1},
pages = {1-18},
year = {2008},
issn = {1053-4822},
doi = {https://doi.org/10.1016/j.hrmr.2008.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1053482208000028},
author = {Jeffrey B. Vancouver},
keywords = {Self-regulation, Control theory, Goals, Computational modeling, Dynamic processes},
abstract = {Instead of merely combining theories of self-regulation, the current paper articulates a dynamic process theory of the underlying cognitive subsystems that explain relationships among long-used constructs like goals, expectancies, and valence. Formal elements of the theory are presented in an attempt to encourage the building of computational models of human actors, thinkers, and learners in organizational contexts. Discussion focuses on the application of these models for understanding the dynamics of individuals interacting in their organizations.}
}
@article{ALDRIDGE1967155,
title = {A wave analogue as a guide to ultrasonic thinking},
journal = {Ultrasonics},
volume = {5},
number = {3},
pages = {155-162},
year = {1967},
issn = {0041-624X},
doi = {https://doi.org/10.1016/S0041-624X(67)80060-X},
url = {https://www.sciencedirect.com/science/article/pii/S0041624X6780060X},
author = {E.E. Aldridge}
}
@article{LIGABO2023102155,
title = {Practical way to apply fourth-generation assessment tools integrated into creating meaningful learning experiences in biology at high school},
journal = {Evaluation and Program Planning},
volume = {96},
pages = {102155},
year = {2023},
issn = {0149-7189},
doi = {https://doi.org/10.1016/j.evalprogplan.2022.102155},
url = {https://www.sciencedirect.com/science/article/pii/S0149718922001094},
author = {Mateus Ligabo and Fabiana Carvalho Silva and Ana Carolina da S.A. Carvalho and Durval Rodrigues and Rita C.L.B. Rodrigues},
keywords = {Concept maps, Meaningful learning, Hermeneutic-dialectic circle, Hofstede's cultural dimensions, Fourth-generation assessment},
abstract = {The learning process for a Biology topic regarding organisms and animal kingdom diversity was investigated through an innovative Interactive Didactic Sequence (IDS) which integrated the idea of “concept maps” with the Hermeneutic-Dialectic Circle (HDC). HDC is a tool for data collection and a reference for pluralist-constructivist thinking, considered a form of fourth-generation evaluation. Hofstede's cultural dimensions were also integrated into the investigation in order to facilitate mediation in an evaluative context. Students' performances (N = 25) from a São Paulo-Brazil public school were statistically evaluated. Their cultural profile was determined via the Hofstede Value Survey Model 1994 questionnaire. The elaborative process of arranging concept maps was individual (CM-individual) and integrated with HDC in groups (CM-HDC). Concept map assessment methods were based off existing literature. An improvement in students' performances (p < 0.05) that presented concept maps integrated to HDC in a more complex structure when compared to individually-built maps was observed. Employment of HDC helped form motivational/interactive dialogues between students and teachers, which, in turn, assisted in achieving greater learning through the use of concept maps. The application of the fourth-generation evaluation was improved via knowledge regarding students' cultural profiles.}
}
@incollection{KLATT200719,
title = {Perspectives for process systems engineering – a personal view from academia and industry},
editor = {Valentin Pleşu and Paul Şerban Agachi},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {24},
pages = {19-32},
year = {2007},
booktitle = {17th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/S1570-7946(07)80027-7},
url = {https://www.sciencedirect.com/science/article/pii/S1570794607800277},
author = {Karsten-Ulrich Klatt and Wolfgang Marquardt},
keywords = {Review, critical assessment, emerging fields, modeling, design, optimization, control, operations, numerical algorithms, software.},
abstract = {Process systems engineering (PSE) has been an active research field for almost 50 years. Modeling, simulation and optimization technologies have been developed to a mature state. These technologies have been penetrating all fields of chemical engineering in academia as well as in industrial practice. Systems thinking has been established in industrial practice largely through powerful commercial process simulation software and through mandatory courses in most chemical engineering programs. This contribution reflects on the past, present and future of PSE. Special emphasis will be on the perspectives of this field from an academic and industrial point of view.}
}
@incollection{WHITTEN201953,
title = {Chapter 4 - Guided Cognition Effects in Learning Mathematics},
editor = {William B. Whitten and Mitchell Rabinowitz and Sandra E. Whitten},
booktitle = {Guided Cognition for Learning},
publisher = {Academic Press},
pages = {53-108},
year = {2019},
isbn = {978-0-12-817538-5},
doi = {https://doi.org/10.1016/B978-0-12-817538-5.00004-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128175385000043},
author = {William B. Whitten and Mitchell Rabinowitz and Sandra E. Whitten},
keywords = {Advance organizers, Consolidators, Effective homework, Efficient homework, Guided Cognition design, Homework, Long-term or long-lasting learning, Middle school mathematics learning},
abstract = {This chapter reports 11 experiments that were done to determine whether Guided Cognition-designed homework facilitates learning middle school mathematics, and if so, to determine how it helps and what is learned. Experiments were performed in two middle schools and included 8th graders in two experiments and 7th graders in nine experiments. Mathematics topics ranged from fractions to integers to geometry. As in the literature experiments, students were in their normal school environment following their regular curriculum and were unaware that their learning was being observed. Guided Cognition design was found to be effective for learning mathematics. Working story problems that were enriched with cognitive events such as role play, divergent thinking, visualizing and illustrating, and relating to prior experience raised scores on unexpected quizzes by about a letter grade. Another unexpected quiz found that the improvements in problem-solving performance persisted for 6months. Guided Cognition homework was also found to be efficient in that students who worked eight problems and then performed four cognitive events performed as well on unexpected quizzes as students who worked 24 problems in the same time interval. Another pair of experiments determined that modest gains could be made from merely reading completed examples of cognitive events, but that these gains were not long-lasting. Performing the cognitive events was found to be most effective for long-term performance. Another experiment found that experiencing cognitive events after working some mathematics problems can help consolidate knowledge of how to work such problems.}
}
@article{BRANDT20051578,
title = {Mental spaces and cognitive semantics: A critical comment},
journal = {Journal of Pragmatics},
volume = {37},
number = {10},
pages = {1578-1594},
year = {2005},
note = {Conceptual Blending Theory},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2004.10.019},
url = {https://www.sciencedirect.com/science/article/pii/S0378216605000603},
author = {Per Aage Brandt},
keywords = {Mental Space Theory, Truth conditions, Spinoza, Semantic domains, Mental architecture, Material anchors},
abstract = {The article criticizes the negative influence of modern analytic, anti-semantic and anti-phenomenological thinking on cognitive semantics, and the errors or weaknesses of analysis it induces in current Mental Space Theory (MST). It also shows how a less inhibited theory of meaning, mental spaces and blending could develop more useful analyses of empirical occurrences, such as the artifacts called ‘material anchors’ and works of art — here exemplified by a painting by Matisse.}
}
@article{WOLLMANN2019278,
title = {Proposal for a model to hierarchize strategic decisions according to criteria of value innovation, sustainability and budgetary constraint},
journal = {Journal of Cleaner Production},
volume = {231},
pages = {278-289},
year = {2019},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2019.05.190},
url = {https://www.sciencedirect.com/science/article/pii/S095965261931724X},
author = {Dewey Wollmann and Ubiratã Tortato},
keywords = {Complex adaptive systems, Analytic network process, BOCR analysis, Linear programming},
abstract = {Organizations need management models, which will enable their executives to develop systemic thinking. In addition, executives should keep in mind that: some decision-making processes should be shared; impose political influence according to their preferences; value innovation strategies may be present; it is essential to consider the environmental, economic and social dimensions of sustainability. In this context, this study describes a model to hierarchize strategic decisions according to criteria innovation value, sustainability and budgetary constraint, developed according to the methodology proposed by Mitroff et al. (1974). In addition to hierarchizing, the model allows identifying the degree of importance of each of the strategic decisions in the performance indicators defined as evaluation criteria and sub-criteria. In the conceptualization phase, the model is influenced by concepts that describe complex adaptive systems. Next, the Analytic Network Process with Benefits, Opportunities, Costs and Risks Analysis and Linear Programming techniques are used in order to define the mathematical structure that operationalizes the model. The use of a hypothetical example demonstrates the capacity of the model proposed in this work to support the decision-making process of an organization in the selection of its decision alternatives. Thus, the model can help the academic and business communities concerned with the progress of sustainable societies, insofar as it subsidizes decision-making for the development and implementation of new products and processes related to cleaner production.}
}
@article{FEIZIZADEH2024103764,
title = {Spatiotemporal mapping of urban trade and shopping patterns: A geospatial big data approach},
journal = {International Journal of Applied Earth Observation and Geoinformation},
volume = {128},
pages = {103764},
year = {2024},
issn = {1569-8432},
doi = {https://doi.org/10.1016/j.jag.2024.103764},
url = {https://www.sciencedirect.com/science/article/pii/S1569843224001183},
author = {Bakhtiar Feizizadeh and Davoud Omarzadeh and Thomas Blaschke},
keywords = {Shopping pattern mapping, GIS, Geospatial big data, Data-driven approaches, Spatially explicit, GIScience, Novel methodology},
abstract = {The economic viability of an urban area in terms of trade and shopping significantly impacts its residents’ quality of life and is crucial for any sustainable development initiative. Geographic information systems (GIS) are well established, but the use of GIS technology within finance and trade analysis is still in its infancy. In this article, we highlighted the potential of GIS technology and big data analytics and demonstrated the importance of thinking in spatial terms for analysing patterns within the trade and finance industries. We studied spatiotemporal trade and shopping patterns in the city of Tabriz using data generated by customer purchase transactions obtained from 5200 stores, shopping, business and service centres. We employed time series transaction data collected from the points of sale in stores, shopping, service and business centres located in different areas of the city. We applied four well known geospatial big data driven approaches including machine learning nearest neighbour, kernel density estimation, space–time pattern mining and spatiotemporal coupling tele-coupling for detecting and mapping of spatial trade hotspot patterns. The results of this study indicated the potential of GIScience methods for the explicit spatial mapping of trade and shopping patterns. The results revealed that the city centre, particularly the Bazaar of Tabriz, acts as the city’s heart of trade, and we identify additional major business hotspots. Furthermore, the results allow for studying the impacts of unbalanced urban development in Tabriz, where the wealthy suburbs with high quality of life, such as Valiasr and Elguli, host the major shopping hotspots. The spatial patterns obtained enable local stakeholders, decision makers and authorities to develop strategic plans for urban sustainable development in Tabriz. The geospatial big data approach used can stimulate novel and progressive research. Results of this study demonstrate methodological advancements in GIScience by ’spatializing’ individual purchase data and therefor proposing an explicit geospatial big data analysis approach.}
}
@article{OSORIOMORA2025837,
title = {A risk-averse latency location-routing problem with stochastic travel times},
journal = {European Journal of Operational Research},
volume = {321},
number = {3},
pages = {837-850},
year = {2025},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2024.10.041},
url = {https://www.sciencedirect.com/science/article/pii/S0377221724008440},
author = {Alan Osorio-Mora and Francisco Saldanha-da-Gama and Paolo Toth},
keywords = {Routing, Cumulative routing, Sampling, Variable neighborhood search},
abstract = {In this paper, a latency location-routing problem with stochastic travel times is investigated. The problem is cast as a two-stage stochastic program. The ex-ante decision comprises the location of the depots. The ex-post decision regards the routing, which adapts to the observed travel times. A risk-averse decision-maker is assumed, which is conveyed by adopting the latency CVaRα as the objective function. The problem is formulated mathematically. An efficient multi-start variable neighborhood search algorithm is proposed for tackling the problem when uncertainty is captured by a finite set of scenarios. This procedure is then embedded into a sampling mechanism so that realistic instances of the problem can be tackled, namely when the travel times are represented by random vectors with an infinite support. An extensive computational analysis is conducted to assess the methodological developments proposed and the relevance of capturing uncertainty in the problem. Additional insights include the impact of the risk level in the solutions.}
}
@article{LITT1993459,
title = {Single neuron computation: T. McKenna, J. Davis and S.F. Zornetzer (Eds.) (Academic Press, San Diego, CA, 1992, 664 p., Price US $59.95)},
journal = {Electroencephalography and Clinical Neurophysiology},
volume = {87},
number = {6},
pages = {459-460},
year = {1993},
issn = {0013-4694},
doi = {https://doi.org/10.1016/0013-4694(93)90160-W},
url = {https://www.sciencedirect.com/science/article/pii/001346949390160W},
author = {Brian Litt}
}
@article{XUE2024108224,
title = {Interaction dynamics of social support expressions predict future support-seeking behaviors in online support groups},
journal = {Computers in Human Behavior},
volume = {156},
pages = {108224},
year = {2024},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2024.108224},
url = {https://www.sciencedirect.com/science/article/pii/S074756322400092X},
author = {Haoning Xue and Wang Liao and Jingwen Zhang},
keywords = {Compensation, Computational methods, Interaction dynamics, Online support groups, Reciprocity, Support-seeking},
abstract = {Maintaining the sustainability of online support groups (OSGs) presents a significant challenge. Integrating the literature on interaction dynamics and supportive communication, this study investigated how interaction dynamics in supportive communication foster long-term support-seeking behaviors that are crucial to sustaining continuous support exchanges in OSGs. Using a large-scale dataset of 48,868 posts and 468,243 comments over ten years from an OSG, this study examined how reciprocity and compensation of emotional and informational support, signaled by emotional expressions and analytical expressions, predicted a poster's future support-seeking behaviors in the OSG. Results showed that a poster's future support-seeking behaviors were positively associated with receiving (a) reciprocity of analytical expressions and (b) compensation of negative emotional expressions with positive emotional expressions in their past posts. However, reciprocity of negative emotional expressions was negatively associated with a poster's future support-seeking behaviors. This study emphasizes social support as an ongoing interactive process and its importance in motivating support-seeking behaviors and fostering a thriving OSG.}
}
@article{MOAVENI2018452,
title = {Modified Hankel Interaction Index Array for Input-Output Pairing with Improved Characteristics},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {18},
pages = {452-457},
year = {2018},
note = {10th IFAC Symposium on Advanced Control of Chemical Processes ADCHEM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.09.342},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318320251},
author = {Bijan Moaveni and Wolfgang Birk},
keywords = {Control configuration selection, Interaction measure, Hankel Interaction Index Array, System Gramians},
abstract = {In this study, a modified version of Hankel Interaction Index Array (HIIA) for control configuration selection is presented which can overcome some of its shortcomings, like e.g. scaling dependency, or not relating to closed loop system properties. Inspired by the relative gain array approach, the HIIA is reformulated in the relative gain thinking by considering the effect of closing loops. The ratio of the Hankel norm of the subsystems in closed and open loop are used to state a modified version of HIIA, which has improved characteristics compared to the original HIIA. Properties of the modified HIIA are discussed and benchmarked with established methods on three example cases.}
}
@article{TONNANG2022100964,
title = {Advances in data-collection tools and analytics for crop pest and disease management},
journal = {Current Opinion in Insect Science},
volume = {54},
pages = {100964},
year = {2022},
issn = {2214-5745},
doi = {https://doi.org/10.1016/j.cois.2022.100964},
url = {https://www.sciencedirect.com/science/article/pii/S2214574522000992},
author = {Henri EZ Tonnang and Daisy Salifu and Bester T Mudereri and Joel Tanui and Andrew Espira and Thomas Dubois and Elfatih M Abdel-Rahman},
abstract = {Innovative methods in data collection and analytics for pest and disease management are advancing together with computational efficiency. Tools, such as the open-data kit, research electronic data capture, fall armyworm monitoring, and early warning- system application and remote sensing have aided the efficiency of all types of data collection, including text, location, images, audio, video, and others. Concurrently, data analytics have also evolved with the application of artificial intelligence and machine learning (ML) for early warning and decision-support systems. ML has repeatedly been used for the detection, diagnosis, modeling, and prediction of crop pests and diseases. This paper thus highlights the innovations, implications, and future progression of these technologies for sustainability.}
}
@article{PHONAPICHAT20143169,
title = {An Analysis of Elementary School Students’ Difficulties in Mathematical Problem Solving},
journal = {Procedia - Social and Behavioral Sciences},
volume = {116},
pages = {3169-3174},
year = {2014},
note = {5th World Conference on Educational Sciences},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2014.01.728},
url = {https://www.sciencedirect.com/science/article/pii/S1877042814007459},
author = {Prathana Phonapichat and Suwimon Wongwanich and Siridej Sujiva},
keywords = {Mathematical problem solving, mathematical difficulties, mathematical skills, elementary school students},
abstract = {The main purpose of mathematics teaching is to enable students to solve problems in daily life. Unfortunately, according to the latest national test results, most students lack mathematical problem solving skills. This proves to be one of the reasons why overall achievement in mathematics is considered quite low. It also reflects that students have difficulties in comprehending mathematical problems affecting the process of problem-solving. Therefore, in order to allow teachers to establish a proper teaching plan suitable for students’ learning process, this research aims to analyze the difficulties in mathematical problem solving among elementary school students. Samples are divided into two groups, elementary school students and mathematics teachers. Data collection was conducted by structured interview, documentary analysis, and survey tests. Data analysis was conducted by descriptive statistics, and content analysis. The results suggest that there are several difficulties in problem solving, namely 1) Students have difficulties in understanding the keywords appearing in problems, thus cannot interpret them in mathematical sentences. 2) Students are unable to figure out what to assume and what information from the problem is necessary to solving it, 3) Whenever students do not understand the problem, they tend to guess the answer without any thinking process, 4) Students are impatient and do not like to read mathematical problems, and 5) Students do not like to read long problems. Therefore, the results found in this research will lead to the creation and the development of mathematical problem solving diagnostic tests for teachers, in order to improve students’ mathematical problem solving skills.}
}
@article{CHERRIER2023104497,
title = {Household heterogeneity in macroeconomic models: A historical perspective},
journal = {European Economic Review},
volume = {158},
pages = {104497},
year = {2023},
issn = {0014-2921},
doi = {https://doi.org/10.1016/j.euroecorev.2023.104497},
url = {https://www.sciencedirect.com/science/article/pii/S0014292123001265},
author = {Beatrice Cherrier and Pedro Garcia Duarte and Aurélien Saïdi},
keywords = {History of macroeconomics, Heterogeneous agents, Bewley models, Permanent income hypothesis, Aggregation, Equity premium puzzle, Precautionary savings},
abstract = {In this paper, we trace the rise of heterogeneous household models in mainstream macroeconomics from the turn of the 1980s to the early 2000s, when these models evolved into an identifiable and consistent literature. We show that different communities across the US and Europe considered heterogeneous agents for various reasons and developed models that differed in their theoretical and empirical strategies. Minnesota economists primarily focused on incorporating stochastic heterogeneity into general equilibrium models. Other researchers refined growth models or tried to find alternatives to the permanent income hypothesis, leading them to explore more structural heterogeneity. We also document the computational challenges that some of these communities faced, how they gradually became aware of each other's work, and how they faced criticisms from macro- and microeconomists, many of them trained in European countries and dissatisfied with the theoretical and empirical aggregation strategies underlying these models.}
}
@article{WANG2023120782,
title = {A closed-loop analysis approach for ensuring stormwater source control design solution to achieve the intended goals},
journal = {Water Research},
volume = {247},
pages = {120782},
year = {2023},
issn = {0043-1354},
doi = {https://doi.org/10.1016/j.watres.2023.120782},
url = {https://www.sciencedirect.com/science/article/pii/S0043135423012228},
author = {Sheng Wang and Lidan Feng and Yezi Yuan},
keywords = {Sponge city, Stormwater source control, Closed-loop analysis, Bioretention, Runoff frequency spectrum},
abstract = {Stormwater source controls have been adopted worldwide to address hydrological and environmental impairments caused by the spread of impervious surfaces in cities. Current design method in China uses 30-year daily rainfall records to generate relationship of rainfall volume capture ratio (αg) and daily design storm, and then uses design storm to propose design solution. However, source control performance differs from rain to rain, and hence the design solution's actual effect may deviate from αg. Borrowing closed-loop feedback concept from business domain, this study proposes closed-loop analysis (CLA) which uses design solution's 30-year simulated result as data feedback to check design solution's effectiveness and then make improvements if necessary. It consists of four methods: 1) hourly design storm statistical method, for addressing the weakness of current daily design storm; 2) design solution model credibility examination method, for guaranteeing credibility of 30-year simulated results for CLA; 3) appropriate design storms determination method for source control without underdrain; 4) additional design parameters optimization method for source control with underdrain. Taking Xiamen city for example, case study results shows that design solution's 30-year simulated results were consistent/comparable with sizing calculation formula that was used to propose design solution, and therefore they were credible for CLA. Appropriate design storms ensured design solutions without underdrain to achieve the intended αg±3 %. Optimal design parameters combinations ensured design solutions with underdrain to achieve αg but also restore natural runoff events with pre- and post-development runoff frequency spectra similarity being 0.670–0.691. Based on stormwater mathematical model, CLA can drive source control design computation to a new methodological stage.}
}
@incollection{DORFMAN1998395,
title = {Chapter 8 Problem solving, inhibition, and frontal lobe function},
editor = {Naftali Raz},
series = {Advances in Psychology},
publisher = {North-Holland},
volume = {125},
pages = {395-448},
year = {1998},
booktitle = {The Other Side of the Error Term},
issn = {0166-4115},
doi = {https://doi.org/10.1016/S0166-4115(98)80010-1},
url = {https://www.sciencedirect.com/science/article/pii/S0166411598800101},
author = {Jennifer Dorfman},
abstract = {Traditionally, cognitive models of problem solving have not incorporated inhibitory mechanisms, conceiving of human thinking as similar to the computations carried out by a serial computer (e.g., Newell & Simon, 1972). This chapter seeks to demonstrate the importance of inhibition in problem solving by examining subject populations with selective impairments of cognitive inhibition associated with frontal lobe pathology. Studies of three groups with putative frontal lobe dysfunction are reviewed: patients with focal lesions of the prefrontal cortex; schizophrenics; and the normal elderly. It is argued that the basic deficits observed in these groups reflect breakdowns in a supervisory attentional system that modulates problem-solving activity and that is subserved by the frontal cortex (Shallice, 1982). It is concluded that it is time to abandon the computer metaphor of human problem solving and adopt a brain metaphor.}
}
@article{FU2013729,
title = {Expert representation of design repository space: A comparison to and validation of algorithmic output},
journal = {Design Studies},
volume = {34},
number = {6},
pages = {729-762},
year = {2013},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2013.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X13000495},
author = {Katherine Fu and Joel Chan and Christian Schunn and Jonathan Cagan and Kenneth Kotovsky},
keywords = {computer supported design, design by analogy, design methods, engineering design},
abstract = {Development of design-by-analogy tools is a promising design innovation research avenue. Previously, a method for computationally structuring patent databases as a basis for an automated design-by-analogy tool was introduced. To demonstrate its strengths and weaknesses, a computationally-generated structure is compared to four expert designers' mental models of the domain. Results indicate that, compared to experts, the computationally-generated structure is sensible in clustering of patents and organization of clusters. The computationally-generated structure represents a space in which experts can find common ground/consensus – making it promising to be intuitive/accessible to broad cohorts of designers. The computational method offers a resource-efficient way of usefully conceptualizing the space that is sensible to expert designers, while maintaining an element of unexpected representation of the space.}
}
@article{SIGALA2018151,
title = {New technologies in tourism: From multi-disciplinary to anti-disciplinary advances and trajectories},
journal = {Tourism Management Perspectives},
volume = {25},
pages = {151-155},
year = {2018},
issn = {2211-9736},
doi = {https://doi.org/10.1016/j.tmp.2017.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S2211973617301435},
author = {Marianna Sigala},
abstract = {Technologies transform tourism management and marketing from a static and utilitarian sense (whereby managers and tourists use technologies as tools) to a transformative conceptualization whereby tourism markets and actors both shape and are shaped by technology. This paper unravels the transformative power of technologies on: the tourism actors and resources (both the traditional but also new actors, i.e. the technology agents); the ways actors interact to (co-)create but also (co-)destruct tourism value; and the context in which tourism actors interact from a linear supply chain tourism ‘industry’ to a complex socio-technical smart tourism ecosystem. To study such complex phenomena and transformations, the paper emphasises that research should not only adopt a multi-disciplinary approach, but it also needs to follow an anti-disciplinary thinking whereby new knowledge and constructs do not simply fall within existing paradigms, disciplinary silos and mindsets once developed by studying the ‘pure’ humans and their behaviours.}
}
@article{WILSON1997575,
title = {Computation and controversy: Value conflicts and social choices: R. KLING (Ed.) 2nd ed. Academic Press, New York (1996). xxiv + 961 pp., ISBN 0-12-415040-3},
journal = {Information Processing & Management},
volume = {33},
number = {4},
pages = {575-577},
year = {1997},
issn = {0306-4573},
doi = {https://doi.org/10.1016/S0306-4573(97)82727-6},
url = {https://www.sciencedirect.com/science/article/pii/S0306457397827276},
author = {Tom Wilson}
}
@incollection{KAKKAR2025215,
title = {Chapter 11 - A neuroinspired journey: Tracing the evolution and objectives of neuromorphic systems},
editor = {Harish Garg and Jyotir {Moy Chatterjee} and R. Sujatha and Shatrughan Modi},
booktitle = {Primer to Neuromorphic Computing},
publisher = {Academic Press},
pages = {215-238},
year = {2025},
isbn = {978-0-443-21480-6},
doi = {https://doi.org/10.1016/B978-0-443-21480-6.00009-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443214806000092},
author = {Mohit Kumar Kakkar},
keywords = {Neuromorphic computing, Von Neumann Model, Neural network architecture, Artificial neural networks, Neuromorphic framework},
abstract = {A subfield of computing known as “neuromorphic computing” (NC) develops cutting-edge and effective computing platforms by drawing inspiration from the anatomy and physiology of the human brain. The potential for energy savings offered by NC is one of its primary benefits. Neuromorphic systems aim to replicate the brain's remarkable energy efficiency by utilizing specialized hardware designs and algorithms that optimize computation and minimize data movement to reduce power consumption. This work presents a thorough investigation of the development and goals of neuromorphic frameworks, taking motivation from the complicated functions of the human mind. It investigates the motives and driving forces behind the pursuit of NC, such as the search for highly parallel and energy-efficient computing architectures. In addition, this work has discussed the thematic areas and benchmarks for progress in NC as well. This chapter serves as a guide through the neuroinspired journey of neuromorphic systems, revealing insight into their past, present, and future.}
}
@article{MOGILNER2019R915,
title = {Alex Mogilner},
journal = {Current Biology},
volume = {29},
number = {19},
pages = {R915-R917},
year = {2019},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2019.07.077},
url = {https://www.sciencedirect.com/science/article/pii/S0960982219309571},
author = {Alex Mogilner}
}