@article{KRINGELBACH2020108128,
title = {Brain States and Transitions: Insights from Computational Neuroscience},
journal = {Cell Reports},
volume = {32},
number = {10},
pages = {108128},
year = {2020},
issn = {2211-1247},
doi = {https://doi.org/10.1016/j.celrep.2020.108128},
url = {https://www.sciencedirect.com/science/article/pii/S2211124720311177},
author = {Morten L. Kringelbach and Gustavo Deco},
abstract = {Summary
Within the field of computational neuroscience there are great expectations of finding new ways to rebalance the complex dynamic system of the human brain through controlled pharmacological or electromagnetic perturbation. Yet many obstacles remain between the ability to accurately predict how and where best to perturb to force a transition from one brain state to another. The foremost challenge is a commonly agreed definition of a given brain state. Recent progress in computational neuroscience has made it possible to robustly define brain states and force transitions between them. Here, we review the state of the art and propose a framework for determining the functional hierarchical organization describing any given brain state. We describe the latest advances in creating sophisticated whole-brain computational models with interacting neuronal and neurotransmitter systems that can be studied fully in silico to predict and design novel pharmacological and electromagnetic interventions to rebalance them in disease.}
}
@article{SUI2022377,
title = {Data-driven based four examinations in TCM: a survey},
journal = {Digital Chinese Medicine},
volume = {5},
number = {4},
pages = {377-385},
year = {2022},
issn = {2589-3777},
doi = {https://doi.org/10.1016/j.dcmed.2022.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S258937772200074X},
author = {Dong SUI and Lei ZHANG and Fei YANG},
keywords = {Traditional Chinese medicine (TCM), Four examinations, Data-driven, Machine learning, Computational intelligence},
abstract = {Traditional Chinese medicine (TCM) diagnosis is a unique disease diagnosis method with thousands of years of TCM theory and effective experience. Its thinking mode in the process is different from that of modern medicine, which includes the essence of TCM theory. From the perspective of clinical application, the four diagnostic methods of TCM, including inspection, auscultation and olfaction, inquiry, and palpation, have been widely accepted by TCM practitioners worldwide. With the rise of artificial intelligence (AI) over the past decades, AI based TCM diagnosis has also grown rapidly, marked by the emerging of a large number of data-driven deep learning models. In this paper, our aim is to simply but systematically review the development of the data-driven technologies applied to the four diagnostic approaches, i.e. the four examinations, in TCM, including data sets, digital signal acquisition devices, and learning based computational algorithms, to better analyze the development of AI-based TCM diagnosis, and provide references for new research and its applications in TCM settings in the future.}
}
@article{CAMERON2017131,
title = {Lateral thinking – Interocular symmetry and asymmetry in neurovascular patterning, in health and disease},
journal = {Progress in Retinal and Eye Research},
volume = {59},
pages = {131-157},
year = {2017},
issn = {1350-9462},
doi = {https://doi.org/10.1016/j.preteyeres.2017.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S135094621630091X},
author = {James R. Cameron and Roly D. Megaw and Andrew J. Tatham and Sarah McGrory and Thomas J. MacGillivray and Fergus N. Doubal and Joanna M. Wardlaw and Emanuele Trucco and Siddharthan Chandran and Baljean Dhillon},
keywords = {Interocular symmetry, Asymmetry, Retina, Retinal imaging, Retinal vasculature, Patterning},
abstract = {No biological system or structure is likely to be perfectly symmetrical, or have identical right and left forms. This review explores the evidence for eye and visual pathway asymmetry, in health and in disease, and attempts to provide guidance for those studying the structure and function of the visual system, where recognition of symmetry or asymmetry may be essential. The principal question with regards to asymmetry is not ‘are the eyes the same?’, for some degree of asymmetry is pervasive, but ‘when are they importantly different?’. Knowing if right and left eyes are ‘importantly different’ could have significant consequences for deciding whether right or left eyes are included in an analysis or for examining the association between a phenotype and ocular parameter. The presence of significant asymmetry would also have important implications for the design of normative databases of retinal and optic nerve metrics. In this review, we highlight not only the universal presence of asymmetry, but provide evidence that some elements of the visual system are inherently more asymmetric than others, pointing to the need for improved normative data to explain sources of asymmetry and their impact on determining associations with genetic, environmental or health-related factors and ultimately in clinical practice.}
}
@article{SCOTT2020107269,
title = {CPC’s 50th Anniversary: Celebrating 50 years of open-source software in computational physics},
journal = {Computer Physics Communications},
volume = {252},
pages = {107269},
year = {2020},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2020.107269},
url = {https://www.sciencedirect.com/science/article/pii/S0010465520300886},
author = {N.S. Scott and A. Hibbert and J. Ballantyne and S. Fritzsche and A.L. Hazel and D.P. Landau and D.W. Walker and Z. Was},
keywords = {Computer Physics Communications, CPC Program Library, Collaborative Computational Project, Mendeley Data repository, Platform for Advanced Scientific Computing, Code Ocean},
abstract = {To celebrate the leading role Computer Physics Communications (CPC) has played in publishing open-source software in computational physics for over 50 years the editors are delighted to announce this Virtual Special Issue. Since 2018, coinciding with the 50th anniversary of the start of the CPC venture, thirty-two invited articles have been published. Each has been peer reviewed and each bears the header ‘CPC 50th anniversary article’. The special issue is in keeping with CPC’s ethos: it is focused on computational physics software and is accompanied by twenty-five software systems. The introduction to the collection also includes a personal reflection on Phil Burke, CPC’s founder, by Alan Hibbert, a lifelong colleague, who joined Queen’s University with Phil in the autumn of 1967. The distinctive feature of CPC is its Program Library which houses and distributes over 3500 open-source programs in computational physics. The introduction concludes with a description of key events in the history of the Program Library, its association with Queen’s University Belfast and its transfer to Elsevier’s Mendeley Data repository.}
}
@article{TAUB201510,
title = {The effect of computer science on physics learning in a computational science environment},
journal = {Computers & Education},
volume = {87},
pages = {10-23},
year = {2015},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2015.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S0360131515000913},
author = {Rivka Taub and Michal Armoni and Esther Bagno and Mordechai (Moti) Ben-Ari},
keywords = {Interdisciplinary projects, Programming and programming languages, Secondary education, Simulations, Teaching/learning strategies},
abstract = {College and high-school students face many difficulties when dealing with physics formulas, such as a lack of understanding of their components or of the physical relationships between the two sides of a formula. To overcome these difficulties some instructors suggest combining simulations' design while learning physics, claiming that the programming process forces the students to understand the physical mechanism activating the simulation. This study took place in a computational-science course where high-school students programmed simulations of physical systems, thus combining computer science (CS) and mathematics with physics learning. The study explored the ways in which CS affected the students' conceptual understanding of the physics behind formulas. The major part of the analysis process was qualitative, although some quantitative analysis was applied as well. Findings revealed that a great amount of the time was invested by the students on representing their physics knowledge in terms of computer science. Three knowledge domains were found to be applied: structural, procedural and systemic. A fourth domain which enabled reflection on the knowledge was found as well, the domain of execution. Each of the domains was found to promote the emergence of knowledge integration processes (Linn & Eylon, 2006, 2011), thus promoting students’ physics conceptual understanding. Based on these findings, some instructional implications are discussed.}
}
@article{DIAS2024103493,
title = {Artificial intelligence in the judiciary: A critical view},
journal = {Futures},
volume = {164},
pages = {103493},
year = {2024},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2024.103493},
url = {https://www.sciencedirect.com/science/article/pii/S0016328724001770},
author = {Stephanie Almeida de Jesus Dias and Renato Máximo Sátiro},
keywords = {Artificial Intelligence, Critical Theory, Judiciary},
abstract = {The main objective of this study is to raise questions about using artificial intelligence (AI) in the judiciary based on critical thinking. The essay approach used in this work aims to foster reflection and debate on the subject, presenting a review of theoretical perspectives with particular attention to the critical theory of the first generation of the Frankfurt School and theories that seek to analyze the relationship between humans and society/technology, such as the critical theory of technology, and stays away from the dominant currents of thinking in organizational studies to contribute unexplored perspectives. In this manner, it will go beyond existing benefits and applications, focusing on a critical view that identifies the elements that guide the technological choices that have been made, thus promoting a discussion that aggregates elements for future developments and improvements. Based on this theoretical context, this essay will raise questions and present three propositions summarizing the identified difficulties and directing future studies.}
}
@article{CHANG2014335,
title = {Computational architecture: Connecting the physical and virtual worlds},
journal = {Frontiers of Architectural Research},
volume = {3},
number = {4},
pages = {335-336},
year = {2014},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2014.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S2095263514000624},
author = {Teng-Wen Chang and Weixin Huang}
}
@incollection{COMBA2021241,
title = {2.14 - Computational Coordination Chemistry},
editor = {Edwin C. Constable and Gerard Parkin and Lawrence {Que Jr}},
booktitle = {Comprehensive Coordination Chemistry III},
publisher = {Elsevier},
address = {Oxford},
pages = {241-255},
year = {2021},
isbn = {978-0-08-102689-2},
doi = {https://doi.org/10.1016/B978-0-08-102688-5.00023-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780081026885000234},
author = {Peter Comba},
keywords = {Ab-initio quantum mechanics, Catalytic cycle, Charge distribution, Complex stability, DFT, Electronic structure, Ligand field theory, Molecular magnetism, Molecular mechanics, Molecular structure, Quantum chemistry, Reaction mechanism, Redox potential, Spectroscopy, Transition state},
abstract = {The computational modeling of metal complexes has been developed to an extent where a large variety of spectroscopic properties, reactivities and stabilities of mono- and oligonuclear complexes can be efficiently and reliably computed. There is a large arsenal of computational methods for the modeling of coordination compounds, spanning a wide range of scales in terms of theoretical basis, accuracy of the data in comparison with experiment, and accessibility in terms of computer power. Relevant current approaches and their limits and possible pitfalls that are discussed include ab-initio and DFT-based quantum-chemical, molecular mechanical, ligand-field-based methods and various combinations thereof, as well as approaches related to machine-learning, artificial intelligence, molecular docking and empirical structure-property correlations.}
}
@article{CESARI2017361,
title = {Frailty and Multimorbidity: Different Ways of Thinking About Geriatrics},
journal = {Journal of the American Medical Directors Association},
volume = {18},
number = {4},
pages = {361-364},
year = {2017},
issn = {1525-8610},
doi = {https://doi.org/10.1016/j.jamda.2016.12.086},
url = {https://www.sciencedirect.com/science/article/pii/S1525861017300348},
author = {Matteo Cesari and Mario Ulises Pérez-Zepeda and Emanuele Marzetti},
keywords = {Diseases, comprehensive geriatric assessment, aging, public health},
abstract = {The terms multimorbidity and frailty are increasingly used in the medical literature to measure the risk profile of an older individual in order to support clinical decisions and design ad hoc interventions. The construct of multimorbidity was initially developed and used in nongeriatric settings. It generates a monodimensional nosological risk profile, grounding its roots in the somewhat inadequate framework of disease. On the other hand, frailty is a geriatric concept that implies a more exhaustive and comprehensive assessment of the individual and his/her environment, facilitating the implementation of multidimensional and tailored interventions. This article aims to promote among geriatricians the use of terms that may better enhance their background and provide more value to their unrivaled expertise in caring for biologically aged persons.}
}
@article{WALSH2021143,
title = {Computational Cognitive Modeling of Human Calibration and Validity Response Scoring for the Graduate Record Examinations (GRE)},
journal = {Journal of Applied Research in Memory and Cognition},
volume = {10},
number = {1},
pages = {143-154},
year = {2021},
note = {Culture & Memory},
issn = {2211-3681},
doi = {https://doi.org/10.1016/j.jarmac.2020.08.012},
url = {https://www.sciencedirect.com/science/article/pii/S2211368120300747},
author = {Matthew M. Walsh and Burcu Arslan and Bridgid Finn},
keywords = {Constructed response scoring, Graduate record examinations, Predictive performance equation, Skill decay},
abstract = {Most research on skill acquisition and retention focuses on the individual being tested. Yet sometimes another person is responsible for evaluating the individual’s performance. Here, we study the acquisition and retention of rater skill using data for the Graduate Record Examinations (GRE). Our work is based on the idea that response scoring, like other cognitive skills, will gradually improve with amount of practice, and decline with elapsed time since that practice occurred. These classic findings are the focus of a computational cognitive model called the Predictive Performance Equation (PPE). However, the generalizability of these findings to response scoring and the applicability of PPE to that domain have not yet been demonstrated. To address this issue, we leveraged a naturalistic dataset containing rating performance from over 23,000 sessions. Our analyses provide empirical support for PPE and establish a basis for using a model like PPE to personalize rater training requirements.}
}
@article{SMITH1990121,
title = {Writing, thinking, computing},
journal = {Poetics},
volume = {19},
number = {1},
pages = {121-142},
year = {1990},
issn = {0304-422X},
doi = {https://doi.org/10.1016/0304-422X(90)90033-2},
url = {https://www.sciencedirect.com/science/article/pii/0304422X90900332},
author = {John B. Smith and Catherine F. Smith},
abstract = {The computer has become the preferred tool for many writers. Over the next few years, it is likely to become the predominant tool. Since writing is fundamentally a mediated activity and since the tool inevitably affects the tool user, we need to consider how a tool as powerful as the computer is affecting writers. To address this issue, we consider the following questions: &#x02022;- What are writers saying about computers?&#x02022;- How are writers using computers?&#x02022;- What does this mean for the teaching of writing?&#x02022;- What does this mean for designers of future writing systems?&#x02022;- How does the computer affect writer's thinking? We are led to the conclusion that new, comprehensive writing environments are both needed and inevitable, and they, in turn, will lead to a form of enhanced, or amplified, thinking. But using these environments and developing this kind of thinking will also require new forms of instruction. Adapting to these changes will pose practical as well as intellectual challenges for the composition community. To meet these challenges tomorrow, we must begin considering the relationships among writing, thinking, and computing today.}
}
@article{SRIHARI20141083,
title = {Role of automation in the examination of handwritten items},
journal = {Pattern Recognition},
volume = {47},
number = {3},
pages = {1083-1095},
year = {2014},
note = {Handwriting Recognition and other PR Applications},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2013.09.032},
url = {https://www.sciencedirect.com/science/article/pii/S0031320313004044},
author = {Sargur N. Srihari and Kirsten Singer},
keywords = {Handwriting examination, Forensic document examination, Writer verification, Writer identification, Computational forensics, Expert system validation},
abstract = {Several automation tools have been developed over the years for forensic document examination (FDE) of handwritten items. Integrating the developed tools into a unified framework is considered and the essential role of the human in the process is discussed. The task framework is developed by considering the approach of computational thinking whose components are abstraction, algorithms, mathematical models and ability to scale. Beginning with the human FDE procedure expressed in algorithmic form, mathematical and software implementations of individual steps of the algorithm are described. Advantages of the framework are discussed, including efficiency (ability to scale to tasks with many handwritten items), reproducibility and validation/improvement of existing manual procedures. It is indicated that as with other expert systems, such as for medical diagnosis, current automation tools are useful only as part of a larger manually intensive procedure. This viewpoint is illustrated with a well-known FDE case, concerning the Lindbergh kidnapping with a new hypothesis – in this case, there are multiple questioned documents, possibility of multiple writers of the same document, determining whether the writing is disguised, known writing is formal while questioned writing is informal, etc. Observations are made for future developments, where human examiners provide handwriting characteristics while computational methods provide the necessary statistical analysis.}
}
@article{FEHER201498,
title = {Computational approaches to mapping allosteric pathways},
journal = {Current Opinion in Structural Biology},
volume = {25},
pages = {98-103},
year = {2014},
note = {Theory and simulation / Macromolecular machines},
issn = {0959-440X},
doi = {https://doi.org/10.1016/j.sbi.2014.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0959440X14000190},
author = {Victoria A Feher and Jacob D Durrant and Adam T {Van Wart} and Rommie E Amaro},
abstract = {Allosteric signaling occurs when chemical and/or physical changes at an allosteric site alter the activity of a primary orthosteric site often many Ångströms distant. A number of recently developed computational techniques, including dynamical network analysis, novel topological and molecular dynamics methods, and hybrids of these methods, are useful for elucidating allosteric signaling pathways at the atomistic level. No single method prevails as best to identify allosteric signal propagation path(s), rather each has particular strengths in characterizing signals that occur over specific timescale ranges and magnitudes of conformational fluctuation. With continued improvement in accuracy and predictive power, these computational techniques aim to become useful drug discovery tools that will allow researchers to identify allostery critical residues for subsequent pharmacological targeting.}
}
@article{KAUFFMAN201525,
title = {Infinite computations and the generic finite},
journal = {Applied Mathematics and Computation},
volume = {255},
pages = {25-35},
year = {2015},
note = {Special issue devoted to the international conference ‘‘Numerical computations: Theory and Algorithms’’ June 17–23, 2013, Falerna, Italy},
issn = {0096-3003},
doi = {https://doi.org/10.1016/j.amc.2014.06.054},
url = {https://www.sciencedirect.com/science/article/pii/S009630031400890X},
author = {Louis H. Kauffman},
keywords = {Grossone, , Finite, Infinite, Generic finite, Category},
abstract = {This paper introduces the concept of a generic finite set and points out that a consistent and significant interpretation of the grossone, ① notation of Sergeyev is that ① takes the role of a generic natural number. This means that ① is not itself a natural number, yet it can be treated as one and used in the generic expression of finite sets and finite formulas, giving a new power to algebra and algorithms that embody this usage. In this view,N={1,2,3,…,①-2,①-1,①}is not an infinite set, it is a symbolic structure representing a generic finite set. We further consider the concept of infinity in categories. An object A in a given category C is infinite relative to that category if and only if there is a injection J:A⟶A in C that is not a surjection. In the category of sets this recovers the usual notion of infinity. In other categories, an object may be non-infinite (finite) while its underlying set (if it has one) is infinite. The computational methodology due to Sergeyev for executing numerical calculations with infinities and infinitesimals is considered from this categorical point of view.}
}
@incollection{LOURDUSAMY202091,
title = {7 - Computational intelligence using ontology—A case study on the knowledge representation in a clinical decision support system},
editor = {Jitendra Kumar Verma and Sudip Paul and Prashant Johri},
booktitle = {Computational Intelligence and Its Applications in Healthcare},
publisher = {Academic Press},
pages = {91-104},
year = {2020},
isbn = {978-0-12-820604-1},
doi = {https://doi.org/10.1016/B978-0-12-820604-1.00007-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128206041000078},
author = {Ravi Lourdusamy and Xavierlal J. Mattam},
keywords = {Clinical decision support systems, Knowledge representation, Computational semantics, Ontology, Ontological engineering},
abstract = {Computational intelligence has been traditionally associated with neural networks, fuzzy systems, and genetic algorithms. Over the years there have been many developments in computational intelligence. At present, many other fields are part of the study and research in computational intelligence. With advances in cognitive sciences, more techniques of information processing by machines that show characteristics closely associated with human intelligence are being found. Some of these techniques have been studied for a long time, but in recent years there has been some maturity in the understanding and use of these techniques. One such technique is the use of semantics in computational intelligence. There has been a long-drawn-out philosophical debate between lingualism, which claims that there is no human thought without language, and “language of thought” theories, which believe that natural language is inessential to private thought. In an attempt to create intelligent machines, the use of semantics for knowledge representation and knowledge-based creation in a system follows the philosophy of lingualism. Different knowledge representations are used in a knowledge-based clinical decision support system. This chapter makes a study of various knowledge representations. The different theories behind the techniques used in the knowledge representations are discussed. The philosophy of lingualism and the use of semantics in computational intelligence are explained, while a study on semantic knowledge representation in clinical decision support systems is made. The conclusion is the explanation as to how ontological engineering can be used to create computational intelligence.}
}
@incollection{BYRNE1997105,
title = {Cognitive Processes in Counterfactual Thinking about what Might Have Been},
editor = {Douglas L. Medin},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {37},
pages = {105-154},
year = {1997},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(08)60501-0},
url = {https://www.sciencedirect.com/science/article/pii/S0079742108605010},
author = {Ruth M.J. Byrne}
}
@article{NITYANANDA2020R159,
title = {Insect Neurobiology: Divergent Neural Computations in Predatory Insects},
journal = {Current Biology},
volume = {30},
number = {4},
pages = {R159-R161},
year = {2020},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2019.12.035},
url = {https://www.sciencedirect.com/science/article/pii/S0960982219316689},
author = {Vivek Nityananda},
abstract = {Summary
A comparative approach to neuroscience can greatly increase our understanding of how mechanisms map onto behaviour. A new study comparing two predatory insects demonstrates how neurons that are homologous can nonetheless mediate different computations and behaviour.}
}
@article{CHI20111937,
title = {Teaching Computing to STEM Students via Visualization Tools},
journal = {Procedia Computer Science},
volume = {4},
pages = {1937-1943},
year = {2011},
note = {Proceedings of the International Conference on Computational Science, ICCS 2011},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2011.04.211},
url = {https://www.sciencedirect.com/science/article/pii/S1877050911002699},
author = {Hongmei Chi and Harsh Jain},
keywords = {Visualization, ChemSketch, ParaView, Computation ;STEM education},
abstract = {Information technology is evolving fast and steady over the years providing more and more tools for society to use. There is an increasing need and implementation of computation in the conduct of modern scientific research and experimentation. Computational thinking has been scarcely understood by STEM undergraduates if their majors are not computer sciences. We explore computation projects into existing courses via visualization computational tools to increase the number of STEM students who graduate with discipline specific computational skills. The goal of this paper was to report our efforts for increasing the number of students with experience using computation in science. Discipline specific tools were chosen and implemented in the respective courses, for example Chemsketch in chemistry. Hands-on labs were designed to familiarize instructors and students so it can be helpful to smooth the learning curve in STEM undergraduate students}
}
@article{SAID2015396,
title = {Exploiting Computational Intelligence Paradigms in e-Technologies and Activities},
journal = {Procedia Computer Science},
volume = {65},
pages = {396-405},
year = {2015},
note = {International Conference on Communications, management, and Information technology (ICCMIT'2015)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.09.101},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915029312},
author = {Hanaa M. Said and Abdel-Badeeh M. Salem},
keywords = {Machine Learning, Intelligent Data Analysis, E- Technologies government, Neural Networks, Fuzzy Logic, Genetic algorithm, Case based reasoning, SVM, Swarm intelligence, computational intelligence;},
abstract = {Computational intelligence (CI) has emerged as a powerful paradigm in e-Science, providing the researchers an immense volume of intelligent computing techniques and algorithms. CI provides knowledge engineers to develop a robust techniques and intelligent tools for e-government applications and tasks. This paper presents a comparative analysis of some techniques used in e-activities and e-government systems. The study includes the following paradigms; artificial neural networks, fuzzy logic, genetic algorithms, case-based reasoning, support vector machines, and swarm intelligence. Additionally, this study found that such paradigms offer many business benefits and advantages; e.g. (a) the ability to acquire, represent, manage and structure the knowledge in the domain under study, (b) the ability to optimize resources, (c) the ability to perform efficient performance, and (d) the ability to conduct planning, budgeting, and forecasting.}
}
@article{BICER2024101462,
title = {Mathematical creativity in upper elementary school mathematics curricula},
journal = {Thinking Skills and Creativity},
volume = {51},
pages = {101462},
year = {2024},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2024.101462},
url = {https://www.sciencedirect.com/science/article/pii/S1871187124000014},
author = {Ali Bicer and Helen Aleksani and Chuck Butler and Traci Jackson and Tricia Dawn Smith and Michael Bostick},
keywords = {Creativity in mathematics, Creativity-directed tasks, Curriculum},
abstract = {Textbooks should promote creative tasks for teachers to enhance students' math skills through creative thinking, instead of relying on memorization of step-by-step procedures. The aim of this study is to analyze elementary school mathematics curricula commonly employed in the U.S., namely GoMath, enVision Math, Math Connects, MyMath, and Investigations. The selection of these curricula is based on their widespread usage, and the analysis seeks to evaluate their effectiveness in fostering the development of students' creative thinking skills. We employed Bicer et al.'s (2021) framework for creativity-directed tasks to analyze 1,000 mathematical tasks within each curriculum. The analysis unveiled that Eureka, followed by Investigations, incorporated a higher proportion of creativity-directed tasks compared to the remaining three curricula. For some categories and subcategories of creativity-directed tasks (e.g., communication, connection), the results are less varied across five curricula. The present study enables school districts, schools, and classroom teachers to know what curricula support the development of creative thinking of students by including various options of creativity-directed tasks in their upper elementary mathematics textbooks.}
}
@article{WINITZKY19921,
title = {Structure and process in thinking about classroom management: An exploratory study of prospective teachers},
journal = {Teaching and Teacher Education},
volume = {8},
number = {1},
pages = {1-14},
year = {1992},
issn = {0742-051X},
doi = {https://doi.org/10.1016/0742-051X(92)90036-3},
url = {https://www.sciencedirect.com/science/article/pii/0742051X92900363},
author = {Nancy Winitzky},
abstract = {Current research on teaching centers on teachers' thinking. What teachers know and, especially, how they reflect on their practice are considered central to understanding teaching. Part of the research on teachers' knowledge concerns cognitive structure (schemata), how teachers organize their knowledge; it is thought that complex, highly structured schemata are related to skillful teaching performance. While it is sensible to assume that schemata and reflection, structure and process, are linked, no data exist to support that assumption. In the present study, cognitive structure and reflection data were collected from 15 prospective teachers. The strength of the correlation between those variables was .48 (p = .05). Implications for theory, research, and practice are discussed.}
}
@article{SOSA2018157,
title = {Innovation Teams and Organizational Creativity: Reasoning with Computational Simulations},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {4},
number = {2},
pages = {157-170},
year = {2018},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2018.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S240587261730076X},
author = {Ricardo Sosa and Andy Connor},
keywords = {Organizational climate, Agent-based simulation, Creative teams, Leadership},
abstract = {A computational social simulation encourages systematic reasoning about the management of innovation teams and organizational creativity. This article draws upon historical literature to identify a potential dilemma faced by business organizations: Is it better to promote creative behavior across a whole organization or focus on the development of small and highly creative teams? We formulate the dilemma from the literature on organizational creativity, and explore it using a multi-agent simulation. Our study models creative behavior abstractly, as the ability to introduce novelty. By varying the scale and scope of non-conformist behavior in the simulation, our research supports the systematic study of the breadth vs. depth dilemma. The results of this study invite an informed examination of strategies to sustain innovation based on the introduction of either a small number of significantly novel ideas, or a large number of novel but more familiar ideas. Results from this study on change agency also indicate that there is a possible trade-off between a highly creative team and its creative efficiency, drawing attention to the importance of a creative critical mass in an organization. We also discuss the implications of these results and our research approach.}
}
@article{GJORGJIEVA2021iii,
title = {Editorial overview: Theoretical and computational approaches to decipher brain function from molecules to behavior},
journal = {Current Opinion in Neurobiology},
volume = {70},
pages = {iii-vii},
year = {2021},
note = {Computational Neuroscience},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2021.11.010},
url = {https://www.sciencedirect.com/science/article/pii/S0959438821001379},
author = {Julijana Gjorgjieva and Ila Fiete}
}
@article{LEE20152858,
title = {The Benin experience: How computational modeling can assist major vaccine policy changes in low and middle income countries},
journal = {Vaccine},
volume = {33},
number = {25},
pages = {2858-2861},
year = {2015},
issn = {0264-410X},
doi = {https://doi.org/10.1016/j.vaccine.2015.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S0264410X15004752},
author = {Bruce Y. Lee and Benjamin Schreiber and Angela R. Wateska and Diana L. Connor and Hamadou M. Dicko and Philippe Jaillard and Mercy Mvundura and Carol Levin and Mélanie Avella and Leila A. Haidari and Shawn T. Brown},
keywords = {Benin, Vaccine, Supply chain, Computational modeling},
abstract = {While scientific studies can show the need for vaccine policy or operations changes, translating scientific findings to action is a complex process that needs to be executed appropriately for change to occur. Our Benin experience provided key steps and lessons learned to help computational modeling inform and lead to major policy change. The key steps are: engagement of Ministry of Health, identifying in-country “champions,” directed and efficient data collection, defining a finite set of realistic scenarios, making the study methodology transparent, presenting the results in a clear manner, and facilitating decision-making and advocacy. Generating scientific evidence is one component of policy change. Enabling change requires orchestration of a coordinated set of steps that heavily involve key stakeholders, earn their confidence, and provide them with relevant information. Our Benin EVM+CCEM+HERMES Process led to a decision to enact major changes and could serve as a template for similar approaches in other countries.}
}
@article{ZILCHAMANO2025100478,
title = {Contrasting individual-specific resilience and compensation personalization frameworks: The case of rumination},
journal = {Biological Psychiatry Global Open Science},
pages = {100478},
year = {2025},
issn = {2667-1743},
doi = {https://doi.org/10.1016/j.bpsgos.2025.100478},
url = {https://www.sciencedirect.com/science/article/pii/S2667174325000321},
author = {Sigal Zilcha-Mano},
keywords = {Rumination, compensation, resilience, complementing, capitalization, personalized treatment, mechanism of change},
abstract = {Background
Rumination has been identified as a potential mechanism of therapeutic change, particularly in directive and focused psychotherapies for depression. Previous research has predominantly centered on either trait-like individual differences or state-like changes in rumination, without integrating these aspects. The present study proposes a computational approach to investigate whether rumination serves as a compensatory or resilience mechanism by integrating both trait-like and state-like effects.
Method
Rumination and depressive symptoms were assessed (in N=100) pre-treatment and repeatedly throughout treatment. Mixed-level models were used to examine whether pre-treatment trait-like rumination interacted with a time-variant variable of in-treatment state-like changes in rumination to predict subsequent changes in treatment outcomes. These models were used to determine whether individuals with higher or lower pre-treatment trait-like levels of rumination benefited more from state-like reductions in rumination, thus contrasting the compensatory and resilience theoretical frameworks.
Results
As hypothesized, the findings support the compensatory framework: individuals with higher pre-treatment trait-like levels of rumination benefited most from greater state-like reductions in rumination during treatment, as evidenced by greater subsequent symptom reduction (p=.04).
Conclusion
The findings refine our understanding of rumination as an individual-specific mechanism of therapeutic change, dependent on the individual's trait-like levels of rumination. The proposed computational approach enabled an empirical comparison of the two main theoretical frameworks of treatment personalization, compensatory and resilience, offering new insights into mechanisms driving therapeutic change. Future studies could leverage the paradigm proposed here to examine for which patients and in what contexts mechanisms of change function as compensatory versus resilience mechanisms.}
}
@article{ANDREJCZUK2019104799,
title = {Synergistic team composition: A computational approach to foster diversity in teams},
journal = {Knowledge-Based Systems},
volume = {182},
pages = {104799},
year = {2019},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0950705119302746},
author = {Ewa Andrejczuk and Filippo Bistaffa and Christian Blum and Juan A. Rodríguez-Aguilar and Carles Sierra},
keywords = {Team composition, Exact algorithms, Heuristic algorithms, Optimisation, Coalition formation},
abstract = {Co-operative learning in heterogeneous teams refers to learning methods in which teams are organised both to accomplish academic tasks and for individuals to gain knowledge. Competencies, personality and the gender of team members are key factors that influence team performance. Here, we introduce a team composition problem, the so-called synergistic team composition problem (STCP), which incorporates such key factors when arranging teams. Thus, the goal of the STCP is to partition a set of individuals into a set of synergistic teams: teams that are diverse in personality and gender and whose members cover all required competencies to complete a task. Furthermore, the STCP requires that all teams are balanced in that they are expected to exhibit similar performances when completing the task. We propose two efficient algorithms to solve the STCP. Our first algorithm is based on a linear programming formulation and is appropriate to solve small instances of the problem. Our second algorithm is an anytime heuristic that is effective for large instances of the STCP. Finally, we thoroughly study the computational properties of both algorithms in an educational context when grouping students in a classroom into teams using actual-world data.}
}
@article{NEILL2024100870,
title = {Designer delectables; exploring the design practice of haute couture and haute cuisine},
journal = {International Journal of Gastronomy and Food Science},
volume = {35},
pages = {100870},
year = {2024},
issn = {1878-450X},
doi = {https://doi.org/10.1016/j.ijgfs.2024.100870},
url = {https://www.sciencedirect.com/science/article/pii/S1878450X24000039},
author = {Lindsay Neill and Nigel Hemmington and Christine McDonald and Francesca Zampollo},
keywords = {Design practice, Haute couture, Haute cuisine, Designerly thinking},
abstract = {This study explores design practice across two domains: haute couture (fashion), and haute cuisine (food). A case study approach was taken using the voice of practitioners as the focus through in-depth qualitative interviews. The cross-domain approach revealed similarities in design practice through four design themes: visualization, ‘conversations’ with materials, co-creation and ‘pushing boundaries’. The data also revealed innovations within the four themes that could apply to other design domains, for example visualization (haute couture) and co-creation (haute cuisine). The practitioners also provided valuable and nuanced insights into their design practice – ‘You have to live something to do it’. These insights from practitioners and their practice reveal how the two domains hold similarities in design practice and provide a deeper understanding of design processes, and designerly thinking, from which creativity and innovation can emerge.}
}
@article{GHAVANLOO20231,
title = {Experimental and computational physics of fullerenes and their nanocomposites: Synthesis, thermo-mechanical characteristics and nanomedicine applications},
journal = {Physics Reports},
volume = {996},
pages = {1-116},
year = {2023},
note = {Experimental and computational physics of fullerenes and their nanocomposites: Synthesis, thermo-mechanical characteristics and nanomedicine applications},
issn = {0370-1573},
doi = {https://doi.org/10.1016/j.physrep.2022.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0370157322003775},
author = {Esmaeal Ghavanloo and Hashem Rafii-Tabar and Ayesha Kausar and Georgios I. Giannopoulos and S. Ahmad Fazelzadeh},
keywords = {Fullerene molecules, Nanocomposites, Synthesis, Computational modeling, Thin films, Mechanical properties, Thermal properties, Vibrational properties, Molecular dynamics, Molecular mechanics, Micromechanics, Nanomedicine, Nanoneuroscience application},
abstract = {It is an established paradigm in the emerging fields of nanoscience, nanotechnology and molecular engineering that a very important domain of fundamental research is associated with carbon-based materials. Ever since the discovery of the first member of the fullerene family (C60) in 1985, and the subsequent discovery of the other members, fullerenes as a nanoscopic allotrope of carbon with anticipated extensive applications in all areas of nanoscience and nanotechnology (both industrial and medical), materials science and engineering, condensed matter physics and chemistry have occupied a central position in research activities across the globe. Detailed investigations, both experimental and theoretical/computational, into their morphology, mechanical, thermal, chemical, biological, electronic, optical and structural properties have led to the emergence of a well-established and independent science of fullerenes, providing very valuable information both in basic and applied sciences. A comprehensive review of these properties of fullerenes, particularly their applications in the above fields will provide valuable up-to-date and essential background information for engaging in new research in this field and also be able to develop new concepts and applications of these exotic carbon structures. For instance, a recent development is their applications in the emerging field of nanoneuroscience, a field interfacing nanoscience and neuroscience. In this extensive, albeit selective survey, related mainly to the C60 fullerenes, the processes involving their experimental synthesis, theoretical formulation of their geometrical structures, their mechanical and thermal properties and nanomedical applications have been reviewed and summarized both within the experimental and theoretical/computational domains. Essential theoretical concepts, ranging from discrete atomistic molecular dynamics and molecular mechanics methods to continuum-based methods have been expounded in order to facilitate the pursuance of the reviewed literature and also to aid in the development of further research in this field.}
}
@article{MCCLELLAND1993209,
title = {Computational approaches to cognition: top-down approaches},
journal = {Current Opinion in Neurobiology},
volume = {3},
number = {2},
pages = {209-216},
year = {1993},
issn = {0959-4388},
doi = {https://doi.org/10.1016/0959-4388(93)90212-H},
url = {https://www.sciencedirect.com/science/article/pii/095943889390212H},
author = {James L. McClelland and David C. Plaut},
abstract = {Computational models are useful tools for exploring the nature of human cognitive processes. In particular, connectionist models are providing researchers with new ways of thinking about the basic nature of cognition and its implementation in the brain. They support novel explanations of important aspects of perception, memory, language, thought and cognitive development, and allow cognitive processes to be linked with the underlying physiological mechanisms. The models also aid our understanding of how disorders of brain function lead to disorders of cognition.}
}
@article{HIBERTY1998237,
title = {Thinking and computing valence bond in organic chemistry1Dedicated to the memory of Professor Joseph Gerratt, in appreciation of his outstanding contributions to modern ab initio valence bond methodology.1},
journal = {Journal of Molecular Structure: THEOCHEM},
volume = {451},
number = {3},
pages = {237-261},
year = {1998},
issn = {0166-1280},
doi = {https://doi.org/10.1016/S0166-1280(98)00208-5},
url = {https://www.sciencedirect.com/science/article/pii/S0166128098002085},
author = {Philippe C. Hiberty},
keywords = {Valence bond, Hybridization, Symmetry breaking, Resonance energy, Breathing orbitals},
abstract = {This paper presents a short survey of some recent ab initio valence bond methods and their applications, and is aimed at justifying and encouraging a valence bond view of organic chemistry, as complementary to the molecular orbital approach. In the first section, the qualitative VB description of the elementary interactions is recalled and compared to the MO model. It is shown that the VB picture is fundamentally correct, even for the well-known cases of the low-lying states of dioxygen and the 4n/4n+2 aromaticity rule. The second section briefly discusses the classical VB method, which deals with atomic orbitals that are optimized for the free atoms and kept unchanged in molecules, then describes modern ab initio VB methods that all perform orbital optimization in molecular calculations. The generalized valence bond and spin-coupled theories both provide a one-configuration wavefunction. While the former is generally used with some time-saving restrictions such as the strong-orthogonality restriction and the perfect-pairing approximation, the latter releases any orthogonality constraint and allows all possible spin couplings. Multiconfiguration methods are also discussed, as well as methods using different orbitals for different structures. Some computational applications of these methods are presented in the last section. It is shown that if given full freedom to optimize its shape with the variational principle as a unique criterion, a one-configuration wavefunction spontaneously takes the form of a VB wavefunction displaying localized orbitals, and presents a picture in terms of hybrid orbitals and/or resonance between limiting structures, very close to the traditional qualitative picture. The concept of hybridization is firmly supported, as the unique outcome of the highest computational level still compatible with the orbital picture. The description of conjugated systems in terms of resonating Kekulé structures is also fully justified and shown to be the best framework for discussing questions such as the distortive tendencies of conjugated π-electronic systems, or violations of Hund's rules. The ab initio VB approach can be used for quantifying some traditional paradigms such as the role of the delocalization energy in the acidity of carboxylic acids and enols, or in the properties of the amide/thioamide functional group. It is also shown to be an elegant solution to some difficult computational problems like the symmetry-breaking artefact or the inclusion of dynamical correlation in the description of the chemical bond. Lastly, some of the methods presented here are shown to be appropriate for the calculation of diabatic potential surfaces, with applications to the Shaik–Pross reactivity model of the VB curve-crossing correlation diagrams.}
}
@article{QUINLAN2007413,
title = {Re-thinking stages of cognitive development: An appraisal of connectionist models of the balance scale task},
journal = {Cognition},
volume = {103},
number = {3},
pages = {413-459},
year = {2007},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2006.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0010027706000552},
author = {Philip T. Quinlan and Han L.J. {van der Maas} and Brenda R.J. Jansen and Olaf Booij and Mark Rendell},
keywords = {Connectionist models, Balance scale task, Latent class analysis},
abstract = {The present paper re-appraises connectionist attempts to explain how human cognitive development appears to progress through a series of sequential stages. Models of performance on the Piagetian balance scale task are the focus of attention. Limitations of these models are discussed and replications and extensions to the work are provided via the Cascade-Correlation algorithm. An application of multi-group latent class analysis for examining performance of the networks is described and these results reveal fundamental functional characteristics of the networks. Evidence is provided that strongly suggests that the networks are unable to acquire a mastery of torque and, although they do recover certain rules of operation that humans do, they also show a propensity to acquire rules never previously seen.}
}
@article{IDEKER2009820,
title = {The Thinking Man's Cell},
journal = {Cell},
volume = {138},
number = {5},
pages = {820-821},
year = {2009},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2009.08.024},
url = {https://www.sciencedirect.com/science/article/pii/S0092867409010411},
author = {Trey Ideker}
}
@article{KONG2023100126,
title = {Complementary role of large language models in educating undergraduate design of distillation column: Methodology development},
journal = {Digital Chemical Engineering},
volume = {9},
pages = {100126},
year = {2023},
issn = {2772-5081},
doi = {https://doi.org/10.1016/j.dche.2023.100126},
url = {https://www.sciencedirect.com/science/article/pii/S2772508123000443},
author = {Zong Yang Kong and Vincentius Surya Kurnia Adi and Juan Gabriel Segovia-Hernández and Jaka Sunarso},
keywords = {ChatGPT, Chemical engineering education, Large language models, Distillation, Industry 4.0, Mass transfer},
abstract = {This paper explores the integration of large language models (LLMs), such as ChatGPT, in chemical engineering education, departing from conventional practices that may not be universally accepted. While there is ongoing debate surrounding the acceptance of LLMs, driven by concerns over computational instability and potential inconsistencies, their inevitability in shaping our communication and interaction with technology cannot be ignored. As educators, we are positioned to play a vital role in guiding students toward the responsible, effective, and synergetic use of LLMs. Focusing specifically on distillation column design in undergraduate mass-transfer courses, this study demonstrates how ChatGPT can be utilized as an auxiliary tool to create interactive learning environments and simulate real-world engineering thinking processes. It emphasizes the need for students to develop critical thinking skills and a thorough understanding of LLM principles, taking responsibility for their use and creations. While ChatGPT should not be solely relied upon, its integration with fundamental principles of chemical engineering is crucial. The effectiveness and limitations of ChatGPT are exemplified through two case studies, showcasing the importance of manual calculations and established simulation software as primary tools for guiding and validating engineering results and analyses. This paper also addresses the pedagogical implications of integrating LLMs into mass transfer courses, encompassing curriculum integration, facilitation, guidance, and ethical considerations. Recommendations are provided for incorporating LLMs effectively into the curriculum. Overall, this study contributes to the advancement of chemical engineering education by examining the benefits and limitations of LLMs as educational aids in the design process.}
}
@incollection{CAPELLI2023105,
title = {4 - 3D-printed and computational models: a combined approach for patient-specific studies},
editor = {Deepak M. Kalaskar},
booktitle = {3D Printing in Medicine (Second Edition)},
publisher = {Woodhead Publishing},
edition = {Second Edition},
pages = {105-125},
year = {2023},
series = {Woodhead Publishing Series in Biomaterials},
isbn = {978-0-323-89831-7},
doi = {https://doi.org/10.1016/B978-0-323-89831-7.00011-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323898317000110},
author = {Claudio Capelli and Michele Bertolini and Silvia Schievano},
keywords = {Patient-specific models, cardiovascular models, segmentation, 3D printing, finite element analyses, computational fluid dynamics, validation},
abstract = {Three-dimensional printed models have been increasingly used in many fields of medicine. The most common benefits include a better understanding of anatomical details, an improved communication between clinicians and patients, a more accurate planning of treatments, and new opportunities for procedural training. In the cardiovascular field, this technology has contributed to improve the management of complex cases, in particular congenital heart disease, by fostering personalized preprocedural planning and increasing medical trainees’ confidence. Cardiovascular structures, however, are extremely challenging to replicate using materials compatible with current 3D printing technologies. Hence, patient-specific computational models, generated from the same set of medical images as printed ones, can be combined to 3D printing technology to simulate different conditions and identify the optimal treatment for each specific patient. A further step forward is represented by the integration of advanced visualization techniques like augmented and virtual reality, to close still existing loopholes. In this chapter, we review the current possibilities associated with the use of patient-specific models, in the context of cardiovascular applications.}
}
@article{SAW2025111884,
title = {Current status and future directions of explainable artificial intelligence in medical imaging},
journal = {European Journal of Radiology},
volume = {183},
pages = {111884},
year = {2025},
issn = {0720-048X},
doi = {https://doi.org/10.1016/j.ejrad.2024.111884},
url = {https://www.sciencedirect.com/science/article/pii/S0720048X24006004},
author = {Shier Nee Saw and Yet Yen Yan and Kwan Hoong Ng},
keywords = {Artificial intelligence, Interpretability, Explainability, Medical imaging, Medical information systems},
abstract = {The inherent “black box” nature of AI algorithms presents a substantial barrier to the widespread adoption of the technology in clinical settings, leading to a lack of trust among users. This review begins by examining the foundational stages involved in the interpretation of medical images by radiologists and clinicians, encompassing both type 1 (fast thinking − ability of the brain to think and act intuitively) and type 2 (slow analytical − slow analytical, laborious approach to decision-making) decision-making processes. The discussion then delves into current Explainable AI (XAI) approaches, exploring both inherent and post-hoc explainability for medical imaging applications and highlighting the milestones achieved. XAI in medicine refers to AI system designed to provide transparent, interpretable, and understandable reasoning behind AI predictions or decisions. Additionally, the paper showcases some commercial AI medical systems that offer explanations through features such as heatmaps. Opportunities, challenges and potential avenues for advancing the field are also addressed. In conclusion, the review observes that state-of-the-art XAI methods are not mature enough for implementation, as the explanations they provide are challenging for medical experts to comprehend. Deeper understanding of the cognitive mechanisms by medical professionals is important in aiming to develop more interpretable XAI methods.}
}
@article{GAMAL20241319,
title = {A computational sustainable approach for energy storage systems performance evaluation based on spherical-fuzzy MCDM with considering uncertainty},
journal = {Energy Reports},
volume = {11},
pages = {1319-1341},
year = {2024},
issn = {2352-4847},
doi = {https://doi.org/10.1016/j.egyr.2023.12.058},
url = {https://www.sciencedirect.com/science/article/pii/S2352484723016529},
author = {Abduallah Gamal and Mohamed Abdel-Basset and Ibrahim M. Hezam and Karam M. Sallam and Ahmad M. Alshamrani and Ibrahim A. Hameed},
keywords = {Energy storage systems, Sustainable, Uncertainty, MCDM, SF-AHP, SF-MACONT, Sensitivity analysis},
abstract = {Incorporating energy storage systems (ESSs) can mitigate the intermittency of renewable energy sources. There are a variety of ESSs for renewable energy with vastly different characteristics. The problem of diversity of characteristics in selecting the most appropriate ESS can be approached as a multi-criteria decision-making (MCDM) problem. This research evaluates sustainable ESSs through a case study in Egypt. A sustainable computational approach is presented through which experts can use verbal expressions to express their opinions in determining the priorities of the dimensions that affect the selection of ESSs. Determining the appropriate energy storage system requires consideration of several main dimensions such as the technology dimension, environmental dimension, economic dimension, and social-political dimension and, in addition to the sub-indicators. Hence, this research applies a hybrid MCDM approach that deals with different indicators and characteristics. Also, uncertainty in applying the proposed approach was dealt with by a spherical fuzzy (SF) environment and by using the spherical fuzzy numbers (SFNs). At first, the SF analytical hierarchy process (SF-AHP) method was used to assess the priorities of the four main dimensions and their sub-indicators. Then, the SF mixed aggregation by comprehensive normalization technique (SF-MACONT) was applied to evaluate and rank the ESSs selected for analysis through research. An illustrative case study was presented that included seven ESSs out of the eighteen systems listed in the research to confirm the feasibility of the developed approach. Sensitivity analysis was carried out by changing some parameters like λ, μ, δ, and ϑ based on the SF-MACONT method and changing the weights of some main dimensions. A comparative analysis with some MCDM approaches was conducted to show the advantages of the developed approach through its flexibility and built-in parameters. The findings show that the technology dimension is the most influential in choosing a sustainable ESS, while the economic dimension is the least influential. Also, the results of the evaluation and ranking of the seven selected ESSs indicate that the "Pumped Hydro" system is the most suitable system for energy storage in Egypt.}
}
@incollection{ALIPPI2024251,
title = {13 - Computational intelligence in cyber-physical systems and the Internet of Things},
editor = {Robert Kozma and Cesare Alippi and Yoonsuck Choe and Francesco Carlo Morabito},
booktitle = {Artificial Intelligence in the Age of Neural Networks and Brain Computing (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {251-267},
year = {2024},
isbn = {978-0-323-96104-2},
doi = {https://doi.org/10.1016/B978-0-323-96104-2.00001-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323961042000014},
author = {Cesare Alippi and Seiichi Ozawa},
keywords = {Intelligent systems, Embedded AI, Learning in nonstationary environments, Cybersecurity},
abstract = {The emergence of nontrivial embedded sensor units and cyber-physical systems and the Internet of Things has made possible the design and implementation of sophisticated applications where large amounts of real-time data are collected, possibly to constitute a big data picture as time passes. Within this framework, intelligence mechanisms based on machine learning, neural networks, and brain computing approaches play a key role to provide systems with advanced functionalities. Intelligent mechanisms are needed to guarantee appropriate performances within an evolving, time-variant environment, optimally harvest the available and manage the residual energy, reduce the energy consumption of the whole system, identify and mitigate the occurrence of faults, and provide shields against cyber-attacks. The chapter introduces the above aspects of intelligence, whose functionalities are needed to boost the next generation of cyber-physical and Internet of Things applications, a smart world generation whose footprint is already around us.}
}
@article{TIRADORAMOS2010855,
title = {Fourth Workshop on Teaching Computational Science (WTCS 2010)},
journal = {Procedia Computer Science},
volume = {1},
number = {1},
pages = {855-856},
year = {2010},
note = {ICCS 2010},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2010.04.093},
url = {https://www.sciencedirect.com/science/article/pii/S1877050910000943},
author = {A. Tirado-Ramos and A.B. Shiflet},
keywords = {Teaching, Computational science, Modeling, Simulation},
abstract = {The Workshop on Teaching Computational Science (WTCS), taking place within the International Conference on Computational Science (ICCS), is a platform for discussing innovations in teaching computational science in its various aspects, e.g. modeling and simulation, at all levels and contexts. Innovations may cover the context of formal courses or self-directed learning, involving, for example, curriculum development, introductory programming, service courses, specialist undergraduate and postgraduate topics, as well as industry-related short courses. This editorial provides an introduction to the work presented during the sessions in Amsterdam.}
}
@incollection{SHARMA202353,
title = {Chapter 2 - Computational approaches in drug discovery and design},
editor = {Rupesh Kumar Gautam and Mohammad Amjad Kamal and Pooja Mittal},
booktitle = {Computational Approaches in Drug Discovery, Development and Systems Pharmacology},
publisher = {Academic Press},
pages = {53-93},
year = {2023},
isbn = {978-0-323-99137-7},
doi = {https://doi.org/10.1016/B978-0-323-99137-7.00009-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323991377000095},
author = {Priyanka Sharma and Kalicharan Sharma and Mukesh Nandave},
keywords = {Molecular modeling, Molecular docking, Molecular dynamics simulation, QSAR, Bioinformatics},
abstract = {Drug development is a costly and time-consuming procedure. The medicine must meet certain characteristics such as nontoxicity, bioavailability, and potency. Establishing a better drug-like compound has now become a difficult and error-prone endeavor in light of ever-increasing expectations for effectiveness, intensity, and stability. The emergence of conformations of chemical therapeutic targets, as well as developments in computational techniques and bioinformatics, have accelerated the use of molecular modeling in pharmaceutical research. Numerous molecular modeling methodologies used in recent pharmacological studies are reviewed in this chapter. Structure- and ligand-based drug design, protein modeling and visualization molecular docking, virtual screening, molecular dynamics simulation, pharmacophore modeling, and QSAR techniques have all been discussed. In addition, we make key database resources and tools available to the researchers and scientists for future prospects.}
}
@article{YOUSEF2024137753,
title = {Biological and computational assessment of new synthesized nicotinamides as potential immunomodulatory VEGFR-2 inhibitors},
journal = {Journal of Molecular Structure},
volume = {1305},
pages = {137753},
year = {2024},
issn = {0022-2860},
doi = {https://doi.org/10.1016/j.molstruc.2024.137753},
url = {https://www.sciencedirect.com/science/article/pii/S002228602400276X},
author = {Reda G. Yousef and Alaa Elwan and Abdallah E. Abdallah and Hazem Elkady and Ahmed B.M. Mehany and Mariam Ali Abo-Saif and Mohamed M. Radwan and Mahmoud A. ElSohly and Ibrahim M. Ibrahim and Mohamed A. Elkady and Mohamed Ayman El-Zahabi and Ibrahim H. Eissa},
keywords = {Nicotinamides, Anticancer, VEGFR-2, Apoptosis, Immunomodulatory, Computational studies},
abstract = {As an extension to our preceding studies on nicotinamide derivatives as anticancer agents, new nicotinamide-based candidates were designed and synthesized as VEGFR-2 inhibitors. The in vitro cytotoxic activity of the synthesized compounds was evaluated against three human cancer cell lines (MCF-7, HepG-2 and HCT-116). The IC50 values for compound 17 were 2.61± 0.01, 3.20 ± 0.02, and 2.46 ± 0.01 µM, respectively, compared to sorafenib (4.21±0.03, 3.40 ± 0.02, and 5.30 ± 0.04 µM) against MCF-7, HePG-2, and HCT-116. This indicated that compound 17 possess double strength relative to sorafenib against both MCF-7 and HCT-116. Compound 17 was the most promising VEGFR-2 inhibitor with IC50 value of 0.34 μM that was slightly better than that of sorafenib (0.38 μM). Further studies displayed the ability of compound 17 to arrest the growth of HCT-116 cells at the Pre-G1 and S phases. Additionally, compound 17 induced a significant increase in the total apoptosis rate of HCT-116 cells from 1.82 % to 26.69 %. Moreover, it showed high selectivity indices against HCT-116, HepG2, and MCF-7 cancer cells. Furthermore, compound 17 showed potent inhibitory activities on TNF-α and IL-6 and showed a notable rise in caspase-3 level. In addition, the potentiality of the designed derivatives to bind with and inhibit the VEGFR-2 enzyme was indicated by molecular docking assessments. MD simulation studies revealed the stability of compound 17 in the active site of VEGFR-2 for 100 ns. Based on the previous findings, compound 17 appears to be a promising apoptotic VEGFR-2 inhibitor and could potentially direct future efforts towards the development of novel anticancer medications.}
}
@article{COSTA201227,
title = {Systems pathology: A critical review},
journal = {Molecular Oncology},
volume = {6},
number = {1},
pages = {27-32},
year = {2012},
issn = {1574-7891},
doi = {https://doi.org/10.1016/j.molonc.2011.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S1574789111001438},
author = {Jose Costa},
keywords = {Systems biology, Systems pathology, Translational research},
abstract = {The technological advances of the last twenty years together with the dramatic increase in computational power have injected new life into systems-level thinking in Medicine. This review emphasizes the close relationship of Systems Pathology to Systems Biology and delineates the differences between Systems Pathology and Clinical Systems Pathology. It also suggests an algorithm to support the application of systems-level thinking to clinical research, proposes applying systems-level thinking to the health care systems and forecasts an acceleration of preventive medicine as a result of the coupling of personal genomics with systems pathology.}
}
@article{HEINZLE201621,
title = {Computational models of eye movements and their application to schizophrenia},
journal = {Current Opinion in Behavioral Sciences},
volume = {11},
pages = {21-29},
year = {2016},
note = {Computational modeling},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2016.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S2352154616300754},
author = {Jakob Heinzle and Eduardo A Aponte and Klaas Enno Stephan},
abstract = {Patients with neuropsychiatric disorders, in particular schizophrenia, show a variety of eye movement abnormalities that putatively reflect alterations of perceptual inference, learning and cognitive control. While these abnormalities are consistently found at the group level, a particularly difficult and important challenge is to translate these findings into clinically useful tests for single patients. In this paper, we argue that generative models of eye movement data, which allow for inferring individual computational and physiological mechanisms, could contribute to filling this gap. We present a selective overview of eye movement paradigms with clinical relevance for schizophrenia and review existing computational approaches that rest on (or could be turned into) generative models. We conclude by outlining desirable clinical applications at the individual subject level and discuss the necessary validation studies.}
}
@article{CHAPLESKI2020101435,
title = {A Molecular-Scale Approach to Rare-Earth Beneficiation: Thinking Small to Avoid Large Losses},
journal = {iScience},
volume = {23},
number = {9},
pages = {101435},
year = {2020},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2020.101435},
url = {https://www.sciencedirect.com/science/article/pii/S2589004220306258},
author = {Robert C. Chapleski and Azhad U. Chowdhury and Anna K. Wanhala and Vera Bocharova and Santanu Roy and Philip C. Keller and Dylan Everly and Santa Jansone-Popova and Alexander Kisliuk and Robert L. Sacci and Andrew G. Stack and Corby G. Anderson and Benjamin Doughty and Vyacheslav S. Bryantsev},
keywords = {Chemical Engineering, Spectroscopy, Physical Inorganic Chemistry, Surface Chemistry},
abstract = {Summary
Separating rare-earth-element-rich minerals from unwanted gangue in mined ores relies on selective binding of collector molecules at the interface to facilitate froth flotation. Salicylhydroxamic acid (SHA) exhibits enhanced selectivity for bastnäsite over calcite in microflotation experiments. Through a multifaceted approach, leveraging density functional theory calculations, and advanced spectroscopic methods, we provide molecular-level mechanistic insight to this selectivity. The hydroxamic acid moiety introduces strong interactions at metal-atom surface sites and hinders subsurface-cation stabilization at vacancy-defect sites, in calcite especially. Resulting from hydrogen-bond-induced interactions, SHA lies flat on the bastnäsite surface and shows a tendency for multilayer formation at high coverages. In this conformation, SHA complexation with bastnäsite metal ions is stabilized, leading to advanced flotation performance. In contrast, SHA lies perpendicular to the calcite surface due to a difference in cationic spacing. We anticipate that these insights will motivate rational design and selection of future collector molecules for enhanced ore beneficiation.}
}
@article{LERON2014126,
title = {Functions via everyday actions: Support or obstacle?},
journal = {The Journal of Mathematical Behavior},
volume = {36},
pages = {126-134},
year = {2014},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2014.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S073231231400056X},
author = {Uri Leron and Tamar Paz},
keywords = {Functions, Composition of functions, Intuitive thinking, Analytical thinking, Dual-process theory, Actions on objects, Changing-the-input misconception},
abstract = {The general context of this paper is the power of intuitive thinking, and how it can help or hinder analytical thinking. The research literature in cognitive psychology teems with tasks where intuitive thinking leads subjects to “non-normative” answers, including tasks for which they have all the knowledge necessary for the normative answer. The best explanation to date for such phenomena is dual-process theory, which stipulates the activation of a quick automatic intuitive process (System 1), together with the failure of the heavy, lazy, and computationally expensive analytical process (System 2) to intervene and correct the intuitive response. In an earlier paper, we have documented a clash between intuitive and analytical thinking concerning functions, which we have termed the changing-the-input phenomenon. The discovery of the changing-the-input phenomenon, however, left us with a puzzle: Why has this phenomenon concerning functions – a purely mathematical concept – been observed in computer science classes but not in mathematics ones? The purpose of the present paper is to address this puzzle. More generally we ask, under what conditions the changing-the-input phenomenon will or will not be manifested? Still more generally, in learning about functions, when is the intuitive scaffolding of functions via actions-on-tangible-objects helpful, and when does it get in the way of deeper understanding?}
}
@article{FORD200437,
title = {Electrophysiological evidence of corollary discharge dysfunction in schizophrenia during talking and thinking},
journal = {Journal of Psychiatric Research},
volume = {38},
number = {1},
pages = {37-46},
year = {2004},
issn = {0022-3956},
doi = {https://doi.org/10.1016/S0022-3956(03)00095-5},
url = {https://www.sciencedirect.com/science/article/pii/S0022395603000955},
author = {Judith M. Ford and Daniel H. Mathalon},
keywords = {Schizophrenia, Corollary discharge, N1, EEG coherence},
abstract = {Failure of corollary discharge, a mechanism for distinguishing self-generated from externally-generated percepts, has been posited to underlie certain positive symptoms of schizophrenia, including auditory hallucinations. Although originally described in the visual system, corollary discharge may exist in the auditory system, whereby signals from motor speech commands prepare auditory cortex for self-generated speech. While associated with sensorimotor systems, it might also apply to inner speech or thought, regarded as our most complex motor act. We had four aims in the studies summarized in this paper: (1) to demonstrate the corollary discharge phenomenon during talking and inner speech in human volunteers using event-related brain potentials (ERPs), (2) to demonstrate that the corollary discharge is abnormal in patients with schizophrenia, (3) to demonstrate the role of frontal speech areas in the corollary discharge during talking, and (4) to relate the dysfunction of the corollary discharge in schizophrenia to auditory hallucinations. Using EEG and ERP measures, we addressed each aim in patients with schizophrenia (DSM IV) and healthy control subjects. The N1 component of the ERP reflected dampening of auditory cortex responsivity during talking and inner speech in control subjects but not in patients. EEG measures of coherence indicated inter-dependence of activity in the frontal speech production and temporal speech reception areas during talking in control subjects, but not in patients, especially those who hallucinated. These data suggest that a corollary discharge from frontal areas where thoughts are generated fails to alert auditory cortex that they are self-generated, leading to the misattribution of inner speech to external sources and producing the experience of auditory hallucinations.}
}
@article{HAN20241,
title = {Ground threat prediction-based path planning of unmanned autonomous helicopter using hybrid enhanced artificial bee colony algorithm},
journal = {Defence Technology},
volume = {32},
pages = {1-22},
year = {2024},
issn = {2214-9147},
doi = {https://doi.org/10.1016/j.dt.2023.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S2214914723001071},
author = {Zengliang Han and Mou Chen and Haojie Zhu and Qingxian Wu},
keywords = {UAH, Path planning, Ground threat prediction, Hybrid enhanced, Collaborative thinking},
abstract = {Unmanned autonomous helicopter (UAH) path planning problem is an important component of the UAH mission planning system. Aiming to reduce the influence of non-complete ground threat information on UAH path planning, a ground threat prediction-based path planning method is proposed based on artificial bee colony (ABC) algorithm by collaborative thinking strategy. Firstly, a dynamic threat distribution probability model is developed based on the characteristics of typical ground threats. The dynamic no-fly zone of the UAH is simulated and established by calculating the distribution probability of ground threats in real time. Then, a dynamic path planning method for UAH is designed in complex environment based on the real-time prediction of ground threats. By adding the collision warning mechanism to the path planning model, the flight path could be dynamically adjusted according to changing no-fly zones. Furthermore, a hybrid enhanced ABC algorithm is proposed based on collaborative thinking strategy. The proposed algorithm applies the leader-member thinking mechanism to guide the direction of population evolution, and reduces the negative impact of local optimal solutions caused by collaborative learning update strategy, which makes the optimization performance of ABC algorithm more controllable and efficient. Finally, simulation results verify the feasibility and effectiveness of the proposed ground threat prediction path planning method.}
}
@article{FABRY2018793,
title = {Turing redux: Enculturation and computation},
journal = {Cognitive Systems Research},
volume = {52},
pages = {793-808},
year = {2018},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2018.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S1389041718301724},
author = {Regina E. Fabry},
keywords = {Enculturation, Mathematical cognition, Computation, Hybrid cognition, Neural plasticity, Embodied cognition},
abstract = {Many of our cognitive capacities are shaped by enculturation. Enculturation is the acquisition of cognitive practices such as symbol-based mathematical practices, reading, and writing during ontogeny. Enculturation is associated with significant changes to the organization and connectivity of the brain and to the functional profiles of embodied actions and motor programs. Furthermore, it relies on scaffolded cultural learning in the cognitive niche. The purpose of this paper is to explore the components of symbol-based mathematical practices. Phylogenetically, these practices are the result of concerted organism-niche interactions that have led from approximate number estimations to the emergence of discrete, symbol-based mathematical operations. Ontogenetically, symbol-based mathematical practices are associated with plastic changes to neural circuitry, action schemata, and motor programs. It will be suggested that these practices rely on previously acquired capacities such as subitizing and counting. With these considerations in place, I will argue that computations, understood in the sense of Turing (1936), are a specific kind of symbol-based mathematical practices that can be realized by human organisms, machines, or by hybrid organism-machine systems. In sum, this paper suggests a new way to think about mathematical cognition and computation.}
}
@article{20167,
title = {What Is the Key Best Practice for Collaborating with a Computational Biologist?},
journal = {Cell Systems},
volume = {3},
number = {1},
pages = {7-11},
year = {2016},
issn = {2405-4712},
doi = {https://doi.org/10.1016/j.cels.2016.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S240547121630223X}
}
@article{BRYANT201034,
title = {Thinking inside the box: A participatory, computer-assisted approach to scenario discovery},
journal = {Technological Forecasting and Social Change},
volume = {77},
number = {1},
pages = {34-49},
year = {2010},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2009.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S004016250900105X},
author = {Benjamin P. Bryant and Robert J. Lempert},
keywords = {Scenario Discovery, Scenario planning, Robust decision making},
abstract = {Scenarios provide a commonly used and intuitively appealing means to communicate and characterize uncertainty in many decision support applications, but can fall short of their potential especially when used in broad public debates among participants with diverse interests and values. This paper describes a new approach to participatory, computer-assisted scenario development that we call scenario discovery, which aims to address these challenges. The approach defines scenarios as a set of plausible future states of the world that represent vulnerabilities of proposed policies, that is, cases where a policy fails to meet its performance goals. Scenario discovery characterizes such sets by helping users to apply statistical or data-mining algorithms to databases of simulation-model-generated results in order to identify easy-to-interpret combinations of uncertain model input parameters that are highly predictive of these policy-relevant cases. The approach has already proved successful in several high impact policy studies. This paper systematically describes the scenario discovery concept and its implementation, presents statistical tests to evaluate the resulting scenarios, and demonstrates the approach on an example policy problem involving the efficacy of a proposed U.S. renewable energy standard. The paper also describes how scenario discovery appears to address several outstanding challenges faced when applying traditional scenario approaches in contentious public debates.}
}
@article{BARGMANN20242999,
title = {Cori Bargmann},
journal = {Neuron},
volume = {112},
number = {18},
pages = {2999-3002},
year = {2024},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2024.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S0896627324006561},
author = {Cori Bargmann},
abstract = {In an interview with Neuron, Cori Bargmann discusses C. elegans as a model organism, the importance of considering the animal’s own world (thinking like a worm), choosing a scientific problem, and her experience as head of science at the Chan Zuckerberg Initiative and co-chair of the BRAIN Initiative.}
}
@article{WATFORD2019114707,
title = {Progress in data interoperability to support computational toxicology and chemical safety evaluation},
journal = {Toxicology and Applied Pharmacology},
volume = {380},
pages = {114707},
year = {2019},
issn = {0041-008X},
doi = {https://doi.org/10.1016/j.taap.2019.114707},
url = {https://www.sciencedirect.com/science/article/pii/S0041008X19303151},
author = {Sean Watford and Stephen Edwards and Michelle Angrish and Richard S. Judson and Katie {Paul Friedman}},
keywords = {Data Interoperability, Computational Toxicology, Bioinformatics, Databases, Applications},
abstract = {New approach methodologies (NAMs) in chemical safety evaluation are being explored to address the current public health implications of human environmental exposures to chemicals with limited or no data for assessment. For over a decade since a push toward “Toxicity Testing in the 21st Century,” the field has focused on massive data generation efforts to inform computational approaches for preliminary hazard identification, adverse outcome pathways that link molecular initiating events and key events to apical outcomes, and high-throughput approaches to risk-based ratios of bioactivity and exposure to inform relative priority and safety assessment. Projects like the interagency Tox21 program and the US EPA ToxCast program have generated dose-response information on thousands of chemicals, identified and aggregated information from legacy systems, and created tools for access and analysis. The resulting information has been used to develop computational models as viable options for regulatory applications. This progress has introduced challenges in data management that are new, but not unique, to toxicology. Some of the key questions require critical thinking and solutions to promote semantic interoperability, including: (1) identification of bioactivity information from NAMs that might be related to a biological process; (2) identification of legacy hazard information that might be related to a key event or apical outcomes of interest; and, (3) integration of these NAM and traditional data for computational modeling and prediction of complex apical outcomes such as carcinogenesis. This work reviews a number of toxicology-related efforts specifically related to bioactivity and toxicological data interoperability based on the goals established by Findable, Accessible, Interoperable, and Reusable (FAIR) Data Principles. These efforts are essential to enable better integration of NAM and traditional toxicology information to support data-driven toxicology applications.}
}
@article{THEODOROPOULOS2021100335,
title = {Augmented Reality and programming education: A systematic review},
journal = {International Journal of Child-Computer Interaction},
volume = {30},
pages = {100335},
year = {2021},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2021.100335},
url = {https://www.sciencedirect.com/science/article/pii/S2212868921000544},
author = {Anastasios Theodoropoulos and George Lepouras},
keywords = {Augmented Reality (AR), Programming learning, Coding learning, CS education, Review study},
abstract = {In recent years, Augmented Reality (AR) usage in the learning process has been growing. AR tools and environments lead to a variety of positive outcomes and impacts for educational purposes. Similarly, AR is changing the learning process in the Computer Science (CS) Education domain. There are numerous studies that adopt the immersive AR technology in order to improve Computational Thinking (CT) or programming skills, in several contexts. However, there are not sufficient studies that analyze the meaningful characteristics or the advantages and disadvantages of AR in the field. In order to better understand the impact of AR in programming education we performed a systematic literature review. This review analyzes 31 studies in the field. It explores the evolution of this developing technology, the challenges and issues that AR offers and discusses how this work can benefit student learning and further research.}
}
@article{LIU2021107410,
title = {A new computational method for acquiring effect knowledge to support product innovation},
journal = {Knowledge-Based Systems},
volume = {231},
pages = {107410},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107410},
url = {https://www.sciencedirect.com/science/article/pii/S0950705121006729},
author = {Hongwei Liu and Wenqiang Li and Yan Li},
keywords = {TRIZ, Product innovation design, Effect knowledge representation, Functional Basis, IPC},
abstract = {Effect provides a scientific principle-level means for product function realization. The unexpected or new application of effects can create high-level innovations enabling products long-term technical advantages and market competitiveness. Acquiring design knowledge is the vital first step of conducting product innovation activities. In order to capture the effect knowledge that can efficiently aid high-level product innovation, this article proposes a new computational method. The method stems from a novel effect knowledge representation considering both functional and technical area features, and utilizes functional-flow terms of Functional Basis and technical area categories of international patent classification (IPC) respectively to standardize the modelling of the two kinds of features. Based on such representation, the method reasonably combines syntactic analysis, WordNet and word vector technologies to extract the desired effect knowledge from IPC text. To evaluate the method, this article first compares the acquired knowledge with those in a comprehensive human-compiled effect database, and then applies the knowledge to aid the innovation design of several mechanical products with different technical backgrounds. Evaluation results and the discussion based on them suggest the feasibility and potential of the proposed method in automatically acquiring well-organized effect knowledge system, as well as in aiding high-level product innovation.}
}
@article{ARFE2020103807,
title = {The effects of coding on children's planning and inhibition skills},
journal = {Computers & Education},
volume = {148},
pages = {103807},
year = {2020},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2020.103807},
url = {https://www.sciencedirect.com/science/article/pii/S0360131520300099},
author = {Barbara Arfé and Tullio Vardanega and Lucia Ronconi},
keywords = {Coding, Computational thinking, Executive function, Primary school children, Problem-solving},
abstract = {Computational thinking (CT) and the coding element of it are progressively entering in the primary school curriculum worldwide. Yet, little is known about the effects of these skills on children's cognitive development. In a cluster-randomized controlled trial, we examined how 1st-grade children's gains in coding skills that follow instructional intervention transfer to two important executive functions (EFs): planning and response inhibition. One-hundred seventy-nine (179) first graders from 5 schools and 10 class groups, with no prior experience of coding, were randomly assigned to an experimental (coding, 5 classes) or control (standard STEM, 5 classes) instructional condition. The experimental intervention involved 8 h of coding activities (two weekly lessons for 4 weeks), through the Code.org platform. Children in the control group were exposed to standard STEM instruction. Four coding tasks drawn from Code.org, two standardized planning tasks (Elithorn maze test and Tower of London, ToL, test) and two standardized response inhibition tasks (NEPSY-II inhibition subtest and numerical Stroop), were used to assess children's skills at the pretest and posttest (after the instructional intervention). To measure retention, the same skills were also assessed for 44 children from the experimental group 5 weeks from the posttest (follow up). The results show that practice with coding through Code.org not only improved measurably children's ability to solve coding problems, but also their EFs, increasing the time children spent planning, their ability to solve standardized planning tasks, and to inhibit prepotent responses. Such findings add to the still limited literature on the cognitive effects of coding, deepening our understanding of the positive implications of introducing Computational Thinking early in the school curriculum.}
}
@article{AGUAYO2023e19205,
title = {Ethical enactivism for smart and inclusive STEAM learning design},
journal = {Heliyon},
volume = {9},
number = {9},
pages = {e19205},
year = {2023},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2023.e19205},
url = {https://www.sciencedirect.com/science/article/pii/S2405844023064137},
author = {Claudio Aguayo and Ronnie Videla and Francisco López-Cortés and Sebastián Rossel and Camilo Ibacache},
keywords = {STEAM, Ethical enactivism, Immersive learning, Systems thinking, Planetary wellbeing},
abstract = {Current global challenges of the 21st century promote STEAM (science, technology, engineering, arts and mathematics) education and digitalization as a means for humans to be the central actors in the construction of a sustainable society that favors a sense of worth and global wellbeing. In this scenario, new educational technology tools and immersive learning affordances (possibilities), offer unprecedented potential for the design of smart and dynamic learning systems and contexts that can enhance learning processes across varied audiences and educational settings. However, current STEAM education practice lacks attention to equipping all citizens with the necessary skills to use digital technologies in an ethical, critical and creative way. This gap calls for attention in design processes, principles and practices that are attentive to ethical considerations and values-based approaches. On the other hand, in its formulation STEAM as an educational approach is framed in four fundamental pillars: creativity, inclusion, citizenship and emerging technologies, which also put attention on the inclusion of disadvantaged and underrepresented social groups during STEAM education design. Following an apparent need to explore ethical and inclusive design in STEAM education, and inspired in the 4E cognition framework, ethical enactivism and embodied and ecosomaesthetics experience design, here we propose a theoretical framework grounded on systems thinking for the design of smart and dynamic STEAM learning systems and settings. The framework is aimed at STEAM educational psychologists, educational technologists, learning designers and educational practitioners who wish to address the global challenges of 21st century education by means of creative, innovative and inclusive education design.}
}
@article{ANDERSEN2024100697,
title = {Infrastructuring digital literacy in K-12 education: A national case study},
journal = {International Journal of Child-Computer Interaction},
volume = {42},
pages = {100697},
year = {2024},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2024.100697},
url = {https://www.sciencedirect.com/science/article/pii/S2212868924000667},
author = {Lars Bo Andersen and Ditte Amund Basballe and Lillian Buus and Christian Dindler and Thomas Illum Hansen and Mikkel Hjorth and Ole Sejer Iversen and Christian Mosbæk Johannessen and Katrine Holm Kanstrup and Rasmus Fink Lorentzen and Morten Misfeldt and Line Have Musaeus and Camilla Balslev Nielsen and Marianne Graves Petersen and Vibeke Schrøder and Marie Falkesgaard Slot},
keywords = {Technology comprehension, Digital literacy, Infrastructuring, Strategy},
abstract = {While much CCI research has dealt with the educational challenge of providing children with knowledge and skills for a digital society, little work has dealt with the strategic challenge of developing and implementing a digital literacy subject in K-12 education. In this paper, we explore how to develop, implement, and sustain a national program on technology comprehension by analyzing the newly established Danish knowledge center for digital technology comprehension. We draw on the concept of infrastructuring to shed light on how to create and sustain the social, material, political and organizational structures that form the basis for introducing the new national initiative. Based on our case, we distill seven propositions that describe more generally how to work strategically with this challenge.}
}
@article{WINTER20181,
title = {The art of the Wunderlich cube and the development of spatial abilities},
journal = {International Journal of Child-Computer Interaction},
volume = {18},
pages = {1-7},
year = {2018},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2018.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S2212868917301010},
author = {Victor Winter and Betty Love and Cindy Corritore},
keywords = {Spatial reasoning, Mathematical analysis, Coding, 3D printing},
abstract = {This paper advocates for a future where the teaching of math and art are harmoniously intertwined as they were in the days of da Vinci. In this future, code provides the “brush” that enables the expression of artistic ideas and mathematical structures in digital and digitally-fabricated mediums. This educational idea is motivated by (1) literature supporting the position that visual thinking and spatial reasoning significantly impact STEAM disciplines, and (2) Piaget’s theory of cognitive development in which children, in the concrete operational stage, solve problems relating to physical objects (i.e., they learn-by-making). A project is then described involving the creation of a 3D artifact we call a Wunderlich cube — a mathematical artifact that embodies numerous spatial reasoning puzzles. An understanding of the properties of the Wunderlich cube is developed through manual construction using LEGO®, mathematical analysis, computational thinking, coding, and 3D printing.}
}
@article{PRADO2019727,
title = {Towards an Extensible Architecture for Ideation},
journal = {Procedia Computer Science},
volume = {159},
pages = {727-735},
year = {2019},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 23rd International Conference KES2019},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.09.228},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919314140},
author = {Hércules A. do Prado and Elaine Coutinho Marcial and Aluizio Haendchen Filho and Edilson Ferneda and Roseane Salvio},
keywords = {design thinking, ideation, debates, sentiment analysis, foresight, Studies of Future},
abstract = {Ideation is an important activity of Design Thinking, a process that may benefit from different levels of automation in its activities. Preceded by Immersion and Analysis activities, Ideation can be enhanced by computational approaches like debate synthesis and mediation, sentiment analysis, and so on. In this paper an architecture for an extensible platform for ideation is addressed. Initially, it comprises a set of components to cope with those functionalities. The extensibility of this platform is in the sense that it shall allow the inclusion of new components like creation of domain ontologies for integration of different studies in the same domain. The general purpose of this platform is to support the creation of ideas by (i) constructing consensus among specialists and (ii) managing dissents in order to keep in track of marginal ideas that can become dominant ones as the discussion advances. It can be applied to many fields, like innovation, Studies of Future, definition of complex diagnoses, etc. Actually, this proposal came up from an experience with a study of future in which some experts had tried to envision trends for some years ahead in order to propose strategic actions for reaching a desired status for Brazil as a successful, fair, and inclusive country. The proposal includes an open and interactive computational environment to enable (i) structuring debates in threads of discussion; (ii) the gathering of ideas about topics of interest; (iii) the debate on the ideas put forward in order to identify the most relevant ones; (iv) synthesis of a debate (anytime summarization); (v) identification of the prevailing sentiments in a debate; and (vi) identification of variables relevant for the sake of the debate target.}
}
@article{KITA202021,
title = {Computational design of generalized centrifugal puzzles},
journal = {Computers & Graphics},
volume = {90},
pages = {21-28},
year = {2020},
issn = {0097-8493},
doi = {https://doi.org/10.1016/j.cag.2020.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S009784932030056X},
author = {Naoki Kita and Takafumi Saito},
keywords = {Computational design, Digital fabrication, Puzzles},
abstract = {Mechanical puzzles have fascinated many people for a long time. While some puzzles require complex procedures to solve, there are puzzles that can be solved easily if the solver understands the underlying mechanism. In this paper, we focus on mechanical puzzles that can be solved by spin such that centrifugal force is applied to the internal mechanical core to unlock the locked state. While traditional centrifugal puzzles are limited to simple shapes, we propose a computational design method to generalize such puzzles by embedding the mechanical core into 3D models. We parameterize the internal core mechanism and optimize the design under several design constraints, and we generate a support structure that helps users solve puzzles easily because generalized puzzles cannot always be spun steadily and easily due to complex surfaces and non-flat contact areas. Additionally, we embed multiple cores into a model. To solve a multi-core puzzle, the user must follow certain orders to unlock each locking mechanism. We fabricate a variety of designed puzzles and demonstrate whether they can be physically unlocked.}
}
@article{BAGGIO2020100005,
title = {Computational modelling and simulations in tourism: A primer},
journal = {Annals of Tourism Research Empirical Insights},
volume = {1},
number = {1},
pages = {100005},
year = {2020},
issn = {2666-9579},
doi = {https://doi.org/10.1016/j.annale.2020.100005},
url = {https://www.sciencedirect.com/science/article/pii/S2666957920300057},
author = {Rodolfo Baggio},
keywords = {Complex systems, Modelling, Simulations, Numerical and computational methods},
abstract = {The aim of this contribution is to briefly sketch and discuss the main issues that concern the activities of modelling and simulating complex phenomena and systems. The focus is on numerical and computational techniques. We discuss the validity of these methods and examine the different steps to be taken for ensuring a correct, accurate and reliable implementation. The approach is essentially of general methodological nature, regardless of specific techniques or tools.}
}
@article{MOTOMURA20103,
title = {Multi-aspect data analysis for investigating human computation mechanism},
journal = {Cognitive Systems Research},
volume = {11},
number = {1},
pages = {3-15},
year = {2010},
note = {Brain Informatics},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2008.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S1389041708000521},
author = {Shinichi Motomura and Ning Zhong},
keywords = {Multi-aspect data analysis, Brain informatics methodology, Human computation mechanism, EEG and fMRI},
abstract = {In the paper, we present a multi-aspect data analysis approach for investigating human computation mechanism. Multi-aspect analysis in multiple human brain data sources is an important methodology in Brain Informatics, which emphasizes on a systematic way for investigating human information processing mechanisms, including measuring, collecting, modeling, transforming, managing, and mining multiple human brain data obtained from various cognitive experiments by using powerful equipments, such as fMRI and EEG. After giving an outline of Brain Informatics methodology, we describe how to design cognitive experiments of mental arithmetic task with multiple difficulty levels for obtaining multiple EEG and fMRI data sources, and how to analyze such data for investigating the spatiotemporal characteristics and flow of human computation processing. Such an investigation can be regarded as a case study using Brain Informatics methodology. Experimental results show the usefulness of our approach.}
}
@article{VANCOUVER201456,
title = {Change one can believe in: Adding learning to computational models of self-regulation},
journal = {Organizational Behavior and Human Decision Processes},
volume = {124},
number = {1},
pages = {56-74},
year = {2014},
issn = {0749-5978},
doi = {https://doi.org/10.1016/j.obhdp.2013.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S0749597813001180},
author = {Jeffrey B. Vancouver and Justin M. Weinhardt and Ronaldo Vigo},
keywords = {Computational model, Motivation, Dynamics, Learning},
abstract = {Theories of self-regulation describe motivation as a dynamic process of goal choice and goal striving. To facilitate those processes, individuals learn about themselves and their environment, which is an internal dynamic process. However, the precise nature of the relationship between these learning and motivational processes is not well specified. This article integrates formal models of learning, goal choice, and goal striving using a single information processing structure found in self-regulatory models of motivation. Results from two published studies (DeShon and Rench, 2009, Schmidt and DeShon, 2007) validate the model. In both cases, the integrated model accounts for findings that previous theories of self-regulation could not explain. Discussion focuses on additional tests to validate the model and on the value of incorporating formal models from the cognitive, learning, and motivational literatures to account for behavior in complex settings and over time.}
}
@article{MULATTI2023100040,
title = {Perceived lack of control promotes creativity},
journal = {Journal of Creativity},
volume = {33},
number = {1},
pages = {100040},
year = {2023},
issn = {2713-3745},
doi = {https://doi.org/10.1016/j.yjoc.2022.100040},
url = {https://www.sciencedirect.com/science/article/pii/S2713374522000231},
author = {Claudio Mulatti and Barbara Treccani},
keywords = {Creativity, Divergent creativity, Lack of control, Compensatory control theory, Semantic control},
abstract = {The sense of lack of control has been shown to foster illusory pattern perception, superstition, conspiracy and religious beliefs. In two identical experiments we investigated whether the feeling of lacking control (vs. control) can also foster creative thinking, which we operationalized as the ability to produce associative and dissociative combinations of either related and unrelated concepts. Participants were asked to think about an incident in their life wherein they felt either to be in control or to lose control of the situation. Immediately afterwards, they had to perform a set of tasks tapping (divergent) creative thinking. In both experiments, we observed higher scores in all creativity tasks for participants who recalled loss-of-control events than for those recalling in-control events. Our findings suggest that compensatory processes, triggered by experiencing lack of control, can promote divergent thinking. We propose an account situated within current models of semantic control.}
}
@article{KHATUN2023127163,
title = {A combined experimental and computational approach on La0.6Sr0.4MnO3 perovskite},
journal = {Materials Chemistry and Physics},
volume = {295},
pages = {127163},
year = {2023},
issn = {0254-0584},
doi = {https://doi.org/10.1016/j.matchemphys.2022.127163},
url = {https://www.sciencedirect.com/science/article/pii/S0254058422014699},
author = {Mst Romana Khatun and Md Khadimul Islam and Monira Jannatul Kobra and Yuji Inagaki and Rajia Sultana and Md Abdur Razzaque Sarker and Md Saiful Islam},
keywords = {Sr-doped La manganite, XRD, Magnetization, Resistivity, Energy dispersion, Thermal properties},
abstract = {We synthesize a high quality Sr-doped lanthanum manganite using solid state reaction route to investigate the various properties for device applications. The crystal structure of the synthesized perovskite was studied by X-ray diffraction (XRD) pattern and also compared with the crystallographic data obtained from the simulation calculations. The magnetization as a function of applied magnetic field and temperature of La0.6Sr0.4MnO3 exhibits ferromagnetic metal phase with the Curie temperature of 361 K. The electrical resistivity with temperature unexpectedly shows semiconducting behavior due to the intergrain effects. On the other hand, the energy dispersion studied by first principles calculations based on density functional theory (DFT) demonstrates metallic conduction in conformity with the available experimental results. No energy gap in the absorption spectrum done by UV–Visible spectrophotometer of this manganite also confirms the nature of identical conductivity. Finally, a quasi-harmonic Debye model was employed to calculate the thermal characteristics like Debye temperature, specific heat capacities, volume expansion coefficient, etc. in this LSMO perovskite.}
}
@article{COONS2015126,
title = {Grease pencils and the persistence of individuality in computationally produced custom objects},
journal = {Design Studies},
volume = {41},
pages = {126-136},
year = {2015},
note = {Special Issue: Computational Making},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2015.08.005},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X15000599},
author = {ginger “all-lower-case” coons and Matt Ratto},
keywords = {computer aided design, design tools, participatory design, social design, computational craft},
abstract = {This article explores the relationship between an established craft production method and a computational adaptation of that method. In looking at a specific tool, the grease pencils used in the fitting and production of prosthetic limbs, we examine the ways in which complexity, tacit understandings, and human movement are translated into a collection of variables and considerations manipulable in a digital environment. We discuss, briefly, the persistent individuality of objects like prosthetic sockets, and the ways in which their materiality and necessarily custom nature push back against assumptions that computational production is generalizing, disembodied, and abstract.}
}
@article{KARSAKOV2015730,
title = {Improving Visualization Courses in Russian Higher Education in Computational Science and High Performance Computing},
journal = {Procedia Computer Science},
volume = {66},
pages = {730-739},
year = {2015},
note = {4th International Young Scientist Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.11.083},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915034328},
author = {Andrey Karsakov and Anna Bilyatdinova and Alexey Bezgodov},
keywords = {Visualization course, Computational science, Higher education},
abstract = {In order to keep up with the fast-paced and widespread technologies and applications of visualization, worldwide education community is actively implementing visualization courses in curricula of undergraduate and graduate programs. A study of the state of the art in the teaching visualization in Russian higher education shows the necessity to improve the quality and breadth of knowledge of the visualization courses. In this paper we propose our approach to overcome the national and historical challenges in teaching visualization in Russian STEM higher education on the example of Computational Science and High Performance Computing double degree Master's programs in ITMO University. We offer a smooth transition to the modern relevant syllabus content by presenting two courses’ designs with same width but with various depth in knowledge that should to be studied. At the end of the paper we give some discussions about future works in development visualization courses in Russia.}
}
@article{LU2025103764,
title = {Detection of structural-functional coupling abnormalities using multimodal brain networks in Alzheimer’s disease: A comparison of three computational models},
journal = {NeuroImage: Clinical},
volume = {46},
pages = {103764},
year = {2025},
issn = {2213-1582},
doi = {https://doi.org/10.1016/j.nicl.2025.103764},
url = {https://www.sciencedirect.com/science/article/pii/S2213158225000348},
author = {Yinping Lu and Luyao Wang and Toshiya Murai and Jinglong Wu and Dong Liang and Zhilin Zhang},
keywords = {Structural-functional coupling, Statistical model, Communication model, Graph harmonic model, Brain network, Alzheimer’s disease, Multimodal MRI},
abstract = {Alzheimer’s disease (AD) is a progressive neurodegenerative disorder characterized by the disconnection of white matter fibers and disrupted functional connectivity of gray matter; however, the pathological mechanisms linking structural and functional changes remain unclear. This study aimed to explore the interaction between the structural and functional brain network in AD using advanced structural–functional coupling (S-F coupling) models to assess whether these changes correlate with cognitive function, Aβ deposition levels, and gene expression. In this study, we utilized multimodal magnetic resonance imaging data from 41 individuals with AD, 112 individuals with mild cognitive impairment, and 102 healthy controls to explore these mechanisms. We applied different computational models to examine the changes in the S-F coupling associated with AD. Our results showed that the communication and graph harmonic models demonstrated greater heterogeneity and were more sensitive than the statistical models in detecting AD-related pathological changes. In addition, S-F coupling increases with AD progression at the global, subnetwork, and regional node levels, especially in the medial prefrontal and anterior cingulate cortices. The S-F coupling of these regions also partially mediated cognitive decline and Aβ deposition. Furthermore, gene enrichment analysis revealed that changes in S-F coupling were strongly associated with the regulation of cellular catabolic processes. This study advances our understanding of the interaction between structural and functional connectivity and highlights the importance of S-F coupling in elucidating the neural mechanisms underlying cognitive decline in AD.}
}
@incollection{COUCLELIS2009245,
title = {Computational Human Geography},
editor = {Rob Kitchin and Nigel Thrift},
booktitle = {International Encyclopedia of Human Geography},
publisher = {Elsevier},
address = {Oxford},
pages = {245-250},
year = {2009},
isbn = {978-0-08-044910-4},
doi = {https://doi.org/10.1016/B978-008044910-4.00669-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780080449104006696},
author = {H. Couclelis},
keywords = {Agent-based models, Cellular automata, Geocomputation, Geo(infor)matics, GIS, Location-based services, Models/modeling, Public participation GIS, Spatial analysis},
abstract = {Computational human geography refers to the use of computational methods and techniques to solve problems in human geography research and applications. Geographic information systems (GIS) and science are a big part of computational human geography but the notion is considerably broader, encompassing spatial process modeling and simulation, the modeling of spatial decision and behavior, visualization techniques, most aspects of spatial analysis, and an increasing number of other areas. Computation in human geography goes back to the beginnings of the quantitative revolution and is philosophically related though methodologically distinct from it. Two major thrusts have persisted through the years: the use of numerical techniques to solve large, complex quantitative problems; and the development of models of complex spatial processes expressed directly in computational terms. Typical exponents of the latter kinds of applications are cellular automata models of urban and environmental processes, and agent-based models of spatial decision and behavior. More recent developments involve applications of mobile and portable computing. Critiques of computational human geography originate from both within the field and from the humanistic and social theory perspectives. The former address a number of epistemological and methodological problems while the latter tend to focus on issues of ontology and representation.}
}
@article{PETZSCHNER2017421,
title = {Computational Psychosomatics and Computational Psychiatry: Toward a Joint Framework for Differential Diagnosis},
journal = {Biological Psychiatry},
volume = {82},
number = {6},
pages = {421-430},
year = {2017},
note = {Computational Psychiatry},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2017.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0006322317315846},
author = {Frederike H. Petzschner and Lilian A.E. Weber and Tim Gard and Klaas E. Stephan},
keywords = {Allostasis, Cybernetics, Hierarchical Bayesian model, Homeostasis, Inference, Metacognition, Prediction error},
abstract = {This article outlines how a core concept from theories of homeostasis and cybernetics, the inference-control loop, may be used to guide differential diagnosis in computational psychiatry and computational psychosomatics. In particular, we discuss 1) how conceptualizing perception and action as inference-control loops yields a joint computational perspective on brain-world and brain-body interactions and 2) how the concrete formulation of this loop as a hierarchical Bayesian model points to key computational quantities that inform a taxonomy of potential disease mechanisms. We consider the utility of this perspective for differential diagnosis in concrete clinical applications.}
}
@article{REN2021105428,
title = {Computational fluid dynamics simulation of adsorption process in a liquid-solids fluidized bed},
journal = {Journal of Environmental Chemical Engineering},
volume = {9},
number = {4},
pages = {105428},
year = {2021},
issn = {2213-3437},
doi = {https://doi.org/10.1016/j.jece.2021.105428},
url = {https://www.sciencedirect.com/science/article/pii/S221334372100405X},
author = {Panfeng Ren and Wenbin Li and Kuotsung Yu},
keywords = {Liquid-solids fluidized bed (LSFB), Computational fluid dynamics (CFD) simulation, Turbulent mass diffusivity, Hydrodynamics, Protein adsorption},
abstract = {For simultaneously predicting the hydrodynamics and protein adsorption process in a liquid-solids fluidized bed (LSFB), a computational fluid dynamics (CFD) model is established by combining the two-fluid model (TFM) for the liquid-particles two-phase fluidization system with the c2¯−εc model for the turbulent mass transfer. In terms of hydrodynamics, the kl−εl−kp−εp−Θ equations are adopted to describe phases turbulence. The simulations of hydrodynamics using various drag models and different modelling parameters are conducted to test their sensitivity. Then for the adsorption process, the recently developed formulations of the c2¯−εc model are adopted to characterize rigorously the turbulent mass diffusion in LSFB. With the proposed model, the velocity field as well as the concentration field can be acquired. Simulated results are compared with the experimental data and a good agreement between them is found.}
}
@article{FLETCHER1998747,
title = {Computational fluid dynamics modelling of an entrained flow biomass gasifier},
journal = {Applied Mathematical Modelling},
volume = {22},
number = {10},
pages = {747-757},
year = {1998},
issn = {0307-904X},
doi = {https://doi.org/10.1016/S0307-904X(98)10025-2},
url = {https://www.sciencedirect.com/science/article/pii/S0307904X98100252},
author = {D.F. Fletcher and B.S. Haynes and J. Chen and S.D. Joseph},
abstract = {A mathematical model, based on the Computational Fluid Dynamics package CFX4, has been developed to study the flow within an entrained flow biomass gasifier. The gasifier is designed to convert sawdust and chopped cotton gin trash into a low calorific value gas which can be burned in a modified engine to run a generator. Calculations of the flowfield are performed using the standard k–ϵ model and a Differential Reynolds Stress Model (DSM). In line with current thinking, it is shown that the k–ϵ model gives unphysical results for complex swirling flows, whereas the DSM model performs well. Particle tracking was performed to determine typical trajectories for the biomass and char and the results used to determine means of avoiding slagging in the gasifier base. The simulations have proved to be very useful to the designers who are now using the model to optimise the design.}
}
@article{KUMAR20223122,
title = {Experimental Spectroscopic, Quantum Computational, Hirshfeld Surface, Molecular Docking, and Electronic Excitation Studies on an Antibiotic Agent: SDZ},
journal = {Polycyclic Aromatic Compounds},
volume = {43},
number = {4},
pages = {3122-3146},
year = {2022},
issn = {1040-6638},
doi = {https://doi.org/10.1080/10406638.2022.2063909},
url = {https://www.sciencedirect.com/science/article/pii/S1040663822010284},
author = {Mukesh Kumar and Aysha Fatima and Meenakshi Singh and Indresh Verma and Ghazala Khanum and S. Muthu and Khaled Althubeiti and khamael M. Abualnaja and Musheer Ahmad and Nazia Siddiqui and Saleem Javed},
keywords = {DFT, NBO, EDD and HDD, molecular docking},
abstract = {In this report sulfadiazine (SDZ) has been experimentally and quantum chemically investigated. Computational analysis was carried out theoretically using the density functional theory (DFT) approach/B3LYP and 6-311++G(d,p) level to obtain optimized geometrical structure and vibrational modes analysis and other various calculations. A detailed description of the intermolecular interactions of the crystal surface were carried out by means of Hirshfeld surface analysis and fingerprint plots. Exploration of electron excitation from occupied to unoccupied orbitals in a single electron pair occurs, with dimethyl sulfoxide (DMSO) and MeOH as solvents and electron density distribution (EDD) and hole density distribution (HDD) maps were drawn in an excited state. The molecule reactivity region MEP, molecular stability, natural bond orbital (NBO), HOMO–LUMO, dipole moment (μ), polarizability (α), and hyperpolarizability (β) nonlinear optical (NLO), have all been taken into account. NBO analysis was carried out and the hybridization of atoms that form bonds was evaluated. The charge transfer of the title molecule has been examined by TD-DFT method The UV–Vis spectrum was obtained by employing the TDDFT/PCM method and compared with experimental spectra. Calculated HOMO→LUMO energy gap and charge transfer in the molecule was investigated. Chemical descriptors indicate the reactivity of the molecule as a whole, and Fukui function calculations were used to examine the reactive locations of the compound. The electrophilicity index was calculated and the bio-activity of the molecule was studied. However, biological research like drug-likeness and molecular docking are also done on the molecule.}
}
@article{FINDLAY1988165,
title = {Thinking creatively about creative thinking},
journal = {Journal of Social and Biological Structures},
volume = {11},
number = {1},
pages = {165-175},
year = {1988},
issn = {0140-1750},
doi = {https://doi.org/10.1016/0140-1750(88)90059-0},
url = {https://www.sciencedirect.com/science/article/pii/0140175088900590},
author = {C.Scott Findlay and Charles J. Lumsden}
}
@article{VARGASROJAS2022110093,
title = {Prescriptive comprehensive approach for the engineering of products made with composites centered on the manufacturing process and structured design methods: Review study performed on filament winding},
journal = {Composites Part B: Engineering},
volume = {243},
pages = {110093},
year = {2022},
issn = {1359-8368},
doi = {https://doi.org/10.1016/j.compositesb.2022.110093},
url = {https://www.sciencedirect.com/science/article/pii/S1359836822004693},
author = {Erik Vargas-Rojas},
keywords = {Composites thinking, Design method, Filament winding, Product engineering, TRIZ},
abstract = {At first, this research seeks to develop the technology required to fabricate two surfaces of revolution via filament winding: a concavity and a convexity. Concerning mandrels technology, the detachable mandrel concept is chosen among others. Their engineering is conducted conventionally based on free-thinking design approaches, as well as expertise and overconfidence. Consequently, the demoulding process of the concave surface is inefficient due to the lack of adequate dismantling functions of the respective mandrel, leading to damage of the composite material during demoulding, mandrel rework and delays. These inconveniences motivated a reexamination of the mandrels design process. Thus, three structured design methods were incorporated: Design for Manufacturing and Assembly (DFMA), Functional Analysis (FA) and Theory of Inventive Problem Solving (TIPS, a.k.a. TRIZ). Their synergistic implementation allowed the correct demoulding of the concave surface of revolution. In a second stage, this experience serves as reference for proposing a comprehensive, iterative, prescriptive and unified approach aimed at filament-wound products. It focuses on the base material (composites) and the manufacturing process (filament winding), being applicable to other fabrication processes of composite products. It is based on three models reported in the literature: one for metals, one for composites and one for filament-wound composites. Each of its steps is carried out with well-known structured design methods employed in products and systems engineering, including but not limited to DFMA, FA and TRIZ. As regards results, at the level of the design problem of the mandrels, the importance of the correct establishment of mechanical functions – dismantling in this case – is observed. In particular, how the lack of demoulding functions impacts unfavorably the quality of the final product. As respects the comprehensive approach, a significant outcome is the “filament winding thinking,” as an evolution of other schemes such as “metals” or “composites thinking” followed to generically develop products according to the nature of their base material.}
}
@incollection{SEJNOWSKI2015480,
title = {Computational Neuroscience},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {480-484},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.55011-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868550119},
author = {Terrence J. Sejnowski},
keywords = {Algorithms, Computational models, Neural systems},
abstract = {The goal of computational neuroscience is to understand how brains generate behaviors using computational approaches. Computational models of the brain explore how populations of highly interconnected neurons are formed during development and how they represent, process, store, act upon, and become altered by information present in the body and the environment. Techniques from physics, computer science, and mathematics are used to simulate and analyze these computational models and provide links between the wide range of levels that brains are investigated, from molecular interactions to large-scale systems. Models are also used for interpreting experimental data and providing a conceptual framework for the dynamical properties of neural systems, which should lead to more comprehensive theories of brain function.}
}
@article{ESCOLAGASCON2022e11303,
title = {'Feeling' or 'sensing' the future? Testing for anomalous cognitions in clinical versus healthy populations},
journal = {Heliyon},
volume = {8},
number = {11},
pages = {e11303},
year = {2022},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2022.e11303},
url = {https://www.sciencedirect.com/science/article/pii/S2405844022025919},
author = {Álex Escolà-Gascón and Abigail C. Wright and James Houran},
keywords = {Boundary functioning, Emotional intelligence, Parapsychology, Premonitions, Schizophrenia, Thinking styles},
abstract = {In the study and treatment of psychosis, emotional intelligence (EI) and thinking styles are important patient characteristics for successful outcomes in clinical intervention. Anticipation of unpredictable stimuli (AUS) may be understood as an anomalous perception and anomalous cognition in which an individual supposedly senses and recognizes future stimuli in an unexpected way, also referred to as “hunches or premonitions.” This examined the roles of EI and thinking styles in AUSs in convenience samples of healthy participants (n = 237) versus patients diagnosed with psychosis (n = 118). We adjusted several quadratic and exponential regression models according to the obtained functions. Group means were also compared to examine differences in EI scores for participants with psychosis compared to healthy participants. In the healthy group, EI predicted AUSs with a weight between 42% and 58%. Thinking styles were not correlated with AUSs. However, EI was not correlated with AUSs in the clinical group. Patients with psychosis tended to score higher on AUSs and lower on EI and thinking styles compared to participants in the healthy group. We discuss EI as a variable that can contextualize some anomalous perceptions which are otherwise difficult to classify or measure within the classic psychosis continuum model.}
}
@incollection{WARE2004351,
title = {Chapter 11 - Thinking with visualizations},
editor = {Colin Ware},
booktitle = {Information Visualization (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {351-386},
year = {2004},
series = {Interactive Technologies},
isbn = {978-1-55860-819-1},
doi = {https://doi.org/10.1016/B978-155860819-1/50014-5},
url = {https://www.sciencedirect.com/science/article/pii/B9781558608191500145},
author = {Colin Ware},
abstract = {Publisher Summary
The best visualizations are not static images to be printed in books, but fluid, dynamic artifacts that respond to the need for a different view or for more detailed information. Visualization can be an interface to a simulation of a complex system; the visualization, combined with the simulation, can create a powerful cognitive augmentation. The visualization is a two-way interface, although highly asymmetric, with far higher bandwidth communication from the machine to the human than in the other direction. The high-bandwidth visualization channel is then used to deliver the results of modeling exercises and database searches. One way to approach the design of an information system is to consider the cost of knowledge. The result of this approach is a kind of cognitive information economics. Activities are analyzed according to the value of what is gained and the cost incurred. There are two kinds of costs: resource costs and opportunity costs. The chapter explores both of these and the economics of cognition and the cognitive cost of knowledge.}
}
@article{ANDREWS2019102188,
title = {Black hole as a model of computation},
journal = {Results in Physics},
volume = {13},
pages = {102188},
year = {2019},
issn = {2211-3797},
doi = {https://doi.org/10.1016/j.rinp.2019.102188},
url = {https://www.sciencedirect.com/science/article/pii/S2211379719304036},
author = {G.R. Andrews},
keywords = {Black hole computation, Kerr/CFT correspondence, Holographic principle, Information theory, Gamma-ray spectroscopy, Shannon entropy},
abstract = {This paper focuses on an alternative, more physically realistic model of computation than Etesi and Németi’s relativistic computer in a Malament-Hogarth spacetime (2002) that uses the black hole itself combined with an external observer equipped with a source and some method of measurement of gamma-rays, as opposed to sending a classical computer into a black hole and exploiting the properties of the spacetime to achieve hypercomputation. The source of output, Hawking radiation, is considered along with the constraints imposed by the holographic principle which limit the number of degrees of freedom in the system and consequently the maximum usable information. The Bekenstein-Hawking entropy is converted from the traditional form in terms of the horizon area to that of the Shannon entropy, establishing an analogy between the physical and computational perspectives of the system. Next examples are considered to establish the approximate order of the necessary excitation energy and the resulting gamma-ray interactions which form the input from the observer. Finally, the Turing completeness of the language for this model is considered through a simulation of the Turing machine. The goal is to introduce a model of computation that can later be used to study the relationship between computability and physical systems.}
}
@article{JAY201976,
title = {Intensional computation with higher-order functions},
journal = {Theoretical Computer Science},
volume = {768},
pages = {76-90},
year = {2019},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2019.02.016},
url = {https://www.sciencedirect.com/science/article/pii/S0304397519301227},
author = {Barry Jay},
keywords = {Intensional computation, Higher-order functions, SF-calculus, Foundations of computation},
abstract = {Intensional computations are those that query the internal structure of their arguments. In a higher-order setting, such queries perform program analysis. This is beyond the expressive power of traditional term rewriting systems, such as lambda-calculus or combinatory logic, as they are extensional. In such settings it has been necessary to encode or quote the program before analysis. However, there are intensional calculi, specifically confluent term rewriting systems, that can analyse higher-order programs within the calculus proper, without quotation; there are even programs that produce the Goedel numbers of their program argument. This paper summarizes the current situation. Highlights include the following observations. We have known since 2011 that the simplest intensional calculus, SF-calculus, supports arbitrary queries of closed normal forms, including equality, pattern-matching, searching and self-interpretation. Recent work, verified using the Coq proof assistant, has shown that all recursive programs can be represented as closed normal forms in SF-calculus, and even in combinatory logic. Thus, we can here deduce that SF-calculus (but not combinatory logic) can define queries of programs. These results are compatible with direct support for lambda-abstraction. Although these results conflict with the traditional understanding of expressive power of combinatory logic and λ-calculus, as developed by Church and Kleene, our recent publication has shown that their approach is compromised by its reliance on encodings. To drive the point home, this paper uses a non-standard encoding to lambda-define a trivial solution of the Halting Problem.}
}
@article{DEV2015232,
title = {Unsolved problems in biology—The state of current thinking},
journal = {Progress in Biophysics and Molecular Biology},
volume = {117},
number = {2},
pages = {232-239},
year = {2015},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2015.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S0079610715000115},
author = {Sukhendu B. Dev},
keywords = {Unsolved biological problems, Millennium Prize, Origin of life},
abstract = {Many outstanding problems have been solved in biology and medicine for which scientists have been awarded prestigious prizes including the Nobel Prize, Lasker Award and Breakthrough Prizes in life sciences. These have been the fruits of years of basic research. From time to time, publications have appeared listing “unsolved” problems in biology. In this article, I ask the question whether it is possible to have such a list, if not a unique one, at least one that is analogous to the Millennium Prize in mathematics. My approach to finding an answer to this question was to gather views of leading biologists. I have also included my own views. Analysis of all the responses received over several years has convinced me that it is difficult, but not impossible, to have such a prize. Biology is complex and very interdisciplinary these days at times involving large numbers of teams, unlike mathematics, where Andrew Wiles spent seven years in complete isolation and secrecy solving Fermat's last theorem. Such an approach is simply not possible in biology. Still I would like to suggest that a similar prize can be established by a panel of distinguished scientists. It would be awarded to those who solved one of the listed problems in biology that warrant a verifiable solution. Despite many different opinions, I found that there is some commonality in the responses I received – I go on to discuss what these are and how they may impact future thinking.}
}
@article{GURSOY201529,
title = {Visualizing making: Shapes, materials, and actions},
journal = {Design Studies},
volume = {41},
pages = {29-50},
year = {2015},
note = {Special Issue: Computational Making},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2015.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X15000617},
author = {Benay Gürsoy and Mine Özkar},
keywords = {material computing, computational models, design activity, parametric design, reasoning},
abstract = {The increasing interest in materiality currently challenges the long existing traditions that consider visual thinking as the primary actor in design creativity. Shape grammars offer a formalism to represent visual reasoning in design, which is never purely limited to the visual aspects of design processes. Aiming to develop ways to explicitly include material manipulation in a computational formalism, we report on an ongoing exploration of how shape computation extends beyond abstract visual shapes to incorporate material shapes that have a physical existence. We present a materially informed process with shape rules and show that we can apply these rules creatively to explore the physical character of the material.}
}
@article{VELUPILLAI201440,
title = {Computable and computational complexity theoretic bases for Herbert Simon’s cognitive behavioral economics},
journal = {Cognitive Systems Research},
volume = {29-30},
pages = {40-52},
year = {2014},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2013.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S1389041713000405},
author = {K.Vela Velupillai and Ying-Fang Kao},
keywords = {Bounded rationality, Satisficing, Heuristics, Computability, Computational complexity},
abstract = {This paper aims to interpret and formalize Herbert Simon’s cognitive notions of bounded rationality, satisficing and heuristics in terms of computability theory and computational complexity theory. Simon’s theory of human problem solving is analyzed in the light of Turing’s work on Solvable and Unsolvable Problems. It is suggested here that bounded rationality results from the fact that the deliberations required for searching computationally complex spaces exceed the actual complexity that human beings can handle. The immediate consequence is that satisficing becomes the general criterion of decision makers and heuristics are the procedures used for achieving their goals. In such decision problems, it is demonstrated that bounded rationality and satisficing are more general than orthodox, non-cognitive, Olympian rationality and optimization, respectively, and not the other way about.}
}
@article{MAYER2017107,
title = {Understanding scientists’ computational modeling decisions about climate risk management strategies using values-informed mental models},
journal = {Global Environmental Change},
volume = {42},
pages = {107-116},
year = {2017},
issn = {0959-3780},
doi = {https://doi.org/10.1016/j.gloenvcha.2016.12.007},
url = {https://www.sciencedirect.com/science/article/pii/S0959378016306197},
author = {Lauren A. Mayer and Kathleen Loa and Bryan Cwik and Nancy Tuana and Klaus Keller and Chad Gonnerman and Andrew M. Parker and Robert J. Lempert},
keywords = {Values-informed mental models, Climate change, Risk management, Decision making under uncertainty},
abstract = {When developing computational models to analyze the tradeoffs between climate risk management strategies (i.e., mitigation, adaptation, or geoengineering), scientists make explicit and implicit decisions that are influenced by their beliefs, values and preferences. Model descriptions typically include only the explicit decisions and are silent on value judgments that may explain these decisions. Eliciting scientists’ mental models, a systematic approach to determining how they think about climate risk management, can help to gain a clearer understanding of their modeling decisions. In order to identify and represent the role of values, beliefs and preferences on decisions, we used an augmented mental models research approach, namely values-informed mental models (ViMM). We conducted and qualitatively analyzed interviews with eleven climate risk management scientists. Our results suggest that these scientists use a similar decision framework to each other to think about modeling climate risk management tradeoffs, including eight specific decisions ranging from defining the model objectives to evaluating the model’s results. The influence of values on these decisions varied between our scientists and between the specific decisions. For instance, scientists invoked ethical values (e.g., concerns about human welfare) when defining objectives, but epistemic values (e.g., concerns about model consistency) were more influential when evaluating model results. ViMM can (i) enable insights that can inform the design of new computational models and (ii) make value judgments explicit and more inclusive of relevant values. This transparency can help model users to better discern the relevance of model results to their own decision framing and concerns.}
}
@article{HUIJSER2018170,
title = {The wandering self: Tracking distracting self-generated thought in a cognitively demanding context},
journal = {Consciousness and Cognition},
volume = {58},
pages = {170-185},
year = {2018},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2017.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S1053810017301927},
author = {Stefan Huijser and Marieke K. {van Vugt} and Niels A. Taatgen},
keywords = {Self-generated thought, Mind wandering, Self-referential processing, Task demand, Computational cognitive modeling, Eye-tracking},
abstract = {We investigated how self-referential processing (SRP) affected self-generated thought in a complex working memory task (CWM) to test the predictions of a computational cognitive model. This model described self-generated thought as resulting from competition between task- and distracting processes, and predicted that self-generated thought interferes with rehearsal, reducing memory performance. SRP was hypothesized to influence this goal competition process by encouraging distracting self-generated thinking. We used a spatial CWM task to examine if SRP instigated such thoughts, and employed eye-tracking to examine rehearsal interference in eye-movement and self-generated thinking in pupil size. The results showed that SRP was associated with lower performance and higher rates of self-generated thought. Self-generated thought was associated with less rehearsal and we observed a smaller pupil size for mind wandering. We conclude that SRP can instigate self-generated thought and that goal competition provides a likely explanation for how self-generated thoughts arises in a demanding task.}
}
@article{ALEXANDRU20221,
title = {Theories of life and computation: Special issue on the occasion of the 65th birthday of Professor Gabriel Ciobanu},
journal = {Theoretical Computer Science},
volume = {926},
pages = {1-2},
year = {2022},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2022.06.041},
url = {https://www.sciencedirect.com/science/article/pii/S0304397522004157},
author = {Andrei Alexandru and Bogdan Aman and Ross Horne}
}
@article{OERS199051,
title = {The development of mathematical thinking in school: a comparison of the action- psychological and information-processing approaches},
journal = {International Journal of Educational Research},
volume = {14},
number = {1},
pages = {51-66},
year = {1990},
issn = {0883-0355},
doi = {https://doi.org/10.1016/0883-0355(90)90016-2},
url = {https://www.sciencedirect.com/science/article/pii/0883035590900162},
author = {Bert Van Oers},
abstract = {The learning and teaching of mathematics can be analyzed from different psychological points of view. Information-processing theories focus on the various information-processing mechanisms underlying mathematical competence and try to foster the development of these mechanisms in order to stimulate the growth of mathematical competence. In the action-psychological approach of the cultural-historical school, mathematics is viewed as a kind of culturally developed human activity governed by the rules that mathematicians themselves follow while doing their job. Consequently, the development of mathematical competence is regarded as the formation of a system of meaningful mathematical actions that constitute that activity. At a general theoretical level these approaches can be shown to be basically different and even incompatible. With regard to mathematics education the differences are illustrated with respect to several themes such as task-analysis, automatization, and the learning of elementary arithmetic. Considering insight and meaningful and sophisticated problem-solving as the core of mathematical thinking, the action-psychological approach appears to be the more promising candidate as an aid in the design of future mathematics education.}
}
@article{ADAMATZKY2017469,
title = {East-West paths to unconventional computing},
journal = {Progress in Biophysics and Molecular Biology},
volume = {131},
pages = {469-493},
year = {2017},
note = {Integral Biomathics 2017: The Necessary Conjunction of Western and Eastern Thought Traditions for Exploring the Nature of Mind and Life},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2017.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0079610717301177},
author = {Andrew Adamatzky and Selim Akl and Mark Burgin and Cristian S. Calude and José Félix Costa and Mohammad Mahdi Dehshibi and Yukio-Pegio Gunji and Zoran Konkoli and Bruce MacLennan and Bruno Marchal and Maurice Margenstern and Genaro J. Martínez and Richard Mayne and Kenichi Morita and Andrew Schumann and Yaroslav D. Sergeyev and Georgios Ch. Sirakoulis and Susan Stepney and Karl Svozil and Hector Zenil},
keywords = {Unconventional computing, East, West, Spirituality},
abstract = {Unconventional computing is about breaking boundaries in thinking, acting and computing. Typical topics of this non-typical field include, but are not limited to physics of computation, non-classical logics, new complexity measures, novel hardware, mechanical, chemical and quantum computing. Unconventional computing encourages a new style of thinking while practical applications are obtained from uncovering and exploiting principles and mechanisms of information processing in and functional properties of, physical, chemical and living systems; in particular, efficient algorithms are developed, (almost) optimal architectures are designed and working prototypes of future computing devices are manufactured. This article includes idiosyncratic accounts of ‘unconventional computing’ scientists reflecting on their personal experiences, what attracted them to the field, their inspirations and discoveries.}
}
@article{NIKOLIC2023107820,
title = {Where is the mind within the brain? Transient selection of subnetworks by metabotropic receptors and G protein-gated ion channels},
journal = {Computational Biology and Chemistry},
volume = {103},
pages = {107820},
year = {2023},
issn = {1476-9271},
doi = {https://doi.org/10.1016/j.compbiolchem.2023.107820},
url = {https://www.sciencedirect.com/science/article/pii/S1476927123000117},
author = {Danko Nikolić},
keywords = {Scaling problem, Explanatory gap, Connectionism, Metabotropic receptors, G protein-gated ion channels, Practopoiesis},
abstract = {Perhaps the most important question posed by brain research is: How the brain gives rise to the mind. To answer this question, we have primarily relied on the connectionist paradigm: The brain’s entire knowledge and thinking skills are thought to be stored in the connections; and the mental operations are executed by network computations. I propose here an alternative paradigm: Our knowledge and skills are stored in metabotropic receptors (MRs) and the G protein-gated ion channels (GPGICs). Here, mental operations are assumed to be executed by the functions of MRs and GPGICs. As GPGICs have the capacity to close or open branches of dendritic trees and axon terminals, their states transiently re-route neural activity throughout the nervous system. First, MRs detect ligands that signal the need to activate GPGICs. Next, GPGICs transiently select a subnetwork within the brain. The process of selecting this new subnetwork is what constitutes a mental operation – be it in a form of directed attention, perception or making a decision. Synaptic connections and network computations play only a secondary role, supporting MRs and GPGICs. According to this new paradigm, the mind emerges within the brain as the function of MRs and GPGICs whose primary function is to continually select the pathways over which neural activity will be allowed to pass. It is argued that MRs and GPGICs solve the scaling problem of intelligence from which the connectionism paradigm suffers.}
}
@article{PINE2017385,
title = {Clinical Advances From a Computational Approach to Anxiety},
journal = {Biological Psychiatry},
volume = {82},
number = {6},
pages = {385-387},
year = {2017},
note = {Computational Psychiatry},
issn = {0006-3223},
doi = {https://doi.org/10.1016/j.biopsych.2016.09.020},
url = {https://www.sciencedirect.com/science/article/pii/S0006322316328669},
author = {Daniel S. Pine}
}
@incollection{ERICSSON199437,
title = {CHAPTER 2 - Contemporary Approaches to the Study of Thinking and Problem Solving},
editor = {Robert J. Sternberg},
booktitle = {Thinking and Problem Solving},
publisher = {Academic Press},
address = {San Diego},
pages = {37-79},
year = {1994},
volume = {2},
series = {Handbook of Perception and Cognition},
isbn = {978-0-08-057299-4},
doi = {https://doi.org/10.1016/B978-0-08-057299-4.50008-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780080572994500086},
author = {K. Anders Ericsson and Reid Hastie},
abstract = {Publisher Summary
This chapter discusses the contemporary approaches to the study of thinking and problem solving. The modal approach to create a comprehensive theory of thinking strives to identify simple conditions under which a given type of thinking can be reliably reproduced. Following the successful example of experimenters in many of the natural sciences, the goal of this approach is to discover general laws and invariant constraints in well-defined tasks that do not require access to complex knowledge and experience. The most popular alternative approach to the study of thinking starts by examining performance in everyday life and identifying stable and reproducible phenomena. Of particular interest is expert performance, because it offers the highest levels of performance and also the largest stable individual differences in performance when compared with that of beginners. An understanding of thinking is incomplete unless it provides an account of how the elements of adult thought—such as concepts, representations, and skills—are acquired. Research on learning and skill acquisition on the whole range of activities ranging from performance on simple laboratory tasks to complex life-long efforts to attain expert performance shows that effective learning is not an automatic consequence of extended experience.}
}
@article{1999209,
title = {99/02041 Strategic thinking about nuclear energy: implications of the emerging market structure in electric generation: Bodde, D. L. Energy Policy, 1998, 26, (12), 957–962},
journal = {Fuel and Energy Abstracts},
volume = {40},
number = {3},
pages = {209},
year = {1999},
issn = {0140-6701},
doi = {https://doi.org/10.1016/S0140-6701(99)97811-6},
url = {https://www.sciencedirect.com/science/article/pii/S0140670199978116}
}
@article{BERNARD20153982,
title = {Developing a Capability to Elicit and Structure Psychosocial Decision Information within Computational Models},
journal = {Procedia Manufacturing},
volume = {3},
pages = {3982-3989},
year = {2015},
note = {6th International Conference on Applied Human Factors and Ergonomics (AHFE 2015) and the Affiliated Conferences, AHFE 2015},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2015.07.945},
url = {https://www.sciencedirect.com/science/article/pii/S2351978915009464},
author = {Michael L. Bernard},
keywords = {Knowledge elicitation, Knowledge structure, Cognitive modeling, Social modeling, Systems modeling, Country assessments},
abstract = {There is a recognized need to develop computational models that can represent and simulate the decision making process of various groups across socio-cultural domains [5]. Yet, developing such models can be greatly hampered by the need to acquire and represent information pertaining to the psychological and social aspects of decision-making within these groups. Currently, there are numerous techniques and tools to help facilitate the elicitation and structuring of knowledge within expert-type systems—particularly those that focus on technical processes such as mechanical troubleshooting [3]. However, few techniques and tools have been developed for models that are intended to represent and assess the decision making of groups within different societies—particularly including cultural elements within these societies. This paper seeks to help address this challenge by discussing an approach to eliciting and structuring cross-cultural psychosocial and behavioral-economic elements within a theory-based assessment model. This work was developed to address the needs of Sandia National Laboratories’ Behavioral Influence Assessment modeling capability, which assesses decision-making within societies. The main component of the knowledge engineering effort is what we call the “knowledge structure.” The knowledge structure acts as scaffolding for the organization of psychosocial processes underlying decision-making, as well as the actual content of that knowledge with respect to a modeled society.}
}
@article{LIU2023340,
title = {Research on the standardization strategy of granular computing},
journal = {International Journal of Cognitive Computing in Engineering},
volume = {4},
pages = {340-348},
year = {2023},
issn = {2666-3074},
doi = {https://doi.org/10.1016/j.ijcce.2023.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S2666307423000323},
author = {Donghang Liu and Xuekui Shangguan and Keyu Wei and Chensi Wu and Xiaoying Zhao and Qifeng Sun and Yaoyu Zhang and Ruijun Bai},
keywords = {Granular computing, Standardization strategy, Methodology, Standard system},
abstract = {As intelligent systems continue to evolve, problems are becoming increasingly complex. The constant abundance of data puts a higher demand on the value of data utilization. Granular computing is a new computational paradigm for complex problem-solving. It takes structured thinking, structured problem-solving methods, and structured information processing patterns as its research objects and belongs to the scope of higher-level human cognitive mechanism research. The development and application of granular computing must be more standardized and unified. The granular computing standardization strategy is the most direct means to promote the regularization of granular computing. In this paper, we first sort out the main applications of granular computing in standards. According to the characteristics of granular computing, a framework of its standard system is proposed to provide a reference for the subsequent research of granular computing standards. The next direction of the granular computing standards strategy is discussed, and solutions are given.}
}
@article{20161,
title = {Credit for Computation},
journal = {Cell Systems},
volume = {3},
number = {1},
pages = {1-2},
year = {2016},
issn = {2405-4712},
doi = {https://doi.org/10.1016/j.cels.2016.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S2405471216302289}
}
@article{LEIRMO2024761,
title = {Digital Twins for Industry 5.0: Unlocking the Human Potential},
journal = {Procedia CIRP},
volume = {130},
pages = {761-766},
year = {2024},
note = {57th CIRP Conference on Manufacturing Systems 2024 (CMS 2024)},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2024.10.161},
url = {https://www.sciencedirect.com/science/article/pii/S2212827124013180},
author = {Torbjørn L. Leirmo},
keywords = {Digital twin, Human aspect, Industry 5.0},
abstract = {The holy grail of Industry 5.0 resides in the intersection of the three dimensions; sustainable, resilient, and human-centric manufacturing. While the Industry 4.0 paradigm addresses resiliency and sustainability through increased flexibility and efficiency, the human component of manufacturing systems has been largely neglected. Despite rapid developments in artificial intelligence, human intelligence remains superior in terms of creative and critical thinking. Digital twins have emerged as a concept that effectively merges the physical and the digital worlds in cyber-physical production systems. The computational power of digital systems is leveraged to collect and aggregate data that are analyzed and presented to a human decision-maker. Taking a human-centric perspective, the digital twin should be designed to enhance human capabilities, accommodate the needs of people, and mitigate shortcomings of the human mind. This paper addresses these issues by discussing how humans may utilize and better interact with digital twins. A conceptual framework for a human-centric digital twin is proposed with use cases for various interfaces for operators, engineers, and managers.}
}
@article{HAN2020382,
title = {A computational approach for using social networking platforms to support creative idea generation},
journal = {Procedia CIRP},
volume = {91},
pages = {382-387},
year = {2020},
note = {Enhancing design through the 4th Industrial Revolution Thinking},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2020.02.190},
url = {https://www.sciencedirect.com/science/article/pii/S2212827120308374},
author = {Ji Han and Dongmyung Park and Hannah Forbes and Dirk Schaefer},
keywords = {Creativity, Social Media, Idea Generation, Social Networking, Ideation},
abstract = {Good design relies upon the generation of good ideas, but producing ideas, especially creative ones, is increasingly challenging. This may be due to limited relevant information, lack of creative skills, design fixation, or as a result of too many previously existing ideas. Conventional creativity tools, such as brainstorming and TRIZ, as well as advanced methods, such as design-by-analogy, are often employed by designers for idea generation to alleviate some of these challenges. In recent years, computational creativity tools have emerged to support creative idea generation. However, most of these computational tools are data-driven, and thereby employ various databases, for example, existing databases such as the ConceptNet containing past common-sense knowledge, and customized ones containing limited information. The limitations of these databases have constrained the capability of the computational creativity tools. Social media platforms, such as Twitter and Wikipedia, which allow users to create web-based content, have been reported to have billions of users. It can be considered a huge ‘unorganized’ database of information created by a crowd. However, to date little work has been done on the utilization of such crowd-generated knowledge from social media to support actual design activities, especially during the early stages of the design process. In this paper, the authors propose a computational approach to retrieve, process, and reuse the textual knowledge from social networks to prompt designers’ creative mind in producing ideas for new product design and development. They also propose a novel approach to construct crowd knowledge databases, which can be employed by computational tools, as well as used individually, for supporting creative idea generation. A case study involving the use of an existing social media analysis tool to construct a crowd database for helping designers produce ideas has been conducted to provide insights on implementing the proposed approach for creative idea generation.}
}
@article{DECAROLIS2011145,
title = {Using modeling to generate alternatives (MGA) to expand our thinking on energy futures},
journal = {Energy Economics},
volume = {33},
number = {2},
pages = {145-152},
year = {2011},
issn = {0140-9883},
doi = {https://doi.org/10.1016/j.eneco.2010.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0140988310000721},
author = {Joseph F. DeCarolis},
keywords = {Mathematical methods (JEL: C02), Optimization, Uncertainty, Modeling},
abstract = {Energy-economy optimization models – encoded with a set of structured, self-consistent assumptions and decision rules – have emerged as a key tool for the analysis of energy and climate policy at the national and international scale. Given the expansive system boundaries and multi-decadal timescales involved, addressing future uncertainty in these models is a critical challenge. The approach taken by many modelers is to build larger models with greater complexity to deal with structural uncertainty, and run a few highly detailed scenarios under different input assumptions to address parametric uncertainty. The result is often large and inflexible models used to conduct analysis that offers little insight. This paper introduces a technique borrowed from the operations research literature called modeling to generate alternatives (MGA) as a way to flex energy models and systematically explore the feasible, near-optimal solution space in order to develop alternatives that are maximally different in decision space but perform well with regard to the modeled objectives. The resultant MGA alternatives serve a useful role by challenging preconceptions and highlighting plausible alternative futures. A simple, conceptual model of the U.S. electric sector is presented to demonstrate the utility of MGA as an energy modeling technique.}
}
@article{NAGURNEY19953,
title = {Massively parallel computation of spatial price equilibrium problems as dynamical systems},
journal = {Journal of Economic Dynamics and Control},
volume = {19},
number = {1},
pages = {3-37},
year = {1995},
issn = {0165-1889},
doi = {https://doi.org/10.1016/0165-1889(93)00772-V},
url = {https://www.sciencedirect.com/science/article/pii/016518899300772V},
author = {Anna Nagurney and Takashi Takayama and Ding Zhang},
keywords = {Spatial price equilibrium, Dynamical systems, Variational inequalities, Massively parallel computation},
abstract = {In this paper we introduce a dynamical system for the formulation and computation of spatial price equilibrium problems in quantity variables. The set of stationary points of the system corresponds to the set of solutions of the variational inequality problem governing the problem. We propose the Euler-type method for the computation of the equilibrium pattern and provide convergence results. We then demonstrate that the algorithm can be implemented on a massively parallel architecture and illustrate its performance on the Thinking Machine's CM-2 architecture. This research represents the first implementation of a massively parallel approach for the computation of either dynamical systems or variational inequality problems arising in economics.}
}
@incollection{HORWITZ2023265,
title = {8 - Improved force models for Euler–Lagrange computations},
editor = {Shankar Subramaniam and S. Balachandar},
booktitle = {Modeling Approaches and Computational Methods for Particle-Laden Turbulent Flows},
publisher = {Academic Press},
pages = {265-298},
year = {2023},
series = {Computation and Analysis of Turbulent Flows},
isbn = {978-0-323-90133-8},
doi = {https://doi.org/10.1016/B978-0-32-390133-8.00015-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323901338000153},
author = {Jeremy A.K. Horwitz},
keywords = {undisturbed fluid velocity, two-way coupling, Euler–Lagrange, spherical particle equation of motion, drag, lift, unsteady effects, wall-bounded flows, multi-particle effects},
abstract = {This chapter focuses on force models governing spherical particle motion which are used in Euler–Lagrange methods. These forces also represent a momentum exchange and are important for modeling how fluid dynamics change in the presence of particles. A survey of widely used drag and lift correlations for single- and multiple-particle systems will be presented along with some physical discussion as to their origins. Our focus is on incompressible applications though a few compressible force formulations will be mentioned. A central quantity that arises in these force correlations is the notion of the undisturbed fluid velocity. Seldom taught in fluid mechanics curricula and often confused with “free-stream” velocity, the undisturbed fluid velocity is discussed in detail to develop the reader's intuition. Modeling of the undisturbed fluid velocity is addressed in the context of two-way coupled correction schemes.}
}
@article{NENSA2025100001,
title = {Embracing generative AI: A necessary evolution in professional writing},
journal = {European Journal of Radiology Artificial Intelligence},
volume = {1},
pages = {100001},
year = {2025},
issn = {3050-5771},
doi = {https://doi.org/10.1016/j.ejrai.2024.100001},
url = {https://www.sciencedirect.com/science/article/pii/S305057712400001X},
author = {Felix Nensa},
keywords = {GenAI, LLM, ChatGPT, AI, Writing},
abstract = {Generative artificial intelligence (AI), particularly large language models (LLMs), has become an integral part of our professional lives. Despite their transformative potential, many professionals remain cautious about using these tools for drafting and editing manuscripts. While it is reasonable for academic journals to request transparency regarding AI usage, fundamental reservations against employing generative AI (GenAI) are outdated. A useful analogy can be drawn from the film Hidden Figures, which depicts the arrival of IBM computers at NASA, eventually replacing human “computers” for manual calculations. Dorothy Vaughan, the supervisor of these human experts, anticipated the change and adapted proactively by teaching her team programming skills. Today, it is unthinkable for scientific calculations to be done without software, just as it will soon be unthinkable to draft professional texts without AI assistance. GenAI should be seen as a tool that enhances human creativity rather than replacing it. By handling mundane aspects of writing, it allows authors to focus on critical thinking and idea generation. Transparency in AI use fosters trust and maintains ethical standards. Authors are encouraged to use GenAI under supervision and disclose its use openly. This will not only improve manuscript quality but also help authors allocate more time to innovation and creative thinking. Embracing GenAI is not merely an option; it represents an essential evolution in the way we approach writing.}
}