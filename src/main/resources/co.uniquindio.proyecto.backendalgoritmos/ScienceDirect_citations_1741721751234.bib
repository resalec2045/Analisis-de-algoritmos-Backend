@article{HUANG2022209,
title = {A Framework for Collaborative Artificial Intelligence in Marketing},
journal = {Journal of Retailing},
volume = {98},
number = {2},
pages = {209-223},
year = {2022},
issn = {0022-4359},
doi = {https://doi.org/10.1016/j.jretai.2021.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0022435921000142},
author = {Ming-Hui Huang and Roland T. Rust},
keywords = {Artificial intelligence, Collaborative AI, Collaborative intelligence, Augmentation, Replacement},
abstract = {We develop a conceptual framework for collaborative artificial intelligence (AI) in marketing, providing systematic guidance for how human marketers and consumers can team up with AI, which has profound implications for retailing, which is the interface between marketers and consumers. Drawing from the multiple intelligences view that AI advances from mechanical, to thinking, to feeling intelligence (based on how difficult for AI to mimic human intelligences), the framework posits that collaboration between AI and HI (human marketers and consumers) can be achieved by 1) recognizing the respective strengths of AI and HI, 2) having lower-level AI augmenting higher-level HI, and 3) moving HI to a higher intelligence level when AI automates the lower level. Implications for marketers, consumers, and researchers are derived. Marketers should optimize the mix and timing of AI-HI marketing team, consumers should understand the complementarity between AI and HI strengths for informed consumption decisions, and researchers can investigate innovative approaches to and boundary conditions of collaborative intelligence.}
}
@article{HADIMOGAVI2024100027,
title = {ChatGPT in education: A blessing or a curse? A qualitative study exploring early adopters’ utilization and perceptions},
journal = {Computers in Human Behavior: Artificial Humans},
volume = {2},
number = {1},
pages = {100027},
year = {2024},
issn = {2949-8821},
doi = {https://doi.org/10.1016/j.chbah.2023.100027},
url = {https://www.sciencedirect.com/science/article/pii/S2949882123000270},
author = {Reza {Hadi Mogavi} and Chao Deng and Justin {Juho Kim} and Pengyuan Zhou and Young {D. Kwon} and Ahmed {Hosny Saleh Metwally} and Ahmed Tlili and Simone Bassanelli and Antonio Bucchiarone and Sujit Gujar and Lennart E. Nacke and Pan Hui},
keywords = {Artificial intelligence (AI), Generative AI, ChatGPT, Education, Human-computer interaction (HCI),, Early adopters, Social media, Qualitative research},
abstract = {To foster the development of pedagogically potent and ethically sound AI-integrated learning landscapes, it is pivotal to critically explore the perceptions and experiences of the users immersed in these contexts. In this study, we perform a thorough qualitative content analysis across four key social media platforms. Our goal is to understand the user experience (UX) and views of early adopters of ChatGPT across different educational sectors. The results of our research show that ChatGPT is most commonly used in the domains of higher education, K-12 education, and practical skills training. In social media dialogues, the topics most frequently associated with ChatGPT are productivity, efficiency, and ethics. Early adopters' attitudes towards ChatGPT are multifaceted. On one hand, some users view it as a transformative tool capable of amplifying student self-efficacy and learning motivation. On the other hand, there is a degree of apprehension among concerned users. They worry about a potential overdependence on the AI system, which they fear might encourage superficial learning habits and erode students’ social and critical thinking skills. This dichotomy of opinions underscores the complexity of Human-AI Interaction in educational contexts. Our investigation adds depth to this ongoing discourse, providing crowd-sourced insights for educators and learners who are considering incorporating ChatGPT or similar generative AI tools into their pedagogical strategies.}
}
@article{PAL20133944,
title = {Title Paper: Natural computing: A problem solving paradigm with granular information processing},
journal = {Applied Soft Computing},
volume = {13},
number = {9},
pages = {3944-3955},
year = {2013},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2013.06.026},
url = {https://www.sciencedirect.com/science/article/pii/S1568494613002159},
author = {Sankar K. Pal and Saroj K. Meher},
keywords = {Natural computing, Granular computing, Soft computing, Hybrid model, Decision systems},
abstract = {Natural computing, inspired by biological course of action, is an interdisciplinary field that formalizes processes observed in living organisms to design computational methods for solving complex problems, or designing artificial systems with more natural behaviour. Based on the tasks abstracted from natural phenomena, such as brain modelling, self-organization, self-repetition, self evaluation, Darwinian survival, granulation and perception, nature serves as a source of inspiration for the development of computational tools or systems that are used for solving complex problems. Nature inspired main computing paradigms used for such development include artificial neural networks, fuzzy logic, rough sets, evolutionary algorithms, fractal geometry, DNA computing, artificial life and granular or perception-based computing. Information granulation in granular computing is an inherent characteristic of human thinking and reasoning process performed in everyday life. The present article provides an overview of the significance of natural computing with respect to the granulation-based information processing models, such as neural networks, fuzzy sets and rough sets, and their hybridization. We emphasize on the biological motivation, design principles, application areas, open research problems and challenging issues of these models.}
}
@incollection{LUCHINI2023195,
title = {Chapter 13 - Brain networks of creative cognition},
editor = {Roni Reiter-Palmon and Sam Hunter},
booktitle = {Handbook of Organizational Creativity (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {195-207},
year = {2023},
isbn = {978-0-323-91840-4},
doi = {https://doi.org/10.1016/B978-0-323-91840-4.00021-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323918404000219},
author = {Simone Luchini and Roger E. Beaty},
keywords = {Creativity, Default network, Divergent thinking, Executive control network, Functional connectivity, Network neuroscience},
abstract = {In recent years there has been an increasing interest in the role of brain networks supporting creative thinking. This chapter provides a summary of the literature on the network neuroscience of creativity, providing a twofold argument by separately detailing research in domain-general and domain-specific creativity. The first section will concern two main lines of research on domain-general creativity: (1) the neurocognitive mechanisms of creative cognition (how brain networks map onto specific cognitive processes involved in creative thinking), and (2) the individual differences in brain network connectivity and creative ability (how brain networks relate to differences in creative abilities). The second section, on domain-specific creativity, will then consider three domains of artistic creativity: (1) music improvisation, (2) figural creativity, and (3) literary creativity. Throughout this chapter we discuss common themes and shared findings between domain-general and domain-specific creativity. We will then conclude by outlining some of the limitations in the literature and by providing some directions for future research.}
}
@article{PEREZRIVEROL2013134,
title = {Computational proteomics pitfalls and challenges: HavanaBioinfo 2012 Workshop report},
journal = {Journal of Proteomics},
volume = {87},
pages = {134-138},
year = {2013},
issn = {1874-3919},
doi = {https://doi.org/10.1016/j.jprot.2013.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S1874391913000493},
author = {Yasset Perez-Riverol and Henning Hermjakob and Oliver Kohlbacher and Lennart Martens and David Creasy and Jürgen Cox and Felipe Leprevost and Baozhen Paul Shan and Violeta I. Pérez-Nueno and Michal Blazejczyk and Marco Punta and Klemens Vierlinger and Pedro A. Valiente and Kalet Leon and Glay Chinea and Osmany Guirola and Ricardo Bringas and Gleysin Cabrera and Gerardo Guillen and Gabriel Padron and Luis Javier Gonzalez and Vladimir Besada},
keywords = {Bioinformatics workshop, Mass spectrometry, Course, Protein identification, Database searching, Proteomic repositories},
abstract = {The workshop “Bioinformatics for Biotechnology Applications (HavanaBioinfo 2012)”, held December 8–11, 2012 in Havana, aimed at exploring new bioinformatics tools and approaches for large-scale proteomics, genomics and chemoinformatics. Major conclusions of the workshop include the following: (i) development of new applications and bioinformatics tools for proteomic repository analysis is crucial; current proteomic repositories contain enough data (spectra/identifications) that can be used to increase the annotations in protein databases and to generate new tools for protein identification; (ii) spectral libraries, de novo sequencing and database search tools should be combined to increase the number of protein identifications; (iii) protein probabilities and FDR are not yet sufficiently mature; (iv) computational proteomics software needs to become more intuitive; and at the same time appropriate education and training should be provided to help in the efficient exchange of knowledge between mass spectrometrists and experimental biologists and bioinformaticians in order to increase their bioinformatics background, especially statistics knowledge.}
}
@article{SAVIN2021106878,
title = {Free associations of citizens and scientists with economic and green growth: A computational-linguistics analysis},
journal = {Ecological Economics},
volume = {180},
pages = {106878},
year = {2021},
issn = {0921-8009},
doi = {https://doi.org/10.1016/j.ecolecon.2020.106878},
url = {https://www.sciencedirect.com/science/article/pii/S0921800920309484},
author = {Ivan Savin and Stefan Drews and Jeroen {van den Bergh}},
keywords = {Structural topic modelling, Growth-vs-environment debate, Public opinion, Scientific opinion, Green growth},
abstract = {The debate about the relationship between economic growth and environmental sustainability triggers a range of associations. Here we analyze open-ended textual responses of citizens and scientists concerning their associations with the terms “economic growth” and “green growth”. We derive from the responses a number of topics and examine how associations differ across distinct opinion segments of people, namely supporters of Green growth, Agrowth and Degrowth. The results indicate that the general public is more critical of the notion of economic growth than academic researchers. Citizens stress problems of corruption, social inequality, unemployment and poverty, with less variation among the three opinion segments compared to scientists. The latter more strongly emphasize the environmental consequences of economic growth. Concerning associations of scientists with the term “green growth”, we find topics questioning its feasibility to be more likely expressed by Degrowth supporters, while topics stressing the possibility of sustainable economic growth by Green growth supporters. We find that topic polarization is stronger for scientists than citizens. Our results provide further validation for opinion clusters identified in previous studies and uncover additional insights about related views on growth and sustainability.}
}
@incollection{YANG20161,
title = {Chapter 1 - Bio-inspired computation and its applications in image processing: an overview},
editor = {Xin-She Yang and João Paulo Papa},
booktitle = {Bio-Inspired Computation and Applications in Image Processing},
publisher = {Academic Press},
pages = {1-24},
year = {2016},
isbn = {978-0-12-804536-7},
doi = {https://doi.org/10.1016/B978-0-12-804536-7.00001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128045367000016},
author = {X.-S. Yang and J.P. Papa},
keywords = {algorithm, ant algorithm, artificial neural networks, bee algorithm, bat algorithm, bio-inspired computation, cuckoo search, firefly algorithm, harmony search, particle swarm optimization, metaheuristics, swarm intelligence, support vector machine, signal and image processing},
abstract = {Almost all design problems in the sciences and engineering can be formulated as optimization problems, and many image processing problems can also be related to or formulated as optimization problems. These optimization problems can be solved by optimization techniques. However, these problems are often highly nonlinear and are subject to multiple nonlinear constraints, which makes them very challenging to solve. The further complication to these challenges is the stringent time requirements and high dimensionality, which means that traditional optimization techniques, such as gradient-based methods cannot deal with such kinds of problems well. Recent trends tend to use bio-inspired optimization techniques as a promising alternative, and it is usually combined with traditional methods, especially in the area of image processing. These bio-inspired computational methods are usually based on swarm intelligence and can be very effective in coping with nonlinearity in real-world problems. This chapter presents an overview of bio-inspired computation and its application in image processing, including some current trends and important issues, such as efficiency and time constraints.}
}
@article{OTOOLE2024100080,
title = {Extending human creativity with AI},
journal = {Journal of Creativity},
volume = {34},
number = {2},
pages = {100080},
year = {2024},
issn = {2713-3745},
doi = {https://doi.org/10.1016/j.yjoc.2024.100080},
url = {https://www.sciencedirect.com/science/article/pii/S2713374524000062},
author = {Katherine O'Toole and Emőke-Ágnes Horvát},
keywords = {Computational creativity, Generative AI, HCI},
abstract = {The development of generative AI has led to novel ways that technology can be integrated into creative activities. However, this has also raised concerns about how human creators will be affected, and what impact it may have on creative industries. As a result, there has been research into how we can design AI tools that work with human creators, rather than replacing them. In this paper we review approaches utilized to build AI tools that facilitate human creativity and allow users to engage fully and authentically in the creative process. These include leveraging AI models to help us shed light on elements of the creative process, building interfaces that encourage exploration of ideas, and designing technological affordances that can support the development of new creative practices.}
}
@incollection{VANCOUVER2020463,
title = {Chapter 12 - Perceptions of control theory in industrial-organizational psychology: disturbances and counter-disturbances},
editor = {Warren Mansell},
booktitle = {The Interdisciplinary Handbook of Perceptual Control Theory},
publisher = {Academic Press},
pages = {463-501},
year = {2020},
isbn = {978-0-12-818948-1},
doi = {https://doi.org/10.1016/B978-0-12-818948-1.00012-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128189481000125},
author = {Jeffrey B. Vancouver},
keywords = {Control theory, Self-regulation, Self-efficacy, Computational modeling},
abstract = {The history of perceptual control theory's growing influence in the field of Industrial-Organizational Psychology is described. This history began in the early 1980's and included mostly conceptual work that described how control theory concepts might be used to understand applied phenomena. Both conceptual and empirical work on control theory ideas continued throughout the 1990's despite a substantial backlash against the theory by prominent scholars in the field. However, it was conceptual and empirical work in the 21st century that defined its potential integrative value and its theoretical rigor. Moreover, research regarding self-efficacy demonstrated how informal theories of human behavior might be better understood from a control theory perspective. Much of the current work with perceptual control theory involves the construction and testing of computational models that represent the links among perceptual, learning, and thinking modes of self-regulation and control.}
}
@article{KUMAR20224712,
title = {Efficient computational stochastic framework for performance optimization of E-waste management plant},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {34},
number = {8, Part A},
pages = {4712-4728},
year = {2022},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2022.05.018},
url = {https://www.sciencedirect.com/science/article/pii/S1319157822001677},
author = {Naveen Kumar and Deepak Sinwar and Monika Saini and Dinesh Kumar Saini and Ashish Kumar and Manjit Kaur and Dilbag Singh and Heung-No Lee},
keywords = {E-waste management plant, Availability, Maintainability, Genetic Algorithm, Differential Evolution, Particle Swarm Optimization, Markov Birth-Death Process},
abstract = {Purpose
Reliability and maintainability are the key system effectiveness measures in process and manufacturing industries, and treatment plants, especially in E-waste management plants. The present work is proposed with a motto to develop a stochastic framework for the e-waste management plant to optimize its availability integrated with reliability, availability, maintainability, and dependability (RAMD) measures and Markovian analysis to estimate the steady-state availability of the E-waste management plant. In the analysis an effort is also made to identify the best performing algorithm for availability optimization of the e-waste plant.
Methodology
A stochastic model for a particular plant is developed and its availability is optimized using various metaheuristic approaches like a genetic algorithm (GA), particle swarm optimization (PSO), and differential evolutions (DE). The most sensitive component is identified using RAMD methodology while the effect of deviation in various failure and repair rates are observed by the proposed model. The failure and repair rates follow an exponential distribution. All time-dependent random variables are statistically independent.
Originality/Novelties
A novel stochastic model is presented for an e-waste management plant and optimum availability is obtained using metaheuristic approaches. The proposed methodology is not so far discussed in the reliability analysis of process industries.
Findings
The numerical results of the proposed model compared to identify the most efficient algorithm. It is observed that genetic algorithm provides the maximum value (0.92330969) of availability at a population size 2500 after 500 iterations. PSO algorithm attained the maximum value (0.99996744) of availability just after 50 iterations and 100 population size. So, its rate of convergence is faster than GA. The optimum value of availability is 0.99997 using differential evolution after 500 iterations and population size of more than 1000. These findings are very beneficial for system designers.
Practical Implications
The proposed methodology can be utilized to find the reliability measures of other process industries.}
}
@article{HONG2006255,
title = {Bruno Buchberger — A life devoted to symbolic computation},
journal = {Journal of Symbolic Computation},
volume = {41},
number = {3},
pages = {255-258},
year = {2006},
note = {Logic, Mathematics and Computer Science: Interactions in honor of Bruno Buchberger (60th birthday)},
issn = {0747-7171},
doi = {https://doi.org/10.1016/j.jsc.2005.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0747717105001306},
author = {Hoon Hong and Deepak Kapur and Peter Paule and Franz Winkler and  {Faculty of RISC-Linz}}
}
@article{SAND2022100955,
title = {Three cases that demonstrate how students connect the domains of mathematics and computing},
journal = {The Journal of Mathematical Behavior},
volume = {67},
pages = {100955},
year = {2022},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2022.100955},
url = {https://www.sciencedirect.com/science/article/pii/S0732312322000232},
author = {Odd Petter Sand and Elise Lockwood and Marcos D. Caballero and Knut Mørken},
keywords = {Computing, Modeling, Programming, Thinking and learning, Connections, Undergraduate students},
abstract = {This study uses actor-oriented transfer perspective to investigate different ways in which students make connections across the domains of mathematics and computing. We interview first-year students at the University of Oslo as they work with a set of tutorials that we designed to integrate knowledge from both domains. The cases we present here demonstrate four different types of cross-domain connections: (a) mathematically reproducing the work of a computer program, (b) cyclically improving a program to produce better output, (c) coupling math to output to justify program improvements and (d) coupling math to code to justify program design. We provide rich examples of the ways in which students make these connections and discuss affordances for mathematical learning in this context.}
}
@article{MUKTI2024117407,
title = {Computer aided sketching in the early-stage design of complex vessels},
journal = {Ocean Engineering},
volume = {305},
pages = {117407},
year = {2024},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2024.117407},
url = {https://www.sciencedirect.com/science/article/pii/S0029801824007443},
author = {M.H. Mukti and R.J. Pawling and D.J. Andrews},
abstract = {Various methods have been developed for automated and semi-automated architecture generation in the computer aided ship design processes. The question remains as to how this can speed up the design process without losing the requirement elucidation intent for concept phase. This paper presents a novel approach with a software toolset to develop design and analysis approaches to early stage ship design and provide a sketching tool. This was done by enhancing the user interface and experience of the UCL Network Block Approach to achieve a “thinking sketch” in a way that is “quick” and “fluid” enough to promote inventive and creative sketching comparable to hand sketching. The UCL Network Block Approach draws on the UCL Design Building Block (DBB) approach and uses network methods applied to the synthesis of distributed ship service systems (DS3) and Computer Aided Ship Design (CASD) to expand DS3 definition in early stage ship design. The UCL originated inside-out/DBB approach to sketch driven synthesis has been made translatable to both DBB ship descriptions and ensuring early stage naval architectural “balance”. The proposed approach has been used for the first time successfully to not only carry out a rapid sketching exercise for a naval ship design but also enable quick preliminary analysis of a set of DS3 networks.}
}
@incollection{DIBBLE20061511,
title = {Chapter 31 Computational Laboratories for Spatial Agent-Based Models},
editor = {L. Tesfatsion and K.L. Judd},
series = {Handbook of Computational Economics},
publisher = {Elsevier},
volume = {2},
pages = {1511-1548},
year = {2006},
issn = {1574-0021},
doi = {https://doi.org/10.1016/S1574-0021(05)02031-9},
url = {https://www.sciencedirect.com/science/article/pii/S1574002105020319},
author = {Catherine Dibble},
keywords = {agent-based simulation, computational laboratory, computational social science, computational economics, spatial economics, spatial social science, spatial networks, small-world networks, scale-free networks, synthetic landscape, inference},
abstract = {An agent-based model is a virtual world comprising distributed heterogeneous agents who interact over time. In a spatial agent-based model the agents are situated in a spatial environment and are typically assumed to be able to move in various ways across this environment. Some kinds of social or organizational systems may also be modeled as spatial environments, where agents move from one group or department to another and where communications or mobility among groups may be structured according to implicit or explicit channels or transactions costs. This chapter focuses on the potential usefulness of computational laboratories for spatial agent-based modeling. Speaking broadly, a computational laboratory is any computational framework permitting the exploration of the behaviors of complex systems through systematic and replicable simulation experiments. By that definition, most of the research discussed in this handbook would be considered to be work with computational laboratories. A narrower definition of computational laboratory (or comp lab for short) refers specifically to specialized software tools to support the full range of agent-based modeling and complementary tasks. These tasks include model development, model evaluation through controlled experimentation, and both the descriptive and normative analysis of model outcomes. The objective of this chapter is to explore how comp lab tools and activities facilitate the systematic exploration of spatial agent-based models embodying complex social processes critical for social welfare. Examples include the spatial and temporal coordination of human activities, the diffusion of new ideas or of infectious diseases, and the emergence and ecological dynamics of innovative ideas or of deadly new diseases.}
}
@article{OLTETEANU201581,
title = {comRAT-C: A computational compound Remote Associates Test solver based on language data and its comparison to human performance},
journal = {Pattern Recognition Letters},
volume = {67},
pages = {81-90},
year = {2015},
note = {Cognitive Systems for Knowledge Discovery},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2015.05.015},
url = {https://www.sciencedirect.com/science/article/pii/S0167865515001609},
author = {Ana-Maria Olteţeanu and Zoe Falomir},
keywords = {Computational creativity, Remote Associates Test, Cognitive systems, Knowledge base, Language corpus, Cognitive modeling},
abstract = {Discovering the processes and types of knowledge organization which are involved in the creative process is a challenge up to this date. Human creativity is usually measured by psychological tests, such as the Remote Associates Test (RAT). In this paper, an approach based on a specific type of knowledge organization and processes which enables automatic solving of RAT queries is implemented (comRAT) as a part of a more general cognitive theoretical framework for creative problem-solving (CreaCogs). This aims to study: (a) whether a convergence process can be used to solve such queries and (b) if frequency of appearance of the test items in language data may influence knowledge association or discovery in solving such problems. The comRAT uses a knowledge base of language data extracted from the Corpus of Contemporary American English. The results obtained are compared to results obtained in empirical tests with humans. In order to explain why some answers might be preferred over others, frequencies of appearance of the queries and solutions are analyzed. The difficulty encountered by humans when solving RAT queries is expressed in response times and percentage of participants solving the query, and a significant moderate correlation between human data on query difficulty and the data provided by this approach is obtained.}
}
@article{PROSPERETTI20031089,
title = {Appendix 3: Report of study group on computational physics},
journal = {International Journal of Multiphase Flow},
volume = {29},
number = {7},
pages = {1089-1099},
year = {2003},
issn = {0301-9322},
doi = {https://doi.org/10.1016/S0301-9322(03)00081-8},
url = {https://www.sciencedirect.com/science/article/pii/S0301932203000818},
author = {Andrea Prosperetti and Grétar Tryggvason},
keywords = {Computational multiphase flow, Direct numerical simulations, Numerical methods},
abstract = {The great improvement of algorithms and computing hardware in the last few years must be ranked as one of the most important turning points in the history of multiphase flow research. After a brief review of some of this recent progress, it is pointed out that, besides its application to solving actual problems, computational physics plays other key roles: (1) As a tool to develop and understand basic physics and as a guide toward asking more penetrating questions; (2) As an aid in closing the averaged equations; (3) As a means to learn to compute better. Roadblocks toward greater effectiveness are the huge complexity of many of the necessary computational tasks but also, at a more practical level, the transmission of “computational knowledge” from one researcher to another, much in the same way as experimentalists can rely on readily available equipment (e.g., lasers, etc.), without having to build each item themselves. The solution to this problem will require a cultural shift––from a “cottage industry” to a “big science” mentality––which can be aided by a different attitude on the part of the funding agencies. Great synergism can be achieved by a closer integration of the multiphase computational physics enterprise with both Applied Mathematics and Computer Science.}
}
@article{DAUCE20101,
title = {Computational neuroscience, from multiple levels to multi-level},
journal = {Journal of Physiology-Paris},
volume = {104},
number = {1},
pages = {1-4},
year = {2010},
note = {Computational Neuroscience, from Multiple Levels to Multi-level},
issn = {0928-4257},
doi = {https://doi.org/10.1016/j.jphysparis.2009.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0928425709000837},
author = {Emmanuel Daucé and Laurent Perrinet}
}
@article{DANOS200773,
title = {Distributed Measurement-based Quantum Computation},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {170},
pages = {73-94},
year = {2007},
note = {Proceedings of the 3rd International Workshop on Quantum Programming Languages (QPL 2005)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2006.12.012},
url = {https://www.sciencedirect.com/science/article/pii/S1571066107000564},
author = {Vincent Danos and Ellie D'Hondt and Elham Kashefi and Prakash Panangaden},
keywords = {Formal language, quantum communication, quantum computing, semantics},
abstract = {We develop a formal model for distributed measurement-based quantum computations, adopting an agent-based view, such that computations are described locally where possible. Because the network quantum state is in general entangled, we need to model it as a global structure, reminiscent of global memory in classical agent systems. Local quantum computations are described as measurement patterns. Since measurement-based quantum computation is inherently distributed, this allows us to extend naturally several concepts of the measurement calculus [V. Danos, E. Kashefi and P. Panangaden, The measurement calculus (2004), arXiv:quant-ph/0412135], a formal model for such computations. Our goal is to define an assembly language, i.e. we assume that computations are well-defined and we do not concern ourselves with verification techniques. The operational semantics for systems of agents is given by a probabilistic transition system, and we define operational equivalence in a way that it corresponds to the notion of bisimilarity. With this in place, we prove that teleportation is bisimilar to a direct quantum channel, and this also within the context of larger networks.}
}
@article{CORDASCO201152,
title = {Efficient on-line algorithms for Euler diagram region computation},
journal = {Computational Geometry},
volume = {44},
number = {1},
pages = {52-68},
year = {2011},
issn = {0925-7721},
doi = {https://doi.org/10.1016/j.comgeo.2010.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S0925772110000581},
author = {Gennaro Cordasco and Rosario {De Chiara} and Andrew Fish},
keywords = {Euler diagrams, Region computation, Diagram generation},
abstract = {Euler diagrams are an accessible and effective visualisation of data involving simple set-theoretic relationships. Sets are represented by closed curves in the plane and often have wellformedness conditions placed on them in order to enhance comprehensibility. The theoretical underpinning for tool support has usually focussed on the problem of generating an Euler diagram from an abstract model. However, the problem of efficient computation of the abstract model from the concrete diagram has not been addressed before, despite this computation being a necessity for computer interpretations of user drawn diagrams. This may be used, together with automated manipulations of the abstract model, for purposes such as semantic information presentation or diagrammatic theorem proving. Furthermore, in interactive settings, the user may update diagrams “on-line” by adding and removing curves, for example, in which case a system requirement is the update of the abstract model (without the necessity of recomputation of the entire abstract model). We define the notion of marked Euler diagrams, together with a method for associating marked points on the diagram with regions in the plane. Utilising these, we provide on-line algorithms which quickly compute the abstract model of a weakly reducible wellformed Euler diagram (constructible as a sequence of additions or removals of curves, keeping a wellformed diagram at each step), and quickly updates both the set of curves in the plane as well as the abstract model according to the on-line operations. Efficiency is demonstrated by comparison with a common, naive algorithm. Furthermore, the methodology enables a straightforward implementation which has subsequently been realised as an application for the user classification domain.}
}
@article{BANDOPADHAYA2020100378,
title = {Integrated healthcare monitoring solutions for soldier using the internet of things with distributed computing},
journal = {Sustainable Computing: Informatics and Systems},
volume = {26},
pages = {100378},
year = {2020},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2020.100378},
url = {https://www.sciencedirect.com/science/article/pii/S2210537919304081},
author = {Shuvabrata Bandopadhaya and Rajiv Dey and Ashok Suhag},
keywords = {Healthcare monitoring system, Internet of things (IoT), Distributed computing, Fuzzy classification, Partten recognistion, Long Range wide area network (LoRaWAN)},
abstract = {This paper has proposed an integrated healthcare monitoring solution for the soldiers deployed in adverse environmental conditions, using the internet of things (IoT) with distributed computing. For these soldiers, the health parameters of every individual need to be monitored on a real-time basis and subsequent analysis of the dataset to be made for initiating appropriate medical support with the lowest possible delay. In this paper, a three-layer service-oriented IoT architecture has been proposed where the computational functionalities are distributed among all the layers. The proposed distributed computing mechanism has implemented two levels of filtration of redundant information that belongs to safe soldiers. The first level of filtering is done at the end-node using the Fuzzy classification approach and the second level of filtering is done at the intermediate node using the time-series pattern analysis approach. This layer-wise filtration process results in a reduction in data flooding and computational burden on the cloud due to which system response time improves to suit emergency applications. A prototype has been developed to validate the effectiveness of the proposed solution.}
}
@article{GIANNOPULU2022e09017,
title = {Synchronised neural signature of creative mental imagery in reality and augmented reality},
journal = {Heliyon},
volume = {8},
number = {3},
pages = {e09017},
year = {2022},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2022.e09017},
url = {https://www.sciencedirect.com/science/article/pii/S240584402200305X},
author = {I. Giannopulu and G. Brotto and T.J. Lee and A. Frangos and D. To},
keywords = {Creativity, Synchronisation, Mental imagery, Real environment, Augmented reality, Complexity},
abstract = {Creativity, transforming imaginative thinking into reality, is a mental imagery simulation in essence. It can be incorporeal, concerns sophisticated and/or substantial thinking, and involves objects. In the present study, a mental imagery task consisting of creating a scene using familiar (FA) or abstract (AB) physical or virtual objects in real (RMI) and augmented reality (VMI) environments, and an execution task involving effectively creating a scene in augmented reality (VE), were utilised. The beta and gamma neural oscillations of healthy participants were recorded via a 32 channel wireless 10/20 international EGG system. In real and augmented environments and for both the mental imagery and execution tasks, the participants displayed a similar cortico-cortical neural signature essentially based on synchronous vs asynchronous beta and gamma oscillatory activities between anterior (i.e. frontal) and posterior (i.e. parietal, occipito-parietal and occipito-temporal) areas bilaterally. The findings revealed a transient synchronised neural architecture that appears to be consistent with the hypothesis according to which, creativity, because of its inherent complexity, cannot be confined to a single brain area but engages various interconnected networks.}
}
@article{MINGERS201767,
title = {Back to the future: A critique of Demetis and Lee's “Crafting theory to satisfy the requirements of systems science”},
journal = {Information and Organization},
volume = {27},
number = {1},
pages = {67-71},
year = {2017},
issn = {1471-7727},
doi = {https://doi.org/10.1016/j.infoandorg.2017.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S1471772717300118},
author = {John Mingers},
abstract = {Demetis and Lee's paper outlines criteria for constructing theory in accordance with systems science. This is a laudable aim but in this comment I suggest that their view of systems thinking is both narrow and somewhat dated. Demetis and Lee equate systems science with only one aspect of it – General Systems Thinking (GST) – and they discuss in detail only one theorist – Niklas Luhmann. I draw attention to a range of other systems approaches including system dynamics, soft systems methodology, complexity theory, critical systems thinking, critical realism and multimethodology. I conclude with tentative guidelines of my own.}
}
@incollection{MARINESCU20171,
title = {Chapter 1 - Complex Systems},
editor = {Dan C. Marinescu},
booktitle = {Complex Systems and Clouds},
publisher = {Elsevier},
address = {Boston},
pages = {1-32},
year = {2017},
series = {Computer Science Reviews and Trends},
isbn = {978-0-12-804041-6},
doi = {https://doi.org/10.1016/B978-0-12-804041-6.00001-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128040416000013},
author = {Dan C. Marinescu},
keywords = {Complexity, Emergence, Phase transitions, Open systems, Nondeterminism, Self-similarity, Fractal geometry, Power Law distribution},
abstract = {After a brief review of the evolution of thinking about systems, consisting of an ensemble of components, the chapter analyzes the nondeterminism, nonlinearity, and phase transitions in complex systems. A range of topics pertinent to complexity, such as self-organization, self-organized criticality, power law distributions, computational irreducibility, and quantitative characterization of complexity are then covered. Cybernetics and the interdisciplinary nature of complexity conclude the chapter.}
}
@article{VIGNONCLEMENTEL20103,
title = {A primer on computational simulation in congenital heart disease for the clinician},
journal = {Progress in Pediatric Cardiology},
volume = {30},
number = {1},
pages = {3-13},
year = {2010},
note = {Proceedings of the 1st International Conference on Computational Simulation in Congenital Heart Disease},
issn = {1058-9813},
doi = {https://doi.org/10.1016/j.ppedcard.2010.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S1058981310000767},
author = {Irene E. Vignon-Clementel and Alison L. Marsden and Jeffrey A. Feinstein},
keywords = {Hemodynamics, Computer modeling, Boundary conditions, Clinical data, Congenital heart disease},
abstract = {Interest in the application of engineering methods to problems in congenital heart disease has gained increased popularity over the past decade. The use of computational simulation to examine common clinical problems including single ventricle physiology and the associated surgical approaches, the effects of pacemaker implantation on vascular occlusion, or delineation of the biomechanical effects of implanted medical devices is now routinely appearing in clinical journals within all pediatric cardiovascular subspecialties. In practice, such collaboration can only work if both communities understand each other's methods and their limitations. This paper is intended to facilitate this communication by presenting in the context of congenital heart disease (CHD) the main steps involved in performing computational simulation—from the selection of an appropriate clinical question/problem to understanding the computational results, and all of the “black boxes” in between. We examine the current state of the art and areas in need of continued development. For example, medical image-based model-building software has been developed based on numerous different methods. However, none of them can be used to construct a model with a simple “click of a button.” The creation of a faithful, representative anatomic model, especially in pediatric subjects, often requires skilled manual intervention. In addition, information from a second imaging modality is often required to facilitate this process. We describe the technical aspects of model building, provide a definition of some of the most commonly used terms and techniques (e.g. meshes, mesh convergence, Navier-Stokes equations, and boundary conditions), and the assumptions used in running the simulations. Particular attention is paid to the assignment of boundary conditions as this point is of critical importance in the current areas of research within the realm of congenital heart disease. Finally, examples are provided demonstrating how computer simulations can provide an opportunity to “acquire” data currently unobtainable by other modalities, with essentially no risk to patients. To illustrate these points, novel simulation examples of virtual Fontan conversion (from preoperative data to predicted postoperative state) and outcomes of different surgical designs are presented. The need for validation of the currently employed techniques and predicted results are required and the methods remain in their infancy. While the daily application of these technologies to patient-specific clinical scenarios likely remains years away, the ever increasing interest in this area among both clinicians and engineers makes its eventual use far more likely than ever before and, some could argue, only a matter of [computing] time.}
}
@article{FLOWERS2025119061,
title = {Context matters: Modeling thermochronologic data in geologic frameworks using the Great Unconformity as a case study},
journal = {Earth and Planetary Science Letters},
volume = {650},
pages = {119061},
year = {2025},
issn = {0012-821X},
doi = {https://doi.org/10.1016/j.epsl.2024.119061},
url = {https://www.sciencedirect.com/science/article/pii/S0012821X2400493X},
author = {R.M. Flowers and B.A. Peak},
keywords = {Geologic context approach, (U-Th)/He, Thermal history, Great Unconformity, Pikes Peak, Tava},
abstract = {The critical importance of sample context and geologic information for interpreting geochronologic data has long been fundamental to the Earth sciences. However, the lack of quantitative uncertainties associated with contextual, observational information means that much geologic data cannot be statistically treated in computational models using the same approaches as quantitative datasets. This challenge is showcased by the current debate over whether and how geologic data should be used when modeling thermochronologic results, which has important implications for deriving time-temperature (tT) paths from which burial and exhumation histories are interpreted. Holistically leveraging observational data to test hypotheses and determine the set of geologically reasonable thermal histories that can explain thermochronologic results has a longstanding history, but some recent studies have criticized this approach as one that arbitrarily limits tT solutions. Here, a geologic context approach to thermal history modeling, in which observational and thermochronologic datasets are combined to design geologically valid models and reach the most geologically likely interpretation, is illustrated using an example of constraining Great Unconformity exhumation in Colorado where this modeling philosophy has been questioned. Although the quality of geologic data and their applicability to modeled samples can vary and be debated, this does not mean that all geologic data are inherently unreliable and therefore discardable. Exploring models with varying or minimal constraints can be useful to test different hypotheses and determine the resolving power of the data, but using an endmember context-blind approach to interpret thermochronologic results can produce outcomes that violate fundamental aspects of the geology. The strategy outlined here is not the only valid approach to modeling thermochronologic data, but if the purpose of the modeling is to derive meaningful interpretations about sample tT paths in order to better illuminate the geologic history, then critical thinking about the sample context, first order geologic observations, and primary relationships should be integral components of the modeling process.}
}
@article{EDELMAN2007253,
title = {Behavioral and computational aspects of language and its acquisition},
journal = {Physics of Life Reviews},
volume = {4},
number = {4},
pages = {253-277},
year = {2007},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2007.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S1571064507000255},
author = {Shimon Edelman and Heidi Waterfall},
keywords = {Computational cognitive linguistics, Psycholinguistics, Machine learning, Language acquisition},
abstract = {One of the greatest challenges facing the cognitive sciences is to explain what it means to know a language, and how the knowledge of language is acquired. The dominant approach to this challenge within linguistics has been to seek an efficient characterization of the wealth of documented structural properties of language in terms of a compact generative grammar—ideally, the minimal necessary set of innate, universal, exception-less, highly abstract rules that jointly generate all and only the observed phenomena and are common to all human languages. We review developmental, behavioral, and computational evidence that seems to favor an alternative view of language, according to which linguistic structures are generated by a large, open set of constructions of varying degrees of abstraction and complexity, which embody both form and meaning and are acquired through socially situated experience in a given language community, by probabilistic learning algorithms that resemble those at work in other cognitive modalities.}
}
@article{MANALU2023641,
title = {Developing Nusantara Mobile Application to Support Local Tourism in Indonesia},
journal = {Procedia Computer Science},
volume = {227},
pages = {641-650},
year = {2023},
note = {8th International Conference on Computer Science and Computational Intelligence (ICCSCI 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.568},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923017350},
author = {Daniella Oktalina Manalu and Yudhistya Ayu Kusumawati and Cuk Tho},
keywords = {mobile application, tourism, local tourism},
abstract = {Tourism is a very important sector and has a major influence on development and national income. Moreover, Indonesia has thousands of tourist destinations that are very beautiful and interesting to visit, both for Indonesians and foreigners. It's just that, there are still many local tours, such as tourist villages, which are still not well known by most people. In fact, there are many cultures, customs, places of recreation, or characteristics of an area that need to be seen and introduced to outsiders, even to Indonesians themselves. Therefore, this study aims to explain the problems that occur in the field of tourism, as well as provide solutions in the form of tourism applications that aim to help promote local Indonesian tourism, as well as make it easy for travel enthusiasts to organize their travel plans. The process of making this travel application is also carried out through various research and interviews with potential users and IT people in order to produce an attractive and effective application. This study uses the design thinking method. Researchers collected data sources from literature studies and surveys through questionnaires, where the results of the data obtained from the questionnaires were numerical or quantitative. The aim is to determine the level of public interest in tourism, as well as determine the level of potential users of this tourism application. That way, the goals of this application will be achieved and effective in helping solve tourism problems.}
}
@incollection{PRIETOMARTINEZ201919,
title = {Chapter 2 - Computational Drug Design Methods—Current and Future Perspectives},
editor = {Kunal Roy},
booktitle = {In Silico Drug Design},
publisher = {Academic Press},
pages = {19-44},
year = {2019},
isbn = {978-0-12-816125-8},
doi = {https://doi.org/10.1016/B978-0-12-816125-8.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012816125800002X},
author = {Fernando D. Prieto-Martínez and Edgar López-López and K. {Eurídice Juárez-Mercado} and José L. Medina-Franco},
keywords = {Artificial intelligence, Big data, Chemical space, Chemoinformatics, Deep learning, Molecular modeling, Polypharmacology, SmART, Target fishing, Virtual screening},
abstract = {Computer-aided drug design (CADD) comprises a broad range of theoretical and computational approaches that are part of modern drug discovery. CADD methods have made key contributions to the development of drugs that are in clinical use or in clinical trials. Such methods have emerged and evolved along with experimental approaches used in drug design. In this chapter we discuss the major CADD methods and examples of recent applications to drugs that have advanced in clinical trials or that have been approved for clinical use. We also comment on representative trends in current drug discovery that are shaping the development of novel methods, such as computer-aided drug repurposing. Similarly we present emerging concepts and technologies in molecular modeling and chemoinformatics. Furthermore, this chapter discusses the authors’ point of view of the challenges of traditional and novel CADD methods to increase their positive impact in drug discovery.}
}
@article{LIEFGREEN2020101332,
title = {Strategies for selecting and evaluating information},
journal = {Cognitive Psychology},
volume = {123},
pages = {101332},
year = {2020},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2020.101332},
url = {https://www.sciencedirect.com/science/article/pii/S001002852030061X},
author = {Alice Liefgreen and Toby Pilditch and David Lagnado},
keywords = {Information search, OED framework, Utility functions, Inquiry, Question asking, Strategies, Probabilistic reasoning, Bayesian Networks},
abstract = {Within the domain of psychology, Optimal Experimental Design (OED) principles have been used to model how people seek and evaluate information. Despite proving valuable as computational-level methods to account for people’s behaviour, their descriptive and explanatory powers remain largely unexplored. In a series of experiments, we used a naturalistic crime investigation scenario to examine how people evaluate queries, as well as outcomes, in probabilistic contexts. We aimed to uncover the psychological strategies that people use, not just to assess whether they deviated from OED principles. In addition, we explored the adaptiveness of the identified strategies across both one-shot and stepwise information search tasks. We found that people do not always evaluate queries strictly in OED terms and use distinct strategies, such as by identifying a leading contender at the outset. Moreover, we identified aspects of zero-sum thinking and risk aversion that interact with people’s information search strategies. Our findings have implications for building a descriptive account of information seeking and evaluation, accounting for factors that currently lie outside the realm of information-theoretic OED measures, such as context and the learner’s own preferences.}
}
@article{WENG20072303,
title = {On developmental mental architectures},
journal = {Neurocomputing},
volume = {70},
number = {13},
pages = {2303-2323},
year = {2007},
note = {Selected papers from the 3rd International Conference on Development and Learning (ICDL 2004) Time series prediction competition: the CATS benchmark},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2006.07.017},
url = {https://www.sciencedirect.com/science/article/pii/S0925231206005194},
author = {Juyang Weng},
keywords = {Mental architecture, Agent architecture, Computational neural science, Cognitive development, Autonomous mental development, Developmental robots, Learning types, Developmental vision, Speech recognition, Language acquisition, Thinking, Reasoning, Autonomous planning},
abstract = {This paper presents a computational theory of developmental mental architectures for artificial and natural systems, motivated by neuroscience. The work is an attempt to approximately model biological mental architectures using mathematical tools. Six types of architecture are presented, beginning with the observation-driven Markov decision process as Type-1. From Type-1 to Type-6, the architecture progressively becomes more complete toward the necessary functions of autonomous mental development. Properties of each type are presented. Experiments are discussed with emphasis on their architectures.}
}
@article{DYER2021101055,
title = {Uncertainty and disciplinary difference: Mapping attitudes towards uncertainty across discipline boundaries},
journal = {Design Studies},
volume = {77},
pages = {101055},
year = {2021},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2021.101055},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X21000661},
author = {Loren Dyer and Jacqueline Power and Andrew Steen and Louise Wallis and Aidan Davison},
keywords = {design processes, design thinking, epistemology, interdisciplinarity, uncertainty},
abstract = {This article investigates the different ways that uncertainty is understood and approached across design disciplines. Structural attitudes toward uncertainty are assessed in design thinking literature before other possible ways of viewing uncertainty in the design process are introduced. Uncertainty is then presented as a source of epistemological difference between design disciplines, and this difference is explicated through a project that uses literature survey and analytical diagramming to map differences between discipline attitudes to uncertainty. Our review identifies uncertainty as a prevalent source of discipline difference with the goal of better describing barriers, and effective responses to them, in inter- and trans-disciplinary design agendas.}
}
@incollection{MADIAJAGAN20191,
title = {Chapter 1 - Parallel Computing, Graphics Processing Unit (GPU) and New Hardware for Deep Learning in Computational Intelligence Research},
editor = {Arun Kumar Sangaiah},
booktitle = {Deep Learning and Parallel Computing Environment for Bioengineering Systems},
publisher = {Academic Press},
pages = {1-15},
year = {2019},
isbn = {978-0-12-816718-2},
doi = {https://doi.org/10.1016/B978-0-12-816718-2.00008-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128167182000087},
author = {M. Madiajagan and S. Sridhar Raj},
keywords = {Deep learning, Parallelization, Graphics processing unit, Hardware architecture, Memory optimization, Computational intelligence},
abstract = {Graphics processing unit (GPU) is an electronic circuit which manipulates and modifies the memory for better image output. Deep learning involves huge amounts of matrix multiplications and other operations which can be massively parallelized and thus sped up on GPUs. A single GPU might have thousands of cores while a CPU usually has no more than 12 cores. GPU's practical applicability is affected by two issues: long training time and limited GPU memory, which is greatly influenced as the neural network size grows. In order to overcome these issues, this chapter presents various technologies in distributed parallel processing which improve the training time and optimize the memory, and hardware engine architectures will be explored for data size reduction. The GPUs generally used for deep learning are limited in memory size compared to CPUs, so even the latest Tesla GPU has only 16 GB of memory. Therefore, GPU memory cannot be increased to that extent easily, so networks must be designed to fit within the available memory. This could be a factor limiting progress, overcoming which would be highly beneficiary to the computational intelligence area.}
}
@article{HERBET2015413,
title = {Rethinking voxel-wise lesion-deficit analysis: A new challenge for computational neuropsychology},
journal = {Cortex},
volume = {64},
pages = {413-416},
year = {2015},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2014.10.021},
url = {https://www.sciencedirect.com/science/article/pii/S0010945214003517},
author = {Guillaume Herbet and Gilles Lafargue and Hugues Duffau}
}
@article{PAPAVLASOPOULOU2019415,
title = {Exploring children's learning experience in constructionism-based coding activities through design-based research},
journal = {Computers in Human Behavior},
volume = {99},
pages = {415-427},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2019.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0747563219300184},
author = {Sofia Papavlasopoulou and Michail N. Giannakos and Letizia Jaccheri},
keywords = {Constructionism, Coding, Computational thinking, Engagement, Children, Design-based research},
abstract = {Over the last few years, the integration of coding activities for children in K-12 education has flourished. In addition, novel technological tools and programming environments have offered new opportunities and increased the need to design effective learning experiences. This paper presents a design-based research (DBR) approach conducted over two years, based on constructionism-based coding experiences for children, following the four stages of DBR. Three iterations (cycles) were designed and examined in total, with participants aged 8–17 years old, using mixed methods. Over the two years, we conducted workshops in which students used a block-based programming environment (i.e., Scratch) and collaboratively created a socially meaningful artifact (i.e., a game). The study identifies nine design principles that can help us to achieve higher engagement during the coding activity. Moreover, positive attitudes and high motivation were found to result in the better management of cognitive load. Our contribution lies in the theoretical grounding of the results in constructionism and the emerging design principles. In this way, we provide both theoretical and practical evidence of the value of constructionism-based coding activities.}
}
@article{SCHWARTZ20192047,
title = {Biophysics and the Genomic Sciences},
journal = {Biophysical Journal},
volume = {117},
number = {11},
pages = {2047-2053},
year = {2019},
issn = {0006-3495},
doi = {https://doi.org/10.1016/j.bpj.2019.07.038},
url = {https://www.sciencedirect.com/science/article/pii/S0006349519306277},
author = {David C. Schwartz},
abstract = {It is now rare to find biological, or genetic investigations that do not rely on the tools, data, and thinking drawn from the genomic sciences. Much of this revolution is powered by contemporary sequencing approaches that readily deliver large, genome-wide data sets that not only provide genetic insights but also uniquely report molecular outcomes from experiments that biophysicists are increasingly using for potentiating structural and mechanistic investigations. In this perspective, I describe a path of how biophysical thinking greatly contributed to this revolution in ways that parallel advancements in computer science through discussion of several key inventions, described as “foundational devices.” These discussions also point at the future of how biophysics and the genomic sciences may become more finely integrated for empowering new measurement paradigms for biological investigations.}
}
@article{XIA20025,
title = {Applications of computational fluid dynamics (cfd) in the food industry: a review},
journal = {Computers and Electronics in Agriculture},
volume = {34},
number = {1},
pages = {5-24},
year = {2002},
issn = {0168-1699},
doi = {https://doi.org/10.1016/S0168-1699(01)00177-6},
url = {https://www.sciencedirect.com/science/article/pii/S0168169901001776},
author = {Bin Xia and Da-Wen Sun},
keywords = {Computational fluid dynamics, , Food, Refrigeration, Cooling, Drying, Sterilisation, Mixing, Chilling, Modelling, Simulation},
abstract = {Computational fluid dynamics (cfd) is a simulation tool, which uses powerful computer and applied mathematics to model fluid flow situations for the prediction of heat, mass and momentum transfer and optimal design in industrial processes. It is only in recent years that cfd has been applied in the food processing industry. This paper reviews the application of cfd in food processing industries including drying, sterilisation, refrigeration and mixing. The advantages of using cfd are discussed and the future of cfd applications is also outlined.}
}
@article{ROTH2023101278,
title = {Reset and restoration. The looming conservative turn of management theory: An extension of Foss et al.},
journal = {Scandinavian Journal of Management},
volume = {39},
number = {3},
pages = {101278},
year = {2023},
issn = {0956-5221},
doi = {https://doi.org/10.1016/j.scaman.2023.101278},
url = {https://www.sciencedirect.com/science/article/pii/S0956522123000192},
author = {Steffen Roth},
keywords = {The Great Reset, Management theory, Cronyism, Stratification, Conservatism, Restorism},
abstract = {This article is a reply to Foss et al.’s (2022) contribution to the special issue of the Scandinavian Journal of Management on The Great Reset of management and organization theory. In their article, the authors make a strong case that “reset thinking” geared towards a more “sustainable” redesign of the global economy promotes extensive state interventionism and cronyism capitalism, and therefore reject the idea of a need for “a fundamental rethink of existing management theory”. Whereas I do agree with the authors on most points, I am less convinced that “existing management theory” will suffice to address the problem of “reset thinking”. In this article, I demonstrate that the economy-bias of existing theories is a gateway for “reset thinking” geared towards an allegedly necessary re-/socialisation of management and organisation. A research agenda on cronyism must therefore be complemented by one on privilege and hierarchy not only as undesirable side-effects of cronyism, but also as desired outcomes of advocacy for specific minorities or missions. As self-identifications with group interests or calls for missions have become popular in management theory, I conclude that this new appetite for privilege might undermine not only the higher ideals of many management theorists, but also the foundations of modern society.}
}
@article{WANG20073776,
title = {Maximum likelihood computation based on the Fisher scoring and Gauss–Newton quadratic approximations},
journal = {Computational Statistics & Data Analysis},
volume = {51},
number = {8},
pages = {3776-3787},
year = {2007},
issn = {0167-9473},
doi = {https://doi.org/10.1016/j.csda.2006.12.037},
url = {https://www.sciencedirect.com/science/article/pii/S0167947306005147},
author = {Yong Wang},
keywords = {Maximum likelihood computation, Fisher scoring, Gauss–Newton method, Constrained optimization, Iteratively reweighted least-squares},
abstract = {The Fisher scoring and Gauss–Newton methods are two known methods for maximum likelihood computation. This paper provides a generalization for each method in a unified manner so that they can be used for some difficult maximum likelihood computation, when, for example, there exist constraints on the parameters. A generalized method does not use directly the Newton-type iteration formulas of these methods, but, instead, uses the corresponding quadratic functions transformed from them. It proceeds by repeatedly approximating the log-likelihood function with the quadratic functions in the neighborhoods of the current iterates and optimizing each quadratic function within the parameter space. It is shown that each quadratic function has a weighted linear regression formulation, which can be conveniently solved. This generalization also extends the applicability of the Fisher scoring method to situations when the expected Fisher information matrices are unavailable in closed form. Fast computation can generally be anticipated, owing to their small rates of convergence and a rapid solution of each linear regression problem. While the generalized Gauss–Newton method may sometimes suffer for the so-called large residual problem, the generalized Fisher scoring method has performed consistently well in the numerical experiments we conducted.}
}
@incollection{NAGURNEY1996335,
title = {Chapter 7 Parallel computation},
series = {Handbook of Computational Economics},
publisher = {Elsevier},
volume = {1},
pages = {335-404},
year = {1996},
issn = {1574-0021},
doi = {https://doi.org/10.1016/S1574-0021(96)01009-X},
url = {https://www.sciencedirect.com/science/article/pii/S157400219601009X},
author = {Anna Nagurney},
abstract = {Publisher Summary
Parallel computation represents not only a new mode of computation, but, a new intellectual paradigm. This chapter provides an overview of the technology of parallel computation in terms of hardware and programming languages; presents some of the fundamental classes of problems encountered in economics and the associated numerical methodologies for their solution; discusses the state-of-the-art computational techniques and focuses on parallel techniques and contrasts them with serial techniques for illumination and instructive purposes; and presents applications of the classes of problems and associated numerical methods to econometrics, microeconomics, macroeconomics, and finance. The emergence of computation as a basic scientific methodology in economics has given access to solutions of fundamental problems that pure analysis, observation, or experimentation could not have achieved. Parallel computation represents the wave of the future. It is considered to be cheaper and faster than serial computing and the only approach to faster computation currently foreseeable. Parallel computation is appealing, hence, for the economies of scale that are possible, for the potentially faster solution of large-scale problems, and also for the possibilities that it presents for imitating adjustment or tatonnement processes.}
}
@article{SUMAR20103980,
title = {Computational intelligence approach to PID controller design using the universal model},
journal = {Information Sciences},
volume = {180},
number = {20},
pages = {3980-3991},
year = {2010},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2010.06.026},
url = {https://www.sciencedirect.com/science/article/pii/S0020025510002872},
author = {Rodrigo Rodrigues Sumar and Antonio Augusto Rodrigues Coelho and Leandro dos Santos Coelho},
keywords = {PID control, Nonlinear systems, Fuzzy systems, Neural networks, Differential evolution, Optimization},
abstract = {Despite the popularity of PID (Proportional-Integral-Derivative) controllers, their tuning aspect continues to present challenges for researches and plant operators. Various control design methodologies have been proposed in the literature, such as auto-tuning, self-tuning, and pattern recognition. The main drawback of these methodologies in the industrial environment is the number of tuning parameters to be selected. In this paper, the design of a PID controller, based on the universal model of the plant, is derived, in which there is only one parameter to be tuned. This is an attractive feature from the viewpoint of plant operators. Fuzzy and neural approaches – bio-inspired methods in the field of computational intelligence – are used to design and assess the efficiency of the PID controller design based on differential evolution optimization in nonlinear plants. The numerical results presented herein indicate that the proposed bio-inspired design is effective for the nonlinear control of nonlinear plants.}
}
@incollection{TSATSE20212033,
title = {Reflections on the development of scenario and problem-based chemical engineering projects},
editor = {Metin Türkay and Rafiqul Gani},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {50},
pages = {2033-2038},
year = {2021},
booktitle = {31st European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-323-88506-5.50314-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780323885065503144},
author = {A. Tsatse and E. Sorensen},
keywords = {problem-based learning, Scenarios, Process Systems Engineering},
abstract = { Abstract
This work reflects on the use of scenario- and problem-based learning as a way of conveying not only fundamental knowledge, but also to provide training in the use of computational Process Systems Engineering (PSE) tools applied to open-ended real world problems. The teaching framework also has a strong emphasis on the development of professional skills and to evaluate the recommended design solutions considering multiple perspectives such as economics, safety, environment and societal context. The framework is implemented through week-long group projects called Scenarios, taking place mainly in the first two years of study, and examples are given of different variations of Scenarios. This teaching approach has multiple benefits, including but not limited to, students’ understanding of PSE tools and the development of their critical engineering thinking.}
}
@article{SCHMID2022,
title = {Mendelian or Multifactorial? Current Undergraduate Genetics Assessments Focus on Genes and Rarely Include the Environment},
journal = {Journal of Microbiology & Biology Education},
volume = {23},
number = {3},
year = {2022},
issn = {1935-7877},
doi = {https://doi.org/10.1128/jmbe.00093-22},
url = {https://www.sciencedirect.com/science/article/pii/S1935787722000302},
author = {Kelly M. Schmid and Dennis Lee and Monica Weindling and Awais Syed and Stephanie-Louise Yacoba Agyemang and Brian Donovan and Gregory Radick and Michelle K. Smith and L. Kate Wright},
keywords = {assessment, curriculum, environment, genes, genetics, undergraduate},
abstract = {Undergraduate genetics courses have historically focused on simple genetic models, rather than taking a more multifactorial approach where students explore how traits are influenced by a combination of genes, the environment, and gene-by-environment interactions. While a focus on simple genetic models can provide straightforward examples to promote student learning, they do not match the current scientific understanding and can result in deterministic thinking among students.
ABSTRACT
Undergraduate genetics courses have historically focused on simple genetic models, rather than taking a more multifactorial approach where students explore how traits are influenced by a combination of genes, the environment, and gene-by-environment interactions. While a focus on simple genetic models can provide straightforward examples to promote student learning, they do not match the current scientific understanding and can result in deterministic thinking among students. In addition, undergraduates are often interested in complex human traits that are influenced by the environment, and national curriculum standards include learning objectives that focus on multifactorial concepts. This research aims to discover to what extent multifactorial genetics is currently being assessed in undergraduate genetics courses. To address this, we analyzed over 1,000 assessment questions from a commonly used undergraduate genetics textbook; published concept assessments; and open-source, peer-reviewed curriculum materials. Our findings show that current genetics assessment questions overwhelmingly emphasize the impact of genes on phenotypes and that the effect of the environment is rarely addressed. These results indicate a need for the inclusion of more multifactorial genetics concepts, and we suggest ways to introduce them into undergraduate courses.}
}
@article{MALDONADO2014177,
title = {Synchronicity among Biological and Computational Levels of an Organism: Quantum Biology and Complexity},
journal = {Procedia Computer Science},
volume = {36},
pages = {177-184},
year = {2014},
note = {Complex Adaptive Systems Philadelphia, PA November 3-5, 2014},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.09.076},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914013258},
author = {Carlos E. Maldonado and Nelson A. Gómez-Cruz},
keywords = {quantum biology, living systems, non-linear systems, complexity science, theory, health.},
abstract = {This paper argues that there is a synchronicity among biological and computational levels on an organism and provides arguments and proofs based on experimental research gathered in the literature. The leading thread is the interplay between quantum biology (QB) and complexity. As the paper asks whether QB does contribute to complexity science (CS), five arguments are provided: (i) Firstly a state-of-the art of QB and its relationship to CS is sketched out. Thereafter, the attention is directed to answering the question set out; (ii) Secondly, it digs into the understanding of life toward deeper levels of reality; (iii) It is shown that non-trivial quantum effects shed insightful lights on the information processing of and within living beings; (iv) Once the distinction is made between increasing levels of complexity and increasing levels of organization, the focus lies in the importance of QB for organization, and not so much for complexity as such; (v) The role of information rises at the center of all concerns, and the intertwining of complexity and information processing. At the end some conclusions are drawn.}
}
@article{POST1987339,
title = {Latest thinking on the Malpasset accident},
journal = {Engineering Geology},
volume = {24},
number = {1},
pages = {339-353},
year = {1987},
note = {Dam Failures},
issn = {0013-7952},
doi = {https://doi.org/10.1016/0013-7952(87)90071-8},
url = {https://www.sciencedirect.com/science/article/pii/0013795287900718},
author = {G. Post and D. Bonazzi}
}
@article{ROSEN20211,
title = {A word is worth a thousand pictures: A 20-year comparative analysis of aberrant abstraction in schizophrenia, affective psychosis, and non-psychotic depression},
journal = {Schizophrenia Research},
volume = {238},
pages = {1-9},
year = {2021},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2021.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0920996421003674},
author = {Cherise Rosen and Martin Harrow and Liping Tong and Thomas H. Jobe and Helen Harrow},
keywords = {Abstraction, Concretism, Aberrant abstraction, Schizophrenia, Affective psychosis, Unipolar depression non-psychotic},
abstract = {Abstract thinking is a cognitive process that involves the assimilation of concepts reduced from diffuse sensory input, organized, and interpreted in a manner beyond the obvious. There are multiple facets by which abstraction is measured that include semantic, visual-spatial and social comprehension. This study examined the prevalence and course of abstract and concrete responses to semantic proverbs and aberrant abstraction (composite score of semantic, visual-spatial, and social comprehension) over 20 years in 352 participants diagnosed with schizophrenia, affective psychosis, and unipolar non-psychotic depression. We utilized linear models, two-way ANOVA and contrasts to compare groups and change over time. Linear models with Generalized Estimation Equation (GEE) to determine association. Our findings show that regardless of diagnosis, semantic proverb interpretation improves over time. Participants with schizophrenia give more concrete responses to proverbs when compared to affective psychosis and unipolar depressed without psychosis. We also show that the underlying structure of concretism encompasses increased conceptual overinclusion at index hospitalization and idiosyncratic associations at follow-up; whereas, abstract thinking overtime encompasses increased visual-spatial abstraction at index and rich associations with increased social comprehension scores at follow-up. Regardless of diagnosis, premorbid functioning, descriptive characteristics, and IQ were not associated with aberrant abstraction. Delusions are highly and positively related to aberrant abstraction scores, while hallucinations are mildly and positively related to this score. Lastly, our data point to the importance of examining the underlying interconnected structures of ‘established’ constructs vis-à-vis mixed methods to provide a description of the rich interior world that may not always map onto current quantitative measures.}
}
@article{ALONSO20092683,
title = {A method to generate computationally efficient reduced order models},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {198},
number = {33},
pages = {2683-2691},
year = {2009},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2009.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0045782509001376},
author = {D. Alonso and A. Velazquez and J.M. Vega},
keywords = {Reduced order model, Proper Orthogonal Decomposition, Incompressible nonisothermal flow},
abstract = {A new method is presented to generate reduced order models (ROMs) in Fluid Dynamics problems. The method is based on the expansion of the flow variables on a Proper Orthogonal Decomposition (POD) basis, calculated from a limited number of snapshots, which are obtained via Computational Fluid Dynamics (CFD). Then, the POD-mode amplitudes are calculated as minimizers of a properly defined overall residual of the equations and boundary conditions. The residual can be calculated using only a limited number of points in the flow field, which can be scattered either all over the whole computational domain or over a smaller projection window. This means that the process is both computationally efficient (reconstructed flow fields require less than 1% of the time needed to compute a full CFD solution) and flexible (the projection window can avoid regions of large localized CFD errors). Also, various definitions of the residual are briefly discussed, along with the number and distribution of snapshots, the number of retained modes, and the effect of CFD errors, to conclude that the method is numerically robust. This is because the results are largely insensitive to the definition of the residual, to CFD errors, and to the CFD method itself, which may contain artificial stabilizing terms. Thus, the method is amenable for practical engineering applications.}
}
@article{KERBER2012239,
title = {A worst-case bound for topology computation of algebraic curves},
journal = {Journal of Symbolic Computation},
volume = {47},
number = {3},
pages = {239-258},
year = {2012},
issn = {0747-7171},
doi = {https://doi.org/10.1016/j.jsc.2011.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0747717111001775},
author = {Michael Kerber and Michael Sagraloff},
keywords = {Topology computation, Algebraic curve, Amortized analysis, Complexity analysis},
abstract = {Computing the topology of an algebraic plane curve C means computing a combinatorial graph that is isotopic to C and thus represents its topology in R2. We prove that, for a polynomial of degree n with integer coefficients bounded by 2ρ, the topology of the induced curve can be computed with Õ(n8ρ(n+ρ)) bit operations (Õ indicates that we omit logarithmic factors). Our analysis improves the previous best known complexity bounds by a factor of n2. The improvement is based on new techniques to compute and refine isolating intervals for the real roots of polynomials, and on the consequent amortized analysis of the critical fibers of the algebraic curve.}
}
@article{METHLING2022100013,
title = {Heuristics in multi-criteria decision-making: The cost of fast and frugal decisions},
journal = {EURO Journal on Decision Processes},
volume = {10},
pages = {100013},
year = {2022},
issn = {2193-9438},
doi = {https://doi.org/10.1016/j.ejdp.2022.100013},
url = {https://www.sciencedirect.com/science/article/pii/S2193943822000024},
author = {Florian Methling and Sara J.M. Abdeen and Rüdiger {von Nitzsch}},
keywords = {MCDM, Decision support, Heuristics, Utility theory, Value-focused thinking},
abstract = {There has been an ongoing debate in research regarding the use of heuristics in decision-making. Advocators have succeeded in showing that applying heuristics not only reduces effort but can even be more accurate than analytical approaches under certain conditions. Others point out the biases and cognitive distortions inherent in disregarding information. Researchers have used both simulations and experiments to study how the use of heuristics affects the decision's outcome. However, a good decision is determined by the process and not a lucky outcome. It is a conscious reflection on the decision-maker's information and preferences. Therefore, a heuristic must be assessed by its ability to match a structured decision processing all available information. Thus, the question remains: how often does the reduction of information considered in heuristic decisions lead to a different recommended alternative? We applied different heuristics to a dataset of 945 real, personal decisions. We have found that by using heuristics instead of a fully developed decision structure, in 60.34% of cases, a different alternative would have been recommended to the decision-maker leading to a mean relative utility loss for the deviating decisions of 34.58%. This shows that a continuous effort to reflect on the weighing of objectives and alternatives leads to better decisions.}
}
@article{GUPTA2009481,
title = {Does phonological short-term memory causally determine vocabulary learning? Toward a computational resolution of the debate},
journal = {Journal of Memory and Language},
volume = {61},
number = {4},
pages = {481-502},
year = {2009},
issn = {0749-596X},
doi = {https://doi.org/10.1016/j.jml.2009.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0749596X09000825},
author = {Prahlad Gupta and Jamie Tisdale},
keywords = {Nonword repetition, Vocabulary learning, Computational modeling, Phonological memory, Word learning, Short-term memory, Language},
abstract = {The relationship between nonword repetition ability and vocabulary size and vocabulary learning has been a topic of intense research interest and investigation over the last two decades, following the demonstration that nonword repetition accuracy is predictive of vocabulary size (Gathercole & Baddeley, 1989). However, the nature of this relationship is not well understood. One prominent account posits that phonological short-term memory (PSTM) is a causal determinant both of nonword repetition ability and of phonological vocabulary learning, with the observed correlation between the two reflecting the effect of this underlying third variable (e.g., Baddeley, Gathercole, & Papagno, 1998). An alternative account proposes the opposite causality: that it is phonological vocabulary size that causally determines nonword repetition ability (e.g., Snowling, Chiat, & Hulme, 1991). We present a theory of phonological vocabulary learning, instantiated as a computational model. The model offers a precise account of the construct of PSTM, of performance in the nonword repetition task, of novel word form learning, and of the relationship between all of these. We show through simulation not only that PSTM causally affects both nonword repetition accuracy and phonological vocabulary size, but also that phonological vocabulary size causally affects nonword repetition ability. The plausibility of the model is supported by the fact that its nonword repetition accuracy displays effects of phonotactic probability and of nonword length, which have been taken as evidence for causal effects on nonword repetition accuracy of phonological vocabulary knowledge and PSTM, respectively. Thus the model makes explicit how the causal links posited by the two theoretical perspectives are both valid, in the process reconciling the two perspectives, and indicating that an opposition between them is unnecessary.}
}
@article{FOLLI2022102458,
title = {Biases in belief reports},
journal = {Journal of Economic Psychology},
volume = {88},
pages = {102458},
year = {2022},
issn = {0167-4870},
doi = {https://doi.org/10.1016/j.joep.2021.102458},
url = {https://www.sciencedirect.com/science/article/pii/S016748702100088X},
author = {Dominik Folli and Irenaeus Wolff},
keywords = {Belief elicitation, Belief formation, Belief-action consistency, Framing effects, Projection, Consensus effect, Wishful thinking,  rationalization},
abstract = {Belief elicitation is important in many different fields of economic research. We show that how a researcher elicits such beliefs – in particular, whether the belief is about the participant’s opponent, an unrelated other, or the population of others – strongly affects the belief reports. We study the underlying processes and find a clear consensus effect. Yet, when matching the opponent’s action would lead to a low payoff and the researcher asks for the belief about this opponent, ex-post rationalization kicks in and beliefs are re-adjusted again. Hence, we recommend to ask about unrelated others or about the population in such cases, as ‘opponent beliefs’ are even more detached from the beliefs participants had when deciding about their actions in the corresponding game. We find no evidence of wishful thinking in any of the treatments.}
}
@incollection{HUDEDAGADDI2017233,
title = {Chapter 7 - Quantum inspired computational intelligent techniques in image segmentation},
editor = {Siddhartha Bhattacharyya and Ujjwal Maulik and Paramartha Dutta},
booktitle = {Quantum Inspired Computational Intelligence},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {233-258},
year = {2017},
isbn = {978-0-12-804409-4},
doi = {https://doi.org/10.1016/B978-0-12-804409-4.00007-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128044094000073},
author = {D.P. Hudedagaddi and B.K. Tripathy},
keywords = {Quantum computing, Computing intelligence, Image segmentation, Evolutionary algorithms},
abstract = {Quantum computing (QC) is a new area of research which incorporates elements from mathematics, physics, and computing. Quantum computing has generated a growing interest among scientists, technologists, and industrialists. Over the past decade it provided a platform for research to people in the scientific, technical, and industrial fields. Quantum physics concepts have been used in developing the basics of QC. In QC, the parallel processing feature has reduced the algorithm complexities which are being used. This feature helped find solutions to several optimization problems and issues that were related to it. Quantum inspired intelligent computational methods have been used in several application areas. Image segmentation is one such area and the exploration of this feature in image segmentation is the primary focus of this chapter.}
}
@incollection{SEJNOWSKI20012460,
title = {Computational Neuroscience},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {2460-2465},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/03419-7},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767034197},
author = {T.J. Sejnowski},
abstract = {The goal of computational neuroscience is to explain in computational terms how brains generate behaviors. Computational models of the brain explore how populations of highly interconnected neurons are formed during development and how they come to represent, process, store, act upon, and become altered by, information present in the environment. Techniques from computer science and mathematics are used to simulate and analyze these computational models and provide links between the widely ranging levels of investigation, from the molecular to the systems levels. Computational neuroscience is a relatively young discipline that is growing rapidly. Most of the models that have been developed thus far have been aimed at interpreting experimental data and providing a conceptual framework for the dynamic properties of neural systems. A more comprehensive theory of brain function should arise as we gain a broader understanding of the computational resources of nervous systems at all levels of organization.}
}
@article{HAMALAINEN2024100050,
title = {Generating policy alternatives for decision making: A process model, behavioural issues, and an experiment},
journal = {EURO Journal on Decision Processes},
volume = {12},
pages = {100050},
year = {2024},
issn = {2193-9438},
doi = {https://doi.org/10.1016/j.ejdp.2024.100050},
url = {https://www.sciencedirect.com/science/article/pii/S2193943824000062},
author = {Raimo P. Hämäläinen and Tuomas J. Lahtinen and Kai Virtanen},
keywords = {Policy decision, Generation of policy alternatives, Portfolio decision analysis, Path dependence, Cognitive biases and heuristics},
abstract = {The generation of alternative policies is essential in complex decision tasks with multiple interests and stakeholders. A diverse set of policies is typically desirable to cover the range of options and objectives. Decision modelling literature has often assumed that clearly defined decision alternatives are readily available. This is not a realistic assumption in practice. We present a structured process model for the generation of policy alternatives in settings that include non-quantifiable elements and where portfolio optimisation approaches are not applicable. Behavioural issues and path dependence as well as heuristics and biases which can occur during the process are discussed. The behavioural experiment compares policy alternatives obtained by using two different portfolio generation techniques. The results of the experiment demonstrate that path dependence can occur in policy generation. We report thinking patterns of subjects which relate to biases and heuristics.}
}
@article{ALPUENTE20153,
title = {Exploring conditional rewriting logic computations},
journal = {Journal of Symbolic Computation},
volume = {69},
pages = {3-39},
year = {2015},
note = {Symbolic Computation in Software Science},
issn = {0747-7171},
doi = {https://doi.org/10.1016/j.jsc.2014.09.028},
url = {https://www.sciencedirect.com/science/article/pii/S0747717114000960},
author = {M. Alpuente and D. Ballis and F. Frechina and J. Sapiña},
keywords = {Rewriting logic, Trace exploration, Maude, Conditional rewrite theories},
abstract = {Trace exploration is concerned with techniques that allow computation traces to be dynamically searched for specific contents. Depending on whether the exploration is carried backward or forward, trace exploration techniques allow provenance tracking or impact tracking to be done. The aim of provenance tracking is to show how (parts of) a program output depends on (parts of) its input and to help estimate which input data need to be modified to accomplish a change in the outcome. The aim of impact tracking is to identify the scope and potential consequences of changing the program input. Rewriting Logic (RWL) is a logic of change that supplements (an extension of) the equational logic by adding rewrite rules that are used to describe (nondeterministic) transitions between states. In this paper, we present a rich and highly dynamic, parameterized technique for the forward inspection of RWL computations that allows the nondeterministic execution of a given conditional rewrite theory to be followed up in different ways. With this technique, an analyst can browse, slice, filter, or search the traces as they come to life during the program execution. The navigation of the trace is driven by a user-defined, inspection criterion that specifies the required exploration mode. By selecting different inspection criteria, one can automatically derive a family of practical algorithms such as program steppers and more sophisticated dynamic trace slicers that compute summaries of the computation tree, thereby facilitating the dynamic detection of control and data dependencies across the tree. Our methodology, which is implemented in the Anima graphical tool, allows users to evaluate the effects of a given statement or instruction in isolation, track input change impact, and gain insight into program behavior (or misbehavior).}
}
@article{POPAT2019365,
title = {Learning to code or coding to learn? A systematic review},
journal = {Computers & Education},
volume = {128},
pages = {365-376},
year = {2019},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2018.10.005},
url = {https://www.sciencedirect.com/science/article/pii/S0360131518302768},
author = {Shahira Popat and Louise Starkey},
keywords = {Coding, Programming, School, Computer, Outcome, Skills},
abstract = {The resurgence of computer programming in the school curriculum brings a promise of preparing students for the future that goes beyond just learning how to code. This study reviewed research to analyse educational outcomes for children learning to code at school. A systematic review was applied to identify relevant articles and a thematic analysis to synthesise the findings. Ten articles were included in the synthesis and an overarching model was developed which depicts the themes. The results demonstrate that although students are learning to code, a range of other educational outcomes can be learnt or practiced through the teaching of coding. These included mathematical problem-solving, critical thinking, social skills, self-management and academic skills. The review also identified the importance of instructional design for developing these educational outcomes through coding.}
}
@article{PALMERI2004378,
title = {Computational approaches to the development of perceptual expertise},
journal = {Trends in Cognitive Sciences},
volume = {8},
number = {8},
pages = {378-386},
year = {2004},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2004.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1364661304001603},
author = {Thomas J. Palmeri and Alan C-N. Wong and Isabel Gauthier},
abstract = {Dog experts, ornithologists, radiologists and other specialists are noted for their remarkable abilities at categorizing, identifying and recognizing objects within their domain of expertise. A complete understanding of the development of perceptual expertise requires a combination of thorough empirical research and carefully articulated computational theories that formalize specific hypotheses about the acquisition of expertise. A comprehensive computational theory of the development of perceptual expertise remains elusive, but we can look to existing computational models from the object-recognition, perceptual-categorization, automaticity and related literatures for possible starting points. Arguably, hypotheses about the development of perceptual expertise should first be explored within the context of existing computational models of visual object understanding before considering the creation of highly modularized adaptations for particular domains of perceptual expertise.}
}
@article{NUNES2020117761,
title = {Thinking the future of membranes: Perspectives for advanced and new membrane materials and manufacturing processes},
journal = {Journal of Membrane Science},
volume = {598},
pages = {117761},
year = {2020},
issn = {0376-7388},
doi = {https://doi.org/10.1016/j.memsci.2019.117761},
url = {https://www.sciencedirect.com/science/article/pii/S0376738819333113},
author = {Suzana P. Nunes and P. Zeynep Culfaz-Emecen and Guy Z. Ramon and Tymen Visser and Geert Henk Koops and Wanqin Jin and Mathias Ulbricht},
abstract = {The state-of-the-art of membrane technology is characterized by a number of mature applications such as sterile filtration, hemodialysis, water purification and gas separation, as well as many more niche applications of successful membrane-based separation and processing of fluid mixtures. The membrane industry is currently employing a portfolio of established materials, mostly standard polymers or inorganic materials (not originally developed for membranes), and easily scalable manufacturing processes such as phase inversion, interfacial polymerization and coating. Innovations in membranes and their manufacturing processes must meet the desired intrinsic properties that determine selectivity and flux, for specific applications. However, tunable and stable performance, as well as sustainability over the entire life cycle of membrane products are becoming increasingly important. Membrane manufacturers are progressively required to share the carbon footprint of their membrane modules with their customers. Environmental awareness among the world's population is a growing phenomenon and finds its reflection in product development and manufacturing processes. In membrane technology one can see initial steps in this direction with the replacement of hazardous solvents, the utilization of renewable materials for membrane production and the reuse of membrane modules. Other examples include increasing the stability of organic membrane polymers and lowering the cost of inorganic membranes. In a long-term perspective, many more developments in materials science will be required for making new, advanced membranes. These include “tools” such as self-assembly or micro- and nano-fabrication, and “building blocks”, e.g. tailored block copolymers or 1D, 2D and 3D materials. Such membranes must be fabricated in a simpler manner and be more versatile than existing ones. In this perspective paper, a vision of such LEGO®-like membranes with precisely adjustable properties will be illustrated with, where possible, examples that already demonstrate feasibility. These include the possibility to switch properties using an external stimulus, adapting a membrane's selectivity to a given separation, or providing the ability to assemble, disassemble and reassemble the membrane on a suitable support as scaffold, in situ, in place and on-demand. Overall, it is foreseen that the scope of future membrane applications will become much wider, based on improved existing membrane materials and manufacturing processes, as well as the combination of novel, tailor-made “building blocks” and “tools” for the fabrication of next-generation membranes tuned to specific applications.}
}
@article{ARLE2014642,
title = {Mechanism of Dorsal Column Stimulation to Treat Neuropathic but not Nociceptive Pain: Analysis With a Computational Model},
journal = {Neuromodulation: Technology at the Neural Interface},
volume = {17},
number = {7},
pages = {642-655},
year = {2014},
issn = {1094-7159},
doi = {https://doi.org/10.1111/ner.12178},
url = {https://www.sciencedirect.com/science/article/pii/S1094715914601410},
author = {Jeffrey E. Arle and Kristen W. Carlson and Longzhi Mei and Nicolae Iftimia and Jay L. Shils},
keywords = {Chronic pain, dorsal column stimulation, gate control theory of pain, neural circuitry modeling, neuromodulation mechanism, neuropathic pain, spinal cord stimulation},
abstract = {Objective:
Stimulation of axons within the dorsal columns of the human spinal cord has become a widely used therapy to treat refractory neuropathic pain. The mechanisms have yet to be fully elucidated and may even be contrary to standard “gate control theory.” Our hypothesis is that a computational model provides a plausible description of the mechanism by which dorsal column stimulation (DCS) inhibits wide dynamic range (WDR) cell output in a neuropathic model but not in a nociceptive pain model.
Materials and Methods:
We created a computational model of the human spinal cord involving approximately 360,000 individual neurons and dendritic processing of some 60 million synapses—the most elaborate dynamic computational model of the human spinal cord to date. Neuropathic and nociceptive “pain” signals were created by activating topographically isolated regions of excitatory interneurons and high-threshold nociceptive fiber inputs, driving analogous regions of WDR neurons. Dorsal column fiber activity was then added at clinically relevant levels (e.g., Aβ firing rate between 0 and 110 Hz by using a 210-μsec pulse width, 50–150 Hz frequency, at 1–3 V amplitude).
Results:
Analysis of the nociceptive pain, neuropathic pain, and modulated circuits shows that, in contradiction to gate control theory, 1) nociceptive and neuropathic pain signaling must be distinct, and 2) DCS neuromodulation predominantly affects the neuropathic signal only, inhibiting centrally sensitized pathological neuron groups and ultimately the WDR pain transmission cells.
Conclusion:
We offer a different set of necessary premises than gate control theory to explain neuropathic pain inhibition and the relative lack of nociceptive pain inhibition by using retrograde DCS. Hypotheses regarding not only the pain relief mechanisms of DCS were made but also regarding the circuitry of pain itself, both nociceptive and neuropathic. These hypotheses and further use of the model may lead to novel stimulation paradigms.}
}
@article{RIESENFELD20151054,
title = {Initiating a CAD renaissance: Multidisciplinary analysis driven design: Framework for a new generation of advanced computational design, engineering and manufacturing environments},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {284},
pages = {1054-1072},
year = {2015},
note = {Isogeometric Analysis Special Issue},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2014.11.024},
url = {https://www.sciencedirect.com/science/article/pii/S0045782514004502},
author = {Richard F. Riesenfeld and Robert Haimes and Elaine Cohen},
keywords = {Multidisciplinary analysis driven design, Integrated computational engineering, CAD/CAE/CAM/IGA},
abstract = {We present a critical analysis of the effectiveness of the current field of CAD, and discuss some of the forces that have taken it so far off course from its strikingly foresighted origins. Armed with the ensuing understanding of the operational forces that have taken CAD adrift, we conclude that the disparity between CAD’s mired state-of-the-art condition relative to more appropriate, inspired and achievable goals for CAD calls for more drastic measures. It is asserted that, well beyond the evolutionary progression of incremental steps characteristic of next version system releases, the field is overdue for developing a class of genuine design-centric, ab initio, CAD systems architectures effecting the original CAD vision through the powerful instruments of contemporary computing tools and technologies.}
}
@article{ANDERSON2015309,
title = {Reading visually embodied meaning from the brain: Visually grounded computational models decode visual-object mental imagery induced by written text},
journal = {NeuroImage},
volume = {120},
pages = {309-322},
year = {2015},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2015.06.093},
url = {https://www.sciencedirect.com/science/article/pii/S1053811915006345},
author = {Andrew James Anderson and Elia Bruni and Alessandro Lopopolo and Massimo Poesio and Marco Baroni},
keywords = {Concept representation, Embodiment, Mental imagery, Perceptual simulation, Language, Multimodal semantic models, Representational similarity},
abstract = {Embodiment theory predicts that mental imagery of object words recruits neural circuits involved in object perception. The degree of visual imagery present in routine thought and how it is encoded in the brain is largely unknown. We test whether fMRI activity patterns elicited by participants reading objects' names include embodied visual-object representations, and whether we can decode the representations using novel computational image-based semantic models. We first apply the image models in conjunction with text-based semantic models to test predictions of visual-specificity of semantic representations in different brain regions. Representational similarity analysis confirms that fMRI structure within ventral-temporal and lateral-occipital regions correlates most strongly with the image models and conversely text models correlate better with posterior-parietal/lateral-temporal/inferior-frontal regions. We use an unsupervised decoding algorithm that exploits commonalities in representational similarity structure found within both image model and brain data sets to classify embodied visual representations with high accuracy (8/10) and then extend it to exploit model combinations to robustly decode different brain regions in parallel. By capturing latent visual-semantic structure our models provide a route into analyzing neural representations derived from past perceptual experience rather than stimulus-driven brain activity. Our results also verify the benefit of combining multimodal data to model human-like semantic representations.}
}
@incollection{ZHENG202411,
title = {Chapter Two - Reviewing the past enables us to learn},
editor = {Wenbo Zheng and Fei-Yue Wang},
booktitle = {Computational Knowledge Vision},
publisher = {Academic Press},
pages = {11-38},
year = {2024},
isbn = {978-0-443-21619-0},
doi = {https://doi.org/10.1016/B978-0-44-321619-0.00008-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044321619000008X},
author = {Wenbo Zheng and Fei-Yue Wang},
keywords = {Computer vision, Artificial intelligence, Knowledge, Knowledge-based vision, Visual information},
abstract = {This chapter reviews the history of computer vision and artificial intelligence. Computer vision is the field of artificial intelligence that studies how computers can simulate the visual system of humans or other living things. It aims to enable computers to perceive and understand through the processing of visual information based on images or videos. From the 20th century onward, computer vision theory has been progressively developed. King-Sun Fu proposed syntactically structured representation and computation and constructed a top-down computational theory of vision. In the 1970s, David Marr then combined the knowledge of neuroscience, psychology, and other subjects of his time to systematically formulate a computational theory of vision, which made it possible to develop a more rigorous theory of the processing of visual information. Since then, computer vision has been flourishing.}
}
@article{IOANNIDOU2009236,
title = {AgentCubes: Incremental 3D end-user development},
journal = {Journal of Visual Languages & Computing},
volume = {20},
number = {4},
pages = {236-251},
year = {2009},
note = {Special Issue on Best Papers from VL/HCC2008},
issn = {1045-926X},
doi = {https://doi.org/10.1016/j.jvlc.2009.04.001},
url = {https://www.sciencedirect.com/science/article/pii/S1045926X09000238},
author = {Andri Ioannidou and Alexander Repenning and David C. Webb},
keywords = {Incremental 3D, Game design, Visual programming, End-user development, IT fluency, Computational thinking},
abstract = {3D game development can be an enticing way to attract K-12 students to computer science, but designing and programming 3D games is far from trivial. Students need to achieve a certain level of 3D fluency in modeling, animation, and programming to be able to create compelling 3D content. The combination of innovative end-user development tools and standards-based curriculum that promotes IT fluency by shifting the pedagogical focus from programming to design, can address motivational aspects without sacrificing principled educational goals. The AgentCubes 3D game-authoring environment raises the ceiling of end-user development without raising the threshold. Our formal user study shows that with Incremental 3D, the gradual approach to transition from 2D to 3D authoring, middle school students can build sophisticated 3D games including 3D models, animations, and programming.}
}
@article{AGGARWAL2023110458,
title = {Quantum healthcare computing using precision based granular approach},
journal = {Applied Soft Computing},
volume = {144},
pages = {110458},
year = {2023},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2023.110458},
url = {https://www.sciencedirect.com/science/article/pii/S1568494623004763},
author = {Lakshita Aggarwal and Shelly Sachdeva and Puneet Goswami},
keywords = {Quantum computing, Qubits, Healthcare, Diagnosis, Classical computing, Precision},
abstract = {Previously, doctors interpreted diseases and their outcomes according to their experience in diagnosis. However, with the rapid increase in technology and population, the task of examining the patient becomes cumbersome and sometimes human efforts produce inconsistent results. Several research is being done for healthcare in terms of improving visualization and accuracy by using machine learning models. The current research targets to explore quantum computing as a different way of processing information compared to classical computer systems such as the use of quantum bits (qubits) along with superposition and entanglement for extending the computation capabilities at an unprecedented level of thinking in the healthcare domain. Quantum computing systems provide exponential benefits in terms of high-speed processing, faster and easier diagnostic assistance, unimaginable reduction in processing throughput, and many more. An extensive comparative analysis of existing approaches has been made which benchmarks the need for quantum healthcare computing. The objective of this work is to interpret whether Quantum computers prove to be more trusted when it comes to patient diagnosis, and faster analysis leading to cost optimization. In order to accelerate patient diagnosis, different approaches have been presented. The authors have proposed a precision-based granular approach for patient diagnosis that incorporates diagnosing the disease with enhanced precision and granularity. It involves reporting symptoms by the patient, encountering by healthcare expert on multiple factors, precise examination, granular health status (understanding past and present medical history), followed by a precise intervention by understanding biomolecular simulations. The algorithm has been presented to describe the flow process for patient diagnosis modeling using quantum computing. It involves qubits initialization, pairing the values, assigning probabilistic values, cross-validation, and quantum circuit formation. Precision-based granular approach has been implemented for a scenario (consisting of medical parameters such as oxygen and heart rate level, with the functionality of diagnosing oxygen level and heart range which lies as either normal or not normal (high/low)). Precision-based granular approach deals specifically with the individual ‘biomolecular simulation by understanding variations in the individual body whereas the umbrella-based approach does not deal with specifically to individual mechanisms. Granular level of encounter is not possible in umbrella-based treatment. Python Jupyter notebook and IBM Composer tool is used for the implementation of results. Bloch sphere and computational state graph are obtained as an output for better visualization and understanding. Falcon r5.11H processor is used with the version of 1.0.24 of IBM Composer to simulate the experiment. The methodology using precision based granular approach provides timely encounter of disease along with umbrella diagnosis and precise treatment. The time is taken and frequency of qubits have been presented with promising results. The diagnosis process and optimizing cost efficiency can aid in an early detection of the disease.}
}
@article{ANDERSON201738,
title = {Isolating blocks as computational tools in the circular restricted three-body problem},
journal = {Physica D: Nonlinear Phenomena},
volume = {343},
pages = {38-50},
year = {2017},
issn = {0167-2789},
doi = {https://doi.org/10.1016/j.physd.2016.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167278916303013},
author = {Rodney L. Anderson and Robert W. Easton and Martin W. Lo},
keywords = {Circular restricted three-body problem, Isolating blocks, Invariant manifolds, Invariant 3-sphere},
abstract = {Isolating blocks may be used as computational tools to search for the invariant manifolds of orbits and hyperbolic invariant sets associated with libration points while also giving additional insight into the dynamics of the flow in these regions. We use isolating blocks to investigate the dynamics of objects entering the Earth–Moon system in the circular restricted three-body problem with energies close to the energy of the L2 libration point. Specifically, the stable and unstable manifolds of Lyapunov orbits and the hyperbolic invariant set around the libration points are obtained by numerically computing the way orbits exit from an isolating block in combination with a bisection method. Invariant spheres of solutions in the spatial problem may then be located using the resulting manifolds.}
}
@article{HUANG2006567,
title = {An integrated computational intelligence approach to product concept generation and evaluation},
journal = {Mechanism and Machine Theory},
volume = {41},
number = {5},
pages = {567-583},
year = {2006},
issn = {0094-114X},
doi = {https://doi.org/10.1016/j.mechmachtheory.2005.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0094114X05001333},
author = {Hong-Zhong Huang and Ruifeng Bo and Wei Chen},
keywords = {Conceptual design, Computational intelligence, Optimal concept, Genetic algorithm, Fuzzy neural network},
abstract = {Product concept generation and evaluation are two major activities for obtaining an optimal concept in conceptual design. In this paper, an integrated computational intelligence approach is proposed for dealing with these two aspects. A group of satisfactory concepts are generated first by using genetic algorithm and incorporating the information from knowledge base. Then concept evaluation and decision making are implemented using fuzzy neural network to obtain an optimal concept. Our procedure of using computational intelligence in conceptual design is described. The key issues in implementing the proposed approach are discussed, and finally the applicability of the proposed method is illustrated with an engineering example.}
}
@article{RAHMAN201872,
title = {Hybrid bio-Inspired computational intelligence techniques for solving power system optimization problems: A comprehensive survey},
journal = {Applied Soft Computing},
volume = {69},
pages = {72-130},
year = {2018},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2018.04.051},
url = {https://www.sciencedirect.com/science/article/pii/S1568494618302424},
author = {Imran Rahman and Junita Mohamad-Saleh},
keywords = {Computational intelligence, Hybrid optimization, Optimization, Bio-inspired computation, Power system},
abstract = {Optimization problems of modern day power system are very challenging to resolve because of its design complexity, wide geographical dispersion and influence from many unpredictable factors. For that reason, it is essential to apply most effective optimization techniques by taking full benefits of simplified formulation and execution of a particular problem. This study presents a summary of significant hybrid bio-inspired computational intelligence (CI) techniques utilized for power system optimization. Authors have reviewed an extensive range of hybrid CI techniques and examined the motivations behind their improvements. Various applications of hybrid bio-inspired CI algorithms have been highlighted in this paper. In addition, few drawbacks regarding the hybrid CI algorithms are explained. Current trends in CI techniques from the past researches have also been discussed in the domain of power system optimization. Lastly, some future research directions are suggested for further advancement of hybrid techniques.}
}
@article{TURHAN20075237,
title = {Statistical and computational intelligence tools for the analyses of warp tension in different back-rest oscillations},
journal = {Information Sciences},
volume = {177},
number = {23},
pages = {5237-5252},
year = {2007},
note = {Including: Mathematics of Uncertainty},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2007.06.029},
url = {https://www.sciencedirect.com/science/article/pii/S0020025507003246},
author = {Yıldıray Turhan and Sezai Tokat and Recep Eren},
keywords = {Neural networks, Radial basis function, Cross-validation, Data regression, Warp tension, Back-rest oscillation, Weft density},
abstract = {In this paper, experimental, computational intelligence based and statistical investigations of warp tensions in different back-rest oscillations are presented. Firstly, in the experimental stage, springs having different stiffnesses are used to obtain different back-rest oscillations. For each spring, fabrics are woven in different weft densities and the warp tensions are measured and saved during weaving process. Secondly, in the statistical investigation, the experimental data are analyzed by using linear multiple and quadratic multiple-regression models. Later, in the computational intelligence based investigation, the data obtained from the experimental study are analyzed by using artificial neural networks that are universal approximators which provide a massively parallel processing and decentralized computing. Specially, radial basis function neural network structure is chosen. In this structure, cross-validation technique is used in order to determine the number of radial basis functions. Finally, the results of regression analysis, the computational intelligence based analysis and experimental measurements are compared by using the coefficient of determination. From the results, it is shown that the computational intelligence based analysis indicates a better agreement with the experimental measurement than the statistical analysis.}
}
@article{CARROLL1999111,
title = {Invented Computational Procedures of Students in a Standards-Based Curriculum},
journal = {The Journal of Mathematical Behavior},
volume = {18},
number = {2},
pages = {111-121},
year = {1999},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(99)00024-3},
url = {https://www.sciencedirect.com/science/article/pii/S0732312399000243},
author = {William M. Carroll},
abstract = {Fourth graders who had been in a standards-based elementary curriculum since kindergarten were individually interviewed and administered a whole-class test that probed their knowledge of facts and multidigit computation. Standard algorithms are not taught as part of the curriculum, which instead emphasizes student-invented procedures and discussions of solution methods. Of interest were the types of student-invented procedures that were used as well as their computational accuracy. Students used several procedures that involved sophisticated mental calculation strategies, such as decomposing numbers or adding from left to right. Many students also used the standard written algorithms. Both invented and standard algorithms used by the students were highly accurate, although invented procedures often indicated better mental flexibility and awareness of place value. On the written test, students' computational abilities were above national normative levels.}
}
@article{CHAUNCEY2023100182,
title = {A framework and exemplars for ethical and responsible use of AI Chatbot technology to support teaching and learning},
journal = {Computers and Education: Artificial Intelligence},
volume = {5},
pages = {100182},
year = {2023},
issn = {2666-920X},
doi = {https://doi.org/10.1016/j.caeai.2023.100182},
url = {https://www.sciencedirect.com/science/article/pii/S2666920X23000619},
author = {Sarah A. Chauncey and H. Patricia McKenna},
keywords = {AI ethics, AI responsibility, AI-Rich learning environments, Cognitive flexibility, Critical thinking, Self-regulation},
abstract = {The aim of this paper is to investigate the ethical and responsible use of AI chatbots in education in support of critical thinking, cognitive flexibility and self-regulation in terms of their potential to enhance and motivate teaching and learning in contemporary education environments. AI chatbots such as ChatGPT by OpenAI appear to be improving in conversational and other capabilities and this paper explores such advances using version 4. Based on a review of the research literature, a conceptual framework is formulated for responsible use of AI chatbots in education supporting cognitive flexibility in AI-rich learning environments. The framework is then operationalized for use in this paper through the development of exemplars for math, english language arts (ELA), and studying with ChatGPT to close learning gaps in an effort to foster more ethical and responsible approaches to the design and development of AI chatbots for application and use in teaching and learning environments. This paper extends earlier foundational work on cognitive flexibility and AI chatbots as well as work on cognitive flexibility in support of creativity and innovation with AI chatbots in urban civic spaces.}
}
@article{HALLINAN2001506,
title = {Thinking Beyond the Fringe},
journal = {Trends in Cognitive Sciences},
volume = {5},
number = {12},
pages = {506-507},
year = {2001},
issn = {1364-6613},
doi = {https://doi.org/10.1016/S1364-6613(00)01802-7},
url = {https://www.sciencedirect.com/science/article/pii/S1364661300018027},
author = {Jennifer Hallinan},
keywords = {explicit models of memory, stochastic generative approach, evolution of the neural modularity, metarepresentation}
}
@incollection{MISHRA2024231,
title = {Chapter Twelve - Unravelling the gut microbiome: Connecting with AI for deeper insights},
editor = {Akanksha Srivastava and Vaibhav Mishra},
series = {Methods in Microbiology},
publisher = {Academic Press},
volume = {55},
pages = {231-246},
year = {2024},
booktitle = {Artificial Intelligence in Microbiology: Scope and Challenges Volume 1},
issn = {0580-9517},
doi = {https://doi.org/10.1016/bs.mim.2024.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S058095172400028X},
author = {Vaibhav Mishra and Chhavi Atri and Raj Pandey and Akanksha Srivastava},
keywords = {Artificial intelligence, Gut microbes, Microbiology, Gastroenterology, Machine learning, Deep learning},
abstract = {Artificial intelligence (AI) remains a relatively unfamiliar concept for many, but its significance in the biomedical field is gaining recognition as the world undergoes transformative changes. Furthermore, AI possesses the potential to emulate critical thinking, reasoning, problem-solving abilities, and logical capacities of machines. Additionally, in the realm of gut microbiota research, AI emerges as a valuable asset. The synergy between gut microbes and AI not only holds promise for treating diverse gastroenterological diseases but also aids in comprehending the intricate relationships between gut microbes and microbes of resides into the other body parts. Moreover, AI facilitates a deeper understanding of different facets within gut-microbes interaction research. These direct communications are governed by chemical messengers, hormones, and neurotransmitters, detectable through biosensor chips employing machine learning (ML). Additionally, the indirect regulation of gut function by the brain via the hypothalamic-pituitary-adrenal (HPA) axis can be analysed using different computational models. This promising prospect remains largely unexplored, and in this chapter, our aim is to delve into and harness the potential of AI in gut microbial research.}
}
@article{SALVATORE2024143,
title = {The affective grounds of the mind. The Affective Pertinentization (APER) model},
journal = {Physics of Life Reviews},
volume = {50},
pages = {143-165},
year = {2024},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2024.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S1571064524000903},
author = {Sergio Salvatore and Arianna Palmieri and Raffaele {De Luca Picione} and Vincenzo Bochicchio and Matteo Reho and Maria Rita Serio and Giampaolo Salvatore},
keywords = {Affective Pertinentization model, Affective Landscape, Phase Space of Meaning, Meaning dimensionality},
abstract = {The paper presents the Affective Pertinentization model (APER), a theory of the affect and its role it plays in meaning-making. APER views the affect as the basic form of making sense of reality. It consists of a global, bipolar pattern of neurophysiological activity through which the organism maps the instant-by-instant variation of its environment. Such a pattern of neuropsychological activity is constituted by a plurality of bipolar affective dimensions, each of which maps a component of the environmental variability. The affect has a pluri-componential structure defining a multidimensional affective landscape that foregrounds (i.e., makes pertinent) a certain pattern of facets of the environment (e.g., its pleasantness/unpleasantness) relevant to survival, while backgrounding the others. Doing so, the affect grounds the following cognitive processes. Accordingly, meaning-making can be modeled as a function of the dimensionality of the affective landscape. The greater the dimensionality of the affective landscape, the more differentiated the system of meaning is. Following a brief review of current theories pertaining to the affect, the paper proceeds discussing the APER's core tenets – the multidimensional view of the affect, its semiotic function, and the concepts of Affective Landscape and Phase Space of Meaning. The paper then proceeds deepening the relationship between the APER model and other theories, highlighting how the APER succeeds in framing original conceptualizations of several challenging issues – the intertwinement between affect and sensory modalities, the manner in which the mind constitutes the content of the experience, the determinants of psychopathology, the intertwinement of mind and culture, and the spreading of affective forms of thinking and behaving in society. Finally, the unsolved issues and future developments of the model are briefly envisaged.}
}
@article{BRIDGES2012780,
title = {Thinking Outside the Cleft to Understand Synaptic Activity: Contribution of the Cystine-Glutamate Antiporter (System xc−) to Normal and Pathological Glutamatergic Signaling},
journal = {Pharmacological Reviews},
volume = {64},
number = {3},
pages = {780-802},
year = {2012},
issn = {0031-6997},
doi = {https://doi.org/10.1124/pr.110.003889},
url = {https://www.sciencedirect.com/science/article/pii/S0031699724010159},
author = {Richard Bridges and Victoria Lutgen and Doug Lobner and David A. Baker},
abstract = {System xc− represents an intriguing target in attempts to understand the pathological states of the central nervous system. Also called a cystine-glutamate antiporter, system xc− typically functions by exchanging one molecule of extracellular cystine for one molecule of intracellular glutamate. Nonvesicular glutamate released during cystine-glutamate exchange activates extrasynaptic glutamate receptors in a manner that shapes synaptic activity and plasticity. These findings contribute to the intriguing possibility that extracellular glutamate is regulated by a complex network of release and reuptake mechanisms, many of which are unique to glutamate and rarely depicted in models of excitatory signaling. Because system xc− is often expressed on non-neuronal cells, the study of cystine-glutamate exchange may advance the emerging viewpoint that glia are active contributors to information processing in the brain. It is noteworthy that system xc− is at the interface between excitatory signaling and oxidative stress, because the uptake of cystine that results from cystine-glutamate exchange is critical in maintaining the levels of glutathione, a critical antioxidant. As a result of these dual functions, system xc− has been implicated in a wide array of central nervous system diseases ranging from addiction to neurodegenerative disorders to schizophrenia. In the current review, we briefly discuss the major cellular components that regulate glutamate homeostasis, including glutamate release by system xc−. This is followed by an in-depth discussion of system xc− as it relates to glutamate release, cystine transport, and glutathione synthesis. Finally, the role of system xc− is surveyed across a number of psychiatric and neurodegenerative disorders.}
}
@article{COSTA20055,
title = {Interactive Computation: Stepping Stone in the Pathway From Classical to Developmental Computation},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {141},
number = {5},
pages = {5-31},
year = {2005},
note = {Proceedings of the Workshop on the Foundations of Interactive Computation (FInCo 2005)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2005.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S157106610505187X},
author = {Antônio Carlos da Rocha Costa and Graçaliz Pereira Dimuro},
keywords = {Interactive computation, developmental computation, domain theory, classical theory of computation},
abstract = {This paper reviews and extends previous work on the domain-theoretic notion of Machine Development. It summarizes the concept of Developmental Computation and shows how Interactive Computation can be understood as a stepping stone in the pathway from Classical to Developmental Computation. A critical appraisal is given of Classical Computation, showing in which ways its shortcomings tend to restrict the possible evolution of real computers, and how Interactive and Developmental Computation overcome such shortcomings. The idea that Developmental Computation is more encompassing than Interactive Computation is stressed. A formal framework for Developmental Computation is sketched, and the current frontier of the work on Developmental Computation is briefly exposed.}
}
@article{NOWROOZI201252,
title = {A general computational recognition primed decision model with multi-agent rescue simulation benchmark},
journal = {Information Sciences},
volume = {187},
pages = {52-71},
year = {2012},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2011.09.039},
url = {https://www.sciencedirect.com/science/article/pii/S0020025511005330},
author = {Alireza Nowroozi and Mohammad E. Shiri and Angeh Aslanian and Caro Lucas},
keywords = {Naturalistic decision making, Recognition primed decision model, Computational modeling, Disaster management, RoboCup, Multi-agent rescue simulation benchmark},
abstract = {Analytical decision making strategies rely on weighing pros and cons of multiple options in an unbounded rationality manner. Contrary to these strategies, recognition primed decision (RPD) model which is a primary naturalistic decision making (NDM) approach assumes that experienced and professional decision makers when encounter problems in real operating conditions are able to use their previous experiences and trainings in order to diagnose the problem, recall the appropriate solution, evaluate it mentally, and implement it to handle the problem in a satisficing manner. In this paper, a computational form of RPD, now called C-RPD, is presented. Unified Modeling Language was used as a modeling language to represent the proposed C-RPD model in order to make the implementation easy and obvious. To execute the model, RoboCup Rescue agent simulation environment, which is one of the best and the most famous complex and multi-agent large-scale environments, was selected. The environment simulates the incidence of fire and earthquakes in urban areas where it is the duty of the police forces, firefighters and ambulance teams to control the crisis. Firefighters of SOS team are first modeled and implemented by utilizing C-RPD and then the system is trained using an expert’s experience. There are two evaluations. To find out the convergence of different versions developed during experience adding, some of the developed versions are chosen and evaluated on seven maps. Results show performance improvements. The SOS team ranked first in an official world championship and three official open tournaments.}
}
@article{GOLDSTONE2005424,
title = {Computational models of collective behavior},
journal = {Trends in Cognitive Sciences},
volume = {9},
number = {9},
pages = {424-430},
year = {2005},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2005.07.009},
url = {https://www.sciencedirect.com/science/article/pii/S1364661305002147},
author = {Robert L. Goldstone and Marco A. Janssen},
abstract = {Computational models of human collective behavior offer promise in providing quantitative and empirically verifiable accounts of how individual decisions lead to the emergence of group-level organizations. Agent-based models (ABMs) describe interactions among individual agents and their environment, and provide a process-oriented alternative to descriptive mathematical models. Recent ABMs provide compelling accounts of group pattern formation, contagion and cooperation, and can be used to predict, manipulate and improve upon collective behavior. ABMs overcome an assumption that underlies much of cognitive science – that the individual is the crucial unit of cognition. The alternative advocated here is that individuals participate in collective organizations that they might not understand or even perceive, and that these organizations affect and are affected by individual behavior.}
}
@article{HASUO2017404,
title = {Semantics of higher-order quantum computation via geometry of interaction},
journal = {Annals of Pure and Applied Logic},
volume = {168},
number = {2},
pages = {404-469},
year = {2017},
note = {Eighth Games for Logic and Programming Languages Workshop (GaLoP)},
issn = {0168-0072},
doi = {https://doi.org/10.1016/j.apal.2016.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S0168007216301336},
author = {Ichiro Hasuo and Naohiko Hoshino},
keywords = {Higher-order computation, Quantum computation, Programming language, Geometry of interaction, Denotational semantics, Categorical semantics},
abstract = {While much of the current study on quantum computation employs low-level formalisms such as quantum circuits, several high-level languages/calculi have been recently proposed aiming at structured quantum programming. The current work contributes to the semantical study of such languages by providing interaction-based semantics of a functional quantum programming language; the latter is, much like Selinger and Valiron's, based on linear lambda calculus and equipped with features like the ! modality and recursion. The proposed denotational model is the first one that supports the full features of a quantum functional programming language; we prove adequacy of our semantics. The construction of our model is by a series of existing techniques taken from the semantics of classical computation as well as from process theory. The most notable among them is Girard's Geometry of Interaction (GoI), categorically formulated by Abramsky, Haghverdi and Scott. The mathematical genericity of these techniques—largely due to their categorical formulation—is exploited for our move from classical to quantum.}
}
@article{EVINS2013230,
title = {A review of computational optimisation methods applied to sustainable building design},
journal = {Renewable and Sustainable Energy Reviews},
volume = {22},
pages = {230-245},
year = {2013},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2013.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S1364032113000920},
author = {Ralph Evins},
keywords = {Review, Optimisation, Sustainable, Building, Design, Multi-objective},
abstract = {This paper presents a comprehensive review of all significant research applying computational optimisation to sustainable building design problems. A summary of common heuristic optimisation algorithms is given, covering direct search, evolutionary methods and other bio-inspired algorithms. The main summary table covers 74 works that focus on the application of these methods to different fields of sustainable building design. Key fields are reviewed in detail: envelope design, including constructions and form; configuration and control of building systems; renewable energy generation; and holistic optimisations of several areas simultaneously, with particular focus on residential and retrofit. Improvements to the way optimisation is applied are also covered, including platforms and frameworks, algorithmic comparisons and developments, use of meta-models and incorporation of uncertainty. Trends, including the rise of multi-objective optimisation, are analysed graphically. Likely future developments are discussed.}
}
@incollection{VASSILOPOULOS2020349,
title = {10 - Computational intelligence methods for the fatigue life modeling of composite materials},
editor = {Anastasios P. Vassilopoulos},
booktitle = {Fatigue Life Prediction of Composites and Composite Structures (Second Edition)},
publisher = {Woodhead Publishing},
edition = {Second Edition},
pages = {349-383},
year = {2020},
series = {Woodhead Publishing Series in Composites Science and Engineering},
isbn = {978-0-08-102575-8},
doi = {https://doi.org/10.1016/B978-0-08-102575-8.00010-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780081025758000103},
author = {Anastasios P. Vassilopoulos and Efstratios F. Georgopoulos},
keywords = {Fatigue, Composites, Artificial neural network, Genetic programming, ANFIS, S-N curves},
abstract = {Novel computational methods such as artificial neural networks, adaptive neuro-fuzzy inference systems and genetic programming are used in this chapter for the modeling of the nonlinear behavior of composite laminates subjected to constant amplitude loading. The examined computational methods are stochastic nonlinear regression tools, and can therefore be used to model the fatigue behavior of any material, provided that sufficient data are available for training. They are material independent methods that simply follow the trend of the available data, in each case giving the best estimate of their behavior. Application on a wide range of experimental data gathered after fatigue testing glass/epoxy and glass/polyester laminates proved that their modeling ability compares favorably with, and is to some extent superior to, other modeling techniques.}
}
@article{ROSSITER20241,
title = {MATLAB files to support learning of simple frequency response design},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {26},
pages = {1-6},
year = {2024},
note = {4th IFAC Workshop on Internet Based Control Education - IBCE 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.10.261},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324020366},
author = {J.A. Rossiter},
keywords = {Control101 toolbox, frequency response, lead and lag compensation, virtual laboratories, independent learning},
abstract = {This paper presents a small number of MATLAB APPs and livescript files designed to help students both understand and implement frequency response tools into feedback design. The paper presents the thinking behind the use of MATLAB and the topic itself before then describing the proposed resources in detail.}
}
@incollection{GOODSON2010175,
title = {Chapter 10 - Using Computational Modeling to Understand Microtubule Dynamics: A Primer for Cell Biologists},
editor = {Leslie Wilson and John J. Correia},
series = {Methods in Cell Biology},
publisher = {Academic Press},
volume = {95},
pages = {175-188},
year = {2010},
booktitle = {Microtubules, in vitro},
issn = {0091-679X},
doi = {https://doi.org/10.1016/S0091-679X(10)95010-3},
url = {https://www.sciencedirect.com/science/article/pii/S0091679X10950103},
author = {Holly V. Goodson and Ivan V. Gregoretti},
abstract = {Experimental cell biology, biochemistry, and structural biology have provided a wealth of information about microtubule function and mechanism, but we are reaching a limit as to what can be understood from experiment alone. Standard biochemical approaches are not sufficient to make quantitative predictions about microtubule behavior, and they are limited in their ability to test existing conceptual models of microtubule mechanism. Because microtubules are so complex, achieving a deep understanding of microtubule behavior and mechanism will require the input of mathematical and computational modeling. However, this type of analysis can be daunting to the uninitiated. The purpose of this chapter is to provide a straightforward introduction to the various types of modeling and how they can be used to study microtubule function, dynamics, and mechanism.}
}
@article{SZUBA1998321,
title = {A molecular quasi-random model of computations applied to evaluate collective intelligence},
journal = {Future Generation Computer Systems},
volume = {14},
number = {5},
pages = {321-339},
year = {1998},
note = {Bio-inspired solutions to parallel processing problems},
issn = {0167-739X},
doi = {https://doi.org/10.1016/S0167-739X(98)00037-5},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X98000375},
author = {Tadeusz Szuba},
keywords = {Collective intelligence, IQ measure, Social structure, PROLOG, Model of computations, Brownian movements},
abstract = {This paper presents a bio-inspired model of computations, the random PROLOG processor (RPP), used for analysis of collective intelligence (CI). In the RPP, clause_molecules (CMs) of facts, rules, goals, or higher-level logical structures enclosed by membranes move quasi-randomly in structured computational_PROLOG_space (CS). When CMs rendezvous, an inference process can occur iff the logical conditions are fulfilled. CI can be evaluated as follows: (1) the mapping is done of a given social structure into the RPP; (2) the beings and their behavior are translated into PROLOG expressions, carried by CMs; (3) the goal(s) of the social structure are translated into N-element inference (NEI); (4) the efficiency of the NEI is evaluated and given as the intelligence quotient of a social structure (IQS) projected onto NEI.}
}
@incollection{ZOHURI202225,
title = {Chapter 2 - A general approach to business resilience system (BRS)},
editor = {Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia},
booktitle = {Knowledge is Power in Four Dimensions: Models to Forecast Future Paradigm},
publisher = {Academic Press},
pages = {25-57},
year = {2022},
isbn = {978-0-323-95112-8},
doi = {https://doi.org/10.1016/B978-0-323-95112-8.00003-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323951128000039},
author = {Bahman Zohuri and Farhang Mossavar-Rahmani and Farahnaz Behgounia},
keywords = {Artificial intelligence, Data analysis and information, Market and market share, Predictive analytics, Super artificial intelligence},
abstract = {The business resilience system (BRS) with its risk atom and processing data point is based on fuzzy logic and cloud computation in real time. Its purpose and objectives define a clear set of expectations for organizations and enterprises, so their network system and supply chain are totally resilient and protected against cyberattacks, man-made threats, and natural disasters. These enterprises include financial, organizational, homeland security, and supply chain operations with multipoint manufacturing across the world. Market share and marketing advantages are expected to result from the implementation of the system. The collected information and defined objectives provide the basis to monitor and analyze the data through cloud computation and will guarantee the success of their survivability against any unexpected threats. Putting this kind of operation in place allows the executive and stakeholders within those organizations and enterprises to make the right decision when encountering threats that interrupt their normal day-to-day operations, as well as, in cases such as defense and homeland security, to predict the next move of an adversary. Given the fact that the BRS, as part of its functionality, processes the incoming data and information if not real time, then near real time with the help of superartificial intelligence in place, this gives the stakeholder an edge against and threats as well as predicting issues with operational intelligence. Artificial intelligence (AI) is one of those technologies that seem to be expanding in every direction. This technology will take center stage at Think 2018. Resilience thinking is inevitably systems thinking, at least as much as sustainable development is. In fact, “when considering systems of humans and nature (social-ecological systems), it is important to consider the system as a whole.” The term “resilience” originated in the 1970s in the field of ecology from the research of C.S. Holling, who defined resilience as “a measure of the persistence of systems and of their ability to absorb change and disturbance and still maintain the same relationships between populations or state variables.” In short, resilience is best defined as “the ability of a system to absorb disturbances and still retain its basic function and structure.” In this chapter, we explain the BRS and how it works. Please note that the with minor editing and manipulation, the materials presented in this chapter have been borrowed from the book published from Zohuri and Moghaddam10 with permission from both authors and publisher as well.}
}
@article{ZHANG2022116187,
title = {Tri-level attribute reduction in rough set theory},
journal = {Expert Systems with Applications},
volume = {190},
pages = {116187},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.116187},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421015050},
author = {Xianyong Zhang and Yiyu Yao},
keywords = {Attribute reduction, Three-way decision, Tri-level analysis, Object-specific attribute reducts, Tri-level attribute reducts, Granular computing},
abstract = {Attribute reduction serves as a pivotal topic of rough set theory for data analysis. The ideas of tri-level thinking from three-way decision can shed new light on three-level attribute reduction. Existing classification-specific and class-specific attribute reducts consider only macro-top and meso-middle levels. This paper introduces a micro-bottom level of object-specific reducts. The existing two types of reducts apply to the global classification with all objects and a local class with partial objects, respectively. The new type applies to an individual object. These three types of reducts constitute tri-level attribute reducts. Their development and hierarchy are worthy of systematical explorations. Firstly, object-specific reducts are defined by object consistency from dependency, and they improve both classification-specific and class-specific reducts. Secondly, tri-level reducts are unified by tri-level consistency. Hierarchical relationships between object-specific reducts and class-specific, classification-specific reducts are analyzed, and relevant connections of three-way classifications of attributes are given. Finally, tri-level reducts are systematically analyzed, and two approaches, i.e., the direct calculation and hierarchical transition, are suggested for constructing a specific reduct. We build a framework of tri-level thinking and analysis of attribute reduction to enrich three-way granular computing. Tri-level reducts lead to the sequential development and hierarchical deepening of attribute reduction, and their results profit intelligence processing and system reasoning.}
}
@article{HAYASHI1999507,
title = {Numerical models of HDR geothermal reservoirs—a review of current thinking and progress},
journal = {Geothermics},
volume = {28},
number = {4},
pages = {507-518},
year = {1999},
issn = {0375-6505},
doi = {https://doi.org/10.1016/S0375-6505(99)00026-7},
url = {https://www.sciencedirect.com/science/article/pii/S0375650599000267},
author = {Kazuo Hayashi and Jonathan Willis-Richards and Robert J Hopkirk and Yuichi Niibori},
keywords = {Reservoir, Hot dry rock, HDR, Modelling, Numerical simulation},
abstract = {A brief review is presented of modelling and simulation of HDR geothermal reservoirs both for hydraulic fracturing/stimulation to create artificial/engineered geothermal reservoirs and for long-term heat extraction operations. Firstly, modelling of the governing factors is surveyed and coupling among mechanical, thermal, hydraulic and chemical effects is discussed. Next the structure and modelling of reservoirs are discussed. Finally, the features of a variety of simulation codes are summarized.}
}
@article{LUCAS20218320,
title = {Implementing a Novel Software Program to Support Pharmacy Students’ Reflective Practice in Scientific Research},
journal = {American Journal of Pharmaceutical Education},
volume = {85},
number = {10},
pages = {8320},
year = {2021},
issn = {0002-9459},
doi = {https://doi.org/10.5688/ajpe8320},
url = {https://www.sciencedirect.com/science/article/pii/S0002945923015012},
author = {Cherie Lucas and Simon Buckingham Shum and Ming Liu and Mary Bebawy},
keywords = {reflection, formative feedback, pharmacy education, pharmaceutical research},
abstract = {ABSTRACT
Objective. To explore pharmacy students’ perceptions of a novel web application tool (AcaWriter) implemented in a Master of Pharmacy curriculum to support reflective thinking in scientific research. Methods. A qualitative research design involving a 50-minute focus group (n=12) was used. The focus group session was audio-taped, transcribed verbatim, and analyzed thematically using the Braun and Clarke framework. Results. Analysis generated four themes related to AcaWriter’s utility in enhancing students’ research thinking and capacity. The themes identified included: ease of use to prompt reflection, tangible tool with non-judgmental capacity; benefits for enhancing self and peer reflection on research techniques and group dynamics; benefits of the reflective writing process to enhance research capacity compared with engaging in reflective dialogue; and benefits beyond the writing process: cultivating self-improvement and self-confidence. Conclusion. The findings of this study show that a novel web application implemented within a pharmacy curriculum can assist students’ self and peer reflection on a research task. Further research is needed to explore the impact of using this tool and its relationship with academic performance and outcomes.}
}
@article{DUBLJEVIC2024102480,
title = {Colleges and universities are important stakeholders for regulating large language models and other emerging AI},
journal = {Technology in Society},
volume = {76},
pages = {102480},
year = {2024},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2024.102480},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X24000289},
author = {Veljko Dubljević},
keywords = {Artificial intelligence (AI), Ethics, Public policy, Legitimacy, Oversight},
abstract = {AI technology has already gone through one “winter,” and alarmist thinking may cause yet another one. Calls for a moratorium on AI research increase the salience of the public request for comment on “AI accountability.” Prohibitive approaches are an overreaction, especially when leveraged on virtual (non-embodied) AI agents. While there are legitimate concerns regarding expansion of AI models like ChatGPT in society, a better approach would be to forge a partnership between academia and industry, and utilize infrastructure of campuses to authenticate users and oversee new AI research. The public could also be involved with public libraries authenticating users. This staged approach to embedding AI in society would facilitate addressing ethical concerns, and implementing virtual AI agents in society in a responsible and safe manner.}
}
@article{JACKSON20091397,
title = {There may be more to reaching than meets the eye: Re-thinking optic ataxia},
journal = {Neuropsychologia},
volume = {47},
number = {6},
pages = {1397-1408},
year = {2009},
note = {Perception and Action},
issn = {0028-3932},
doi = {https://doi.org/10.1016/j.neuropsychologia.2009.01.035},
url = {https://www.sciencedirect.com/science/article/pii/S0028393209000475},
author = {Stephen R. Jackson and Roger Newport and Masud Husain and Jane E. Fowlie and Michael O’Donoghue and Nin Bajaj},
keywords = {Optic ataxia, Neuropsychology of action, Reaching},
abstract = {Optic ataxia (OA) is generally thought of as a disorder of visually guided reaching movements that cannot be explained by any simple deficit in visual or motor processing. In this paper we offer a new perspective on optic ataxia; we argue that the popular characterisation of this disorder is misleading and is unrepresentative of the pattern of reaching errors typically observed in OA patients. We begin our paper by reviewing recent neurophysiological, neuropsychological, and functional brain imaging studies that have led to the proposal that the medial parietal cortex in the vicinity of the parietal-occipital junction (POJ) – the key anatomical site associated with OA – represents reaching movements in eye-centred coordinates, and that this ability is impaired in optic ataxia. Our perspective stresses the importance of the POJ and superior parietal regions of the human PPC for representing reaching movements in both extrinsic (eye-centred) and intrinsic (postural) coordinates, and proposes that it is the ability to simultaneously represent multiple spatial locations that must be directly compared with one another that is impaired in non-foveal OA patients. In support of this idea we review recent fMRI and behavioural studies conducted by our group that have investigated the anatomical correlates of posturally guided movements, and the movements guided by postural cues in patients presenting with optic ataxia.}
}
@article{KEOGH2007249,
title = {Keeping the patient asleep and alive: Towards a computational cognitive model of disturbance management in anaesthesia},
journal = {Cognitive Systems Research},
volume = {8},
number = {4},
pages = {249-261},
year = {2007},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2006.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041707000034},
author = {K. Keogh and E.A. Sonenberg},
keywords = {Behavioural analysis, Computational cognitive modelling, Disturbance management},
abstract = {We have analysed rich, dynamic data about the behaviour of anaesthetists during the management of a simulated critical incident in the operating theatre. We use a paper based analysis and a partial implementation to further the development of a computational cognitive model for disturbance management in anaesthesia. We suggest that our data analysis pattern may be used for the analysis of behavioural data describing cognitive and observable events in other complex dynamic domains.}
}
@article{PALKOVICS2016144,
title = {Exploration of cognition–affect and Type 1–Type 2 dichotomies in a computational model of decision making},
journal = {Cognitive Systems Research},
volume = {40},
pages = {144-160},
year = {2016},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2016.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S1389041715300115},
author = {Michael Anton Palkovics and Martin Takáč},
keywords = {Affective computing, Dual process theory, Decision-making},
abstract = {This paper studies the role of cognition and affect in decision-making as well as notions of Type 1 and 2 processes and behaviors typically used in dual process theories. In order to demonstrate that there is no 1:1 correspondence between types of observed behavior and internal processes causing them, and that Type 1 and Type 2 processes can be produced by a single system, we implemented a computational model integrating affective and cognitive processing. Our model is based on the model of Marinier, Laird, and Lewis (2009). We modified it by increasing the agent’s visual field, adding a GOFAI-style cognitive module (sub-goal management) and expanding the environment by a high-threat tile, to which the agent responds with a hard-wired automatic reaction. This allowed us to generate and observe different types of behavior and study interesting interactions between cognitive and affective control. By comparing our re-implementation to the modified agent, we demonstrated clear cases of Type 1 (fast, automatic) and Type 2 (slow, deliberative) behavior, providing further evidence for the “single-system, two processes” hypothesis.}
}
@article{HOU2025141,
title = {Data-driven modeling of 600 MW supercritical unit under full operating conditions based on Transformer-XL},
journal = {ISA Transactions},
volume = {158},
pages = {141-166},
year = {2025},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2024.12.049},
url = {https://www.sciencedirect.com/science/article/pii/S0019057824006359},
author = {Guolian Hou and Tianhao Zhang and Ting Huang},
keywords = {Supercritical unit, Transformer-XL, Once-through/recirculation/shut-down mode, Quantum chaotic nutcracker optimization algorithm},
abstract = {Improving the flexible and deep peak shaving capability of supercritical (SC) unit under full operating conditions to adapt a larger-scale renewable energy integrated into the power grid is the main choice of novel power system. However, it is particularly challenging to establish an accurate SC unit model under large-scale variable loads and deep peak shaving. To this end, a data-driven modeling strategy combining Transformer-Extra Long (Transformer-XL) and quantum chaotic nutcracker optimization algorithm is proposed. Firstly, three models of the SC unit under once-through/recirculation/shut-down are built via analyzing its mechanism of the operation process, respectively. Secondly, the superior performance of Transformer-XL in obtaining global feature information is employed to effectively solve the problem of high information dependence caused by the strong coupling and nonlinearity of SC unit. Then, the improved quantum chaotic nutcracker optimization algorithm with higher search accuracy is proposed to obtain the optimal parameters of Transformer-XL based on the logistic chaotic mapping and quantum thinking. Feature information dependencies and optimal parameter settings are fully considered in the proposed modeling scheme, which results in an accurate model of SC unit under full operating conditions. Finally, various simulations and comparisons are conducted based on the on-site data of 600 MW SC unit to demonstrate the superiority of the proposed data-driven modeling strategy. According to the improved Transformer-XL, the mean square errors of the proposed SC unit model under once-through/recirculation/shut-down modes are less than 2.500E-03, which verifies the high accuracy of the model. Consequently, the developed model is suitable for application in the controller designing and the operating efficiency and flexibility improvement of SC unit.}
}
@article{VARGO2017260,
title = {A systems perspective on markets – Toward a research agenda},
journal = {Journal of Business Research},
volume = {79},
pages = {260-268},
year = {2017},
issn = {0148-2963},
doi = {https://doi.org/10.1016/j.jbusres.2017.03.011},
url = {https://www.sciencedirect.com/science/article/pii/S014829631730098X},
author = {Stephen L. Vargo and Kaisa Koskela-Huotari and Steve Baron and Bo Edvardsson and Javier Reynoso and Maria Colurcio},
keywords = {Markets, Systems thinking, Marketing, Complex systems, Research agenda},
abstract = {This paper addresses the implications of an emerging, increasingly important way of thinking about markets: systems thinking. A market is one of the most founational abstractions in marketing and business research; yet, it often receives too little attention. As a result, the taken-for-granted assumptions about markets spur from over-simplified conceptualizations of neoclassical economics that depict markets as static and mechanistic. Systems thinking represents a major change in perspective that involves transcending this mechanistic worldview and thinking instead in terms of wholes, relationships, processes, and patterns. We argue that building a theory of markets based on systems thinking, would enable scholars to develop more realistic models that correspond with fast-changing business environment and therefore, increase both the rigor and relevance of future research. To further this aim, we identify the main implications of systems thinking and formulate them into a research agenda to further the systemic understanding of markets.}
}
@incollection{REIMERS2006119,
title = {[8] Bioconductor: An Open Source Framework for Bioinformatics and Computational Biology},
series = {Methods in Enzymology},
publisher = {Academic Press},
volume = {411},
pages = {119-134},
year = {2006},
booktitle = {DNA Microarrays, Part B: Databases and Statistics},
issn = {0076-6879},
doi = {https://doi.org/10.1016/S0076-6879(06)11008-3},
url = {https://www.sciencedirect.com/science/article/pii/S0076687906110083},
author = {Mark Reimers and Vincent J. Carey},
abstract = {This chapter describes the Bioconductor project and details of its open source facilities for analysis of microarray and other high‐throughput biological experiments. Particular attention is paid to concepts of container and workflow design, connections of biological metadata to statistical analysis products, support for statistical quality assessment, and calibration of inference uncertainty measures when tens of thousands of simultaneous statistical tests are performed.}
}
@article{IOAKIMIDIS2017280,
title = {Caustics, pseudocaustics and the related illuminated and dark regions with the computational method of quantifier elimination},
journal = {Optics and Lasers in Engineering},
volume = {88},
pages = {280-300},
year = {2017},
issn = {0143-8166},
doi = {https://doi.org/10.1016/j.optlaseng.2016.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S0143816616301348},
author = {Nikolaos I. Ioakimidis},
keywords = {Caustics, Pseudocaustics, Illuminated and dark regions, Cracks, Plates, Elasticity},
abstract = {The method of caustics is a powerful experimental method in elasticity and particularly in fracture mechanics for crack problems. The related method of pseudocaustics is also of interest. Here we apply the computational method of quantifier elimination implemented in the computer algebra system Mathematica in order to determine (i) the non-parametric equation and two properties of the caustic at a crack tip and especially (ii) the illuminated and the dark regions related to caustics and pseudocaustics in plane elasticity and plate problems. The present computations concern: (i) The derivation of the non-parametric equation of the classical caustic about a crack tip through the elimination of the parameter involved (here the polar angle) as well as two geometrical properties of this caustic. (ii) The derivation of the inequalities defining the illuminated region on the screen in the problem of an elastic half-plane loaded normally by a concentrated load with the boundary of this illuminated region related to some extent to the caustic formed. (iii) Similarly for the problem of a clamped circular plate under a uniform loading with respect to the caustic and the pseudocaustic formed. (iv) Analogously for the problem of an equilateral triangular plate loaded by uniformly distributed moments along its whole boundary, which defines the related pseudocaustic. (v) The determination of quantities of interest in mechanics from the obtained caustics or pseudocaustics. The kind of computations in the applications (ii) to (iv), i.e. the derivation of inequalities defining the illuminated region on the screen, seems to be completely new independently of the use here of the method of quantifier elimination. Additional applications are also possible, but some of them require the expansion of the present somewhat limited power of the quantifier elimination algorithms in Mathematica. This is expected to take place in the future.}
}
@article{KE201426,
title = {An implementation of design-based learning through creating educational computer games: A case study on mathematics learning during design and computing},
journal = {Computers & Education},
volume = {73},
pages = {26-39},
year = {2014},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2013.12.010},
url = {https://www.sciencedirect.com/science/article/pii/S0360131513003345},
author = {Fengfeng Ke},
keywords = {Learning by design, Game-based learning, Mathematical disposition, Thinking mathematically, Computer game making},
abstract = {This mixed-method case study examined the potential of computer-assisted, math game making activities in facilitating design-based math learning for school children. Sixty-four middle school children participated in Scratch-based, math game making activities. Data were collected via activity and conversation observation, artifact analysis, interviewing, and survey. The study findings indicated that participants developed significantly more positive dispositions toward mathematics after computer game making. The study also found that experience-driven game design processes helped to activate children's reflection on everyday mathematical experiences. Mathematical thinking and content experience were intertwined within the process of computer game authoring. On the other hand, children designers were involved in game-world and story crafting more than mathematical representation. And it was still challenging for them to perform computer game coding with abstract reasoning.}
}
@article{GILHOOLY2024100071,
title = {AI vs humans in the AUT: Simulations to LLMs},
journal = {Journal of Creativity},
volume = {34},
number = {1},
pages = {100071},
year = {2024},
issn = {2713-3745},
doi = {https://doi.org/10.1016/j.yjoc.2023.100071},
url = {https://www.sciencedirect.com/science/article/pii/S2713374523000304},
author = {Ken Gilhooly},
keywords = {AI, Alternative uses, Divergent thinking},
abstract = {This paper reviews studies of proposed creative machines applied to a prototypical creative task, i.e., the Alternative Uses Task (AUT). Although one system (OROC) did simulate some aspects of human strategies for the AUT, most recent attempts have not been simulation-oriented, but rather have used Large Language Model (LLM) systems such as GPT-3 which embody extremely large connectionist networks trained on huge volumes of textual data. Studies reviewed here indicate that LLM based systems are performing on the AUT at near or somewhat above human levels in terms of scores on originality and usefulness. Moreover, similar patterns are found in the data of humans and LLM models in the AUT, such as output order effects and a negative association between originality and value or utility. However, it is concluded that GPT-3 and similar systems, despite generating novel and useful responses, do not display creativity as they lack agency and are purely algorithmic. LLM studies so far in this area have largely been exploratory and future studies should guard against possible training data contamination.}
}
@incollection{MARS20253,
title = {What every cognitive neuroscientist should know about prefrontal cortex evolution},
editor = {Jordan Henry Grafman},
booktitle = {Encyclopedia of the Human Brain (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {3-11},
year = {2025},
isbn = {978-0-12-820481-8},
doi = {https://doi.org/10.1016/B978-0-12-820480-1.00127-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204801001273},
author = {Rogier B. Mars},
keywords = {Prefrontal cortex, Brain evolution, Foraging, Granular prefrontal cortex, Dorsolateral prefrontal cortex, Cognitive control, Comparative neuroscience, Primate, Connectivity, Human},
abstract = {Most theories of cognitive control assign a vital role to human prefrontal cortex (PFC). Although models of PFC function are abundant, most fail to capture the complexity of this part of the brain. Here we argue that an improved understanding of the evolution of PFC can aid in the formulation of better models. By better understanding what PFC is, why it evolved, and what benefit it provided to our ancestors, we can constrain our thinking and put the plethora of neuroimaging data showing PFC activation into context.}
}
@article{AMADORHIDALGO2021103694,
title = {Cognitive abilities and risk-taking: Errors, not preferences},
journal = {European Economic Review},
volume = {134},
pages = {103694},
year = {2021},
issn = {0014-2921},
doi = {https://doi.org/10.1016/j.euroecorev.2021.103694},
url = {https://www.sciencedirect.com/science/article/pii/S0014292121000477},
author = {Luis Amador-Hidalgo and Pablo Brañas-Garza and Antonio M. Espín and Teresa García-Muñoz and Ana Hernández-Román},
keywords = {Decision making under uncertainty, Cognitive abilities, Online experiment, Risk and loss aversion, Factor analysis},
abstract = {There is an intense debate whether risk-taking behavior is partially driven by cognitive abilities. The critical issue is whether choices arising from subjects with lower cognitive abilities are more likely driven by errors or lack of understanding than pure preferences for risk. The latter implies that the often-argued link between risk preferences and cognitive abilities (a common finding is that abilities relate negatively to risk aversion and positively to loss aversion) might be a spurious correlation. This experiment reports evidence from a sample of 556 participants who made choices in two risk-related tasks and completed three cognitive tasks, all with real monetary incentives: number-additions (including incentive-compatible expected number of correct additions), the Cognitive Reflection Test (to measure analytical/reflective thinking) and the Remote Associates Test (for convergent thinking). Results are unambiguous: none of our cognition measures plays any systematic role on risky decision making. Using structural equation modeling and factor analysis, we show that cognitive abilities are negatively associated with noisy, inconsistent choices and this effect may make higher ability individuals appear to be less risk averse and more loss averse. Yet we show that errors are more likely to appear when the two payoffs in a given decision exhibit similar probability. Therefore, our results suggest that failing to account for noisy decision making might have led to erroneously inferring a correlation between cognitive abilities and risk preferences in previous studies.}
}
@article{BAHLS2015104,
title = {LaTeXnics: The effect of specialized typesetting software on STEM students’ composition processes},
journal = {Computers and Composition},
volume = {37},
pages = {104-116},
year = {2015},
issn = {8755-4615},
doi = {https://doi.org/10.1016/j.compcom.2015.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S8755461515000511},
author = {Patrick Bahls and Amanda Wray},
keywords = {Computer-Mediated communication, STEM writing, Writing process, LaTeX, Markup languages, Typesetting tools},
abstract = {Undergraduate science, technology, engineering, and mathematics (STEM) students are often trained to use technical typesetting software in order to produce authentic mathematical prose, though little research exists about how this writing technology impacts students’ thinking and computation process. Drawing upon survey and interview research conducted at two liberal arts institutions, the authors investigate student writing practices across several undergraduate mathematics courses that required the use of LaTeX (a common markup language allowing users to specify the appearance of text and its layout on the printed page). This article presents findings about how the use of LaTeX slowed down students’ writing process, encouraging greater revision and reflection as well as allowing students to identify errors in their work at more than one stage in the process. We also explore the affective learning outcomes of STEM students using typesetting software, including increased feelings of confidence and professionalization. This article seeks to contribute to the growing conversation about how STEM students transfer knowledge about writing across disciplines.}
}
@article{TOFFOLI20103,
title = {From Such Simple a Beginning: The Momentous Consequences of Physics' Microscopic Reversibility for Communication and Computation—and Almost Anything Else},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {253},
number = {6},
pages = {3-16},
year = {2010},
note = {Proceedings of the Workshop on Reversible Computation (RC 2009)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2010.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S1571066110000150},
author = {Tommaso Toffoli},
keywords = {invertibility, irreversibility, computation, dynamics, thermodynamics, entropy, second law of thermodynamics},
abstract = {Darwin concludes The Origin of Species with a splendid one-phrase poem,From so simple a beginningendless forms most beautiful and most wonderfulhave been, and are being, evolved. Darwin's “simple beginning” may be identified, in today's terminology, with dissipation—evolution's basic fuel. All the rest is commentary—or, more precisely, corollary. One can aptly apply Darwin's phrase to another kind of “simple beginning,” from which as well “endless forms most beautiful and most wonderful have been, and are being, evolved.” What I have in mind is a concept that is apparently the very antithesis of dissipation, namely, physics' fundamental assumption of invertibility—or “microscopic reversibility.” To paraphrase Dobzhansky, no sensible step can be taken today in information, communication, and computer sciences, as well as in fundamental physics, except in the light of invertibility.}
}