@article{SIGAYRET2022104505,
title = {Unplugged or plugged-in programming learning: A comparative experimental study},
journal = {Computers & Education},
volume = {184},
pages = {104505},
year = {2022},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2022.104505},
url = {https://www.sciencedirect.com/science/article/pii/S0360131522000768},
author = {Kevin Sigayret and André Tricot and Nathalie Blanc},
keywords = {Elementary education, Improving classroom teaching, Programming and programming languages, Teaching/learning strategies},
abstract = {In recent years, computer programming has reappeared in school curricula with the aim of transmitting knowledge and skills beyond the simple ability to code. However, there are different ways of teaching this subject and very few experimental studies compare plugged-in and unplugged programming learning. The purpose of this study is to highlight the impact of plugged-in or unplugged learning on students' performance and subjective experience. To this end, we designed an experimental study with 217 primary school students divided into two groups and we measured their knowledge of computational concepts, ability to solve algorithmic problem, motivation toward the instruction, self-belief and attitude toward science. The programming sessions were designed to be similar between the two conditions, only the tools were different. Computers and Scratch software were used in the plugged-in group while the unplugged group used paper instructions, pictures, figurines and body movements instead. The results show better learning performance in the plugged-in group. Furthermore, although motivation dropped slightly in both groups, this drop was only significant in the unplugged condition. Gender also seems to be an important factor, as girls exhibit a lower post-test motivation and a lower willingness to pursue their practice in programming outside the school context. However, this effect on motivation was only observable in the plugged-in group which suggests that educational programming software may have a positive but gendered motivational impact.}
}
@article{EDELMANN20181,
title = {Formal studies of culture: Issues, challenges, and current trends},
journal = {Poetics},
volume = {68},
pages = {1-9},
year = {2018},
issn = {0304-422X},
doi = {https://doi.org/10.1016/j.poetic.2018.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0304422X18301323},
author = {Achim Edelmann and John W. Mohr},
keywords = {Formal study of culture, Cultural matrix approach, Measuring duality, Formalist theorization of culture, Computational hermeneutics},
abstract = {Over the last two decades, the formal study of culture has grown into one of the most exciting, systematic, and dynamic sub-fields in sociology. In this essay, we take stock of recent developments in this field. We highlight four emerging themes: (1) the maturation of the field that has occurred over the last two decades, (2) the rise and formalization of the “cultural matrix” approach to studying culture, (3) the development of various efforts to advance a more formal theory of culture, and (4) the proliferation of Big Data and the development of new kinds of quantitative and computational approaches to the study of culture, including the emergence of a new area focused on “computational hermeneutics.” We conclude by discussing future opportunities, challenges, and questions in formalizing culture.}
}
@article{FISCHER199721,
title = {Computational environments supporting creativity in the context of lifelong learning and design},
journal = {Knowledge-Based Systems},
volume = {10},
number = {1},
pages = {21-28},
year = {1997},
note = {Information Technology Support for Creativity},
issn = {0950-7051},
doi = {https://doi.org/10.1016/S0950-7051(97)00010-5},
url = {https://www.sciencedirect.com/science/article/pii/S0950705197000105},
author = {Gerhard Fischer and Kumiyo Nakakoji},
keywords = {Creativity support, Domain-oriented design environments (DODEs), Lifelong-learning},
abstract = {Much of our intelligence and creativity results from the collective memory of communities of practice and of the artifacts and technology surrounding them. Rather than studying individual creativity in isolation, we have developed a conceptual framework of creativity in the context of everyday practice — where design activities prevail and learning is constantly required. The conceptual framework explores new role distributions between people and computers based on theories that view design as reflection-in-action and breakdowns as opportunities for learning and creativity. We use an example from the domain of multimedia information design to illustrate how creativity is supported by domain-oriented design environments. The paper describes the mechanisms, architectures and processes underlying these environments.}
}
@article{EDELSON2021100986,
title = {How fuzzy-trace theory predicts development of risky decision making, with novel extensions to culture and reward sensitivity},
journal = {Developmental Review},
volume = {62},
pages = {100986},
year = {2021},
issn = {0273-2297},
doi = {https://doi.org/10.1016/j.dr.2021.100986},
url = {https://www.sciencedirect.com/science/article/pii/S0273229721000411},
author = {Sarah M. Edelson and Valerie F. Reyna},
keywords = {Risk-taking, Risky decision making, Reward sensitivity, COVID-19, Fuzzy-trace theory, Adolescence},
abstract = {Comprehensive meta-analyses of risky decision making in children, adolescents, and adults have revealed that age trends in disambiguated laboratory tasks confirmed fuzzy-trace theory’s prediction that preference for risk decreases monotonically from childhood to adulthood. These findings are contrary to predictions of dual systems or neurobiological imbalance models. Assumptions about increasing developmental reliance on mental representations of the gist of risky options are essential to account for this developmental trend. However, dual systems theory appropriately emphasizes how cultural context changes behavioral manifestation of risk preferences across age and neurobiological imbalance models appropriately emphasize developmental changes in reward sensitivity. All of the major theories include the assumption of increasing behavioral inhibition. Here, we integrate these theoretical constructs—representation, cultural context, reward sensitivity, and behavioral inhibition—to provide a novel framework for understanding and improving risky decision making in youth. We also discuss the roles of critical tests, scientific falsification, disambiguating assessments of psychological and neurological processes, and the misuse of such concepts as ecological validity and reverse inference. We illustrate these concepts by extending fuzzy-trace theory to explain why youth are a major conduit of viral infections, including the virus that causes COVID-19. We conclude by encouraging behavioral scientists to embrace new ways of thinking about risky decision making that go beyond traditional stereotypes about adolescents and that go beyond conceptualizing ideal decision making as trading off degrees of risk and reward.}
}
@article{GABRIEL2008330,
title = {A friend is a present you give to your “Self”: Avoidance of intimacy moderates the effects of friends on self-liking},
journal = {Journal of Experimental Social Psychology},
volume = {44},
number = {2},
pages = {330-343},
year = {2008},
issn = {0022-1031},
doi = {https://doi.org/10.1016/j.jesp.2007.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0022103107001126},
author = {Shira Gabriel and Mauricio Carvallo and Lisa M. Jaremka and Brooke Tippin},
keywords = {The self, Social comparison, Friendship, Avoidance of intimacy, Attachment style},
abstract = {The current research proposes that thinking about friends improves feelings about the self and does so differentially depending on avoidance of intimacy. Based on previous findings that individuals who avoid intimacy in relationships (avoidant individuals) contrast their self-concepts with primed friends whereas those who pursue intimacy in relationships (non-avoidant individuals) assimilate their self-concepts to primed friends [Gabriel, S., Carvallo, M., Dean, K., Tippin, B. D., & Renaud, J. (2005). How I see “Me” depends on how I see “We”: The role of avoidance of intimacy in social comparison. Personality and Social Psychology Bulletin, 31, 156–157], we predicted that friends who embody negative aspects of self would lead avoidant individuals to like themselves more, whereas friends who embody positive aspects of self would lead non-avoidant individuals to like themselves more. A pretest determined that good friends were seen as more similar to positive and ideal aspects of the self, whereas friends about whom participants had more mixed feelings (ambivalent friends) were seen as more similar to disliked and feared aspects of the self. Four experiments supported the main hypotheses. In Experiment 1, non-avoidant individuals like themselves more when good friends were primed. In Experiment 2, avoidant individuals like themselves more when ambivalent friends were primed. In Experiment 3, non-avoidant individuals liked themselves better after thinking about a friend’s positive traits, whereas avoidant individuals liked themselves better after thinking about a friend’s negative traits. In Experiment 4, all individuals under self-esteem threat strategically brought friends to mind who would help them like themselves more.}
}
@article{CHASTAIN200083,
title = {Cultivating design competence: online support for beginning design studio},
journal = {Automation in Construction},
volume = {9},
number = {1},
pages = {83-91},
year = {2000},
issn = {0926-5805},
doi = {https://doi.org/10.1016/S0926-5805(99)00053-9},
url = {https://www.sciencedirect.com/science/article/pii/S0926580599000539},
author = {Thomas Chastain and Ame Elliott},
abstract = {A primary lesson of a beginning design studio is the development of a fundamental design competence. This entails acquiring skills of integration, projection, exploration, as well as critical thinking—forming the basis of thinking “like a designer”. Plaguing the beginning architectural design student as she develops this competence are three typical problems: a lagging visual intelligence, a linking of originality with creativity, and the belief that design is an act of an individual author instead of a collaborative activity. We believe that computation support for design learning has particular attributes for helping students overcome these problems. These attributes include its inherent qualities for visualization, for explicitness, and for sharing. This paper describes five interactive multi-media exercises exploiting these attributes which were developed to support a beginning design studio. The paper also reports how they have been integrated into the course curriculum. Le développement des compétences en design: support on-line pour le studio de design élémentaire Une des premières leçons lors du studio de design est le développement d’une compétence fondamentale en conception. Ceci implique l’acquisition des habiletés d’intégration, de projection, d’exploration ainsi que la pensée critique—antérieurement les bases de la façon de penser nommée “comme un concepteur”. Il y a trois problèmes fondamentaux qui pèsent sur l’étudiant débutant en architecture lors du développement cette compétence: une intelligence visuelle insuffisante, le fait de lier l’originalité à la créativité, et la croyance que le processus de conception est une activité individuelle, plutôt que collaborative. Nous sommes de l’avis que le soutien en informatique lors de l’apprentissage de la conception architecturale posséde des attributs bien particuliers pour aider les étudiants à surmonter ces difficultés. Ces attributs comprennent des qualités inhérentes pour la visualisation, pour être explicite, et pour le partage. Ce papier décrit cinq exercices de médias interactifs qui exploitent ces attributs, et qui ont été développés pour supporter un studio de design élémentaire. Il présente aussi un reportage sur la façon dont ces exercices ont été intégrés dans le curriculum du cours.}
}
@incollection{CARPENDALE2013125,
title = {Chapter Six - A Relational Developmental Systems Approach to Moral Development},
editor = {Richard M. Lerner and Janette B. Benson},
series = {Advances in Child Development and Behavior},
publisher = {JAI},
volume = {45},
pages = {125-153},
year = {2013},
booktitle = {Embodiment and Epigenesis: Theoretical and Methodological Issues in Understanding the Role of Biology within the Relational Developmental System},
issn = {0065-2407},
doi = {https://doi.org/10.1016/B978-0-12-397946-9.00006-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780123979469000063},
author = {Jeremy I.M. Carpendale and Stuart I. Hammond and Sherrie Atwood},
keywords = {Developmental systems theory, Moral development, Moral norms, Nativism, Social interaction},
abstract = {Morality and cooperation are central to human life. Psychological explanations for moral development and cooperative behavior will have biological and evolutionary dimensions, but they can differ radically in their approach to biology. In particular, many recent proposals have pursued the view that aspects of morality are innate. We briefly review and critique two of these claims. In contrast to these nativist assumptions about the role of biology in morality, we present an alternative approach based on a relational developmental systems view of moral development. The role for biology in this approach is in setting up the conditions—the developmental system—in which forms of interaction and later forms of thinking emerge.}
}
@article{CHANG2023101823,
title = {Stakeholder requirement evaluation of smart industrial service ecosystem under Pythagorean fuzzy environment for complex industrial contexts: A case study of renewable energy park},
journal = {Advanced Engineering Informatics},
volume = {55},
pages = {101823},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2022.101823},
url = {https://www.sciencedirect.com/science/article/pii/S1474034622002816},
author = {Yuan Chang and Xinguo Ming and Zhihua Chen and Tongtong Zhou and Xiaoqiang Liao and Wenyan Song},
keywords = {Smart industrial product-service system (IPS), Requirement evaluation, Service ecosystem, Pythagorean fuzzy sets, Multi-criteria decision making, Viable systems model},
abstract = {This study focuses on ways to systematically evaluate stakeholder requirements when developing a smart industrial service ecosystem (SISE) in a complex industrial context. The SISE development requires considering the service requirement from both the complex industrial context and service ecosystem manners. This study proposes a systematic framework for stakeholder requirement evaluation in SISE. The first part of the framework is the industrial context-viable system model with ecological thinking (IC-VESM) to elicit the service requirements for the SISE, which facilitates a systematic analysis of the service value proposition and service requirement elicitation in the operational lifecycle of an entire industrial context. This second part of the framework proposes a method for evaluating service requirements that is both feasible and systematic. This is achieved by combining the Fuzzy Kano and AHP methods in a Pythagorean fuzzy (PF) environment. The PF Kano computes the categories and determines the weights of service requirements from a consumer perspective, while the PF AHP hierarchically analyzes the service requirements and provides pairwise comparison paths for design experts. Finally, an illustrative case study in a renewable energy context was used to demonstrate the feasibility and effectiveness of the methodology. The proposed theoretical model provides more reliable and systematic outcomes than traditional methods when eliciting service requirements and evaluating complex smart industrial service solutions. The study has practical implications by providing useful insights for companies to recognize key smart service requirements in complex industrial contexts and to improve sustainable development.}
}
@article{WISE2024144,
title = {Naturalistic reinforcement learning},
journal = {Trends in Cognitive Sciences},
volume = {28},
number = {2},
pages = {144-158},
year = {2024},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2023.08.016},
url = {https://www.sciencedirect.com/science/article/pii/S1364661323002127},
author = {Toby Wise and Kara Emery and Angela Radulescu},
keywords = {reinforcement learning, decision-making, naturalistic, computational modeling},
abstract = {Humans possess a remarkable ability to make decisions within real-world environments that are expansive, complex, and multidimensional. Human cognitive computational neuroscience has sought to exploit reinforcement learning (RL) as a framework within which to explain human decision-making, often focusing on constrained, artificial experimental tasks. In this article, we review recent efforts that use naturalistic approaches to determine how humans make decisions in complex environments that better approximate the real world, providing a clearer picture of how humans navigate the challenges posed by real-world decisions. These studies purposely embed elements of naturalistic complexity within experimental paradigms, rather than focusing on simplification, generating insights into the processes that likely underpin humans’ ability to navigate complex, multidimensional real-world environments so successfully.}
}
@article{RASANAN2024857,
title = {Beyond discrete-choice options},
journal = {Trends in Cognitive Sciences},
volume = {28},
number = {9},
pages = {857-870},
year = {2024},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2024.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S136466132400175X},
author = {Amir Hosein Hadian Rasanan and Nathan J. Evans and Laura Fontanesi and Catherine Manning and Cynthia Huang-Pollock and Dora Matzke and Andrew Heathcote and Jörg Rieskamp and Maarten Speekenbrink and Michael J. Frank and Stefano Palminteri and Christopher G. Lucas and Jerome R. Busemeyer and Roger Ratcliff and Jamal Amani Rad},
abstract = {While decision theories have evolved over the past five decades, their focus has largely been on choices among a limited number of discrete options, even though many real-world situations have a continuous-option space. Recently, theories have attempted to address decisions with continuous-option spaces, and several computational models have been proposed within the sequential sampling framework to explain how we make a decision in continuous-option space. This article aims to review the main attempts to understand decisions on continuous-option spaces, give an overview of applications of these types of decisions, and present puzzles to be addressed by future developments.}
}
@article{CALUDE2023844,
title = {What perceptron neural networks are (not) good for?},
journal = {Information Sciences},
volume = {621},
pages = {844-857},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2022.11.083},
url = {https://www.sciencedirect.com/science/article/pii/S0020025522013743},
author = {Cristian S. Calude and Shahrokh Heidari and Joseph Sifakis},
keywords = {Perceptrons, Sensitive and robust functions, Quantum computing},
abstract = {Perceptron Neural Networks (PNNs) are essential components of intelligent systems because they produce efficient solutions to problems of overwhelming complexity for conventional computing methods. Many papers show that PNNs can approximate a wide variety of functions, but comparatively, very few discuss their limitations and the scope of this paper. To this aim, we define two classes of Boolean functions – sensitive and robust –, and prove that an exponentially large set of sensitive functions are exponentially difficult to compute by multi-layer PNNs (hence incomputable by single-layer PNNs). A comparatively large set of functions in the second one, but not all, are computable by single-layer PNNs. Finally, we used polynomial threshold PNNs to compute all Boolean functions with quantum annealing and present in detail a QUBO computation on the D-Wave Advantage. These results confirm that the successes of PNNs, or lack of them, are in part determined by properties of the learned data sets and suggest that sensitive functions may not be (efficiently) computed by PNNs.}
}
@article{GREENE201766,
title = {The rat-a-gorical imperative: Moral intuition and the limits of affective learning},
journal = {Cognition},
volume = {167},
pages = {66-77},
year = {2017},
note = {Moral Learning},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2017.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0010027717300690},
author = {Joshua D. Greene},
keywords = {Deontology, Utilitarianism, Consequentialism, Reinforcement learning, Model-free learning, Machine learning, Ethics, Normative ethics, Moral judgment},
abstract = {Decades of psychological research have demonstrated that intuitive judgments are often unreliable, thanks to their inflexible reliance on limited information (Kahneman, 2003, 2011). Research on the computational underpinnings of learning, however, indicates that intuitions may be acquired by sophisticated learning mechanisms that are highly sensitive and integrative. With this in mind, Railton (2014) urges a more optimistic view of moral intuition. Is such optimism warranted? Elsewhere (Greene, 2013) I’ve argued that moral intuitions offer reasonably good advice concerning the give-and-take of everyday social life, addressing the basic problem of cooperation within a “tribe” (“Me vs. Us”), but that moral intuitions offer unreliable advice concerning disagreements between tribes with competing interests and values (“Us vs. Them”). Here I argue that a computational perspective on moral learning underscores these conclusions. The acquisition of good moral intuitions requires both good (representative) data and good (value-aligned) training. In the case of inter-tribal disagreement (public moral controversy), the problem of bad training looms large, as training processes may simply reinforce tribal differences. With respect to moral philosophy and the paradoxical problems it addresses, the problem of bad data looms large, as theorists seek principles that minimize counter-intuitive implications, not only in typical real-world cases, but in unusual, often hypothetical, cases such as some trolley dilemmas. In such cases the prevailing real-world relationships between actions and consequences are severed or reversed, yielding intuitions that give the right answers to the wrong questions. Such intuitions—which we may experience as the voice of duty or virtue—may simply reflect the computational limitations inherent in affective learning. I conclude, in optimistic agreement with Railton, that progress in moral philosophy depends on our having a better understanding of the mechanisms behind our moral intuitions.}
}
@incollection{MUBAYI2017249,
title = {Chapter 10 - Computational Modeling Approaches Linking Health and Social Sciences: Sensitivity of Social Determinants on the Patterns of Health Risk Behaviors and Diseases},
editor = {Arni S.R. {Srinivasa Rao} and Saumyadipta Pyne and C.R. Rao},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {36},
pages = {249-304},
year = {2017},
booktitle = {Disease Modelling and Public Health, Part A},
issn = {0169-7161},
doi = {https://doi.org/10.1016/bs.host.2017.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0169716117300172},
author = {Anuj Mubayi},
keywords = {Health risk behaviors, Dynamic models, Data mining, Sensitivity and uncertainty analysis, Ecological models, Social influences},
abstract = {Developing health promotion programs that support healthy lifestyle behaviors require comprehensive understanding of mechanisms that drive such complex social systems. Policy makers can use models and theories to guide this process at the individuals, groups, and communities levels. Individuals can have multiple risky health behaviors including physical inactivity, unhealthy diets, smoking, and alcohol drinking that are often shaped by social and ecological factors. Collective understanding of these factors can provide ability to design and evaluate intervention programs that can change unhealthy or risky behaviors over long period of time. However, it is overwhelming task to optimize intervention based on only empirical and/or cross-sectional studies. Effective long lasting intervention needs a thorough understanding of the role of social and environmental mechanisms at multiple scales on the dynamics of health behaviors. Recent mathematical and computational methods developed in other fields, such as epidemiology and finance, can provide systematic and in-depth understanding of mechanisms. However, the use of such methods in social and behaviors sciences have been limited. In this chapter, some real life working examples of social health behaviors problems are provided which uses some cutting edge methods from dynamical systems and data mining to uncertainty quantification.}
}
@article{WARD202154,
title = {On value-laden science},
journal = {Studies in History and Philosophy of Science Part A},
volume = {85},
pages = {54-62},
year = {2021},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2020.09.006},
url = {https://www.sciencedirect.com/science/article/pii/S0039368120301783},
author = {Zina B. Ward},
keywords = {Values, Values in science, Argument from inductive risk, Motivating reasons, Justifying reasons},
abstract = {Philosophical work on values in science is held back by widespread ambiguity about how values bear on scientific choices. Here, I disambiguate several ways in which a choice can be value-laden and show that this disambiguation has the potential to solve and dissolve philosophical problems about values in science. First, I characterize four ways in which values relate to choices: values can motivate, justify, cause, or be impacted by the choices we make. Next, I put my proposed taxonomy to work, using it to clarify one version of the argument from inductive risk. The claim that non-epistemic values must play a role in scientific choices that run inductive risk makes most sense as a claim about values being needed to justify such choices. The argument from inductive risk is not unique: many philosophical arguments about values in science can be more clearly understood and assessed by paying close attention to how values and choices are related.}
}
@article{XU2024102430,
title = {A temporal approach to online discussion during disasters: Applying SIR infectious disease model to predict topic growth and examining effects of temporal distance},
journal = {Public Relations Review},
volume = {50},
number = {2},
pages = {102430},
year = {2024},
issn = {0363-8111},
doi = {https://doi.org/10.1016/j.pubrev.2024.102430},
url = {https://www.sciencedirect.com/science/article/pii/S0363811124000092},
author = {Sifan Xu and Xinyan Zhao and Jie Chen},
keywords = {Disaster, SIR, Computational modeling, SIR model, Twitter big data, Climate change, Topic growth, Construal level},
abstract = {Discussions on social media during major disasters are robust and often have multiple frames of reference. Temporal perspectives, however, are still lacking in current understandings of social-mediated discussions during disasters and crises, but incorporating temporal perspectives can significantly enhance environmental scanning efforts as prescribed in the issues management framework. The purpose of the current research is twofold: to apply and validate the SIR (Susceptible-Infectious-Recovered) model to examine topics’ growth over time on social media and to understand how future orientation of social media users (an indicator of temporal distance) affects their construal of a disaster through supervised machine learning. We based our analysis on Twitter discussions during the Texas winter storm in 2021. Results of the study show great fit of the SIR model for topic growth, and that temporal distance affects users’ construal of the event in line with core predictions of construal level theory. Theoretical, methodological, and practical implications on social-mediated discussions related to climate change-induced and -intensified disasters and issues management are discussed.}
}
@article{WAHLHEIM2024,
title = {Memory updating and the structure of event representations},
journal = {Trends in Cognitive Sciences},
year = {2024},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2024.11.008},
url = {https://www.sciencedirect.com/science/article/pii/S1364661324003152},
author = {Christopher N. Wahlheim and Jeffrey M. Zacks},
keywords = {memory updating, prediction error, interference, event cognition, pattern separation, hippocampus},
abstract = {People form memories of specific events and use those memories to make predictions about similar new experiences. Living in a dynamic environment presents a challenge: How does one represent valid prior events in memory while encoding new experiences when things change? There is evidence for two seemingly contradictory classes of mechanism: One differentiates outdated event features by making them less similar or less accessible than updated event features. The other integrates updated features of new events with outdated memories, and the relationship between them, into a structured representation. Integrative encoding may occur when changed events trigger inaccurate predictions based on remembered prior events. We propose that this promotes subsequent recollection of events and their order, enabling adaptation to environmental changes.}
}
@article{SHAFFER199795,
title = {Learning mathematics through design: The anatomy of Escher's world},
journal = {The Journal of Mathematical Behavior},
volume = {16},
number = {2},
pages = {95-112},
year = {1997},
issn = {0732-3123},
doi = {https://doi.org/10.1016/S0732-3123(97)90019-5},
url = {https://www.sciencedirect.com/science/article/pii/S0732312397900195},
author = {David Williamson Shaffer},
abstract = {This article explores one example of an open learning environment created by combining mathematics and design activities in a “mathematics studio”. Two iterations of the mathematics studio experiment in a project at the MIT Media Laboratory known as Escher's World suggest that: (a) students can learn about the mathematical concept of symmetry in a studio learning environment, (b) students learn to use visual thinking to solve mathematical problems in a studio learning environment, and (c) students develop a more positive attitude towards mathematics as a result of working in a studio learning environment. This article uses a qualitative research model to explore the specific characteristics of the mathematics studio that were influential in creating a successful learning environment—in particular, how expressive mathematics activities and expressive computational media give students a sense of control over their learning.}
}
@article{LOPEZSILVA202446,
title = {‘Are these my thoughts?’: A 20-year prospective study of thought insertion, thought withdrawal, thought broadcasting, and their relationship to auditory verbal hallucinations},
journal = {Schizophrenia Research},
volume = {265},
pages = {46-57},
year = {2024},
note = {Hallucinations: Neurobiology and Patient Experience},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2022.07.005},
url = {https://www.sciencedirect.com/science/article/pii/S0920996422002778},
author = {Pablo López-Silva and Martin Harrow and Thomas H. Jobe and Michele Tufano and Helen Harrow and Cherise Rosen},
keywords = {Schizophrenia, Psychosis, Thought insertion, Thought withdrawal, Thought broadcasting, Auditory-verbal hallucinations},
abstract = {The co-occurrence of delusions and other symptoms at the onset of psychosis is a challenge for theories about the aetiology of psychosis. This paper explores the relatedness of delusions about the experience of thinking (thought insertion, thought withdrawal, and thought broadcasting) and auditory verbal hallucinations by describing their trajectories over a 20-year period in individuals diagnosed with schizophrenia, affective and other psychosis, and unipolar depression nonpsychosis. The sample consisted of 407 participants who were recruited at index hospitalization and evaluated over six follow-ups over 20 years. The symptom structure associated with thought insertion included auditory verbal hallucinations, somatic hallucinations, other hallucinations, delusions of thought-dissemination, delusions of control, delusion of self-depreciation, depersonalization and anxiety. The symptom constellation of thought withdrawal included somatic hallucinations, other hallucinations, delusions of thought dissemination, delusions of control, sexual delusions, depersonalization, negative symptoms, depression, and anxiety. The symptom constellation of thought broadcasting included auditory verbal hallucinations, somatic hallucinations, delusions of thought-dissemination, delusion of self-depreciation, fantastic delusions, sexual delusions, and depersonalization. Auditory verbal hallucinations and delusions of self-depreciation were significantly associated with both thought insertion and thought broadcasting. Thought insertion and thought withdrawal were significantly associated with other hallucinations, delusions of control, and anxiety; thought withdrawal and thought broadcasting were significantly related to sexual delusions. We hypothesize that specific symptom constellations over time might be explained as the product of pseudo-coherent realities created to give meaning to the experience of the world and the self of individuals in psychosis based on both prior top-down and ongoing bottom-up elements.}
}
@article{WANG20071997,
title = {DIANA: A computer-supported heterogeneous grouping system for teachers to conduct successful small learning groups},
journal = {Computers in Human Behavior},
volume = {23},
number = {4},
pages = {1997-2010},
year = {2007},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2006.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0747563206000094},
author = {Dai-Yi Wang and Sunny S.J. Lin and Chuen-Tsai Sun},
keywords = {Cooperative learning, Small-group learning, Computer assisted grouping system, Group composition, Thinking styles, University students},
abstract = {Teachers interested in small-group learning can benefit from using psychological factors to create heterogeneous groups. In this paper we describe a computer-supported grouping system named DIANA that uses genetic algorithms to achieve fairness, equity, flexibility, and easy implementation. Grouping was performed so as to avoid the creation of exceptionally weak groups. We tested DIANA with 66 undergraduate computer science students assigned to groups of three either randomly (10 groups) or using an algorithm reflecting [Sternberg, R. J. (1994). Thinking styles: theory and assessment at the interface between intelligence and personality. In R. J. Sterberg, & P. Ruzgis (Eds.), Personality and Intelligence (pp. 169–187). New York: Cambridge University Press.] three thinking styles (12 groups). The results indicate that: (a) the algorithm-determined groups were more capable of completing whatever they were “required to do” at a statistically significant level, (b) both groups were equally capable of solving approximately 80% of what they “chose to do,” and (c) the algorithm-determined groups had smaller inter-group variation in performance. Levels of satisfaction with fellow group member attitudes, the cooperative process, and group outcomes were also higher among members of the algorithm-determined groups. Suggestions for applying computer-supported group composition systems are offered.}
}
@article{BAKER2021101933,
title = {Who is marginalized in energy justice? Amplifying community leader perspectives of energy transitions in Ghana},
journal = {Energy Research & Social Science},
volume = {73},
pages = {101933},
year = {2021},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2021.101933},
url = {https://www.sciencedirect.com/science/article/pii/S2214629621000268},
author = {Erin Baker and Destenie Nock and Todd Levin and Samuel A. Atarah and Anthony Afful-Dadzie and David Dodoo-Arhin and Léonce Ndikumana and Ekundayo Shittu and Edwin Muchapondwa and Charles Van-Hein Sackey},
abstract = {There is a divide in energy access studies, between technologically-focused modeling papers in engineering and economics, and energy justice frameworks and principles grounded in social sciences. Quantitative computational models are necessary when analyzing energy, and more specifically electricity, systems, as they are technologically-complex systems that can diverge from intuitive patterns. To assure energy justice, these models must be reflective of, and informative to, a wide range of stakeholders, including households and communities alongside utilities, governments, and others. Yet, moving from a qualitative understanding of preferences to quantitative modeling is challenging. In this perspective piece, we pilot the use of the value-focused thinking framework to inform stakeholder engagement. The result is a strategic objective hierarchy that highlights the tradeoffs and the social, economic and technological factors that need to be measured in models. We apply the process in Ghana, using a survey, stakeholder workshops, and follow-up interviews to uncover key tradeoffs and stakeholder-derived objectives. We discuss three key areas that have been rarely, if ever, well-represented in energy models: (1) the relationship between the dynamics of electricity end-use and the technology and economic structure of the system; (2) explicit tradeoffs between electricity access, cost, and reliability as defined by stakeholders; and (3) the definition of new objectives, such as minimizing hazards related to theft. We conclude that this model of engagement provides an opportunity to tie together rigorous qualitative analysis and stakeholder engagement with crucial quantitative models of the electricity system.}
}
@incollection{ZHUGE2016149,
title = {15 - Limitations and challenges},
editor = {Hai Zhuge},
booktitle = {Multi-Dimensional Summarization in Cyber-Physical Society},
publisher = {Morgan Kaufmann},
pages = {149-151},
year = {2016},
series = {Computer Science Reviews and Trends},
isbn = {978-0-12-803455-2},
doi = {https://doi.org/10.1016/B978-0-12-803455-2.00015-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128034552000159},
author = {Hai Zhuge},
keywords = {Summarization, limitations, challenges, representations, computing},
abstract = {The limitation of summarisation lies in the natural differences between human and machine, between languages, and between the ways of observation and thinking of authors and those of readers. The significant evolution of documents in form and function in cyber-physical society challenges the paradigm of summarization research.}
}
@article{ROMAN1992161,
title = {Pavane: a system for declarative visualization of concurrent computations},
journal = {Journal of Visual Languages & Computing},
volume = {3},
number = {2},
pages = {161-193},
year = {1992},
issn = {1045-926X},
doi = {https://doi.org/10.1016/1045-926X(92)90014-D},
url = {https://www.sciencedirect.com/science/article/pii/1045926X9290014D},
author = {Gruia-Catalin Roman and Kenneth C Cox and C.Donald Wilcox and Jerome Y Plun},
abstract = {This paper describes the conceptual model, specification method and visualization methodology for Pavane—a visualization environment concerned with exploring, monitoring and presenting concurrent computations. The underlying visualization model is declarative in the sense that visualization is treated as a mapping from program states to a three-dimensional world of geometric objects. The latter is rendered in full color and may be examined freely by a viewer who is allowed to navigate through the geometric world. The state-to-geometry mapping is defined as a composition of several simpler mappings. The choice is determined by methodological and architectural considerations. This paper shows how this decomposition was molded by two methodological objectives: (1) the desire visually to capture abstract formal properties of programs (e.g. safety and progress) rather than operational details; and (2) the need to support complex animations of atomic computational events. All mappings are specified using a rule-based notation; rules may be added, deleted and modified at any time during the visualization. An algorithm for termination detection in diffusing computations is used to illustrate the specification method and to demonstrate its conceptual elegance and flexibility. A concurrent version of a popular artificial intelligence program provides a vehicle for demonstrating how we derive graphical representations and animation scenarios from key formal properties of the program, i.e. from those safety and progress assertions about the program which turn out to be important in verifying its correctness.}
}
@article{ALANO20221,
title = {Professor Richard Carter (1945–2021)},
journal = {Trends in Parasitology},
volume = {38},
number = {1},
pages = {1-3},
year = {2022},
issn = {1471-4922},
doi = {https://doi.org/10.1016/j.pt.2021.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S1471492221002609},
author = {Pietro Alano and Richard Culleton and Christian Doerig and Louis Miller},
abstract = {The malaria research community lost a pioneer when Professor Richard Carter passed away at the age of 76 on 4 September 2021. Richard was an exceptionally brilliant malariologist, always inquisitive and gifted with an unorthodox way of thinking.}
}
@article{LI2024e40037,
title = {The application and impact of artificial intelligence technology in graphic design: A critical interpretive synthesis},
journal = {Heliyon},
volume = {10},
number = {21},
pages = {e40037},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e40037},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024160689},
author = {Hong Li and Tao Xue and Aijia Zhang and Xuexing Luo and Lingqi Kong and Guanghui Huang},
keywords = {Atificial intelligence, Graphic design, Machine learning, Computer vision, Visual communication design, Systematic review},
abstract = {In the field of graphic design, the application of Artificial Intelligence (AI) is reshaping the design process. This study employs the Critical Interpretive Synthesis (CIS) approach to explore the impacts and challenges of AI on graphic design. Through a comprehensive review of 33 papers, this research reveals four research paradigms of AI in graphic design: Artificial Intelligence Driven Design Automation and Generation (AIDAG), Artificial Intelligence Assisted Graphic Design and Image Processing (AGDIP), Artificial Intelligence in Art and Creative Design Processes (AACDP), and Artificial Intelligence Enhanced Visual Attention and Emotional Response Modeling (AVERM). These paradigms demonstrate the multidimensional role of AI in design, ranging from automation to emotional interaction. The findings suggest that AI serves a dual role as both a design tool and a medium for innovation. AI not only enhances the automation and efficiency of the design process but also fosters designers' creative thinking and understanding of users' emotional needs. This study also proposes a path for the application of the four paradigms in the graphic design process, providing effective design ideas for future design workflows.}
}
@article{FAHIMI2024,
title = {Improving the Efficiency of Inferences From Hybrid Samples for Effective Health Surveillance Surveys: Comprehensive Review of Quantitative Methods},
journal = {JMIR Public Health and Surveillance},
volume = {10},
year = {2024},
issn = {2369-2960},
doi = {https://doi.org/10.2196/48186},
url = {https://www.sciencedirect.com/science/article/pii/S2369296024000188},
author = {Mansour Fahimi and Elizabeth C Hair and Elizabeth K Do and Jennifer M Kreslake and Xiaolu Yan and Elisa Chan and Frances M Barlas and Abigail Giles and Larry Osborn},
keywords = {hybrid samples, composite estimation, optimal composition factor, unequal weighting effect, composite weighting, weighting, surveillance, sample survey, data collection, risk factor},
abstract = {Background
Increasingly, survey researchers rely on hybrid samples to improve coverage and increase the number of respondents by combining independent samples. For instance, it is possible to combine 2 probability samples with one relying on telephone and another on mail. More commonly, however, researchers are now supplementing probability samples with those from online panels that are less costly. Setting aside ad hoc approaches that are void of rigor, traditionally, the method of composite estimation has been used to blend results from different sample surveys. This means individual point estimates from different surveys are pooled together, 1 estimate at a time. Given that for a typical study many estimates must be produced, this piecemeal approach is computationally burdensome and subject to the inferential limitations of the individual surveys that are used in this process.
Objective
In this paper, we will provide a comprehensive review of the traditional method of composite estimation. Subsequently, the method of composite weighting is introduced, which is significantly more efficient, both computationally and inferentially when pooling data from multiple surveys. With the growing interest in hybrid sampling alternatives, we hope to offer an accessible methodology for improving the efficiency of inferences from such sample surveys without sacrificing rigor.
Methods
Specifically, we will illustrate why the many ad hoc procedures for blending survey data from multiple surveys are void of scientific integrity and subject to misleading inferences. Moreover, we will demonstrate how the traditional approach of composite estimation fails to offer a pragmatic and scalable solution in practice. By relying on theoretical and empirical justifications, in contrast, we will show how our proposed methodology of composite weighting is both scientifically sound and inferentially and computationally superior to the old method of composite estimation.
Results
Using data from 3 large surveys that have relied on hybrid samples composed of probability-based and supplemental sample components from online panels, we illustrate that our proposed method of composite weighting is superior to the traditional method of composite estimation in 2 distinct ways. Computationally, it is vastly less demanding and hence more accessible for practitioners. Inferentially, it produces more efficient estimates with higher levels of external validity when pooling data from multiple surveys.
Conclusions
The new realities of the digital age have brought about a number of resilient challenges for survey researchers, which in turn have exposed some of the inefficiencies associated with the traditional methods this community has relied upon for decades. The resilience of such challenges suggests that piecemeal approaches that may have limited applicability or restricted accessibility will prove to be inadequate and transient. It is from this perspective that our proposed method of composite weighting has aimed to introduce a durable and accessible solution for hybrid sample surveys.}
}
@article{CHAUDHARI2024100953,
title = {PSOGSA: A parallel implementation model for data clustering using new hybrid swarm intelligence and improved machine learning technique},
journal = {Sustainable Computing: Informatics and Systems},
volume = {41},
pages = {100953},
year = {2024},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2023.100953},
url = {https://www.sciencedirect.com/science/article/pii/S2210537923001087},
author = {Shruti Chaudhari and Anuradha Thakare and Ahmed M. Anter},
keywords = {Clustering, Swarm intelligence, PSO, Gravitational search algorithm, Neural network, GPU},
abstract = {With the digitization of the entire world and huge requirements of understanding unknown patterns from the data, clustering becomes an important research area. The quick and accurate division of large datasets with a range of properties or features becomes challenging. The parallel implementation of clustering algorithms must satisfy stringent computational requirements to handle large amounts of data. This can be achieved by designing a GPU based optimal computational model with a heuristic approach. Swarm Intelligence (SI), a family of bio-inspired algorithms, that has been effectively applied to a number of real-world clustering problems. The Gravitational Search Algorithm (GSA) is a heuristic search optimization approach based on Newton's Law of Gravitation and mass interactions. Although it has a slow searching rate in the last iterations, this strategy has been proved to be capable of discovering the global optimum. This paper presents GPU based hybrid parallel algorithms for data clustering. A newly developed, hybrid Particle Swarm Optimization (PSO) and Gravitational Search Algorithm (GSA) i.e., PSOGSA achieves the global optima. PSOGSA utilizes novel training methods for enhanced Neural Networks (NN) in order to examine the efficiency of algorithms and resolves the challenges of trapping in local minima. This also shows the sluggish convergence rate of standard evolutionary learning algorithms. The Nearest Neighbour Partition (Partitioning of the Neighbourhood) algorithm can be used to improve the performance of NN. A parallel version of Hybrid PSOGSA with NN is implemented to achieve optimal results with better computational time. Compared to the CPU-based regular PSO, the suggested Hybrid PSOGSA with NN achieved optimal clustering with 71% improved computational time.}
}
@article{YAO2022107747,
title = {Regional attention reinforcement learning for rapid object detection},
journal = {Computers & Electrical Engineering},
volume = {98},
pages = {107747},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.107747},
url = {https://www.sciencedirect.com/science/article/pii/S004579062200057X},
author = {Hongge Yao and Peng Dong and Siyi Cheng and Jun Yu},
keywords = {Regional attention, Reinforcement learning, Object detection, Information fusion, Location and recognition},
abstract = {When people observe a picture, they first pay attention to local areas of the picture, rather than the whole areas, then combine them with previous experience in the brain, and finally make judgments through thinking. This is human visual logic. In this paper, we propose a regional attention reinforcement learning model for object detection. The proposed model uses human visual logical to solve the detection problem of small and complex targets in the picture. The model uses a recurrent network structure as the main framework to extract historical information, and fuse the historical information with the current concerned information. At each recurrent time step, it can pay attention to the fused information, especially pay more attention to the information that may have objects. Experimental results show that the proposed method has more than 5% improved in recognition accuracy to the conventional methods. In terms of FLOPs, the conventional methods normally require 170 M, while the proposed method only needs 25.4M This means that the proposed method has higher detection efficiency.}
}
@article{NISHANT2020102104,
title = {Artificial intelligence for sustainability: Challenges, opportunities, and a research agenda},
journal = {International Journal of Information Management},
volume = {53},
pages = {102104},
year = {2020},
issn = {0268-4012},
doi = {https://doi.org/10.1016/j.ijinfomgt.2020.102104},
url = {https://www.sciencedirect.com/science/article/pii/S0268401220300967},
author = {Rohit Nishant and Mike Kennedy and Jacqueline Corbett},
keywords = {Agenda for practice, AI, Artificial intelligence, Climate change, Environmental governance, Natural environment, Research agenda, Sustainability},
abstract = {Artificial intelligence (AI) will transform business practices and industries and has the potential to address major societal problems, including sustainability. Degradation of the natural environment and the climate crisis are exceedingly complex phenomena requiring the most advanced and innovative solutions. Aiming to spur groundbreaking research and practical solutions of AI for environmental sustainability, we argue that AI can support the derivation of culturally appropriate organizational processes and individual practices to reduce the natural resource and energy intensity of human activities. The true value of AI will not be in how it enables society to reduce its energy, water, and land use intensities, but rather, at a higher level, how it facilitates and fosters environmental governance. A comprehensive review of the literature indicates that research regarding AI for sustainability is challenged by (1) overreliance on historical data in machine learning models, (2) uncertain human behavioral responses to AI-based interventions, (3) increased cybersecurity risks, (4) adverse impacts of AI applications, and (5) difficulties in measuring effects of intervention strategies. The review indicates that future studies of AI for sustainability should incorporate (1) multilevel views, (2) systems dynamics approaches, (3) design thinking, (4) psychological and sociological considerations, and (5) economic value considerations to show how AI can deliver immediate solutions without introducing long-term threats to environmental sustainability.}
}
@article{WATHEN2022176,
title = {Some observations on preconditioning for non-self-adjoint and time-dependent problems},
journal = {Computers & Mathematics with Applications},
volume = {116},
pages = {176-180},
year = {2022},
note = {New trends in Computational Methods for PDEs},
issn = {0898-1221},
doi = {https://doi.org/10.1016/j.camwa.2021.05.037},
url = {https://www.sciencedirect.com/science/article/pii/S0898122121002388},
author = {Andy Wathen},
keywords = {Iterative methods, Preconditioning, Non-self-adjoint problems, Parallel-in-time computation},
abstract = {Numerical Linear Algebra—specifically the computational solution of equations—forms a significant part of Computational Methods for Partial Differential Equations. Here we discuss the contrast between the solution of symmetric systems of equations that arise from self-adjoint problems and non-symmetric systems that arise from non-self-adjoint problems when iterative methods are employed; such methods are the only feasible methods for very large scale computation with PDEs. We then go on to consider non-symmetric all-at-once systems that arise in approximation of time-dependent problems, discuss causality and the parallel-in-time paradigm, suggesting an approach that involves preconditioning initial value problems with time-periodic problems.}
}
@article{NADOLSKI2019210,
title = {Complex systems analysis of hybrid warfare},
journal = {Procedia Computer Science},
volume = {153},
pages = {210-217},
year = {2019},
note = {17th Annual Conference on Systems Engineering Research (CSER)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.05.072},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919307318},
author = {Molly Nadolski and James Fairbanks},
keywords = {Multi-level modelling, Sociotechnical systems, Complex Systems, Toolsets, Unstructured Spaces, Conceptual Modeling, Quantitative Modeling},
abstract = {Being empowered with the appropriate toolset will enable decision-makers to analyze how best to intervene in ever-changing complex systems. This research project explored deconstructing qualitative methods to identify and document requirements to connect the models to computational social science approaches. Previous efforts from our research provided a customizable toolset that assesses the current and future impact that decisions, policies, or strategies can deliver in a system to tackle particularly complex problems. This paper presents a portion of the research effort that developed a threat analysis framework by establishing formally documented research methods to effectively combine conceptual and computational tools. This enables more accurate, efficient, and foresightful knowledge capture and depictions of a particular problem space. The case that the tools and approach are tested against is Russia’s application of grey zone warfare tools in Moldova.}
}
@article{JACKSON2012370,
title = {Information technology use and creativity: Findings from the Children and Technology Project},
journal = {Computers in Human Behavior},
volume = {28},
number = {2},
pages = {370-376},
year = {2012},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2011.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0747563211002147},
author = {Linda A. Jackson and Edward A. Witt and Alexander Ivan Games and Hiram E. Fitzgerald and Alexander {von Eye} and Yong Zhao},
keywords = {Videogames, Creativity, Children, Technology use},
abstract = {This research examined relationships between children’s information technology (IT) use and their creativity. Four types of information technology were considered: computer use, Internet use, videogame playing and cell phone use. A multidimensional measure of creativity was developed based on Sternberg and Lubart, 1999, Subrahmanyam et al., 2006 test of creative thinking. Participants were 491 12-year olds; 53% were female, 34% were African American and 66% were Caucasian American. Results indicated that videogame playing predicted of all measures of creativity. Regardless of gender or race, greater videogame playing was associated with greater creativity. Type of videogame (e.g., violent, interpersonal) was unrelated to videogame effects on creativity. Gender but not race differences were obtained in the amount and type of videogame playing, but not in creativity. Implications of the findings for future research to test the causal relationship between videogame playing and creativity and to identify mediator and moderator variables are discussed.}
}
@article{NIKIFORIDOU2010795,
title = {Statistical literacy at university level: the current trends},
journal = {Procedia - Social and Behavioral Sciences},
volume = {9},
pages = {795-799},
year = {2010},
note = {World Conference on Learning, Teaching and Administration Papers},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2010.12.236},
url = {https://www.sciencedirect.com/science/article/pii/S1877042810023414},
author = {Zoi Nikiforidou and Aspasia Lekka and Jenny Pange},
keywords = {statistical literacy, Statistics Education},
abstract = {Active and critical citizens, in contemporary information-driven societies, are considered to possess capacities and skills of statistical literacy. There are numerous definitions and descriptions concerning statistical literacy, statistical reasoning and statistical thinking. Thus, all these terms converge to the principle that statistical citizenship develops from school settings and relates mainly to the processes of evaluating, interpreting and communicating data. If these are not acquired on time, then students build up errors and misunderstandings. In the current paper general issues concerning Statistics Education at the University level are addressed and aspects for future research are stressed in terms of technology use, content and pedagogic approaches.}
}
@article{LIU2024101910,
title = {Understanding ceiling temperature as a predictive design parameter for circular polymers},
journal = {Cell Reports Physical Science},
volume = {5},
number = {4},
pages = {101910},
year = {2024},
issn = {2666-3864},
doi = {https://doi.org/10.1016/j.xcrp.2024.101910},
url = {https://www.sciencedirect.com/science/article/pii/S2666386424001462},
author = {Xiaoyang Liu and Shivani Kozarekar and Alexander Shaw and Tie-Qi Xu and Eugene Y.-X. Chen and Linda J. Broadbelt},
keywords = {circular polymers, ceiling temperature, thermodynamic parameters, uncertainty propagation, density functional theory},
abstract = {Summary
The rise of polymeric materials marks a notable achievement of the past century, yet challenges in recycling have led to their accumulation in various environments. Efforts to address this include advancements in mechanical recycling, degradation processes, and chemical recycling techniques, particularly chemical recycling to monomer, which offers a path toward a circular economy for plastics. In this perspective, we discuss how ceiling temperature (Tc) can be used as a design parameter for circular (closed-loop recyclable) polymers and provide an overview of typical experimental approaches for deriving Tc, focusing on ΔHp and ΔSp as the key parameters for prediction. The concept of Tc is heavily embedded in the polymer literature and provides a simple but still useful way of quickly ranking different polymers in terms of their relative thermodynamic stability of polymer versus monomer states. While Tc in the bulk state as an intrinsic value is a desirable quantity, it is infeasible in many cases to measure equilibrium states in the bulk; thus, many researchers have focused on investigating Tc in solution, where there may be dependencies of Tc on the solvent, concentration, or other factors, resulting in a family of apparent Tc values at each set of conditions. We thus explore computational studies as a complement to experimental measurements of Tc. To this end, we focus here on the advantages, obstacles, and outlook of the establishment of predictive computational approaches to calculate key thermodynamic parameters related to polymer circularity, namely ΔHp, ΔSp, ΔGp, and Tc values.}
}
@article{GUO2017677,
title = {Research on Element Importance of Shafting Installation Based on QFD and FMEA},
journal = {Procedia Engineering},
volume = {174},
pages = {677-685},
year = {2017},
note = {13th Global Congress on Manufacturing and Management Zhengzhou, China 28-30 November, 2016},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2017.01.205},
url = {https://www.sciencedirect.com/science/article/pii/S1877705817302059},
author = {Qi Guo and Kuangjie Sheng and Zheng Wang and Xilin Zhang and hengyi Yang and Rui Miao},
keywords = {Quality Function Deployment, Failure Mode and Effects Analysis, Marine Shafting, Comprehensive Importance},
abstract = {Development in today's shipbuilding economy is transforming from the quantitative growth to the quality growth. Quality function deployment (QFD) and failure mode and effects analysis (FMEA) adopt different ways of thinking, they remedy their respective limitations for each other, that can effectively guide the quality control. This paper is combined of HuDong ZhongHua Shipbuilding (group) co. LTD.’s shafting installation process, starting from the QFD customer requirements for finding the importance of production process elements and correction by FMEA, ultimately acquire the comprehensive importance of shafting installation process elements.}
}
@article{SZYJEWSKI20203476,
title = {Future management},
journal = {Procedia Computer Science},
volume = {176},
pages = {3476-3485},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.049},
url = {https://www.sciencedirect.com/science/article/pii/S187705092031944X},
author = {Zdzisław Szyjewski},
keywords = {Future management, forecasting the future, new technologies},
abstract = {Accurate forecasting, good identification of trends is the basis of business success. Strategic management methods and techniques that use experience and historical economic data are not adequate to the rapidly changing business environment. In particular, technological changes, and in particular the widespread use of ICT, forces a new approach to management style and changes in the way data is acquired on the basis of which future decisions are made. Innovation thinking, a flexible and dynamic approach to making future-oriented decisions using new technologies are the foundations of future management. Therefore, the aim of the paper is to show the role and position of technology in creating the future.}
}
@incollection{RIVELA202293,
title = {Chapter 6 - Life Cycle Sustainability Assessment-based tools},
editor = {Carmen Teodosiu and Silvia Fiore and Almudena Hospido},
booktitle = {Assessing Progress Towards Sustainability},
publisher = {Elsevier},
pages = {93-118},
year = {2022},
isbn = {978-0-323-85851-9},
doi = {https://doi.org/10.1016/B978-0-323-85851-9.00018-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323858519000183},
author = {Beatriz Rivela and Brandon Kuczenski and Dolores Sucozhañay},
keywords = {Life Cycle Thinking, Life Cycle Assessment, Life Cycle Costing, Social Life Cycle Assessment, Life Cycle Sustainability Assessment},
abstract = {This chapter establishes a baseline of ideas of what Life Cycle Thinking means: going beyond the traditional focus, understanding and including the whole environmental, social, and economic implications of decision-making processes to identify potential conflicts, synergies, and trade-offs. The life cycle methodologies for sustainability assessment are described, providing an overview of the tools and criteria currently applied, available software and databases, and ongoing challenges. While Environmental Life Cycle Assessment (LCA) is a consolidated tool, based on the ISO standards, Life Cycle Costing (LCC), the tool aimed at the assessment of the economic domain using a life cycle perspective, has not been widely integrated into sustainability assessment until the last decade. Concerning the social dimension, Social Life Cycle Assessment (S-LCA) is still at an early stage of development, but it is a promising methodology to face the challenge of integrating the social aspects towards a holistic approach to sustainable development.}
}
@article{GRAF2021100836,
title = {A cycle for validating a learning progression illustrated with an example from the concept of function},
journal = {The Journal of Mathematical Behavior},
volume = {62},
pages = {100836},
year = {2021},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2020.100836},
url = {https://www.sciencedirect.com/science/article/pii/S0732312320301000},
author = {Edith Aurora Graf and Peter W. {van Rijn} and Cheryl L. Eames},
keywords = {Learning progressions, Learning trajectories, Validation, Empirical recovery, Mathematics assessment},
abstract = {A learning progression, or learning trajectory, describes the evolution of student thinking from early conceptions to the target understanding within a particular domain. As a complex theory of development, it requires conceptual and empirical support. In earlier work, we proposed a cycle for the validation of a learning progression with four steps: 1) Theory Development, 2) Examination of Empirical Recovery, 3) Comparison to Competing Models, and 4) Evaluation of Instructional Efficacy. A group of experts met to discuss the application of learning sciences to the design, use, and validation of classroom assessment. Learning progressions, learning trajectories, and how they can support classroom assessment were the main focuses. Revisions to the cycle were suggested. We describe the adapted cycle and illustrate how the first third of it has been applied towards the validation of a learning progression for the concept of function.}
}
@article{AGARWAL1992251,
title = {Computational fluid dynamics on parallel processors},
journal = {Computing Systems in Engineering},
volume = {3},
number = {1},
pages = {251-259},
year = {1992},
note = {High-Performance Computing for Flight Vehicles},
issn = {0956-0521},
doi = {https://doi.org/10.1016/0956-0521(92)90110-5},
url = {https://www.sciencedirect.com/science/article/pii/0956052192901105},
author = {R.K. Agarwal and J.C. Lewis},
abstract = {Greater computational power is needed for solving computational fluid dynamics problems of interest in engineering design. Parallel computers offer the promise of providing orders of magnitude increases in computational power compared with current uniprocessor vector supercomputers. This paper is mainly concerned with the implementation of a three-dimensional Navier-Stokes code MDNS3D on concurrent computers with grain sizes ranging from fine to coarse. An overview of commercially available parallel machines and the current state of the art in parallel algorithms is presented. The implementation of MDNS3D on machines such as the CRAY Y-MP/8, IBM 3090S, BBN Butterfly II, Intel iPSC/2, Symult 2010, MASPAR, and the Connection Machine CM-2, is described. Particular attention is paid to differences in implementation on SIMD and MIMD architectures. Factors affecting the performance of the code on different architectures are addressed. In addition, user interface and software portability issues are considered for various machines. Finally, future trends in parallel hardware and software development are assessed, and the factors important in determining the most suitable architecture for performing very large scale calculations are discussed.}
}
@article{TRAENKLE1994197,
title = {Solving microstructure electrostatistics with MIMD parallel supercomputers and Split-C},
journal = {Journal of Non-Newtonian Fluid Mechanics},
volume = {53},
pages = {197-213},
year = {1994},
issn = {0377-0257},
doi = {https://doi.org/10.1016/0377-0257(94)85049-6},
url = {https://www.sciencedirect.com/science/article/pii/0377025794850496},
author = {Frank Traenkle and Matthew I. Frank and Mary K. Vernon and Sangtae Kirn},
keywords = {Microstructure electrostatics, Multiple Instruction Multiple Data (MIMD) model, Parallel computational algorithms, Split-C},
abstract = {We consider parallel computational algorithms for the boundary integral solutions of the Laplace equation for use in the simulation of electrorheological fluids and as a model study of a class of elliptic partial differential equations that appear in basic microscopic descriptions of heterogeneous structured continua. The viewpoint is that of constructing large scale simulations that bridge micro- and macro-length and time scales on state-of-the-art parallel supercomputers. Because of long range interactions, fast communications are the key to scalable N-Body algorithms. The communication scheduling strategies of Fuentes and Kim are examined in two contexts on the Thinking Machines CM-5 parallel computer. First, an N-Body simulation implementation in C using the standard send-receive message passing primitives in the CMMD 2.0 library shows that communication scheduling leads to dramatic improvements in computational performance. Second, an implementation in Split-C, which uses highly efficient activemessages to implement shared memory communication, reduces communication overhead by an order of magnitude. Taken together, these two developments portend great promise for the development of efficient large scale simulations using portable, high level parallel programming languages.}
}
@article{SCHUELLER1997197,
title = {A state-of-the-art report on computational stochastic mechanics},
journal = {Probabilistic Engineering Mechanics},
volume = {12},
number = {4},
pages = {197-321},
year = {1997},
note = {A State-of-the-Art Report on Computational Stochastic Mechanics},
issn = {0266-8920},
doi = {https://doi.org/10.1016/S0266-8920(97)00003-9},
url = {https://www.sciencedirect.com/science/article/pii/S0266892097000039},
author = {G.I. Schuëller}
}
@article{GROUT2014680,
title = {Taking Computer Science and Programming into Schools: The Glyndŵr/BCS Turing Project},
journal = {Procedia - Social and Behavioral Sciences},
volume = {141},
pages = {680-685},
year = {2014},
note = {4th World Conference on Learning Teaching and Educational Leadership (WCLTA-2013)},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2014.05.119},
url = {https://www.sciencedirect.com/science/article/pii/S1877042814035435},
author = {Vic Grout and Nigel Houlden},
keywords = {Computer science, programming, computing curriculum, teacher training, British Computer Society (BCS) Academy, Computing At School (CAS), Council of Professors and Heads of Computing (CPHC), Lego NXT Mindstorm, Raspberry Pi, Robot C, Scratch, Picoboards ;},
abstract = {2012 and 2013 have been challenging years for Computer Science (CS) education in the UK. After decades of national neglect, there has been a sudden impetus to reintroduce CS into the 11-16 age school curriculums. Immediate obstacles include a generation of children with no CS background and an estimated need for 20,000 new CS teachers - existing UK IT teachers being insufficiently qualified and experienced. The Computing at School (CAS) movement has been instrumental in this quantum transition from an IT to Computing syllabus, as have the British Computer Society (BCS), leading UK universities and a number of major international technology companies, including Microsoft, Google, IBM, British Telecom and Facebook.This paper discusses the background to this position and the progress being made to address these challenges. It describes, in particular, the work of the BCS-funded Glyndwr University ‘Turing Project’ in introducing Welsh high-school students and staff to high-level programming and ‘computational thinking’. The Turing Project uses an innovative combination of Lego NXT Mindstorm robots, Raspberry Pi computers and PicoBoard hardware together with the Robot C and Scratch programming platforms. The paper discusses initial objectives and the general approach, describes focused delivery across different age groups and ability ranges and presents results and analysis demonstrating the effectiveness of the programme. Lessons learnt and future directions are considered in conclusion.}
}
@incollection{LI2014249,
title = {Chapter 8 - Image Processing at Your Fingertips: The New Horizon of Mobile Imaging},
editor = {Joel Trussell and Anuj Srivastava and Amit K. Roy-Chowdhury and Ankur Srivastava and Patrick A. Naylor and Rama Chellappa and Sergios Theodoridis},
series = {Academic Press Library in Signal Processing},
publisher = {Elsevier},
volume = {4},
pages = {249-264},
year = {2014},
booktitle = {Academic Press Library in Signal Processing: Volume 4},
issn = {2351-9819},
doi = {https://doi.org/10.1016/B978-0-12-396501-1.00008-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012396501100008X},
author = {Xin Li},
keywords = {Mobile imaging, Mobile computing, Interactive image processing, Human network interaction},
abstract = {In this chapter, we briefly review the history of mobile imaging and current trend of mobile computing—namely interacting with a computer without an interface. Specifically, we highlight a list of image processing problems at fingertips: intelligent image acquisition, interactive image matting, dynamic image mosaicing and supervised image restoration. The unifying theme is how human interaction can reshape our thinking of conventional image processing algorithms. Several promising applications related to fingertip image processing are discussed at the end.}
}
@article{DUENASDIEZ2019514,
title = {How Chemistry Computes: Language Recognition by Non-Biochemical Chemical Automata. From Finite Automata to Turing Machines},
journal = {iScience},
volume = {19},
pages = {514-526},
year = {2019},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2019.08.007},
url = {https://www.sciencedirect.com/science/article/pii/S2589004219302858},
author = {Marta Dueñas-Díez and Juan Pérez-Mercader},
keywords = {Chemistry, Chemical Reaction, Computer Science, Theory of Computation},
abstract = {Summary
Every problem in computing can be cast as decision problems of whether strings are in a language or not. Computations and language recognition are carried out by three classes of automata, the most complex of which is the Turing machine. Living systems compute using biochemistry; in the artificial, computation today is mostly electronic. Thinking of chemical reactions as molecular recognition machines, and without using biochemistry, we realize one automaton in each class by means of one-pot, table top chemical reactors: from the simplest, Finite automata, to the most complex, Turing machines. Language acceptance/rejection criteria by automata can be formulated using energy considerations. Our Turing machine uses the Belousov-Zhabotinsky chemical reaction and checks the same symbol in an Avogadro′s number of processors. Our findings have implications for chemical and general computing, artificial intelligence, bioengineering, the study of the origin and presence of life on other planets, and for artificial biology.}
}
@article{DORESWAMY2023,
title = {Attributes That Influence Human Decision-Making in Complex Health Services: Scoping Review},
journal = {JMIR Human Factors},
volume = {10},
year = {2023},
issn = {2292-9495},
doi = {https://doi.org/10.2196/46490},
url = {https://www.sciencedirect.com/science/article/pii/S2292949523001153},
author = {Nandini Doreswamy and Louise Horstmanshof},
keywords = {human attributes, human decision-making, rationality, rational decision-making, health policy, health regulation, health services, },
abstract = {Background
Humans currently dominate decision-making in both clinical health services and complex health services such as health policy and health regulation. Many assumptions inherent in health service models today are underpinned by Ramsey’s Expected Utility Theory, a prominent theory in the field of economics that is rooted in rationality. Rational, evidence-based metrics currently dominate the culture of decision-making in health policy and regulation. However, as the COVID-19 pandemic has shown, rational metrics alone may not suffice in making better policy and regulatory decisions. There are ethical and moral considerations and other complex factors that cannot be reduced to evidence-based rationality alone. Therefore, this scoping review was undertaken to identify and map the attributes that influence human decision-making in complex health services.
Objective
The objective is to identify and map the attributes that influence human decision-making in complex health services that have been reported in the peer-reviewed literature.
Methods
This scoping review was designed to answer the following research question: what attributes have been reported in the literature that influence human decision-making in complex health services? A clear, reproducible methodology is provided. It is reported in accordance with the PRISMA-ScR (Preferred Reporting Items for Systematic Reviews and Meta-Analyses Extension for Scoping Reviews) standards and a recognized framework. As the topic of interest merited broad review to scope and understand literature from a holistic viewpoint, a scoping review of literature was appropriate here. Inclusion and exclusion criteria were developed, and a database search undertaken within 4 search systems—ProQuest, Scopus, PubMed, and Web of Science.
Results
The results span 46 years, from 1976 to 2022. A total of 167 papers were identified. After removing duplicates, 81 papers remained. Of these, 77 papers were excluded based on the inclusion and exclusion criteria. The remaining 4 papers were found to be relevant. Citation tracking was undertaken, identifying 4 more relevant papers. Thus, a total of 8 papers were included. These papers were reviewed in detail to identify the human attributes mentioned and count the frequency of mentions. A thematic analysis was conducted to identify the themes.
Conclusions
The results highlight key themes that underline the complex and nuanced nature of human decision-making. The results suggest that rationality is entrenched and may influence the lexicon of our thinking about decision-making. The results also highlight the counter narrative of decision-making underpinned by uniquely human attributes. This may have ramifications for decision-making in complex health services today. The review itself takes a rational approach, and the methods used were suited to this.
International Registered Report Identifier (IRRID)
RR2-10.2196/42353}
}
@article{CORTI19942717,
title = {A computational study of metastability in vapor—liquid equilibrium},
journal = {Chemical Engineering Science},
volume = {49},
number = {17},
pages = {2717-2734},
year = {1994},
issn = {0009-2509},
doi = {https://doi.org/10.1016/0009-2509(94)E0093-6},
url = {https://www.sciencedirect.com/science/article/pii/0009250994E00936},
author = {David S. Corti and Pablo G. Debenedetti},
abstract = {Computer simulations are ideally suited to study systems under arbitrary constraints; hence they are useful for the investigation of metastability. Different types of constraints were applied to the three-dimensional Lennard—Jones fluid in the vapor—liquid coexistence region. Constraining the magnitude of allowed density fluctuations (restricted ensemble) has little effect on the equation of state and on phase equilibrium predictions for reduced temperatures lower than 0.95. Thermodynamic integrations along constrained and unstable paths are in good agreement with chemical potential calculations, indicating that imposing the density constraint does not violate microscopic reversibility. Restricted ensemble calculations were also used to calculate the width of the transition region where the mechanism of phase separation in the superheated liquid changes from nucleation to spinodal decomposition. The width of this region decreases as the temperature is reduced away from criticality. Free energy barriers to isotropic compression were used to determine the width of the transition region from nucleation to spinodal decomposition in the supercooled vapor. This transition region also becomes narrower as the distance from the critical point increases. The pressure of the deeply superheated liquid was found to be sensitive to the maximum size of voids that are allowed to form.}
}
@article{GUPTA20062290,
title = {Towards a new paradigm for innovative training methods for capacity building in remote sensing},
journal = {Advances in Space Research},
volume = {38},
number = {10},
pages = {2290-2298},
year = {2006},
note = {Remote Sensing of Oceanographic Processes and Land Surfaces; Space Science Education and Outreach},
issn = {0273-1177},
doi = {https://doi.org/10.1016/j.asr.2006.06.017},
url = {https://www.sciencedirect.com/science/article/pii/S0273117706004285},
author = {R.K. Gupta and P.M. Bala Manikavelu and D. Vijayan and T.S. Prasad},
keywords = {Thinking curricula, Innovative training methods, Capacity building, Remote sensing},
abstract = {Everybody uses a bulb to illustrate an idea but nobody shows where the current comes from. Majority of remote sensing user community comes from natural and social sciences domain while remote sensing technology evolves from physical and engineering sciences. To ensure inculcation and internalization of remote sensing technology by application/resource scientists, trainer needs to transfer physical and engineering concepts in geometric manner. Here, the steering for the transfer of knowledge (facts, procedures, concepts and principles) and skills (thinking, acting, reacting and interacting) needs to take the trainees from Known to Unknown, Concrete to Abstract, Observation to Theory and Simple to Complex. In the initial stage of training/education, experiential learning by instructor led exploring of thematic details in false colour composite (FCC) as well as in individual black and white spectral band(s) imagery by trainees not only creates interest, confidence build-up and orientation towards purposeful learning but also helps them to overcome their inhibitions towards the physical and engineering basal. The methodology to be adopted has to inculcate productive learning, emphasizing more on thinking and trial and error aspects as opposed to reproductive learning based dominantly on being told and imitation. The delivery by trainer needs to ensure dynamic, stimulating and effective discussions through deluging questions pertaining to analysis, synthesis and evaluation nature. This would ensure proactive participation from trainees. Hands-on module leads to creative concretization of concepts. To keep the trainees inspired to learn in an auto mode during post-training period, they need to consciously swim in the current and emerging knowledge pool during training programme. This is achieved through assignment of seminar delivery task to the trainees. During the delivery of seminar, peers and co-trainees drive the trainee to communicate the seminar content not only in what but also in how and why mode. The interest culminated in this manner keeps the entropy of the trainee minimized even during post-training professional life. So, such germinated trainee would always generate positive induction among colleagues; thus, helping in realizing multiplier effect. Based upon above thought process(es), the paper discusses the concept of “thinking curricula” and associated cares needed in training deliveries.}
}
@article{PUDANE2017517,
title = {Human Emotional Behavior Simulation in Intelligent Agents: Processes and Architecture},
journal = {Procedia Computer Science},
volume = {104},
pages = {517-524},
year = {2017},
note = {ICTE 2016, Riga Technical University, Latvia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.01.167},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917301680},
author = {Mara Pudane and Egons Lavendelis and Michael A. Radin},
keywords = {Affective agents, Emotive agents, Human behavior simulation, Agent internal architecture},
abstract = {The paper describes and discusses processes needed for human emotional behaviour simulation, in particular, emotion incorporation into rational thinking, as well as presents corresponding agent architecture. Such system would enable various application fields, perhaps one of the most important being enhancing smart devices with emotions. Decreasing frequency of social contact has become an urgent issue, particularly among young people. Emotional and social intelligence are however highly desired set of skills which is impossible to develop without interacting with others. Although this problem has been acknowledged, and there are some efforts to facilitate social contact, e.g., by augmented virtual reality games, that is still not enough. There is a need to develop environment that would allow learning exactly social and emotional skills. This on-going research aims at developing intelligent agents that are able to express and incorporate affects into rational processes.}
}
@article{MIYAMOTO2022231473,
title = {Data-driven optimization of 3D battery design},
journal = {Journal of Power Sources},
volume = {536},
pages = {231473},
year = {2022},
issn = {0378-7753},
doi = {https://doi.org/10.1016/j.jpowsour.2022.231473},
url = {https://www.sciencedirect.com/science/article/pii/S0378775322004803},
author = {Kaito Miyamoto and Scott R. Broderick and Krishna Rajan},
keywords = {Lithium-ion batteries, 3D miniature batteries, Optimization of 3D battery architecture, Machine learning, Multiobjective optimization},
abstract = {To power microelectronics for the internet-of-things applications, high-performance miniature batteries, called microbatteries, are critically important. Given their limited size, the three-dimensional design of microbatteries is key to maximizing their performance. Therefore, a computational strategy to identify the target battery architecture has major implications for performance improvement. In this paper, we propose a data-driven 3D battery optimization system at the full cell level that combines an automatic geometry generator based on Monte Carlo Tree Search and highly accurate machine-learning-based performance simulators. The performance of the proposed method is demonstrated by designing high-performance 3D batteries with more than 5.5 times efficiency compared with the approach based on a randomized algorithm. One of the designed geometries displayed greater power and energy densities due to more than 10% reduced internal resistance than the reported state-of-the-art geometry at the current density of higher than 15.8 mA/cm2. The results demonstrate the effectiveness of the method.}
}
@incollection{JONATHANCOHEN1986597,
title = {Semantics and the Computational Metaphor},
editor = {Ruth {Barcan Marcus} and Georg J.W. Dorn and Paul Weingartner},
series = {Studies in Logic and the Foundations of Mathematics},
publisher = {Elsevier},
volume = {114},
pages = {597-621},
year = {1986},
booktitle = {Logic, Methodology and Philosophy of Science VII},
issn = {0049-237X},
doi = {https://doi.org/10.1016/S0049-237X(09)70715-3},
url = {https://www.sciencedirect.com/science/article/pii/S0049237X09707153},
author = {L. {Jonathan Cohen}},
abstract = {Publisher Summary
The computational hypothesis, irrespective of any particular experimental outcomes, has been claimed to carry some rather specific implications for the semantics of natural language. The purpose of the chapter is to criticize that claim. The chapter is occupied solely with the implications of the computational hypothesis and not at all with its merits as a strategy for scientific research. The computational hypothesis, which has come to dominate cognitive psychology, in the sense that it expects the computational analogy to be more successful than any other in generating a variety of theories that are not only testable, but also worthwhile testing, about how particular mental processes operate. Memory, visual imagery, concept formation, problem solving, speech comprehension, etc., are treated as fields of research in which experiments may be used to test theories that such-and-such a combination of iteration, recursion, chunking, horizontal searching, vertical searching, geometrical coding, linguistic coding or other mode of information-processing is at work. Commonly the researcher first constructs or sketches or surveys a suitably wide range of computer-programs (implementable on a suitably wide range of computer-architectures) that, when compared with the mental process in question, would provide analogous outputs for analogous inputs. He then devises experiments on the performance of human subjects to determine, which of these computer-simulations is closest to the kind of explanatory mechanism required by the results of the experiments.}
}
@article{TRAYVICK2024116109,
title = {Speech and language patterns in autism: Towards natural language processing as a research and clinical tool},
journal = {Psychiatry Research},
volume = {340},
pages = {116109},
year = {2024},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2024.116109},
url = {https://www.sciencedirect.com/science/article/pii/S0165178124003949},
author = {Jadyn Trayvick and Sarah B. Barkley and Alessia McGowan and Agrima Srivastava and Arabella W. Peters and Guillermo A. Cecchi and Jennifer H. Foss-Feig and Cheryl M. Corcoran},
keywords = {Autism, Speech, Language, Natural language processing, Automated speech analysis, Acoustics, Computational phenotyping},
abstract = {Speech and language differences have long been described as important characteristics of autism spectrum disorder (ASD). Linguistic abnormalities range from prosodic differences in pitch, intensity, and rate of speech, to language idiosyncrasies and difficulties with pragmatics and reciprocal conversation. Heterogeneity of findings and a reliance on qualitative, subjective ratings, however, limit a full understanding of linguistic phenotypes in autism. This review summarizes evidence of both speech and language differences in ASD. We also describe recent advances in linguistic research, aided by automated methods and software like natural language processing (NLP) and speech analytic software. Such approaches allow for objective, quantitative measurement of speech and language patterns that may be more tractable and unbiased. Future research integrating both speech and language features and capturing “natural language” samples may yield a more comprehensive understanding of language differences in autism, offering potential implications for diagnosis, intervention, and research.}
}
@article{SOUSA2015113,
title = {Symmetry-based generative design and fabrication: A teaching experiment},
journal = {Automation in Construction},
volume = {51},
pages = {113-123},
year = {2015},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2014.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0926580514002283},
author = {José Pedro Sousa and João Pedro Xavier},
keywords = {Architecture, Geometry, Symmetry, Computational design, Digital fabrication, Design education},
abstract = {Throughout history, symmetry has been widely explored as a geometric strategy to conceive architectural forms and spaces. Nonetheless, its concept has changed and expanded overtime. Nowadays, it is understood as an ordering principle resulting from the application of isometric transformations that keep the original object invariant. Departing from this notion, scientists, philosophers and designers have extended it to embrace other geometric scenarios. Following this idea, exploring symmetry does not mean the generation of simple and predictable design solutions. On the contrary, it is a creative window to achieve geometric complexity based on very simple rules. In this context, this paper aims at discussing the relevance of exploring symmetry in architectural design today by means of digital technologies. It argues that the coupled use of computational design and digital fabrication processes allows designers to explore and materialize a higher level of design complexity in a structured and controlled way, especially when non-isometric transformations are involved. As the background for testing and illustrating its arguments, this paper describes a teaching experiment conducted in the Constructive Geometry course at the FAUP, following design-to-fabrication methodologies.}
}
@article{MCGOWEN2010169,
title = {Metaphor or Met-Before? The effects of previouos experience on practice and theory of learning mathematics},
journal = {The Journal of Mathematical Behavior},
volume = {29},
number = {3},
pages = {169-179},
year = {2010},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2010.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S0732312310000404},
author = {Mercedes A. McGowen and David O. Tall},
keywords = {Metaphor, Met-before, Epistemological obstacle, Embodiment, Local straightness},
abstract = {While the general notion of ‘metaphor’ may offer a thoughtful analysis of the nature of mathematical thinking, this paper suggests that it is even more important to take into account the particular mental structures available to the individual that have been built from experience that the individual has ‘met-before.’ The notion of ‘met-before’ offers not only a principle to analyse the changing meanings in mathematics and the difficulties faced by the learner—which we illustrate by the problematic case of the minus sign—it can also be used to analyse the met-befores of mathematicians, mathematics educators and those who develop theories of learning to reveal implicit assumptions that support our thinking in some ways and act as impediments in others.}
}
@article{SUN20112118,
title = {How digital scaffolds in games direct problem-solving behaviors},
journal = {Computers & Education},
volume = {57},
number = {3},
pages = {2118-2125},
year = {2011},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2011.05.022},
url = {https://www.sciencedirect.com/science/article/pii/S036013151100128X},
author = {Chuen-Tsai Sun and Dai-Yi Wang and Hui-Ling Chan},
keywords = {Human–computer interface, Interactive learning environments, Secondary education, Teaching/learning strategies},
abstract = {Digital systems offer computational power and instant feedback. Game designers are using these features to create scaffolding tools to reduce player frustration. However, researchers are finding some unexpected effects of scaffolding on strategy development and problem-solving behaviors. We used a digital Sudoku game named Professor Sudoku to classify built-in critical features, frustration control and demonstration scaffolds, and to investigate their effects on player/learner behaviors. Our data indicate that scaffolding support increased the level at which puzzles could be solved, and decreased frustration resulting from excessive numbers of retries. However, it also reduced the number of unassisted placements (i.e., independently filled cells), and increased reliance on scaffolding tools, both of which are considered disadvantageous for learning. Among the three scaffold types, frustration control reduced the potential for players to feel stuck at certain levels, but also reduced the frequency of use of critical feature-making tools, which are thought to have greater heuristic value. We conclude that the simultaneous provision of critical feature and frustration control scaffolds may increase player reliance on available support, thereby reducing learning opportunities. Providing players with critical features and demonstration scaffolds at the same time increases reliance on available support for some players, but for most it encourages the development of solving strategies.}
}
@incollection{RUNCO200771,
title = {Chapter 3 - Biological Perspectives on Creativity},
editor = {Mark A. Runco},
booktitle = {Creativity},
publisher = {Academic Press},
address = {Burlington},
pages = {71-113},
year = {2007},
isbn = {978-0-12-602400-5},
doi = {https://doi.org/10.1016/B978-012602400-5/50003-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780126024005500034},
author = {Mark A. Runco},
abstract = {Publisher Summary
This chapter discusses various aspects of biological perspectives on creativity. Some of the research on creativity as of late involves the brain and biological correlates of originality, novelty, and insight. Handedness is sometimes used as an indication of hemispheric dominance or hemisphericity, with right-handed people being compared to left-handed people. There are several reports of left-handed persons outnumbering the right-handed in creative and eminent samples. Hemisphericity and other important brain structures and processes contributing to creative thinking and behavior have been studied with EEG, PET, cerebral blood flow, and MRI techniques. Numerous EEG studies suggest that there are particular brain-wave patterns and brain structures that are associated with creative problem solving, or at least specific phases within the problem solving process. EEGs suggest a complex kind of activity while individuals work on divergent thinking tasks. The complexity disappears when those same individuals work on convergent thinking tasks. It is found that the role of the prefrontal cortex in creative thinking and behavior comes from several sources and uses different methodologies.}
}
@article{PENG202484,
title = {Multi-perspective thought navigation for source-free entity linking},
journal = {Pattern Recognition Letters},
volume = {178},
pages = {84-90},
year = {2024},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2023.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S0167865523003677},
author = {Bohua Peng and Wei He and Bin Chen and Aline Villavicencio and Chengfu Wu},
keywords = {Information retrieval, Question generation, Entity linking, Chain-of-thought reasoning},
abstract = {Neural entity-linking models excel at bridging the lexical gap of multiple facets of facts, such as entity-related claims or evidence documents. Despite advancements in self-supervised learning and pretrained language models, challenges persist in entity linking, particularly in interpretability and transferability. Moreover, these models need many aligned documents to adapt to emerging entities, which may not be available due to data scarcity. In this work, we propose a novel Demonstrative Self-TrAining fRamework (D-STAR) that leverages multi-perspective thought navigation. D-STAR iteratively optimizes a question generator and an entity retriever by navigating thoughts on a dynamic graph reasoning across multiple perspectives for question generation. The generated question–answer pairs, along with hard negatives shared in the graph, enable adaptation with minimal computational overhead. Additionally, we introduce a new task, source-free entity linking, focusing on unsupervised transfer learning without direct access to original domain data. To demonstrate the feasibility of this task, we provide a generated question–answering dataset, FandomWiki, for novel entities. Our experiments show that D-STAR significantly improves baselines on SciFact, Zeshel, and FandomWiki.}
}
@article{LU2025121076,
title = {The neuroscientific basis of flow: Learning progress guides task engagement and cognitive control},
journal = {NeuroImage},
volume = {308},
pages = {121076},
year = {2025},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2025.121076},
url = {https://www.sciencedirect.com/science/article/pii/S1053811925000783},
author = {Hairong Lu and Dimitri {Van der Linden} and Arnold B. Bakker},
keywords = {Learning progress, Flow experience, Engagement, Cognitive control, Goal-directed behavior},
abstract = {People often strive for deep engagement in activities, a state typically associated with feelings of flow - full task absorption accompanied by a sense of control and enjoyment. The intrinsic factors driving such engagement and facilitating subjective feelings of flow remain unclear. Building on computational theories of intrinsic motivation, this study examines how learning progress predicts engagement and directs cognitive control. Results showed that task engagement, indicated by feelings of flow and low distractibility, is a function of learning progress. Electroencephalography data further revealed that learning progress is associated with enhanced proactive preparation (e.g., reduced pre-stimulus contingent negativity variance and parietal alpha desynchronization) and improved feedback processing (e.g., increased P3b amplitude and parietal alpha desynchronization). The impact of learning progress on cognitive control is observed at the task-block and goal-episode levels, but not at the trial level. This suggests that learning progress shapes cognitive control over extended periods as progress accumulates. These findings highlight the critical role of learning progress in sustaining engagement and cognitive control in goal-directed behavior.}
}
@article{KOWALSKI2020103693,
title = {Effects of attention training technique on brain function in high- and low-cognitive-attentional syndrome individuals: Regional dynamics before, during, and after a single session of ATT},
journal = {Behaviour Research and Therapy},
volume = {132},
pages = {103693},
year = {2020},
issn = {0005-7967},
doi = {https://doi.org/10.1016/j.brat.2020.103693},
url = {https://www.sciencedirect.com/science/article/pii/S0005796720301479},
author = {Joachim Kowalski and Małgorzata Wierzba and Marek Wypych and Artur Marchewka and Małgorzata Dragan},
keywords = {Attention training technique, Metacognitve therapy, Attention, fMRI, S-REF},
abstract = {Objective
Attention Training Technique (ATT) is a key therapeutic tool in metacognitive therapy. There are numerous studies on the behavioral effects of ATT, however the neural mechanisms at work in the training are yet to be uncovered. To date there have been no controlled fMRI studies of ATT.
Method
We conducted a randomized double-blind controlled study of two groups with varying levels of cognitive-attentional syndrome (CAS). Groups with high (n = 43) and low (n = 46) levels of CAS underwent a single session of ATT or a control condition (CON) in an MRI scanner. Participants underwent resting state functional MRI (rsfMRI) sessions and rumination induction sessions both pre- and post-intervention Functional connectivity analyses and inter-subject correlations analyses were computed. We also collected data on emotion and attention functioning pre- and post-intervention.
Results
We did not observe any behavioral effects of ATT. However, direct comparison between ATT and CON sessions revealed greater inter-subject correlations in almost all hubs belonging to the studied functional networks. Moreover, subjects who received ATT showed diminished connectivity in the fronto-parietal network during ruminations and diminished connectivity of the precuneus with lateral occipital cortices and the intraparietal sulcus in abstract thinking and rsfMRI, respectively. Furthermore, some of the observed effects in functional connectivity and inter-subject correlations were specific to different levels of CAS.
Conclusions
Our results may support a proposed neural mechanism for ATT: disengagement of attention from CAS-type processing in either low- or high-CAS individuals. It is also possible that some neural effects of ATT are specific to individuals with different levels of CAS.}
}
@incollection{MACHINMASTROMATTEO2025376,
title = {Literacy of the Future},
editor = {David Baker and Lucy Ellis},
booktitle = {Encyclopedia of Libraries, Librarianship, and Information Science (First Edition)},
publisher = {Academic Press},
edition = {First Edition},
address = {Oxford},
pages = {376-387},
year = {2025},
isbn = {978-0-323-95690-1},
doi = {https://doi.org/10.1016/B978-0-323-95689-5.00197-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323956895001978},
author = {Juan D. Machin-Mastromatteo},
keywords = {Adaptation, Collaboration, Critical engagement, Democratic engagement, Digital literacy, Educational integration, Ethical dimensions, Futures Literacy, Information literacy, Lifelong learning, Media literacy, Multiliteracies, Programming skills, Social participation, Technological advancements},
abstract = {This entry summarizes the development of the literacy concepts most commonly associated with LIS, namely information literacy, digital literacy, and media literacy, which frame a synthesis of the future perspectives of these and other literacies that have been proposed in the literature.11An alphabetical and non-exhaustive list could include: academic literacy, artificial intelligence or algorithmic literacy, civic literacy, context literacy, data literacy, emotional literacy, financial literacy, focus literacy, futures literacies, game literacy, graphic literacy, health literacy, literacy education, legal literacy, media literacy, multiliteracies, new literacies, new media literacies, navigation literacy, numerical literacy, participatory/participation literacy, personal literacy, psycho-literacy, scientific literacy, search engine literacy, skepticism literacy, statistical literacy, transliteracy, and visual literacy or visuacy. Note: not all of these are covered in this entry for space limitations. These future perspectives are organized in nine sections: the educational implications of literacy, information literacy, digital literacy, literacy education, multiliteracies and holistic perspectives, media literacy, futures literacy, algorithmic literacy and artificial intelligence implications, and other literacies. The purpose of this entry is to offer a brief overview and commentary on the types of literacies that we need to be aware of and competent in for the near future. As these future trends are derived from the specialized literature, they include some already occurring considerations. However, they might become more salient topics in the upcoming years, and they might entail many different implications for the future of LIS professionals, libraries, and even for education in general.}
}
@article{BAILEY20158,
title = {Metacognitive beliefs moderate the relationship between catastrophic misinterpretation and health anxiety},
journal = {Journal of Anxiety Disorders},
volume = {34},
pages = {8-14},
year = {2015},
issn = {0887-6185},
doi = {https://doi.org/10.1016/j.janxdis.2015.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0887618515000791},
author = {Robin Bailey and Adrian Wells},
keywords = {Health anxiety, Metacognition, Catastrophic misinterpretation, Moderation, S-REF model},
abstract = {Catastrophic misinterpretations of bodily symptoms have a central role in cognitive-behavioural models of health anxiety. However, the metacognitive (S-REF) model postulates that psychological disturbance is linked more to beliefs about thinking i.e., metacognition. Equally the relationship between catastrophic misinterpretation and health anxiety should be moderated by metacognition, in particular negative beliefs about the uncontrollability and danger of thinking (MCQNeg). Participants (N=351) completed measures to examine the relationship between these variables. Results indicated positive relationships between metacognition, catastrophic misinterpretation, and health anxiety. Moderation analysis showed that the effect of catastrophic misinterpretations on health anxiety was explained by the proposed interaction with metacognition. Follow-up regression analysis demonstrated the interaction term explained variance in health anxiety when controlling for other variables, and was a stronger unique predictor of health anxiety than catastrophic misinterpretation. Metacognition appears to be an important factor in the relationship between catastrophic misinterpretation and health anxiety, and would have important implications for existing models and treatment.}
}
@incollection{PAGEL201749,
title = {Chapter Four - Testing for Machine Consciousness},
editor = {J.F. Pagel and Philip Kirshtein},
booktitle = {Machine Dreaming and Consciousness},
publisher = {Academic Press},
address = {San Diego},
pages = {49-65},
year = {2017},
isbn = {978-0-12-803720-1},
doi = {https://doi.org/10.1016/B978-0-12-803720-1.00004-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128037201000049},
author = {J.F. Pagel and Philip Kirshtein},
keywords = {Thinking, intelligence, attention, intentionality, volition, self-awareness, artificial intelligence, AI, autonomous entity, Turing Test, Chinese Room Test},
abstract = {Thinking, intelligence, data integration, and attention are aspects of consciousness for which tests have been designed. A short history of the Computer Science field, a description, and an assessment of results obtained to this point for the Turing Test and Chinese Room Test are part of this chapter. Alternative definitions of artificial intelligence are presented. Applied tests for consciousness including those for intelligence, attention, intentionality, volition, and self-awareness are discussed as applied to the assessment of machine systems. Strong AI and the concept of autonomous entities are defined and addressed. The presence of dream-equivalent states is discussed as a potential marker for human-equivalent consciousness.}
}
@article{MENG201248,
title = {Extracting linguistic rules from data sets using fuzzy logic and genetic algorithms},
journal = {Neurocomputing},
volume = {78},
number = {1},
pages = {48-54},
year = {2012},
note = {Selected papers from the 8th International Symposium on Neural Networks (ISNN 2011)},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2011.05.029},
url = {https://www.sciencedirect.com/science/article/pii/S0925231211004711},
author = {Dan Meng and Zheng Pei},
keywords = {Computing with Words, Linguistic rules, Fuzzy logic, Genetic algorithms},
abstract = {Linguistic rules in natural language are useful and consistent with human way of thinking. They are very important in multi-criteria decision making due to their interpretability. In this paper, our discussions concentrate on extracting linguistic rules from data sets. In the end, we firstly analyze how to extract complex linguistic data summaries based on fuzzy logic. Then, we formalize linguistic rules based on complex linguistic data summaries, in which, the degree of confidence of linguistic rules from a data set can be explained by linguistic quantifiers and its linguistic truth from the fuzzy logical point of view. In order to obtain a linguistic rule with a higher degree of linguistic truth, a genetic algorithm is used to optimize the number and parameters of membership functions of linguistic values. Computational results show that the proposed method is an alternative method for extracting linguistic rules with linguistic truth from data sets.}
}
@article{CHANG2017160,
title = {Dynamic modeling approaches to characterize the functioning of health systems: A systematic review of the literature},
journal = {Social Science & Medicine},
volume = {194},
pages = {160-167},
year = {2017},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2017.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0277953617305300},
author = {Angela Y. Chang and Osondu Ogbuoji and Rifat Atun and Stéphane Verguet},
keywords = {Health systems, Dynamic modeling, Systems thinking, System dynamics},
abstract = {Universal Health Coverage (UHC) is one of the targets for the United Nations Sustainable Development Goal 3. The impetus for UHC has led to an increased demand for time-sensitive tools to enhance our knowledge of how health systems function and to evaluate impact of system interventions. We define the field of “health system modeling” (HSM) as an area of research where dynamic mathematical models can be designed in order to describe, predict, and quantitatively capture the functioning of health systems. HSM can be used to explore the dynamic relationships among different system components, including organizational design, financing and other resources (such as investments in resources and supply chain management systems) – what we call “inputs” – on access, coverage, and quality of care – what we call “outputs”, toward improved health system “outcomes”, namely increased levels and fairer distributions of population health and financial risk protection. We undertook a systematic review to identify the existing approaches used in HSM. We identified “systems thinking” – a conceptual and qualitative description of the critical interactions within a health system – as an important underlying precursor to HSM, and collated a critical collection of such articles. We then reviewed and categorized articles from two schools of thoughts: “system dynamics” (SD)” and “susceptible-infected-recovered-plus” (SIR+). SD emphasizes the notion of accumulations of stocks in the system, inflows and outflows, and causal feedback structure to predict intended and unintended consequences of policy interventions. The SIR + models link a typical disease transmission model with another that captures certain aspects of the system that impact the outcomes of the main model. These existing methods provide critical insights in informing the design of HSM, and provide a departure point to extend this research agenda. We highlight the opportunity to advance modeling methods to further understand the dynamics between health system inputs and outputs.}
}
@article{CAO2020118,
title = {Computational parameter identification of strongest influence on the shear resistance of reinforced concrete beams by fiber reinforcement polymer},
journal = {Structures},
volume = {27},
pages = {118-127},
year = {2020},
issn = {2352-0124},
doi = {https://doi.org/10.1016/j.istruc.2020.05.031},
url = {https://www.sciencedirect.com/science/article/pii/S2352012420302435},
author = {Yan Cao and Qingming Fan and Sadaf {Mahmoudi Azar} and Rayed Alyousef and Salim T. Yousif and Karzan Wakil and Kittisak Jermsittiparsert and Lanh {Si Ho} and Hisham Alabduljabbar and Abdulaziz Alaskar},
keywords = {FRP: reinforced concrete, Shear resistance, Selection procedure, ANFIS},
abstract = {Bars made of fiber reinforcement polymer (FRP) are in common usage for concrete reinforcing instead of steel reinforcing since steel could be affected by corrosion. The concrete beams reinforced by FRP bars have been studied mostly in longitudinal direction without shear reinforcement. The primary objective of this investigation was to design and advance an algorithm for selection procedure of the parameters influence on prediction of shear resistance of reinforced concrete beams by FRP. Six input parameters were used which represent geometric and mechanical properties of the bars as well as shear features. These parameters are: web width, tensile reinforcement depth, ratio of shear and depth, concrete compressive strength, ratio of FRP reinforcement, FRP modulus of elasticity and beam shear resistance. The searching algorithm is based on combination of artificial neural network and fuzzy logic principle or adaptive neuro fuzzy inference system (ANFIS). Based on the obtained results ratio of shear and depth has the strongest influence on the prediction of shear resistance of reinforced concrete beams by FRP. Moreover, combination of tensile reinforcement depth and ratio of shear and depth is the most influential combination of two parameters on the prediction of shear resistance of reinforced concrete beams by FRP. Finally, combination of tensile reinforcement depth, ratio of shear and depth and FRP modulus of elasticity is the most influential combination of three parameters on the prediction of shear resistance of reinforced concrete beams by FRP.}
}
@article{GAO2025102628,
title = {An organic artificial synaptic memristor for neuromorphic computing},
journal = {Applied Materials Today},
volume = {43},
pages = {102628},
year = {2025},
issn = {2352-9407},
doi = {https://doi.org/10.1016/j.apmt.2025.102628},
url = {https://www.sciencedirect.com/science/article/pii/S2352940725000472},
author = {Kaikai Gao and Bai Sun and Bo Yang and Zelin Cao and Yu Cui and Mengna Wang and Chuncai Kong and Guangdong Zhou and Sihai Luo and Xiaoliang Chen and Jinyou Shao},
keywords = {Artificial synaptic, Organic material, Neuromorphic computing, Dataset recognition, Artificial intelligence},
abstract = {Developing an artificial synaptic device that can simulate the learning and memory abilities of the human brain is a key step toward achieving neuromorphic computing. Although traditional transistors and emerging memristors are considered potential candidates for achieving these functions, their manufacturing often relies on non-renewable semiconductor materials. Here, we have successfully fabricated a flexible artificial synaptic device with a typical memristive sandwich structure (Ag/PMMA/SLE/Ti) utilizing the cost-effective organic material sodium lignosulfonate (SLE) as the dielectric layer. This device effectively achieves short-term plasticity (STP), spike-number-dependent plasticity (SNDP), and long-term potentiation/depression (LTP/LTD). Furthermore, the conductance of the designed artificial synapse corresponds to synaptic weights, which can be recognized by neuromorphic computation on CYCLE and MNIST datasets (small/large sizes) with an accuracy of 33.8 % and 89.3 %/91.0 %, respectively. Therefore, this artificial synaptic device exhibits the flexibility to serve in various wearable scenarios, including intelligent electronic skin (e-skin). Additionally, the excellent biocompatibility of SLE aligns well with the concept of green electronics.}
}
@article{LEE1993255,
title = {Interval computation as deduction in chip},
journal = {The Journal of Logic Programming},
volume = {16},
number = {3},
pages = {255-276},
year = {1993},
issn = {0743-1066},
doi = {https://doi.org/10.1016/0743-1066(93)90045-I},
url = {https://www.sciencedirect.com/science/article/pii/074310669390045I},
author = {J.H.M. Lee and M.H. {Van Emden}},
abstract = {Logic programming realizes the ideal of “computation is deduction,” but not when floating-point numbers are involved. In that respect logic programming languages are as careless as conventional computation: they ignore the fact that floating-point operations are only approximate and that it is not easy to tell how good the approximation is. It is our aim to extend the benefits of logic programming to computation involving floating-point arithmetic. Our starting points are the ideas of Cleary and the CHIP programming language. Cleary proposed a relational form of interval arithmetic that was incorporated in BNR Prolog in such a way that variables already bound can be bound again. In this way the usual logical interpretation of computation no longer holds. In this paper we develop a technique for narrowing intervals that we relate both to Cleary's work and to the constraint-satisfaction techniques of artificial intelligence. We then modify CHIP by allowing domains to be intervals of real numbers. To reduce arithmetic primitives with interval domains, we use our interval narrowing technique as an implementation of the looking-ahead inference rule. We show that the result is a system where answers are logical consequences of a declarative logic program, even when floating-point computations have been used. We believe ours is the first system with this property.}
}
@article{SHEARER2021,
title = {Foodborne Illness Outbreak Investigation for One Health Postsecondary Education},
journal = {Journal of Microbiology & Biology Education},
volume = {22},
number = {2},
year = {2021},
issn = {1935-7877},
doi = {https://doi.org/10.1128/jmbe.00129-21},
url = {https://www.sciencedirect.com/science/article/pii/S193578772100143X},
author = {Adrienne E. H. Shearer and Kalmia E. Kniel},
keywords = {food safety, investigation, One Health, education, microbiology, public health, escape room, problem-based learning, epidemiology, environment},
abstract = {One Health concepts were incorporated in a foodborne disease outbreak investigation with game features of data presented as visual and manipulative clues. Postsecondary pre-veterinary medicine and animal biosciences students and food science students (n = 319) enrolled in an introductory animal and food sciences course over a 3-year period received a brief introduction to foodborne illness, an outbreak scenario, and investigative tasks to complete individually or in groups.
ABSTRACT
One Health concepts were incorporated in a foodborne disease outbreak investigation with game features of data presented as visual and manipulative clues. Postsecondary pre-veterinary medicine and animal biosciences students and food science students (n = 319) enrolled in an introductory animal and food sciences course over a 3-year period received a brief introduction to foodborne illness, an outbreak scenario, and investigative tasks to complete individually or in groups. Tasks addressed epidemiology, laboratory, environment, traceback, recall, and prevention concepts. Gamification of the exercise involved generation of a numerical code to unlock a combination lock as an indication of successful organization, compilation, and interpretation of data. Students presented investigation findings and responses to critical thought questions on their roles. Student surveys on engagement and self-perceived change in conceptual understanding indicated that nearly all expressed increased understanding of outbreak investigations, safe food production, and environmental water as a transmission vehicle. Volunteered learned concepts indicated enhanced appreciation for the complexity of food safety and interdisciplinary connections. Students enjoyed the exercise (92%) and cited the clues and group interaction among the most enjoyable features. Objective assessment of student conceptual learning with the subset of students who conducted the investigation individually (n = 58) demonstrated significant increase in correct test responses (49% pretest; 76% posttest) after completion of the investigation for all questions combined and across all learning objectives. These data demonstrate the value of a foodborne disease investigation with escape room gamification features for engaging students in One Health concepts and exercising problem-solving, critical thinking, and skills for independent and collaborative work.}
}
@article{ADENIJI2023,
title = {Draft genome sequence of active gold mine isolate Pseudomonas iranensis strain ABS_30},
journal = {Microbiology Resource Announcements},
volume = {12},
number = {12},
year = {2023},
issn = {2576-098X},
doi = {https://doi.org/10.1128/MRA.00849-23},
url = {https://www.sciencedirect.com/science/article/pii/S2576098X23009234},
author = {Adetomiwa A. Adeniji and Ayansina S. Ayangbenro and Olubukola O. Babalola and Julie C. {Dunning Hotopp}},
keywords = {bioremediation, biosynthetic clusters, genome sequence, gold mine, , secondary metabolites},
abstract = {ABSTRACT
Pseudomonas iranensis ABS_30, isolated from gold mining soil, exhibits metal-resistant properties valuable for heavy metal removal. We report the draft genome sequencing of the P. iranensis ABS_30 strain, which is 5.9 Mb in size.}
}
@incollection{ANGELETOS2023613,
title = {Chapter 20 - Dampening general equilibrium: incomplete information and bounded rationality☆☆This chapter subsumes an older paper of ours, entitled “Dampening General Equilibrium: From Micro to Macro” (Angeletos and Lian, 2017). We have benefited from the comments of various colleagues, especially those of the editors, Rüdiger Bachmann, Wilbert van der Klaauw, and Giorgio Topa. Angeletos acknowledges the support of the National Science Foundation under Grant Number SES-1757198.},
editor = {Rüdiger Bachmann and Giorgio Topa and Wilbert {van der Klaauw}},
booktitle = {Handbook of Economic Expectations},
publisher = {Academic Press},
pages = {613-645},
year = {2023},
isbn = {978-0-12-822927-9},
doi = {https://doi.org/10.1016/B978-0-12-822927-9.00028-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128229279000288},
author = {George-Marios Angeletos and Chen Lian},
keywords = {General equilibrium, incomplete information, higher-order beliefs, Level- Thinking, reflective equilibrium, coordination, beauty contests},
abstract = {We review how realistic frictions in information and/or rationality arrest general equilibrium (GE) feedbacks. In one specification, we maintain rational expectations but remove common knowledge of aggregate shocks. In another, we replace rational expectations with Level-k Thinking or a smooth variant thereof. Two other approaches, heterogeneous priors and cognitive discounting, capture the same essence while offering a gain in tractability. Relative to the full-information rational-expectation (FIRE) benchmark, all these modifications amount to attenuation of GE effects, especially in the short run. This in turn translates to either under- or overreaction in aggregate outcomes, depending on whether GE feedbacks are positive or negative in the first place. We review a few applications, with emphasis on monetary and fiscal policy. We finally discuss how the available evidence on expectations, along with other considerations, can help guide the choice among the various alternatives, as well as between them and FIRE.}
}
@article{BLELLOCH199490,
title = {Parallel solutions to geometric problems in the scan model of computation},
journal = {Journal of Computer and System Sciences},
volume = {48},
number = {1},
pages = {90-115},
year = {1994},
issn = {0022-0000},
doi = {https://doi.org/10.1016/S0022-0000(05)80023-6},
url = {https://www.sciencedirect.com/science/article/pii/S0022000005800236},
author = {Guy E. Blelloch and James J. Little},
abstract = {This paper describes several parallel algorithms that solve geometric problems. The algorithms are based on a vector model of computation-the scan model. The purpose of this paper is both to show how the model can be used and to formulate a set of practical algorithms. The scan model is based on a small set of operations on vectors of atomic values. It differs from the P-RAM models both in that it includes a set of scan primitives, also called parallel prefix computations, and in that it is a strictly data-parallel model. A very useful abstraction in the scan model is the segment abstraction, the subdivision of a vector into a collection of independent smaller vectors. The segment abstraction permits a clean formulation of divide-and-conquer algorithms and is used heavily in the algorithms described in this paper. Within the scan model, using the operations and routines defined, the paper describes a k-D tree algorithm requiring O(lg n) calls to the primitives for n points, a closest-pair algorithm requiring O(lg n) calls to the primitives, a line-drawing algorithm requiring O(1) calls to the primitives, a line-of-sight algorithm requiring O(1) calls to the primitives, and finally, three different convex-hull algorithms. The last convex-hull algorithm, merge-hull, utilizes a generalized binary search technique using divide-and-conquer with the segment abstraction. The paper also describes how to implement the CREW version of Cole's merge sort in O(lg n) calls to the primitives. All these algorithms should be noted for their simplicity rather than their complexity; many of them are parallel versions of known serial algorithms. Most of the algorithms discussed in this paper have been implemented on the Connection Machine, a highly parallel single instruction multiple data computer.}
}
@article{BRESSANELLI2024142512,
title = {Are digital servitization-based Circular Economy business models sustainable? A systemic what-if simulation model},
journal = {Journal of Cleaner Production},
volume = {458},
pages = {142512},
year = {2024},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2024.142512},
url = {https://www.sciencedirect.com/science/article/pii/S0959652624019607},
author = {Gianmarco Bressanelli and Nicola Saccani and Marco Perona},
keywords = {Circular economy, Digital servitization, Sustainability impact assessment, Electrical and electronics equipment, Life cycle thinking, Systemic perspective},
abstract = {Manufacturing companies are struggling with the implementation of Circular Economy, especially due to the uncertainty regarding its potential sustainability benefits. In particular, and despite digital servitization is advocated by several studies as a way to achieve environmental gains, circular business models based on digital servitization are not always sustainable due to burden shifting and unexpected consequences which are difficult to assess before implementation. This is particularly relevant for the Electrical and Electronics Equipment industry, which suffers structural weaknesses such as the dependance on critical raw materials and an increasing waste generation. However, literature lacks models and tools able to address the complexity inherent in the systemic micro-macro perspective envisioned by Circular Economy, while studies that quantitatively assess the sustainability impacts and trade-offs of digital servitization-based circular scenarios are limited. This article aims to develop a better understanding of how the sustainability impacts of circular and servitized scenarios can be assessed and quantified at the economic, environmental, and social level, adopting a systemic perspective through the development of a what-if simulation model. The model is implemented in a spreadsheet tool and applied to a digital servitization-based Circular Economy scenario inspired by the case of a company offering long-lasting, high-efficient washing machines as-a-service. Results show that digital servitization can actually lead to a win-win-win situation with net positive effects to the environment, the society, and the economy. This result is based on the joint application of product design for digitalization and life extension, pay-per-use business models, and product reuse. These results are robust within a significant range of key parameters values. Practitioners and policymakers may use the model to support the evaluation of different circular and servitized scenarios before implementation.}
}
@article{WANG2025112377,
title = {Identifying Falling-from-Height Hazards in Building Information Models: From the Perspectives of Time and Location},
journal = {Journal of Building Engineering},
pages = {112377},
year = {2025},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2025.112377},
url = {https://www.sciencedirect.com/science/article/pii/S235271022500614X},
author = {Qiankun WANG and Chuxiong SHEN and Zeng GUO and Chao Tang},
keywords = {Falling-from-Height, Building Information Modeling, Voxelization, Temporal information, Spatial information},
abstract = {Falling-from-Height (FFH) is the leading hazard among the 'Fatal Five' in the architectural/engineering/construction (AEC) industry, often resulting in severe consequences. Therefore, preventing these accidents is critical at the earliest stage. However, current FFH hazard identification methods suffer from a high workload, computational intensity, and lack of comprehensiveness. We propose a voxelization-based method to identify the temporal and spatial information on FFH hazards in Building Information Modeling (BIM). This method considers the falling, accessibility, and temporal conditions. We compared this method with a traditional FFH hazard identification method. The results indicate that the proposed method can rapidly identify all potential FFH hazard information, with a recall of 100% and a precision of 81.5%. It is 30 times faster than the offset geometry method and is not affected by the curvature of model surfaces, improving the effectiveness of FFH hazard identification. The proposed method is highly suitable for preventing FFH accidents and can be implemented during the design and construction preparation stages, increasing the effectiveness of FFH accident control.}
}
@article{GINOSAR20231858,
title = {Are grid cells used for navigation? On local metrics, subjective spaces, and black holes},
journal = {Neuron},
volume = {111},
number = {12},
pages = {1858-1875},
year = {2023},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2023.03.027},
url = {https://www.sciencedirect.com/science/article/pii/S0896627323002234},
author = {Gily Ginosar and Johnatan Aljadeff and Liora Las and Dori Derdikman and Nachum Ulanovsky},
abstract = {Summary
The symmetric, lattice-like spatial pattern of grid-cell activity is thought to provide a neuronal global metric for space. This view is compatible with grid cells recorded in empty boxes but inconsistent with data from more naturalistic settings. We review evidence arguing against the global-metric notion, including the distortion and disintegration of the grid pattern in complex and three-dimensional environments. We argue that deviations from lattice symmetry are key for understanding grid-cell function. We propose three possible functions for grid cells, which treat real-world grid distortions as a feature rather than a bug. First, grid cells may constitute a local metric for proximal space rather than a global metric for all space. Second, grid cells could form a metric for subjective action-relevant space rather than physical space. Third, distortions may represent salient locations. Finally, we discuss mechanisms that can underlie these functions. These ideas may transform our thinking about grid cells.}
}
@article{BELLO2025101316,
title = {Self-control on the path toward artificial moral agency},
journal = {Cognitive Systems Research},
volume = {89},
pages = {101316},
year = {2025},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101316},
url = {https://www.sciencedirect.com/science/article/pii/S1389041724001104},
author = {Paul Bello and Will Bridewell},
keywords = {Self-control, Attention, Cognitive architecture},
abstract = {The ability of agents to commit to their plans and see them through is a core concept in the philosophy of action (Bratman, 1987, Holton, 2009) and is considered to be a defining feature of having an intention. Seeing plans through in the face of highly compelling opportunities for action that are incompatible with our current commitments requires self-control. In this review paper, we draw upon ancient and modern literature on self-control along with contemporary ideas about the cognitive architecture supporting intentional action to argue that any computational account of moral agency must include an approach to self-control. In addition, we extract and develop a list of necessary features of the phenomena against which individual modeling efforts can be compared. The ARCADIA cognitive system will be discussed in light of this list of features and used to demonstrate both success and failure in a highly simplified self-control dilemma. Finally, we end by discussing a path toward more functionally complete models of agency and control, along with offering perfunctory thoughts on some of the more conceptually challenging issues to address in the future.}
}
@article{ZHANG2025100889,
title = {Identification of Critical Phosphorylation Sites Enhancing Kinase Activity With a Bimodal Fusion Framework},
journal = {Molecular & Cellular Proteomics},
volume = {24},
number = {1},
pages = {100889},
year = {2025},
issn = {1535-9476},
doi = {https://doi.org/10.1016/j.mcpro.2024.100889},
url = {https://www.sciencedirect.com/science/article/pii/S1535947624001798},
author = {Menghuan Zhang and Yizhi Zhang and Keqin Dong and Jin Lin and Xingang Cui and Yong Zhang},
keywords = {kinase activity, critical phosphorylation site, deep learning, phosphorylation mass spectrometry, embedding},
abstract = {Phosphorylation is an indispensable regulatory mechanism in cells, with specific sites on kinases that can significantly enhance their activity. Although several such critical phosphorylation sites (phos-sites) have been experimentally identified, many more remain to be explored. To date, no computational method exists to systematically identify these critical phos-sites on kinases. In this study, we introduce PhoSiteformer, a transformer-inspired foundational model designed to generate embeddings of phos-sites using phosphorylation mass spectrometry data. Recognizing the complementary insights offered by protein sequence data and phosphorylation mass spectrometry data, we developed a classification model, CSPred, which employs a bimodal fusion strategy. CSPred combines embeddings from PhoSiteformer with those from the protein language model ProtT5. Our approach successfully identified 77 critical phos-sites on 58 human kinases. Two of these sites, T517 on PKG1 and T735 on PRKD3, have been experimentally verified. This study presents the first systematic and computational approach to identify critical phos-sites that enhance kinase activity.}
}
@article{ADENIJI2023,
title = {Draft genome sequence of Priestia megaterium AB-S79 strain isolated from active gold mine},
journal = {Microbiology Resource Announcements},
volume = {13},
number = {2},
year = {2023},
issn = {2576-098X},
doi = {https://doi.org/10.1128/mra.01055-23},
url = {https://www.sciencedirect.com/science/article/pii/S2576098X23010629},
author = {Adetomiwa A. Adeniji and Ayansina S. Ayangbenro and Olubukola O. Babalola},
keywords = {bioremediation, biosynthetic traits, genome analysis, , secondary metabolites, genomics},
abstract = {ABSTRACT
We screened and isolated Priestia megaterium strain AB-S79 from active gold mine soil, then sequenced its genome to unravel its biosynthetic traits. The isolate with a 5.7-Mb genome can be utilized as a reference in genome-guided strain selection for metabolic engineering and other biotechnological operations.}
}
@article{CHUNG201356,
title = {Table-top role playing game and creativity},
journal = {Thinking Skills and Creativity},
volume = {8},
pages = {56-71},
year = {2013},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2012.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S1871187112000478},
author = {Tsui-shan Chung},
keywords = {Creativity, Role playing game, Divergent thinking test, Priming},
abstract = {The current study aims to observe whether individuals who engaged in table-top role playing game (TRPG) were more creative. Participants total 170 (52 TRPG players, 54 electronic role playing game (ERPG) players and 64 Non-players) aged from 19 to 63. In the current study, an online questionnaire is used, adopting the verbal subtests of Wallach–Kogan Creativity Tests and the McCrae and Costa Big Five Personality Inventory. It is found that TRPG players score higher in divergent thinking tests. Priming and instruction giving methods lower the performance of all participants, in particular, when the instruction is memory provoking. ERPG players score lowest among the three groups. TRPG could be regarded as a form of improvisation. It could also be a preferable activity for the promotion of creativity. It is low cost and no formal setting is required to play. Many ERPGs are originated from TRPGs, therefore, with the popularity of ERPG, there should be advantages in promoting TRPG.}
}
@article{CHEN2010573,
title = {Generating ontologies with basic level concepts from folksonomies},
journal = {Procedia Computer Science},
volume = {1},
number = {1},
pages = {573-581},
year = {2010},
note = {ICCS 2010},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2010.04.061},
url = {https://www.sciencedirect.com/science/article/pii/S1877050910000621},
author = {Wen-hao Chen and Yi Cai and Ho-fung Leung and Qing Li},
keywords = {Folksonomy, Ontology, Basic level categories, Category utility},
abstract = {This paper deals with the problem of ontology generation. Ontology plays an important role in knowledge representation, and it is an artifact describing a certain reality with specific vocabulary. Recently many researchers have realized that folksonomy is a potential knowledge source for generating ontologies. Although some results have already been reported on generating ontologies from folksonomies, most of them do not consider what a more acceptable and applicable ontology for users should be, nor do they take human thinking into consideration. Cognitive psychologists find that most human knowledge is represented by basic level concepts which is a family of concepts frequently used by people in daily life. Taking cognitive psychology into consideration, we propose a method to generate ontologies with basic level concepts from folksonomies. Using Open Directory Project (ODP) as the benchmark, we demonstrate that the ontology generated by our method is reasonable and consistent with human thinking.}
}
@article{CAMARGOJUNIOR2016190,
title = {Optimal economic result and risk of parallel development of concept options in dynamic markets},
journal = {RAI Revista de Administração e Inovação},
volume = {13},
number = {3},
pages = {190-198},
year = {2016},
issn = {1809-2039},
doi = {https://doi.org/10.1016/j.rai.2016.06.006},
url = {https://www.sciencedirect.com/science/article/pii/S1809203916300377},
author = {Alceu Salles {Camargo Júnior} and Abraham Sin Oih Yu},
keywords = {New product development, Economic result and risk of projects, Option thinking},
abstract = {New product development is an essential competence to organizations. Launching success products requires elaborate and precise knowledge about the technological platforms, like the most important market needs and characteristics, and the project team have to employ information systems to support the project decisions, which must be rapid and accurate. However, when the market characteristics are much dynamic and change rapidly or the development project aims at a really new product, the levels of uncertainties are greater, and the project team must employ more robust strategies of risk management. Option thinking is useful to develop several concept alternatives of some crucial subsystems of the new product in order to achieve new technical and market knowledge by repeating cycles of design, built and tested by several and different prototypes in parallel. These different prototypes develop, test and can accumulate knowledge about each one, different technologies, architectures and quality attributes or the usability for potential customers. This study achieves the optimal number of concept options to develop in parallel in order to maximize the economic performance of the development project of a new product constituted of two important subsystems. Mathematical models simulating the sequential decision process are developed to determine the economic result and risk of a two-subsystem product innovation project. Our results point the parallel development of concept options as a robust strategy to manage new product development mostly in adverse conditions, that is, with greater levels of uncertainties.}
}
@incollection{GALLISTEL199235,
title = {Classical Conditioning as an Adaptive Specialization: A Computational Model},
editor = {Douglas L. Medin},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {28},
pages = {35-67},
year = {1992},
issn = {0079-7421},
doi = {https://doi.org/10.1016/S0079-7421(08)60487-9},
url = {https://www.sciencedirect.com/science/article/pii/S0079742108604879},
author = {C.R. Gallistel},
abstract = {Publisher Summary
This chapter analyzes the results of some modern classical conditioning experiments from the perspective of a computational model based on the assumption that the underlying learning process is specifically adapted to the domain of multivariate, nonstationary time series. It focuses on the quantitative results from experiments on the effects of partial reinforcement on the rate of acquisition and extinction because the other predictions of the model have been discussed and associative models are conspicuously unsuccessful at making quantitative predictions in this area. The model gives a mathematical characterization of the learning process from which one can derive the results of conditioning experiments. It is unlike these models in the sense that it is not in the associative tradition. The model replaces the associative explanatory framework with a framework that treats the conditioning process as a computational mechanism adapted through evolution to the peculiarities of one domain-a mechanism that solves one and only one of the several fundamentally distinct learning problems that confront mobile, multicellular organisms.}
}
@incollection{KUMAR20031,
title = {1 - An introduction to computational development},
editor = {Sanjeev Kumar and Peter J. Bentley},
booktitle = {On Growth, Form and Computers},
publisher = {Academic Press},
address = {London},
pages = {1-43},
year = {2003},
isbn = {978-0-12-428765-5},
doi = {https://doi.org/10.1016/B978-012428765-5/50034-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780124287655500347},
author = {Sanjeev Kumar and Peter J. Bentley}
}
@incollection{ROWLAND2003341,
title = {Chapter 16 - Interpreting Analytical Spectra with Evolutionary Computation},
editor = {Gary B. Fogel and David W. Corne},
booktitle = {Evolutionary Computation in Bioinformatics},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {341-365},
year = {2003},
series = {The Morgan Kaufmann Series in Artificial Intelligence},
isbn = {978-1-55860-797-2},
doi = {https://doi.org/10.1016/B978-155860797-2/50018-4},
url = {https://www.sciencedirect.com/science/article/pii/B9781558607972500184},
author = {Jem J. Rowland},
abstract = {Publisher Summary
This chapter deals with analytical techniques that are used to probe the activity and chemical makeup of cells. Metabolomics, the study of the entire biochemical constituents of a cell at any one time, is found to provide a rich means of monitoring organism activity. It can reveal explanations for different characteristics of seemingly similar organisms and can be used to relate function with gene. Spectroscopies are well suited to the study and interpretation of the metabolome in functional genomics. Another important technique in functional genomics is the measurement of gene expression via transcriptome arrays. This chapter outlines the various ways in which evolutionary computation (EC) can provide the basis for powerful tools for spectral interpretation and thus for functional genomics. It mentions various methods of forming predictive models from multivariate, often quasi-continuous data. It also discusses ways in which the effectiveness of such conventional techniques may be enhanced by combining them with evolutionary techniques.}
}
@article{WILSON1989171,
title = {Grand challenges to computational science},
journal = {Future Generation Computer Systems},
volume = {5},
number = {2},
pages = {171-189},
year = {1989},
note = {Grand Challenges to Computational Science},
issn = {0167-739X},
doi = {https://doi.org/10.1016/0167-739X(89)90038-1},
url = {https://www.sciencedirect.com/science/article/pii/0167739X89900381},
author = {Kenneth G. Wilson},
abstract = {Computational Science is at the very beginning of centuries of growth, comparable to the four centuries of experimental advances since Galileo. The Grand Challenges to Computational Science are unsolved scientific problems of extraordinary breadth and importance which will demand continuing computational advances throughout the forthcoming computational era. Supercomputers can be used to see phenomena not directly accessible to experiment in key scientific and engineering areas such as atmospheric science, astronomy, materials science, molecular biology, aerodynamics, and elementary particle physics. However, the benefits of supercomputers will be greatly increased if some major difficulties are overcome. In this paper, I address some of the tougher requirements on current grand challenge research to ensure that it has enduring value. The problems of algorithm development, error control, software productivity, and the fostering of technological advances are especially important.}
}
@incollection{MCKELVEY199687,
title = {Chapter 2 Computation of equilibria in finite games},
series = {Handbook of Computational Economics},
publisher = {Elsevier},
volume = {1},
pages = {87-142},
year = {1996},
issn = {1574-0021},
doi = {https://doi.org/10.1016/S1574-0021(96)01004-0},
url = {https://www.sciencedirect.com/science/article/pii/S1574002196010040},
author = {Richard D. McKelvey and Andrew McLennan},
abstract = {Publisher Summary
This chapter provides an overview of the latest state of the art of methods for numerical computation of Nash equilibria —and refinements of Nash equilibria —for general finite n-person games. The appropriate method for computing Nash equilibria for a game depends on a number of factors. The first and most important factor involves, whether it is required to simply find one equilibrium (a sample equilibrium), or find all equilibria. The problem of finding one equilibrium is a well studied problem, and there exist number of different methods for numerically computing a sample equilibrium. The problem of finding all equilibria has been addressed recently. While, there exist methods for computation of all equilibria, they are computationally intensive. With current methods, they are only feasible on small problems. The chapter overviews methods for computing sample equilibria in normal form games, and discusses the computation of equilibria on extensive form games.}
}
@article{DO2020110730,
title = {Capturing creative requirements via requirements reuse: A machine learning-based approach},
journal = {Journal of Systems and Software},
volume = {170},
pages = {110730},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110730},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220301631},
author = {Quoc Anh Do and Tanmay Bhowmik and Gary L. Bradshaw},
keywords = {Requirements reuse, Requirements engineering, Creativity in RE, Boilerplate, Natural language processing, Machine learning},
abstract = {The software industry has become increasingly competitive as we see multiple software serving the same domain and striving for customers. To that end, modern software needs to provide creative features to improve sustainability. To advance software creativity, research has proposed several techniques, including multi-day workshops involving experienced requirements analysts, and semi-automated tools to support creative thinking in a limited scope. Such approaches are either useful only for software with already rich issue tracking systems, or require substantial engagement from analysts with creative minds. In a recent work, we have demonstrated a novel framework that is beneficial for both novel and existing software and allows end-to-end automation promoting creativity. The framework reuses requirements from similar software freely available online, utilizes advanced natural language processing and machine learning techniques, and leverages the concept of requirement boilerplate to generate candidate creative requirements. An application of our framework on software domains: Antivirus, Web Browser, and File Sharing followed by a human subject evaluation have shown promising results. In this invited extension, we present further analysis for our research questions and report an additional evaluation by human subjects. The results exhibit the framework’s ability in generating creative features even for a relatively matured application domain, such as Web Browser, and provoking creative thinking among developers irrespective of their experience levels.}
}
@article{ROSSITER202017604,
title = {Using interactive tools to facilitate student self-testing of dynamics and PI compensation},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {17604-17609},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2677},
url = {https://www.sciencedirect.com/science/article/pii/S240589632033439X},
author = {J.A. Rossiter},
keywords = {Virtual laboratories, staff efficiency, student engagement, independent learning},
abstract = {Virtual laboratories have become a common tool in recent years for supporting student learning and engagement. This paper presents a new tool for helping students self-assess their competence in basic dynamics for 1st and 2nd order systems alongside simple PI compensation techniques. The tools provide a supported environment for helping students work towards the correct answer by providing succinct feedback on incorrect responses and opportunities to try again, while displaying relevant information. A partner interactive tool is also provided which focuses solely on assessment with no feedback, so that students can assess their ability to get correct answers in a scenario that only the first attempt counts. This paper gives the thinking behind the tools, their coding and also accessibility for students.}
}
@incollection{VERSCHAFFEL2010401,
title = {Mathematics Learning},
editor = {Penelope Peterson and Eva Baker and Barry McGaw},
booktitle = {International Encyclopedia of Education (Third Edition)},
publisher = {Elsevier},
edition = {Third Edition},
address = {Oxford},
pages = {401-406},
year = {2010},
isbn = {978-0-08-044894-7},
doi = {https://doi.org/10.1016/B978-0-08-044894-7.00517-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780080448947005170},
author = {L. Verschaffel and B. Greer and E. {De Corte}},
keywords = {Adaptive expertise, Assessment, Collaboration, Competence, Constructivism, Design experiment, High-stakes testing, Mathematical, Mathematics education, Mathematics learning, Mathematics teaching, Prior knowledge, Routine expertise, Situated cognition, Standards},
abstract = {This article presents a review of important recent themes and developments in research on the learning and teaching of mathematical knowledge and thinking. As a framework, we use a model for designing a powerful environment for learning and teaching mathematics; this model is structured according to four interrelated components, namely competence, learning, intervention, and assessment (CLIA-model) (De Corte et al., 2004). We argue and illustrate that our empirically based knowledge of each of these four interconnected components has substantially advanced over the past decades, enabling a progressively better understanding of not only the components that constitute a mathematical disposition, but also the nature of the learning and developmental processes that should be induced in students to facilitate the acquisition of competence, the characteristics of learning environments that are powerful in initiating and evoking those processes, and finally, the kind of assessment instruments that are appropriate to help monitor and support learning and teaching.}
}
@article{DEVINK20222744,
title = {Cooperativity as quantification and optimization paradigm for nuclear receptor modulators††Electronic supplementary information (ESI) available: Experimental details, supporting figures and tables. See DOI: 10.1039/d1sc06426f},
journal = {Chemical Science},
volume = {13},
number = {9},
pages = {2744-2752},
year = {2022},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d1sc06426f},
url = {https://www.sciencedirect.com/science/article/pii/S2041652023017297},
author = {Pim J. {de Vink} and Auke A. Koops and Giulia D'Arrigo and Gabriele Cruciani and Francesca Spyrakis and Luc Brunsveld},
abstract = {ABSTRACT
Nuclear Receptors (NRs) are highly relevant drug targets, for which small molecule modulation goes beyond a simple ligand/receptor interaction. NR–ligands modulate Protein–Protein Interactions (PPIs) with coregulator proteins. Here we bring forward a cooperativity mechanism for small molecule modulation of NR PPIs, using the Peroxisome Proliferator Activated Receptor γ (PPARγ), which describes NR–ligands as allosteric molecular glues. The cooperativity framework uses a thermodynamic model based on three-body binding events, to dissect and quantify reciprocal effects of NR–coregulator binding (KID) and NR–ligand binding (KIID), jointly recapitulated in the cooperativity factor (α) for each specific ternary ligand·NR·coregulator complex formation. These fundamental thermodynamic parameters allow for a conceptually new way of thinking about structure–activity-relationships for NR–ligands and can steer NR modulator discovery and optimization via a completely novel approach.}
}
@article{AUMER2024100111,
title = {Impaired cognitive flexibility in schizophrenia: A systematic review of behavioral and neurobiological findings},
journal = {Biomarkers in Neuropsychiatry},
volume = {11},
pages = {100111},
year = {2024},
issn = {2666-1446},
doi = {https://doi.org/10.1016/j.bionps.2024.100111},
url = {https://www.sciencedirect.com/science/article/pii/S2666144624000297},
author = {Philipp Aumer and Geva A. Brandt and Dusan Hirjak and Florian Bähner},
keywords = {Schizophrenia, Cognitive flexibility, Set-shifting, Wisconsin card sorting test, Intra-extra dimensional set shift, Cambridge Neuropsychological Test Automated Battery},
abstract = {Background and hypothesis
Impaired cognitive flexibility in schizophrenia (SZ) is well documented and correlation with worse functional outcome indicates clinical relevance. Paradigms that assess cognitive flexibility include the Wisconsin Card Sorting Test (WCST) and the Cambridge Neuropsychological Test Automated Battery’s (CANTAB) Intra-Extra Dimensional Set Shift (IED). This systematic review provides an overview of the current state of research on cognitive flexibility in schizophrenia and points out relevant areas of non-consensus.
Methods
Two electronic databases (Embase and PubMed) were searched for records published from 1993 to 2024 on adult SZ patients that were assessed for cognitive flexibility/set-shifting ability using WCST and/or IED.
Results
38 studies were included in the review, most of which reported significantly worse performance of SZ patients in WCST and/or IED compared to healthy controls (HC). Most publications focused on the specific profile of cognitive inflexibility. Other aspects included progression of cognitive inflexibility over the course of the illness, neurobiological correlates, IQ as a possible confounder and whether cognitive inflexibility is a heritable trait.
Conclusion
Included studies show that cognitive inflexibility rather reflects a stable trait than a state, indicating a lasting prefrontal impairment in SZ. Further longitudinal studies are needed to clarify how these deficits evolve during progression of the disorder. Neither antipsychotic medication nor intelligence seem to explain impaired cognitive flexibility. However, a disease-specific cognitive phenotype has not yet been established and additional research on neuro-computational mechanisms is thus needed to identify possible targets for interventional studies.}
}
@article{ZHANG2024103588,
title = {Anonymous data sharing scheme for resource-constrained internet of things environments},
journal = {Ad Hoc Networks},
volume = {163},
pages = {103588},
year = {2024},
issn = {1570-8705},
doi = {https://doi.org/10.1016/j.adhoc.2024.103588},
url = {https://www.sciencedirect.com/science/article/pii/S1570870524001999},
author = {Zetian Zhang and Jingyu Wang and Lixin Liu and Yongfeng Li and Yun Hao and Hanqing Yang},
keywords = {Anonymity, Resource-constrained, Integrity verification, Data sharing, Accountability, Revocation},
abstract = {With the rapid development of Internet of Things (IoT) technology in industrial, agricultural, medical and other fields, IoT terminal devices face security and privacy challenges when sharing data. Among them, ensuring data confidentiality, achieving dual-side privacy protection, and performing reliable data integrity verification are basic requirements. Especially in resource-constrained environments, limitations in the storage, computing, and communication capabilities of devices increase the difficulty of implementing these security safeguards. To address this problem, this paper proposes a resource-constrained anonymous data-sharing scheme (ADS-RC) for the IoT. In ADS-RC, we use elliptic curve operations to replace computation-intensive bilinear pairing operations, thereby reducing the computational and communication burden on end devices. We combine an anonymous verifiable algorithm and an attribute encryption algorithm to ensure double anonymity and data confidentiality during the data-sharing process. To deal with potential dishonest behavior, this solution supports the revocation of malicious user permissions. In addition, we designed a batch data integrity verification algorithm and stored verification evidence on the blockchain to ensure the security and traceability of data during transmission and storage. Through experimental verification, the ADS-RC scheme achieves reasonable efficiency in correctness, security and efficiency, providing a new solution for data sharing in resource-constrained IoT environments.}
}
@article{REISS1967193,
title = {Individual thinking and family interaction—II. A study of pattern recognition and hypothesis testing in families of normals, character disorders and schizophrenics},
journal = {Journal of Psychiatric Research},
volume = {5},
number = {3},
pages = {193-211},
year = {1967},
issn = {0022-3956},
doi = {https://doi.org/10.1016/0022-3956(67)90002-7},
url = {https://www.sciencedirect.com/science/article/pii/0022395667900027},
author = {David Reiss},
abstract = {The present study investigated the relationship between family interaction and individual pattern recognition in five families of normals, five families of character disorders and five families of schizophrenics. Following a period of family interaction, members of normal families showed improvement in pattern recognition; members of families of schizophrenics showed deterioration or no change and members of character disorder families were in between. During the period of family interaction, members of normal families were independent and adventuresome in testing their pattern concepts whereas members of families of schizophrenics were cautios, copied each other's performance but showed little pooling of ideas. These findings support the hypothesis that family interaction can influence perceptual process in its individual members in a short time and points to some particular relationships between family interaction and individual perception.}
}
@article{SHUBBAR2024382,
title = {Bridging Qatar's food demand and self-sufficiency: A system dynamics simulation of the energy–water–food nexus},
journal = {Sustainable Production and Consumption},
volume = {46},
pages = {382-399},
year = {2024},
issn = {2352-5509},
doi = {https://doi.org/10.1016/j.spc.2024.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S2352550924000423},
author = {Haya Talib Shubbar and Furqan Tahir and Tareq Al-Ansari},
keywords = {Carbon emissions, Energy-water-food nexus, Food self-sufficiency, Food security, Qatar, System dynamics},
abstract = {The food sector in Qatar is confronted with formidable challenges due to its harsh environmental conditions. Striving for total food self-sufficiency in such an environment would inevitably exert pressure on the energy and water sectors. This heightened demand for energy and water translates into increased costs and escalates environmental impacts. Consequently, this study embarks on an in-depth analysis of food production within the context of Qatar's energy-water-food nexus, aiming to demonstrate how varying degrees of food self-sufficiency may impact the demand on Qatar's water and energy sectors, as well as on greenhouse gas (GHG) emissions. Moreover, this study demonstrates to what extent specific subsystems within the nexus can be modified to enhance sustainability. An energy-water-food nexus is meticulously crafted within the proposed framework to elucidate the intricate interdependencies among these sectors, incorporating pertinent external variables. These interconnections are then transmuted into a system dynamics model (SDM), facilitating a nuanced exploration of potential transformations and their ripple effects. Furthermore, a life-cycle thinking approach explicitly tailored to Qatar was implemented to estimate GHG emissions accurately. Four distinct scenarios are rigorously examined using the SDM, spanning from a status quo perspective to ambitious transitions toward full food self-sufficiency. The findings of the scenarios indicate that scenario 4, which partially provides the country with its food demands locally using desalinated water, treated wastewater, and groundwater and satisfies 20 % of its energy demand from solar energy, is the most ideal with an annual 5.36 × 1010 kWh/year energy consumption, 1.73 × 1012 l/year water demand, and 3.26 × 1010 kg CO2 eq./year emissions. The outcomes underscore the imperative for prioritizing less energy-intensive resources to mitigate overall energy consumption. Additionally, achieving an optimal national scenario necessitates a judicious equilibrium between food imports and domestic production.}
}
@article{ROBERTSON2009136,
title = {Impact of CAD tools on creative problem solving in engineering design},
journal = {Computer-Aided Design},
volume = {41},
number = {3},
pages = {136-146},
year = {2009},
note = {Computer Support for Conceptual Design},
issn = {0010-4485},
doi = {https://doi.org/10.1016/j.cad.2008.06.007},
url = {https://www.sciencedirect.com/science/article/pii/S0010448508001334},
author = {B.F. Robertson and D.F. Radcliffe},
keywords = {CAD, Creativity, Conceptual design},
abstract = {This paper presents the results of a survey of CAD users that examined the ways in which their computational environment may influence their ability to design creatively. This extensive online survey builds upon the findings of an earlier observational case study of the use of computer tools by a small engineering team. The case study was conducted during the conceptual and detailed stages of the design of a first-to-world product. Four mechanisms by which CAD tools may influence the creative problem solving process were investigated: enhanced visualisation and communication, circumscribed thinking, premature design fixation and bounded ideation. The prevalence of these mechanisms was examined via a series of questions that probed the user’s mode of working, attitudes, and responses to hypothetical situations. The survey showed good support for the first three mechanisms and moderate support for the fourth. The results have important implications for both the users and designers of CAD tools.}
}
@incollection{MARTIGNON2001382,
title = {Algorithms},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {382-385},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/00549-0},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767005490},
author = {L. Martignon},
abstract = {The concept of algorithm is central to the modern view of a thinking machine, be it the human mind or the modern computer. An algorithm is a well-defined mathematical recipe for the solution of a well-defined task. It is presented as a finite set of steps or instructions that can be applied to unlimited sets of possibilities. There is a clear-cut rule for the operation to be performed at each step, as well as a clear-cut specification of the conditions under which to terminate the process. An algorithm may contain loops, that is, there may be steps that return to previous steps. Algorithms can be sequential or parallel. An algorithm that produces a ‘yes’ or ‘no’ answer is, decision algorithm. An algorithm that constructs or determines a specific solution to a given problem is a computation algorithm.}
}
@article{KOPPAKA2024,
title = {Mechanism and Selectivity of Bi(V)-Aryl Oxyfunctionalization in Trifluoroacetic Acid Solvents},
journal = {Organometallics},
year = {2024},
issn = {0276-7333},
doi = {https://doi.org/10.1021/acs.organomet.4c00319},
url = {https://www.sciencedirect.com/science/article/pii/S0276733324003509},
author = {Anjaneyulu Koppaka and Dongdong Yang and Sanaz Mohammadzadeh Koumleh and Burjor Captain and Roy A. Periana and Daniel H. Ess},
abstract = {The oxidative functionalization of aromatic sp2 C–H bonds to C–O bonds is a difficult transformation. For main-group metals, the oxyfunctionalization step of a metal-aryl bond is generally slow and potentially problematic if carried out in a relatively strong acid solvent where protonation could prevent oxyfunctionalization. In this work, we experimentally and computationally analyzed the oxyfunctionalization reaction of (Ph)3BiV(TFA)2 (TFA = trifluoroacetate) in a trifluoroacetic acid (TFAH) solvent. Experiments showed a single oxyfunctionalization product phenyl TFA (PhTFA) and two equivalents of benzene. Explicit/continuum solvent density functional theory calculations revealed that a direct intramolecular reductive functionalization pathway is lower in energy than radical or ionic pathways, and surprisingly from (Ph)3BiV(TFA)2, the reductive functionalization pathway is potentially competitive with protonation. In contrast, for (Ph)2BiV(TFA)3 oxyfunctionalization is significantly lower in energy than protonation. For BiIII-phenyl intermediates, redox neutral protonation is significantly lower in energy than a second functionalization. We also examined the oxyfunctionalization versus protonation of BiV-phenyl complexes with a coordinated biphenyl ligand and a coordinated biphenyl sulfone ligand, which both resulted in oxyfunctionalization. For the biphenyl ligand complex, a protonation-first mechanism is proposed, while for the biphenyl sulfone ligand, an oxyfunctionalization first mechanism is consistent with both calculations and experiments.
}
}
@article{NYSTROM201077,
title = {Ontological musings on how nature computes},
journal = {Procedia Computer Science},
volume = {1},
number = {1},
pages = {77-86},
year = {2010},
note = {ICCS 2010},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2010.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S1877050910000116},
author = {J.F. Nystrom},
keywords = {Universe as computation, Quantum vacuum, Computational cosmography},
abstract = {Modern physical theory and modern computational techniques are used to provide conjecture on how nature computes. I utilize time-domain simulation of physical phenomena and build analogies between elements of computation and the “things” of Universe computation, resulting, for example, in the identification of the quantum vacuum as the power source for Universe computation. While reviewing how Universe can be viewed as a computation, we find the need for Negative Universe (which is a part of the quantum vacuum mechanism). This idea is compared with Penrose’s current model which utilizes a separate Platonic world outside of physical Universe. Lastly, in the Discussion, I present an updated version of computational cosmography as a model for Universe as computation.}
}
@article{GOMEZMARTINEZ2025129806,
title = {A bioinspired model of decision making guided by reward dimensions and a motivational state},
journal = {Neurocomputing},
volume = {634},
pages = {129806},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129806},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225004783},
author = {Diana G. Gómez-Martínez and Alison Muñoz-Capote and Oscar Hernández and Francisco Robles and Félix Ramos},
keywords = {Decision-making, Magnitude reward, Probability reward, Decision criteria, Motivational state, Nondeterministic model},
abstract = {The decision-making process is a critical component of computational systems, whose processing involves the evaluation of various alternatives presented as possible solutions to a given problem, depending on the current context. This paper seeks to show how a neuroscience-based decision-making mechanism (DMM) integrating decision criteria, knowledge of reward stimuli, and motivational information helps to contribute to producing human-like adaptive behavior. To fulfill this objective, a computational model on DMM is proposed. The alternatives in this proposed model are constructed based on preferences, and the selection of the best alternative is guided by a goal-directed control scheme influenced by a motivational state (MS). The formation of preferences considers some dimensions of the reward, e.g., magnitude, probability of receiving the reward, incentive salience, and affective value. To validate the model exhibits a behavior considering parameters human being uses to compute its behavior, a case study was proposed. The case study’s objective is to gain the maximum reward (food) from the choice of a 4-choice card (a variation of Iowa Gambling Test), each card has a reward and a contingency probability associated with it. The analysis of the results of the case study shows that the model presents a short exploitation stage to find the contingency rule and choose the best option frequently according to some studies, also observed that the utility value of the card influenced the MS of hunger and other factors play a critical role in the DMM.}
}
@article{DAS2025111780,
title = {ANN-based prediction of aerodynamic force coefficients and shape optimization using MOEO of a high-rise building with varying cross-section along height},
journal = {Journal of Building Engineering},
volume = {100},
pages = {111780},
year = {2025},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2025.111780},
url = {https://www.sciencedirect.com/science/article/pii/S2352710225000166},
author = {Arghyadip Das and Sujit Kumar Dalui},
keywords = {Tall building, Computational fluid dynamics (CFD), Corner recession, Force coefficient, Artificial neural network (ANN), Multi-objective equilibrium optimization (MOEO)},
abstract = {The main goal of this study is to find the effect of various geometrical configurations with varying cross-sections along the height of a corner recessed square tall building, considering the wind flow around the buildings from 0° to 360°. The design variables considered in this study are the amount of corner recessing (S), the height of the square cross-section (h) and the wind incidence angle (Ø). The design variables are generated using the simple random sampling technique in MATLAB. A series of numerical analyses have been performed for those randomly generated design variables using Computational Fluid Dynamics (CFD) in the ANSYS-CFX module to evaluate the along and across-wind force coefficients (Cfx and Cfy). The numerical simulations have been carried out using the k-ε turbulence model on a length scale of 1:300. The results of the CFD simulations are used to train the artificial neural network (ANN) of Cfx and Cfy. After simulating the networks of Cfx and Cfy, a shape optimization study has been carried out using Multi-Objective Equilibrium Optimization (MOEO) to find the optimal shapes of various building configurations considering different weights of the design function values (Cfx and Cfy). The results of the shape optimization study have been validated with separate CFD investigations and several wind tunnel experiments.}
}
@article{BREIT2025102621,
title = {Mathematics achievement and learner characteristics: A systematic review of meta-analyses},
journal = {Learning and Individual Differences},
volume = {118},
pages = {102621},
year = {2025},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2024.102621},
url = {https://www.sciencedirect.com/science/article/pii/S1041608024002140},
author = {Moritz Breit and Michael Schneider and Franzis Preckel},
keywords = {Mathematics achievement, meta-analysis, Systematic review, Math talent, Predictors},
abstract = {Learners' individual differences in mathematics achievement are associated with individual differences in psychological characteristics. A number of meta-analyses have quantified the strengths of these correlations. However, these findings are scattered across different strands of the literature. The present systematic review aims to integrate these strands by providing an overview of meta-analyses of psychological correlates of mathematics achievement. We conducted a systematic literature search and included 30 meta-analyses, reporting correlations between mathematics achievement and 66 variables based on 13,853 effect sizes and an estimated 4,658,717 participants. The correlations are rank-ordered by size and complemented with information about the meta-analyses, their inclusion criteria, and methods. The results show strong associations of mathematics achievement with verbal skills and abilities, prior knowledge, intelligence, creativity, math-specific skills, math self-concept, self-regulation, meta-cognition, and executive functions. Relatively weaker relations were observed for emotional intelligence, achievement goals, academic emotions, and the Big Five personality traits.}
}
@article{BIRJALI201765,
title = {Machine Learning and Semantic Sentiment Analysis based Algorithms for Suicide Sentiment Prediction in Social Networks},
journal = {Procedia Computer Science},
volume = {113},
pages = {65-72},
year = {2017},
note = {The 8th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN 2017) / The 7th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2017) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.08.290},
url = {https://www.sciencedirect.com/science/article/pii/S187705091731699X},
author = {Marouane Birjali and Abderrahim Beni-Hssane and Mohammed Erritali},
keywords = {Sentiment Analysis, Machine Learning, Suicide, Social Networks, Tweets, Semantic Sentiment Analysis},
abstract = {Sentiment analysis is one of the new challenges appeared in automatic language processing with the advent of social networks. Taking advantage of the amount of information is now available, research and industry have sought ways to automatically analyze sentiments and user opinions expressed in social networks. In this paper, we place ourselves in a difficult context, on the sentiments that could thinking of suicide. In particular, we propose to address the lack of terminological resources related to suicide by a method of constructing a vocabulary associated with suicide. We then propose, for a better analysis, to investigate Weka as a tool of data mining based on machine learning algorithms that can extract useful information from Twitter data collected by Twitter4J. Therefore, an algorithm of computing semantic analysis between tweets in training set and tweets in data set based on WordNet is proposed. Experimental results demonstrate that our method based on machine learning algorithms and semantic sentiment analysis can extract predictions of suicidal ideation using Twitter Data. In addition, this work verify the effectiveness of performance in term of accuracy and precision on semantic sentiment analysis that could thinking of suicide.}
}
@article{READ200577,
title = {Early computational processing in binocular vision and depth perception},
journal = {Progress in Biophysics and Molecular Biology},
volume = {87},
number = {1},
pages = {77-108},
year = {2005},
note = {Biophysics of Excitable Tissues},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2004.06.005},
url = {https://www.sciencedirect.com/science/article/pii/S007961070400063X},
author = {Jenny Read},
abstract = {Stereoscopic depth perception is a fascinating ability in its own right and also a useful model of perception. In recent years, considerable progress has been made in understanding the early cortical circuitry underlying this ability. Inputs from left and right eyes are first combined in primary visual cortex (V1), where many cells are tuned for binocular disparity. Although the observation of disparity tuning in V1, combined with psychophysical evidence that stereopsis must occur early in visual processing, led to initial suggestions that V1 was the neural correlate of stereoscopic depth perception, more recent work indicates that this must occur in higher visual areas. The firing of cells in V1 appears to depend relatively simply on the visual stimuli within local receptive fields in each retina, whereas the perception of depth reflects global properties of the stimulus. However, V1 neurons appear to be specialized in a number of respects to encode ecologically relevant binocular disparities. This suggests that they carry out essential pre-processing underlying stereoscopic depth perception in higher areas. This article reviews recent progress in developing accurate models of the computations carried out by these neurons. We seem close to achieving a mathematical description of the initial stages of the brain's stereo algorithm. This is important in itself––for instance, it may enable improved stereopsis in computer vision––and paves the way for a full understanding of how depth perception arises.}
}