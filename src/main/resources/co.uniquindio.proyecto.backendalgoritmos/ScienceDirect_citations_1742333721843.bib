@article{HAMZI2023133853,
title = {Learning dynamical systems from data: A simple cross-validation perspective, part IV: Case with partial observations},
journal = {Physica D: Nonlinear Phenomena},
volume = {454},
pages = {133853},
year = {2023},
issn = {0167-2789},
doi = {https://doi.org/10.1016/j.physd.2023.133853},
url = {https://www.sciencedirect.com/science/article/pii/S0167278923002075},
author = {Boumediene Hamzi and Houman Owhadi and Yannis Kevrekidis},
keywords = {Learning dynamical systems, Kernel flows, Partial observations, Computational graph completion},
abstract = {A simple and interpretable way to learn a dynamical system from data is to interpolate its governing equations with a kernel. In particular, this strategy is highly efficient (both in terms of accuracy and complexity) when the kernel is data-adapted using Kernel Flows (KF) (Owhadi and Yoo, 2019), (which uses gradient-based optimization to learn a kernel based on the premise that a kernel is good if there is no significant loss in accuracy if half of the data is used for interpolation). In this work, we extend previous work on learning dynamical systems using Kernel Flows (Hamzi and Owhadi, 2021; Darcy et al. 2021; Lee et al. 2023; Darcy et al. 2023; Owhadi and Romit Maulik, 2021) to the case of learning vector-valued dynamical systems from time-series observations that are partial/incomplete in the state space. The method combines Kernel Flows with Computational Graph Completion.}
}
@article{RODRIGUEZJORDA2025101702,
title = {Linguistic relativity from an enactive perspective: the entanglement of language and cognition},
journal = {Language Sciences},
volume = {108},
pages = {101702},
year = {2025},
issn = {0388-0001},
doi = {https://doi.org/10.1016/j.langsci.2024.101702},
url = {https://www.sciencedirect.com/science/article/pii/S0388000124000913},
author = {Ulises {Rodríguez Jordá} and Ezequiel A. {Di Paolo}},
keywords = {Linguistic relativity, Enactive approach, Modularity, Post-cognitivism, Languaging},
abstract = {We seek to relate the fields of linguistic relativity (LR) and the enactive approach in cognitive science. We distinguish contemporary research on LR, starting after the mid-1990s, from earlier approaches to the field. Current studies are characterised by a nuanced methodology rooted in the psycholinguistics tradition. While improving on earlier research, they also move away from philosophically oriented discussions about the relation between language and cognition and focus instead on experimentally testing relativistic effects for specific cognitive domains. We claim that this procedure retains some fundamental assumptions from classical cognitive science, precisely those that are challenged by an enactive perspective. These include a commitment to the modularity of mind and a computational understanding of the interactions between cognitive domains. We contend that contemporary LR research is, in fact, compatible with these classical cognitivist ideas, despite superficial points of tension. We then survey recent post-cognitivist approaches to language in cognitive science and explore ways in which LR and the enactive framework could be mutually enriched. Whereas the structural or categorial aspects of language are central for LR research, these are usually downplayed in post-cognitivist approaches, often influenced by the integrationist distinction between first-order linguistic practices and second-order constructs. We advance a specifically enactive perspective that seeks to preserve the systematic features of language while also integrating them within a dynamical understanding of the relation between language and cognition at multiple timescales.}
}
@article{BIRHANE2021100205,
title = {Algorithmic injustice: a relational ethics approach},
journal = {Patterns},
volume = {2},
number = {2},
pages = {100205},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100205},
url = {https://www.sciencedirect.com/science/article/pii/S2666389921000155},
author = {Abeba Birhane},
keywords = {justice, ethics, Afro-feminism, relational epistemology, data science, complex systems, enaction, embodiment, artificial intelligence, machine learning},
abstract = {Summary
It has become trivial to point out that algorithmic systems increasingly pervade the social sphere. Improved efficiency—the hallmark of these systems—drives their mass integration into day-to-day life. However, as a robust body of research in the area of algorithmic injustice shows, algorithmic systems, especially when used to sort and predict social outcomes, are not only inadequate but also perpetuate harm. In particular, a persistent and recurrent trend within the literature indicates that society's most vulnerable are disproportionally impacted. When algorithmic injustice and harm are brought to the fore, most of the solutions on offer (1) revolve around technical solutions and (2) do not center disproportionally impacted communities. This paper proposes a fundamental shift—from rational to relational—in thinking about personhood, data, justice, and everything in between, and places ethics as something that goes above and beyond technical solutions. Outlining the idea of ethics built on the foundations of relationality, this paper calls for a rethinking of justice and ethics as a set of broad, contingent, and fluid concepts and down-to-earth practices that are best viewed as a habit and not a mere methodology for data science. As such, this paper mainly offers critical examinations and reflection and not “solutions.”}
}
@article{BUCHBERGER2006470,
title = {Theorema: Towards computer-aided mathematical theory exploration},
journal = {Journal of Applied Logic},
volume = {4},
number = {4},
pages = {470-504},
year = {2006},
note = {Towards Computer Aided Mathematics},
issn = {1570-8683},
doi = {https://doi.org/10.1016/j.jal.2005.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S1570868305000716},
author = {Bruno Buchberger and Adrian Crǎciun and Tudor Jebelean and Laura Kovács and Temur Kutsia and Koji Nakagawa and Florina Piroi and Nikolaj Popov and Judit Robu and Markus Rosenkranz and Wolfgang Windsteiger},
keywords = {Mathematical assistant, Automated reasoning, Theory exploration, “Lazy Thinking”, Theorema},
abstract = {Theorema is a project that aims at supporting the entire process of mathematical theory exploration within one coherent logic and software system. This survey paper illustrates the style of Theorema-supported mathematical theory exploration by a case study (the automated synthesis of an algorithm for the construction of Gröbner Bases) and gives an overview on some reasoners and organizational tools for theory exploration developed in the Theorema project.}
}
@incollection{JACOBLOPES202177,
title = {Chapter 5 - Assistant’s tools toward life cycle assessment},
editor = {Eduardo Jacob-Lopes and Leila Queiroz Zepka and Mariany Costa Deprá},
booktitle = {Sustainability Metrics and Indicators of Environmental Impact},
publisher = {Elsevier},
pages = {77-90},
year = {2021},
isbn = {978-0-12-823411-2},
doi = {https://doi.org/10.1016/B978-0-12-823411-2.00006-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128234112000062},
author = {Eduardo Jacob-Lopes and Leila Queiroz Zepka and Mariany Costa Deprá},
keywords = {Theoretical approach, Sustainability metrics, Life cycle thinking, Social life cycle assessment, Life cycle costing, Life cycle sustainability assessment},
abstract = {This chapter aims to elucidate the main assistant’s tools created to assist in a global assessment of sustainability metrics and indicators. To this end, the chapter will provide a general review of the main assistant’s tools, considering the Life cycle thinking, social life cycle assessment, life cycle cost, and life cycle sustainability assessment tool. In addition, it guides some necessary criteria to be followed to apply each of these tools. Finally, this compilation of information strongly suggests, at the end of the chapter, the application of sensitivity analyses at the end of the process evaluations.}
}
@article{LI2023110701,
title = {Graph neural network architecture search for rotating machinery fault diagnosis based on reinforcement learning},
journal = {Mechanical Systems and Signal Processing},
volume = {202},
pages = {110701},
year = {2023},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2023.110701},
url = {https://www.sciencedirect.com/science/article/pii/S088832702300609X},
author = {Jialin Li and Xuan Cao and Renxiang Chen and Xia Zhang and Xianzhen Huang and Yongzhi Qu},
keywords = {Rotating machinery, Fault diagnosis, Graph neural network, Neural architecture search, Reinforcement learning},
abstract = {In order to improve the accuracy of fault diagnosis, researchers are constantly trying to develop new diagnostic models. However, limited by the inherent thinking of human beings, it has always been difficult to build a pioneering architecture for rotating machinery fault diagnosis. In order to solve this problem, this paper uses reinforcement learning algorithm based on adjacency matrix to carry out network architecture search (NAS) of rotating machinery fault diagnosis model. A reinforcement learning agent for deep deterministic policy gradient (DDPG) is developed based on actor–critic neural networks. The observation state of reinforcement learning is used to develop the graph neural network (GNN) diagnosis model, and the diagnosis accuracy is fed back to the agent as a reward for updating the reinforcement learning parameters. The MFPT bearing fault datasets and the developed gear pitting fault experimental data are used to validate the proposed network architecture search method based on reinforcement learning (RL-NAS). The proposed method is proved to be practical and effective in various aspects such as fault diagnosis ability, search space, search efficiency and multi-working condition performance.}
}
@article{HIPOLITO2023103510,
title = {Breaking boundaries: The Bayesian Brain Hypothesis for perception and prediction},
journal = {Consciousness and Cognition},
volume = {111},
pages = {103510},
year = {2023},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2023.103510},
url = {https://www.sciencedirect.com/science/article/pii/S1053810023000478},
author = {Inês Hipólito and Michael Kirchhoff},
keywords = {Bayesian Brain Hypothesis, Modularity of the Mind, Cognitive processes, Informational boundaries},
abstract = {This special issue aims to provide a comprehensive overview of the current state of the Bayesian Brain Hypothesis and its standing across neuroscience, cognitive science and the philosophy of cognitive science. By gathering cutting-edge research from leading experts, this issue seeks to showcase the latest advancements in our understanding of the Bayesian brain, as well as its potential implications for future research in perception, cognition, and motor control. A special focus to achieve this aim is adopted in this special issue, as it seeks to explore the relation between two seemingly incompatible frameworks for the understanding of cognitive structure and function: the Bayesian Brain Hypothesis and the Modularity Theory of the Mind. In assessing the compatibility between these theories, the contributors to this special issue open up new pathways of thinking and advance our understanding of cognitive processes.}
}
@article{STROMMER2022134322,
title = {Forward-looking impact assessment – An interdisciplinary systematic review and research agenda},
journal = {Journal of Cleaner Production},
volume = {377},
pages = {134322},
year = {2022},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2022.134322},
url = {https://www.sciencedirect.com/science/article/pii/S095965262203894X},
author = {Kiia Strömmer and Jarrod Ormiston},
keywords = {Impact assessment, Forward-looking, Temporality, Futures thinking},
abstract = {New and established ventures are under increasing pressure to consider how their current actions impact our future world. Whilst many practitioners are paying greater attention to their future impact, most impact assessment research focuses on the retrospective measurement of impact. Limited studies have explored how impact assessment is used as a tool to forecast or predict the intended impact of organisational action. This study aims to overcome this gap by exploring forward-looking approaches to impact assessment. An interdisciplinary systematic review of the impact assessment literature was conducted to answer the question: “How and why do organisations utilise forward-looking, future-oriented approaches to impact assessment?“. The findings elaborate on the common research themes, challenges, and gaps in understanding forward-looking impact assessment. An integrated process model is developed to show the relationships between various antecedents, methods, and effects of forward-looking impact assessment. Based on the review, the paper puts forward a research agenda to provoke further inquiry on forward-looking, future-oriented approaches to impact assessments related to four research themes: uncertainty, values and assumptions, stakeholder cooperation, and learning. The study contributes to the impact assessment literature by providing an overview of how the current literature comprehends forward-looking approaches and insights into how a more holistic view of temporality in impact assessment can be developed.}
}
@article{201119,
title = {Evolution of cognition might be down to brain chemistry},
journal = {New Scientist},
volume = {210},
number = {2806},
pages = {19},
year = {2011},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(11)60726-4},
url = {https://www.sciencedirect.com/science/article/pii/S0262407911607264},
abstract = {The prefrontal cortex, the “thinking” part of our brain, has a radically different chemical balance to that of chimps and macaques}
}
@article{ZHAO2023106750,
title = {A cooperative population-based iterated greedy algorithm for distributed permutation flowshop group scheduling problem},
journal = {Engineering Applications of Artificial Intelligence},
volume = {125},
pages = {106750},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106750},
url = {https://www.sciencedirect.com/science/article/pii/S095219762300934X},
author = {Hui Zhao and Quan-Ke Pan and Kai-Zhou Gao},
keywords = {Distributed permutation flowshop, Group scheduling, Total flowtime, Iterated greedy algorithm, Co-evolutionary},
abstract = {This paper studies the distributed permutation flowshop group scheduling problem (DPFGSP) with the consideration of minimizing total flowtime (TF), which has important applications in the modern manufacturing process. Based on the characteristics of the problem, a cooperative population-based iterated greedy (CPIG) algorithm is proposed by combining the advantages of the divide-and-rule policy, population-based evolution and iterated greedy algorithm. The CPIG divides the DPFGSP into two coupled sub-problems of group scheduling sub-problem and job scheduling sub-problem, and starts with a single population for simplicity. Unlike in the traditional cooperative co-evolutionary algorithms, the two-coupled sub-problems are addressed with a certain probability that can be determined in favor of solving the whole scheduling problem. Some advanced technologies are used, including the constructive heuristics based initialization, the critical factories based destruction and construction, the new best solution based population updating mechanism. The comprehensive experimental evaluation of 810 instances shows that the CPIG algorithm performs much better than the five state-of-the-art metaheuristics in the literature which are closely related to the considered scheduling problem.}
}
@article{GAGNE201889,
title = {When planning to survive goes wrong: predicting the future and replaying the past in anxiety and PTSD},
journal = {Current Opinion in Behavioral Sciences},
volume = {24},
pages = {89-95},
year = {2018},
note = {Survival circuits},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2018.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S2352154618300305},
author = {Christopher Gagne and Peter Dayan and Sonia J Bishop},
abstract = {We increase our probability of survival and wellbeing by minimizing our exposure to rare, extremely negative events. In this article, we examine the computations used to predict and avoid such events and to update our models of the world and action policies after their occurrence. We also consider how these computations might go wrong in anxiety disorders and Post Traumatic Stress Disorder (PTSD). We review evidence that anxiety is linked to increased simulations of the future occurrence of high cost negative events and to elevated estimates of the probability of occurrence of such events. We also review psychological theories of PTSD in the light of newer, computational models of updating through replay and simulation. We consider whether pathological levels of re-experiencing symptomatology might reflect problems reconciling the traumatic outcome with overly optimistic priors or difficulties terminating off-line simulation focused on negative events and over-generalization to states sharing features with those antecedent to the trauma.}
}
@article{CEGIELSKI2016283,
title = {Rethinking the role of Agent-Based Modeling in archaeology},
journal = {Journal of Anthropological Archaeology},
volume = {41},
pages = {283-298},
year = {2016},
issn = {0278-4165},
doi = {https://doi.org/10.1016/j.jaa.2016.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S0278416516000118},
author = {Wendy H. Cegielski and J. Daniel Rogers},
keywords = {ABM, Agent-Based Modeling, Archaeological methods, Simulation, Computational modeling},
abstract = {Agent-Based Modeling (ABM) represents a methodology with significant potential for altering archaeological analytical practice. The continued growth in the number of publications that use ABM provides evidence for the significance of this emerging approach. However, the scope of the research topics investigated has not increased accordingly. A consensus exists among ABM practitioners, that once generally accepted by the field, ABM can make revolutionary advances within the overall archaeological research paradigm. Unresolved concerns within the archaeological community center on whether ABMs are sufficiently grounded in empirical data, are aligned with theoretical trajectories, and on the difficult task of mastering the computational systems. It is worth exploring these aspects of the disjuncture between the mainstream and ABM practitioners for two reasons – to frame a discussion of qualities of ABM that make it transformative and to provide guidelines for broadening ABM’s applicability. With capacity-building in mind, offered here is a practical reference for the non-practitioner archaeologist considering ABM. A glossary is included of key terms used in the text to describe ABM methods and theory.}
}
@article{NKONGOLO2022182,
title = {Using Deep Packet Inspection Data to Examine Subscribers on the Network},
journal = {Procedia Computer Science},
volume = {215},
pages = {182-191},
year = {2022},
note = {4th International Conference on Innovative Data Communication Technology and Application},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.12.021},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922020920},
author = {Mike Nkongolo and Jacobus Phillipus {van Deventer} and Sydney Mambwe Kasongo},
keywords = {Deep packet inspection, machine learning, UGRansome, telecommunication, data science},
abstract = {This article proposes the creation of the deep packet inspection (DPI) dataset to study subscribers’ behavior on the network, applying ensemble learning to this dataset, and comparing it with the UGRansome dataset. The subscriber can be thought of as a person or a group of users using a network service or connectivity. The DPI features represent the subscriber network usage, and the ensemble learning approach is implemented on the DPI dataset to predict the subscriber's service category on the network. The classification and prediction problem addressed on the DPI dataset reached a precision of 100%. The paper predicts that the web and streaming categories with Netflix, Facebook, and YouTube services will be the most utilized in the next few years. This study will lead to a better understanding of the idiosyncratic behavior of active subscribers on the network, exposing novel network anomalies and facilitating the development of novel DPI systems.}
}
@article{BRYANSMITH2023105405,
title = {Real-time social media sentiment analysis for rapid impact assessment of floods},
journal = {Computers & Geosciences},
volume = {178},
pages = {105405},
year = {2023},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2023.105405},
url = {https://www.sciencedirect.com/science/article/pii/S0098300423001097},
author = {Lydia Bryan-Smith and Jake Godsall and Franky George and Kelly Egode and Nina Dethlefs and Dan Parsons},
keywords = {Social media, Sentiment analysis, Flooding, Artificial Intelligence},
abstract = {Traditional approaches to flood modelling mostly rely on hydrodynamic physical simulations. While these simulations can be accurate, they are computationally expensive and prohibitively so when thinking about real-time prediction based on dynamic environmental conditions. Alternatively, social media platforms such as Twitter are often used by people to communicate during a flooding event, but discovering which tweets hold useful information is the key challenge in extracting information from posts in real time. In this article, we present a novel model for flood forecasting and monitoring that makes use of a transformer network that assesses the severity of a flooding situation based on sentiment analysis of the multimodal inputs (text and images). We also present an experimental comparison of a range of state-of-the-art deep learning methods for image processing and natural language processing. Finally, we demonstrate that information induced from tweets can be used effectively to visualise fine-grained geographical flood-related information dynamically and in real-time.}
}
@incollection{KUMAR2024147,
title = {Chapter Eight - Machine learning model for teaching and emotional intelligence},
editor = {Muskan Garg and Deepika Koundal},
booktitle = {Emotional AI and Human-AI Interactions in Social Networking},
publisher = {Academic Press},
pages = {147-168},
year = {2024},
isbn = {978-0-443-19096-4},
doi = {https://doi.org/10.1016/B978-0-443-19096-4.00014-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443190964000146},
author = {Mohit Kumar and Syam Machinathu Parambil Gangadharan and Nabanita Choudhury},
keywords = {Cognitive thinking, Design thinking, E-Learning, Emotional intelligence, Intelligent quotient, Social neuroscience},
abstract = {Education that is ongoing and permanent for the purpose of adaptable up-skilling and retraining has been identified as a contributory factor, along with a relentless race against time, fast scientific progress, and unanticipated challenges. Students in postsecondary learning require mechanisms that can enable long-term, dependable knowledge production and storage, and this is particularly true in the age after a pandemic that occurred during the development of new technologies. There has been an explosion of e-learning platforms and methodologies that have been developed to remedy this issue; however, not all of them have been as successful as would be ideal. This type of new knowledge is very difficult to execute properly; it needs complex, careful educational and interface design to perform as well as it does and keep learners interested. This framework was designed and developed in this study. This technology was employed to support a cutting-edge pedagogic study based on neuroscience that was offered to faculty at universities of higher education. This framework was designed and developed in this study. This technology was employed to support a cutting-edge pedagogic study based on neuroscience that was offered to faculty at universities of higher education. Having a high intelligence quotient does not guarantee a successful and happy life. Success also necessitates self-awareness and emotional control. Our idea was to create a computational paradigm that would educate students in both programming and emotional intelligence. This experiment was successful in addressing the problem of excessive screen use by providing a student interface without displays.}
}
@article{BINKOWSKA201435,
title = {Computational and experimental study of charge distribution in the α-disulfonyl carbanions},
journal = {Journal of Molecular Structure},
volume = {1062},
pages = {35-43},
year = {2014},
issn = {0022-2860},
doi = {https://doi.org/10.1016/j.molstruc.2014.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0022286014000258},
author = {Iwona Binkowska and Jacek Koput and Arnold Jarczewski},
keywords = {Proton transfer, Carbon acids, Charge distribution,  computation},
abstract = {The electron densities of the disulfonyl carbanions were determined using experimental 13C chemical shifts. The 13C NMR spectra and electron densities for the disulfonyl, nitro, and cyano carbon acids were calculated at the MP2/cc-pVDZ level of theory. The calculated chemical shifts for disulfonyl carbanions show satisfying correlation with our own experimental data. The calculated π electron densities at the Cα atom correspond roughly to the “experimental” π electron densities estimated from the 13C chemical shifts. The natural charges at Cα in disulfonyl stabilized carbanions are significantly more negative than with other types of carbanions, partly because of the significant negative natural charge of the α carbon in parent carbon acids. The calculated increase of the negative charge caused by ionization is larger for sulfonyl carbon acids than for cyano- and nitroalkanes. The 13C chemical shifts δ of Cα in disulfonyl stabilized carbanions decrease with more negative calculated negative natural charge at Cα, with a slope of 220ppm/electron. The influence of phenyl ring para-substitution on the charge distribution in carbanions and relationship between the 13C chemical shifts and charge density have been discussed. It appears that the π electron density in these planar or nearly planar carbanions has a decisive impact on the chemical shifts.}
}
@article{KNIGHT20061084,
title = {‘When I first came here, I thought medicine was black and white’: Making sense of medical students’ ways of knowing},
journal = {Social Science & Medicine},
volume = {63},
number = {4},
pages = {1084-1096},
year = {2006},
issn = {0277-9536},
doi = {https://doi.org/10.1016/j.socscimed.2006.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S0277953606000451},
author = {Lynn Valerie Knight and Karen Mattick},
keywords = {Medical training, Professional knowledge, Epistemology, Evidence-based medicine, United Kingdom},
abstract = {Personal beliefs about what knowledge is and how we understand, integrate and apply knowledge (known as personal epistemologies) are entrenched in the process of decision-making. Evidence-based medicine in all its forms brings with it the need for an ever more sophisticated appreciation of individual patients’ perspectives and ‘scientific’ perspectives within the clinical encounter. However, current theoretical perspectives on personal epistemology focus more on scientific ways of knowing where knowledge is abstracted and logical. We conducted semi-structured interviews to investigate medical students’ personal epistemological thinking towards the end of their second year of training at a new medical school in the South West of England. Whilst responses were varied, students appeared to express predominantly simplistic levels of epistemological thinking according to current developmental models of personal epistemology. However, the process of professional identity formation together with epistemological thinking brought together both scientific and experiential ways of knowing in a way that has largely been ignored by current theorists in the domain of personal epistemology.}
}
@article{FU2022107,
title = {Everyday Creativity is Associated with Increased Frontal Electroencephalography Alpha Activity During Creative Ideation},
journal = {Neuroscience},
volume = {503},
pages = {107-117},
year = {2022},
issn = {0306-4522},
doi = {https://doi.org/10.1016/j.neuroscience.2022.09.005},
url = {https://www.sciencedirect.com/science/article/pii/S0306452222004596},
author = {Lei Fu and Jia Zhao and Jiangzhou Sun and Yuchi Yan and Mujie Ma and Qunlin Chen and Jiang Qiu and Wenjing Yang},
keywords = {Everyday creativity, Alpha power, Alpha coherence, Creative ideation, Frontal cortex},
abstract = {Everyday creativity is the basic ability of human survival and penetrates every aspect of life. Nevertheless, the neural mechanisms underlying everyday creativity was largely unexplored. In this study, seventy-five participants completed the creative behaviour inventory, a tool for assessing creative behaviour in daily life. The participants also completed the alternate uses task (AUT) during an electroencephalography (EEG) assessment to evaluate creative thinking. Alpha power was used to quantify neural oscillations during the creative process, while alpha coherence was used to quantify information communication between frontal regions and other sites during creative ideation. Moreover, these two task-related quantitative measures were combined to investigate the relationship between individual differences in everyday creativity and EEG alpha activity during creative idea generation. Compared with the reference period, increased alpha power was observed in the frontal cortex of the right hemisphere and increased functional coupling was observed between frontal and parietal/temporal regions during the activation period. Interestingly, individual differences in everyday creativity were associated with distinct patterns of EEG alpha activity. Specifically, individuals with higher everyday creativity had increased alpha power in the frontal cortex, and increased changes in coherence in frontal-temporal regions of the right hemisphere while performing the AUT. It might indicate that individuals with higher everyday creativity had an enhanced ability to focus on internal information processing and control bottom-up stimuli, as well as better selection of novel semantic information when performing creative ideation tasks.}
}
@article{KARI2022102843,
title = {The Sabatier principle as a tool for discovery and engineering of industrial enzymes},
journal = {Current Opinion in Biotechnology},
volume = {78},
pages = {102843},
year = {2022},
issn = {0958-1669},
doi = {https://doi.org/10.1016/j.copbio.2022.102843},
url = {https://www.sciencedirect.com/science/article/pii/S095816692200177X},
author = {Jeppe Kari and Kay Schaller and Gustavo A Molina and Kim Borch and Peter Westh},
abstract = {The recent breakthrough in all-atom, protein structure prediction opens new avenues for a range of computational approaches in enzyme design. These new approaches could become instrumental for the development of technical biocatalysts, and hence our transition toward more sustainable industries. Here, we discuss one approach, which is well-known within inorganic catalysis, but essentially unexploited in biotechnology. Specifically, we review examples of linear free-energy relationships (LFERs) for enzyme reactions and discuss how LFERs and the associated Sabatier Principle may be implemented in algorithms that estimate kinetic parameters and enzyme performance based on model structures.}
}
@article{KIREEV1994143,
title = {Approximate molecular electrostatic potential computations: applications to quantitative structure-activity relationships},
journal = {Journal of Molecular Structure: THEOCHEM},
volume = {304},
number = {2},
pages = {143-150},
year = {1994},
issn = {0166-1280},
doi = {https://doi.org/10.1016/S0166-1280(96)80006-6},
url = {https://www.sciencedirect.com/science/article/pii/S0166128096800066},
author = {Dmitry B. Kireev and Valery I. Fetisov and Nikolai S. Zefirov},
abstract = {Two new methods for calculating molecular electrostatic potentials are considered, taking into account QSAR requirements. The first of these is based on quantum chemical approximations; the other uses the topology of molecules. A program for displaying potential contour maps generated by various methods is presented. Examples of the successful use of these methods are given.}
}
@article{BIDERMAN2020542,
title = {What Are Memories For? The Hippocampus Bridges Past Experience with Future Decisions},
journal = {Trends in Cognitive Sciences},
volume = {24},
number = {7},
pages = {542-556},
year = {2020},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2020.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S1364661320301066},
author = {Natalie Biderman and Akram Bakkour and Daphna Shohamy},
keywords = {memory, decision-making, amnesia, hippocampus, value},
abstract = {Many decisions require flexible reasoning that depends on inference, generalization, and deliberation. Here, we review emerging findings indicating that the hippocampus, known for its role in long-term memory, contributes to these flexible aspects of value-based decision-making. This work offers new insights into the role of memory in decision-making and suggests that memory may shape decisions even in situations that do not appear, at first glance, to depend on memory at all. Uncovering the pervasive role of memory in decision-making challenges the way we define what memory is and what it does, suggesting that memory’s primary purpose may be to guide future behavior and that storing a record of the past is just one way to do so.}
}
@article{BANERJEE2015143,
title = {Z*-numbers: Augmented Z-numbers for machine-subjectivity representation},
journal = {Information Sciences},
volume = {323},
pages = {143-178},
year = {2015},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2015.06.026},
url = {https://www.sciencedirect.com/science/article/pii/S0020025515004582},
author = {Romi Banerjee and Sankar K. Pal},
keywords = {Artificial-mindfulness, Machine-consciousness, Machine-qualia, Machine-self, Perception-operators, Thinking machine},
abstract = {Envisaging a futuristic environment of man–machine and machine–machine synergy, this article documents our research on the augmented Z-numbers, the Z*-numbers, for machine-perception encapsulation. The Z*-numbers have been envisioned as operands of endogenous machine-mind processes underlying bespoke comprehension of the real world. Besides information-certainty, as in a Z-number, a Z*-number incorporates context, time and affects as essential factors of subjectivity representation. We have proposed: (a) definitions for certainty and affect parameters—arising out of socio-cultural influences on machine-knowledge, (b) a Z*-number based rudimentary procedure for natural-language comprehension emulation, and (c) primitive perception-operators for ‘machine-mentalese’ simulation using Z*-number information-equivalents. Our work draws from non-symbolic theories of cognition and ‘mindfulness’, human-mind processes—studied through behavioral experiments, and theories of the ‘self’ and ‘qualia’. The article includes detailed discussions of these experiments and consequent insights, analysis of a theoretical run-through of the defined procedure, and correspondence-studies between the Z*-number paradigm and philosophies of the self. Our research raises questions on cognitive biases and autogenous mind-processes that highlight crucial practical challenges in the current realization of a synthetic-mind. All ideas herein aim to contribute to studies on the ‘self’ and its machine-embodiment for the synthesis of an empathetic machine-mind.}
}
@article{GARLING1994355,
title = {Computational-process modelling of household activity scheduling},
journal = {Transportation Research Part B: Methodological},
volume = {28},
number = {5},
pages = {355-364},
year = {1994},
issn = {0191-2615},
doi = {https://doi.org/10.1016/0191-2615(94)90034-5},
url = {https://www.sciencedirect.com/science/article/pii/0191261594900345},
author = {Tommy Gärling and Mei-Po Kwan and Reginald G. Golledge},
abstract = {Models of households' travel choices are an important focus of research. For some time, it has been known that such models need to incorporate how travel depends on activity choices. It is argued that production system models constitute an alternative or necessary complementary approach if the goal is to develop models of interdependent activity and travel choices, or activity scheduling, which are based on behavioral science theories of higher cognitive processes. Several computational-process models (CPMs) which implement production systems as computer programs are reviewed. Currently, no encompassing CPM exists but some may be possible to integrate in a descriptive model of activity scheduling.}
}
@article{GARCIACAIRASCO2021107930,
title = {Searching for a paradigm shift in the research on the epilepsies and associated neuropsychiatric comorbidities. From ancient historical knowledge to the challenge of contemporary systems complexity and emergent functions},
journal = {Epilepsy & Behavior},
volume = {121},
pages = {107930},
year = {2021},
note = {NEWroscience 2018},
issn = {1525-5050},
doi = {https://doi.org/10.1016/j.yebeh.2021.107930},
url = {https://www.sciencedirect.com/science/article/pii/S1525505021001645},
author = {Norberto Garcia-Cairasco and Guilherme Podolsky-Gondim and Julian Tejada},
keywords = {Ancestral knowledge, Superstitious versus scientific knowledge, Epilepsies and neuropsychiatric comorbidities, Clinical semiology and neurosurgery methods, experimental and computational modeling, Complexity and emergent properties},
abstract = {In this review, we will discuss in four scenarios our challenges to offer possible solutions for the puzzle associated with the epilepsies and neuropsychiatric comorbidities. We need to recognize that (1) since quite old times, human wisdom was linked to the plural (distinct global places/cultures) perception of the Universe we are in, with deep respect for earth and nature. Plural ancestral knowledge was added with the scientific methods; however, their joint efforts are the ideal scenario; (2) human behavior is not different than animal behavior, in essence the product of Darwinian natural selection; knowledge of animal and human behavior are complementary; (3) the expression of human behavior follows the same rules that complex systems with emergent properties, therefore, we can measure events in human, clinical, neurobiological situations with complexity systems’ tools; (4) we can use the semiology of epilepsies and comorbidities, their neural substrates, and potential treatments (including experimental/computational modeling, neurosurgical interventions), as a source and collection of integrated big data to predict with them (e.g.: machine/deep learning) diagnosis/prognosis, individualized solutions (precision medicine), basic underlying mechanisms and molecular targets. Once the group of symptoms/signals (with a myriad of changing definitions and interpretations over time) and their specific sequences are determined, in epileptology research and clinical settings, the use of modern and contemporary techniques such as neuroanatomical maps, surface electroencephalogram and stereoelectroencephalography (SEEG) and imaging (MRI, BOLD, DTI, SPECT/PET), neuropsychological testing, among others, are auxiliary in the determination of the best electroclinical hypothesis, and help design a specific treatment, usually as the first attempt, with available pharmacological resources. On top of ancient knowledge, currently known and potentially new antiepileptic drugs, alternative treatments and mechanisms are usually produced as a consequence of the hard, multidisciplinary, and integrated studies of clinicians, surgeons, and basic scientists, all over the world. The existence of pharmacoresistant patients, calls for search of other solutions, being along the decades the surgeries the most common interventions, such as resective procedures (i.e., selective or standard lobectomy, lesionectomy), callosotomy, hemispherectomy and hemispherotomy, added by vagus nerve stimulation (VNS), deep brain stimulation (DBS), neuromodulation, and more recently focal minimal or noninvasive ablation. What is critical when we consider the pharmacoresistance aspect with the potential solution through surgery, is still the pursuit of localization-dependent regions (e.g.: epileptogenic zone (EZ)), in order to decide, no matter how sophisticated are the brain mapping tools (EEG and MRI), the size and location of the tissue to be removed. Mimicking the semiology and studying potential neural mechanisms and molecular targets – by means of experimental and computational modeling – are fundamental steps of the whole process. Concluding, with the conjunction of ancient knowledge, coupled to critical and creative contemporary, scientific (not dogmatic) clinical/surgical, and experimental/computational contributions, a better world and of improved quality of life can be offered to the people with epilepsy and neuropsychiatric comorbidities, who are still waiting (as well as the scientists) for a paradigm shift in epileptology, both in the Basic Science, Computational, Clinical, and Neurosurgical Arenas. This article is part of the Special Issue “NEWroscience 2018”.}
}
@article{GOBERT201581,
title = {Using educational data mining to assess students’ skills at designing and conducting experiments within a complex systems microworld},
journal = {Thinking Skills and Creativity},
volume = {18},
pages = {81-90},
year = {2015},
note = {21st Century Skills: International Advancements and Recent Developments},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2015.04.008},
url = {https://www.sciencedirect.com/science/article/pii/S1871187115300067},
author = {Janice D. Gobert and Yoon Jeon Kim and Michael A. {Sao Pedro} and Michael Kennedy and Cameron G. Betts},
keywords = {Complex systems, Inquiry assessment, Performance assessment, Educational data mining, 21st century skills},
abstract = {Many national policy documents underscore the importance of 21st century skills, including critical thinking. In parallel, recent American frameworks for K-12 science education call for the development of critical thinking skills in science, also referred to as science inquiry skills/practices. Assessment of these skills is necessary, as indicated in policy documents; however, this has posed a great challenge for assessment researchers. Recently, some science learning environments seek to assess these science skills. These systems log all students’ interactions within the given system, and if fully leveraged, these logs provide rich assessments of inquiry skills. Here, we describe our environment Inq-ITS (inquiry intelligent tutoring system), that uses educational data mining to assess science inquiry skills, as described as 21st century skills. Additionally, here, we describe how we measure students’ skills at designing controlled experiments, a lynchpin skill of inquiry, in the context of complex systems. In doing so, our work addresses 21st century skill assessment in two ways, namely of inquiry (designing and conducting experiments), and in the context of complex systems, a key topic area of 21st century skills. We use educational data mining to develop our assessment of this skill for complex systems.}
}
@article{MANCINI2022102697,
title = {Out of sight, out of mind? The importance of local context and trust in understanding the social acceptance of biogas projects: A global scale review},
journal = {Energy Research & Social Science},
volume = {91},
pages = {102697},
year = {2022},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2022.102697},
url = {https://www.sciencedirect.com/science/article/pii/S2214629622002018},
author = {Eliana Mancini and Andrea Raggi},
keywords = {Social acceptance, Bioenergy, Non-technical barriers, Biogas, Socio-cultural factors, Life Cycle Thinking},
abstract = {Social acceptance is considered the main non-technical barrier to the development of bioenergy projects. This paper presents the results of a systematic literature review aimed to cover a lack in state-of-the-art literature about socio-cultural factors affecting the acceptance of biogas projects at a global scale. Moreover, this study is aimed at identifying which methods are used for studying this phenomenon, with a focus on the Life Cycle Thinking-oriented ones. Journal articles and conference proceedings were considered. At the end of the screening phases, 54 documents were selected and reviewed. The results showed that acceptance concerns two main issues: biogas plants and its presence in a given location and digestate application on fields. This review showed different results between high-income and low-middle-income countries. As regards the former, trust was the most mentioned socio-cultural factor. Education, as well as women's living conditions were considered important in the latter. However, a contextualisation of every outcome based on local peculiarities is needed in order to understand in a better way the accepting/refuting phenomena of the projects. As regards the second objective of this study, Life Cycle Analysis resulted the most widespread Life Cycle Thinking methodology. In conclusion, the outcomes of this work may be useful to identify the non-technical factors and the most suitable approach that should be considered for a successful implementation of site-specific biogas projects.}
}
@article{CRUTCHFIELD199411,
title = {The calculi of emergence: computation, dynamics and induction},
journal = {Physica D: Nonlinear Phenomena},
volume = {75},
number = {1},
pages = {11-54},
year = {1994},
issn = {0167-2789},
doi = {https://doi.org/10.1016/0167-2789(94)90273-9},
url = {https://www.sciencedirect.com/science/article/pii/0167278994902739},
author = {James P. Crutchfield},
abstract = {Defining structure and detecting the emergence of complexity in nature are inherently subjective, though essential, scientific activities. Despite the difficulties, these problems can be analyzed in terms of how model-building observers infer from measurements the computational capabilities embedded in nonlinear processes. An observer's notion of what is ordered, what is random, and what is complex in its environment depends directly on its computational resources: the amount of raw measurement data, of memory, and of time available for estimation and inference. The discovery of structure in an environment depends more critically and subtlely though on how those resources are organized. The descriptive power of the observer's chosen (or implicit) computational model class, for example, can be an overwhelming determinant in finding regularity in data. This paper presents an overview of an inductive framework-hierarchical ϵ-machine reconstruction—in which the emergence of complexity is associated with the innovation of new computational model classes. Complexity metrics for detecting structure and quantifying emergence, along with an analysis of the constraints on the dynamics of innovation, are outlined. Illustrative examples are drawn from the onset of unpredictability in nonlinear systems, finitary nondeterministic processes, and cellular automata pattern recognition. They demonstrate how finite inference resources drive the innovation of new structures and so lead to the emergence of complexity.}
}
@article{OREILLY2016547,
title = {Creative Engineers: Is Abductive Reasoning Encouraged enough in Degree Project Work?},
journal = {Procedia CIRP},
volume = {50},
pages = {547-552},
year = {2016},
note = {26th CIRP Design Conference},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2016.04.155},
url = {https://www.sciencedirect.com/science/article/pii/S221282711630395X},
author = {Ciarán J. O’Reilly},
keywords = {Creative design, abductive reasoning, education, degree project},
abstract = {Creativity is considered to be an important ability for an engineer to have, and it is therefore important that the development of this ability is structured into the education of engineering students, along with the ability to apply, analyse and evaluate based on existent knowledge. In this paper, the importance of abduction in creative engineering processes is briefly reviewed. It has been shown that abductive reasoning plays a key role in design as it is the only logical operation that introduces new ideas. Its encouragement within the KTH Royal Institute of Technology's degree projects at the Department of Aeronautical and Vehicle Engineering is analysed by examining the stated intended learning outcomes, and through interviewing students. It is found that abductive reasoning is not explicitly encouraged within the intended learning outcomes of these degree project courses, despite its importance in creative thinking. Although, it is very likely that at least some abduction takes place in the project work, its absence from the intended learning outcomes means that students may not have a felt need to demonstrate their abductive reasoning, and supervisors may encourage only non-creative deductive or inductive reasoning. A more explicit inclusion of abductive reasoning in the intended learning outcomes would help both students and supervisors to include creative thinking in the degree project courses.}
}
@article{JIANG2021106740,
title = {Accelerator for crosswise computing reduct},
journal = {Applied Soft Computing},
volume = {98},
pages = {106740},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106740},
url = {https://www.sciencedirect.com/science/article/pii/S1568494620306785},
author = {Zehua Jiang and Keyu Liu and Jingjing Song and Xibei Yang and Jinhai Li and Yuhua Qian},
keywords = {Accelerator, Attribute reduction, Cross computation, Rough set},
abstract = {Attribute reduction, as a technique for selecting qualified attributes which can satisfy the intended constraint related to considered measure, has been widely explored. Notably, one and only one reduct is derived through using one searching strategy in most cases. Nevertheless, only one reduct may be not enough for us to evaluate its effectiveness. To fill such gap, an approach of crosswise computing reduct is proposed for obtaining multiple reducts. The computation of reduct is realized through partitioning the whole data into several groups, and crosswise selecting some groups to form different subsets of data, then computing reducts over these different subsets of data. Moreover, to speed up the process of crosswise computing reduct, an acceleration strategy is designed. The main thinking of our acceleration strategy is to compute the reduct over different subsets of data on the basis of reduct over the whole data. The experimental results over 16 data sets show the following superiorities of our strategy: (1) our approach can decrease the elapsed time of crosswise computing reducts significantly; (2) our approach can not only provide reduct with higher stability, but also maintain the classification performance; (3) the attributes in reduct can provide more stable classification results.}
}
@article{KATTERFELDT201872,
title = {Physical computing with plug-and-play toolkits:Key recommendations for collaborative learning implementations},
journal = {International Journal of Child-Computer Interaction},
volume = {17},
pages = {72-82},
year = {2018},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2018.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212868917300351},
author = {Eva-Sophie Katterfeldt and Mutlu Cukurova and Daniel Spikol and David Cuartielles},
keywords = {Collaborative learning, Education, Motivation, Physical computing, Programming, Toolkit},
abstract = {Physical computing toolkits have long been used in educational contexts to learn about computational concepts by engaging in the making of interactive projects. This paper presents a comprehensive toolkit that can help educators teach programming with an emphasis on collaboration, and provides suggestions for its effective pedagogical implementation. The toolkit comprises the Talkoo kit with physical computing plug-and-play modules and a visual programming environment. The key suggestions are inspired by the results of the evaluation studies which show that children (aged 14–18 in a sample group of 34 students) are well motivated when working with the toolkit but lack confidence in the kit’s support for collaborative learning. If the intention is to move beyond tools and code in computer education to community and context, thus encouraging computational participation, collaboration should be considered as a key aspect of physical computing activities. Our approach expands the field of programming with physical computing for teenage children with a focus on empowering teachers and students with not only a kit but also its appropriate classroom implementation for collaborative learning.}
}
@article{FURLAN2022163,
title = {The earth vibrates with analogies: The Dirac sea and the geology of the vacuum},
journal = {Studies in History and Philosophy of Science},
volume = {93},
pages = {163-174},
year = {2022},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2022.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S0039368122000590},
author = {Stefano Furlan and Rocco Gaudenzi},
keywords = {Analogies, Analogical thinking, Heuristics, History of quantum physics, Vacuum, Spontaneous symmetry breaking},
abstract = {The debate around analogy in modern physics that focuses on its role as a logical inference often correspondingly overlooks its historical dimension and the other equally important functions and aspects that are intertwined with this dimension. Inspired by a close investigation of the primary sources and archival material of a few historical actors, this paper lays out a framework on analogy-making which preserves as much as possible its historical complexity. While not losing sight of the logical role, our framework puts a special emphasis on the heuristic process, and aims at offering to the historian and philosopher of science as well as the physicist some tools to capture the subtle functions of analogical reasoning involved in such a process. After having traced it out theoretically, we make use of this framework to interpret the growth of the ideas of two remarkable physicists dealing with the multifaceted notion of vacuum in 20th century physics. We first consider the trajectory followed by John A. Wheeler, between the 1960s and 1970s, towards (in his own words) a “geology of the vacuum”; and then examine, starting from the hitherto neglected Japanese reception of the idea of Dirac sea in the early 1930s, the pathway that led Yoichiro Nambu to the discovery of spontaneous symmetry breaking.}
}
@article{ROMEROCRISTOBAL2025502215,
title = {Why your doctor is not an algorithm: Exploring logical principles of different clinical inference methods using liver transplantation as a model},
journal = {Gastroenterología y Hepatología (English Edition)},
volume = {48},
number = {3},
pages = {502215},
year = {2025},
issn = {2444-3824},
doi = {https://doi.org/10.1016/j.gastre.2025.502215},
url = {https://www.sciencedirect.com/science/article/pii/S244438242500015X},
author = {Mario Romero-Cristóbal and Magdalena {Salcedo Plaza} and Rafael Bañares},
keywords = {Epistemology, Logic, Clinical reasoning, Machine learning, Liver transplantation, Epistemología, Lógica, Razonamiento clínico, Algoritmos automáticos, Trasplante hepático},
abstract = {The development of machine learning (ML) tools in many different medical settings is largely increasing. However, the use of the resulting algorithms in daily medical practice is still an unsolved challenge. We propose an epistemological approach (i.e., based on logical principles) to the application of computational tools in clinical practice. We rely on the classification of scientific inference into deductive, inductive, and abductive comparing the characteristics of ML tools with those derived from evidence-based medicine [EBM] and experience-based medicine, as paradigms of well-known methods for generation of knowledge. While we illustrate our arguments using liver transplantation as an example, this approach can be applied to other aspects of the specialty. Regarding EBM, it generates general knowledge that clinicians apply deductively, but the certainty of its conclusions is not guaranteed. In contrast, automatic algorithms primarily rely on inductive reasoning. Their design enables the integration of vast datasets and mitigates the emotional biases inherent in human induction. However, its poor capacity for abductive inference (a logical mechanism inherent to human clinical experience) constrains its performance in clinical settings characterized by uncertainty, where data are heterogeneous, results are highly influenced by context, or where prognostic factors can change rapidly.
Resumen
Asistimos en la actualidad a un desarrollo asombroso de las herramientas de aprendizaje automático, lo que se traduce en un número creciente de estudios que ensayan su desempeño ante diferentes problemas médicos. Sin embargo, la implementación de estos algoritmos en la práctica diaria permanece como un reto no completamente abordado. Proponemos una aproximación epistemológica (según los fundamentos lógicos) de la aplicabilidad de las herramientas computacionales en la clínica. Nos basamos en la clasificación de los tipos de inferencia científica en deductiva, inductiva y abductiva, y comparamos las características fundamentales de estas herramientas con las de otros métodos de trasferencia del conocimiento a la práctica diaria (medicina basada en la evidencia [MBE] y medicina basada en la experiencia). Ejemplificamos nuestros razonamientos con el caso del trasplante hepático, si bien pueden ser aplicables a otras áreas de la especialidad. La MBE genera conocimientos generales que el clínico aplica de manera deductiva, a pesar de lo cual la certeza de sus conclusiones no es segura. La inducción predomina en el caso de los algoritmos automáticos. Su diseño permite interrelacionar gran cantidad de datos de manera menos sensible a los sesgos emocionales propios de la inducción humana. Sin embargo, su menor capacidad para la inferencia abductiva (mecanismo lógico propio de la experiencia clínica humana) limita su desempeño en aquellos contextos clínicos que están sujetos a gran incertidumbre, en donde los datos son heterogéneos, los resultados están muy influenciados por el contexto o en los que los factores pronósticos pueden cambiar rápidamente.}
}
@article{KASNECI2023102274,
title = {ChatGPT for good? On opportunities and challenges of large language models for education},
journal = {Learning and Individual Differences},
volume = {103},
pages = {102274},
year = {2023},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2023.102274},
url = {https://www.sciencedirect.com/science/article/pii/S1041608023000195},
author = {Enkelejda Kasneci and Kathrin Sessler and Stefan Küchemann and Maria Bannert and Daryna Dementieva and Frank Fischer and Urs Gasser and Georg Groh and Stephan Günnemann and Eyke Hüllermeier and Stephan Krusche and Gitta Kutyniok and Tilman Michaeli and Claudia Nerdel and Jürgen Pfeffer and Oleksandra Poquet and Michael Sailer and Albrecht Schmidt and Tina Seidel and Matthias Stadler and Jochen Weller and Jochen Kuhn and Gjergji Kasneci},
keywords = {Large language models, Artificial intelligence, Education, Educational technologies},
abstract = {Large language models represent a significant advancement in the field of AI. The underlying technology is key to further innovations and, despite critical views and even bans within communities and regions, large language models are here to stay. This commentary presents the potential benefits and challenges of educational applications of large language models, from student and teacher perspectives. We briefly discuss the current state of large language models and their applications. We then highlight how these models can be used to create educational content, improve student engagement and interaction, and personalize learning experiences. With regard to challenges, we argue that large language models in education require teachers and learners to develop sets of competencies and literacies necessary to both understand the technology as well as their limitations and unexpected brittleness of such systems. In addition, a clear strategy within educational systems and a clear pedagogical approach with a strong focus on critical thinking and strategies for fact checking are required to integrate and take full advantage of large language models in learning settings and teaching curricula. Other challenges such as the potential bias in the output, the need for continuous human oversight, and the potential for misuse are not unique to the application of AI in education. But we believe that, if handled sensibly, these challenges can offer insights and opportunities in education scenarios to acquaint students early on with potential societal biases, criticalities, and risks of AI applications. We conclude with recommendations for how to address these challenges and ensure that such models are used in a responsible and ethical manner in education.}
}
@article{HUANG202233634,
title = {Transition from synaptic simulation to nonvolatile resistive switching behavior based on an Ag/Ag:ZnO/Pt memristor},
journal = {RSC Advances},
volume = {12},
number = {52},
pages = {33634-33640},
year = {2022},
issn = {2046-2069},
doi = {https://doi.org/10.1039/d2ra05483c},
url = {https://www.sciencedirect.com/science/article/pii/S2046206922032296},
author = {Yong Huang and Jiahao Yu and Yu Kong and Xiaoqiu Wang},
abstract = {ABSTRACT
The advent of memristors and the continuing research and development in the field of brain-inspired computing could allow realization of a veritable “thinking machine”. In this study, ZnO-based memristors were fabricated using a radio frequency magnetron sputtering method. The ZnO oxide layer was prepared by incorporating silver nanocrystals (NCs). Several synaptic functions, i.e. nonlinear transmission characteristics, short-term potentiation, long-term potentiation/depression, and pair-pulse facilitation, were imitated in the memristor successfully. Furthermore, the transition from synaptic behaviors to bipolar resistive switching behaviors of the device was also observed under repeated stimulus. It is speculated that the switching mechanism is due to the formation and rupture of the conductive Ag filaments and the corresponding electrochemical metallization. The experimental results demonstrate that the Ag/Ag:ZnO/Pt memristor with resistive switching and several synaptic behaviors has a potential application in neuromorphic computing and data storage systems.}
}
@article{BEDNORZ2024101169,
title = {Effects of domain-specific linguistic factors on the difficulty of mathematics tasks},
journal = {The Journal of Mathematical Behavior},
volume = {75},
pages = {101169},
year = {2024},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2024.101169},
url = {https://www.sciencedirect.com/science/article/pii/S0732312324000464},
author = {David Bednorz and Michael Kleine and Rudolf {vom Hofe}},
keywords = {Mathematical tasks, Task features, linguistic complexity, Task difficulty},
abstract = {Linguistic features as a task-related feature influence the difficulty of mathematical tasks. To reduce this influence (e.g., in testing situations), studies on linguistic simplification focus on modifying linguistic features. These studies show little or no effect on increasing test performance. An open question is whether a quantitative–exploratory approach with texts from a specific domain can be an additional model for reducing the linguistic influence on mathematical tasks. To answer this question, generalized linear mixed models were used to determine the effects of linguistic factors, the requirements of the items, and the effects of linguistic factors when differentiating the requirements of the items, while controlling for further person- and item-related effects. The results show that linguistic factors can have either a negative or positive influence on test performance. The findings indicate that for mathematics assessments and teaching, it might be essential to consider the influence of language factors and task requirements.}
}
@article{BINA2020102475,
title = {Beyond techno-utopia and its discontents: On the role of utopianism and speculative fiction in shaping alternatives to the smart city imaginary},
journal = {Futures},
volume = {115},
pages = {102475},
year = {2020},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2019.102475},
url = {https://www.sciencedirect.com/science/article/pii/S0016328719303374},
author = {Olivia Bina and Andy Inch and Lavínia Pereira},
keywords = {Smart cities, Ways of knowing, Urban imaginaries, Utopianism, Fiction},
abstract = {In recent years, the ösmart city’ has become established in policy and planning discourse, embedding visions of an urban future where ubiquitous technology offers efficient solutions to the pathologies of the contemporary city. In response, a rapidly growing social-scientific literature is critically exploring how the smart city imaginary (SCI) promotes ötechno-utopian’ fantasies, ignoring the risks of a technologically determined future. In this paper we begin by considering SCI as emblematic of the colonization of contemporary (urban) futures by vested interests, arguing for the need for diverse and plural imaginaries and thus for a re-engagement of the social sciences. We explore how critical social scientific contributions to shaping futures might be deepened through further engagement with utopian theory and speculative fiction, two traditions of future-orientated thinking that seek to combine critique with constructive thinking about alternatives. We therefore contribute to ö50 + 50 Theme 2: Framing Futures in 2068-the limits of and opportunities for futures research’ by 1) extending critique of contemporary claims about (smart urban) futures, and; 2) exploring how utopianism and fiction can expand ways of thinking, imagining and knowing futures.}
}
@article{SIMPSON2017166,
title = {Preparing industry for additive manufacturing and its applications: Summary & recommendations from a National Science Foundation workshop},
journal = {Additive Manufacturing},
volume = {13},
pages = {166-178},
year = {2017},
issn = {2214-8604},
doi = {https://doi.org/10.1016/j.addma.2016.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S2214860416302019},
author = {Timothy W. Simpson and Christopher B. Williams and Michael Hripko},
keywords = {Additive manufacturing, Design for additive manufacturing, STEM education, 3D printing, Workforce development},
abstract = {Accompanying the increasing advances and interest in Additive Manufacturing (AM) technologies is an increasing demand for an industrial workforce that is knowledgeable about the technologies and how to apply them to solve real-world problems. As a step towards addressing this knowledge gap, a workshop was held at the National Science Foundation (NSF) to discuss the educational needs to prepare industry for AM and its use in different fields. The workshop participants – 66 representatives from academia, industry, and government – identified several key educational themes: (1) AM processes and process/material relationships, (2) engineering fundamentals with an emphasis on materials science and manufacturing, (3) professional skills for problem solving and critical thinking, (4) design practices and tools that leverage the design freedom enabled by AM, and (5) cross-functional teaming and ideation techniques to nurture creativity. This paper summarizes the industry speakers and presentations from the workshop, along with several new educational partnerships identified by small working groups. Based on the presentations and partnerships, the following recommendations are offered to advance the AM workforce. First, ensure that all AM curricula provide students with an understanding of (i) AM and traditional manufacturing processes to enable them to effectively select the appropriate process for product realization; (ii) the relationships between AM processes and material properties; and (iii) “Design for AM”, including computational tools for AM design as well as frameworks for process selection, costing, and solution generation that take advantage of AM capabilities. Second, establish a national network for AM education that, by leveraging existing “distributed” educational models and NSF’s Advanced Technology Education (ATE) Programs, provides open source resources as well as packaged activities, courses, and curricula for all educational levels (K-Gray). Third, support K-12 educational programs in STEAM (STEM plus the arts) and across all formal and informal learning environments in order to learn the unique capabilities of AM while engaging students in hands-on, tactile, and visual learning activities to prepare them for jobs in industry while learning how to think differently when designing for AM. Fourth, provide support for collaborative and community-oriented maker spaces that promote awareness of AM among the public and provide AM training programs for incumbent workers in industry and students seeking alternative pathways to gain AM knowledge and experience. Recommendations for scaling and coordination across local, regional, and national levels are also discussed to create synergies among the proposed activities and existing efforts.}
}
@article{MAHOWALD2024517,
title = {Dissociating language and thought in large language models},
journal = {Trends in Cognitive Sciences},
volume = {28},
number = {6},
pages = {517-540},
year = {2024},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2024.01.011},
url = {https://www.sciencedirect.com/science/article/pii/S1364661324000275},
author = {Kyle Mahowald and Anna A. Ivanova and Idan A. Blank and Nancy Kanwisher and Joshua B. Tenenbaum and Evelina Fedorenko},
keywords = {large language models, language and thought, cognitive neuroscience, linguistic competence, computational modeling},
abstract = {Large language models (LLMs) have come closest among all models to date to mastering human language, yet opinions about their linguistic and cognitive capabilities remain split. Here, we evaluate LLMs using a distinction between formal linguistic competence (knowledge of linguistic rules and patterns) and functional linguistic competence (understanding and using language in the world). We ground this distinction in human neuroscience, which has shown that formal and functional competence rely on different neural mechanisms. Although LLMs are surprisingly good at formal competence, their performance on functional competence tasks remains spotty and often requires specialized fine-tuning and/or coupling with external modules. We posit that models that use language in human-like ways would need to master both of these competence types, which, in turn, could require the emergence of separate mechanisms specialized for formal versus functional linguistic competence.}
}
@article{WANG2022e09982,
title = {Applying the post-digital strategy of anexact architecture to non-standard design practices within the challenging construction contexts},
journal = {Heliyon},
volume = {8},
number = {8},
pages = {e09982},
year = {2022},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2022.e09982},
url = {https://www.sciencedirect.com/science/article/pii/S2405844022012701},
author = {Sining Wang and Dandan Lin},
keywords = {Design practice strategy, Post-digital architecture, Parametric design, Developing region, Non-standard architecture},
abstract = {New architectural forms offered by digital design approaches often appear incompatible with the prescribed precision and control in construction, especially in developing regions where advanced implementation means are limited. In response, this paper suggests working with design practice indeterminacy. Named ‘anexact architecture’, the post-digital design practice strategy presents a convergent diagram of seeking the feasible design solution space. It relies on the procedural parametric modelling to constantly integrate computation and humanisation, so that a rigorous built outcome is capable of accommodating project-specific idiosyncrasies and constraints. The demonstrator projects are discussed based on the combination of the Participatory Action Research method and the idea of anexact architecture. This paper aims to illustrate the peculiarity of anexact architecture and its ideology of treating design delivery uncertainties as essentials rather than negatives when practicing in a volatile construction context.}
}
@article{SURYARAJ2024124407,
title = {Block based motion estimation model using CNN with representative point matching algorithm for object tracking in videos},
journal = {Expert Systems with Applications},
volume = {255},
pages = {124407},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.124407},
url = {https://www.sciencedirect.com/science/article/pii/S0957417424012739},
author = {C.K. Suryaraj and M.R. Geetha},
keywords = {Motion Estimation, Object Tracking, CNN, RPM, SSIM, Video Sequence, Computation Time},
abstract = {Motion estimation is considered significant for tracking the movement of an object in video sequences, and it is widely used in various video processing applications. Traditionally, many researchers focus on pixel-based motion estimation for object tracking, but it experienced increased computation time and cost. To reduce computation time, the utilization of a block-based motion estimation approach for object tracking is a recent trend. The existing block-based approach faces difficulty in finding representative points within the intensity domain. Therefore, this current research merged the deep learning approach with a block-matching algorithm for achieving efficient object tracking. In this proposed work, initially, video sequences are collected from a benchmark video dataset. Then, the acquired video sequences are segmented into frames. From the segmented frames, current and previous frames are considered for motion estimation. Frames are sent for the data augmentation process in which the process of flipping, cropping, and rotation is carried out. Then, the augmented frames are sent into Convolutional Neural Network (CNN) for feature extraction. Representative Point Matching (RPM) is used to estimate the motion vector based on the extracted features. After estimating the motion vector, the similarity between two consecutive frames is found using Structural Similarity Index (SSIM) technique. Finally, based on the similarity score, the movement of an object in the video is tracked effectively. Simulation analysis of the proposed block-based motion estimation model is done by evaluating some performance metrics. RMSE, PSNR, Execution Time, SSIM, and accuracy obtained for the proposed model are 27.5, 26.5 db, 31 sec, 0.91, and 94 %. This analysis suggested that the proposed CNN-RPM motion estimation model performs better in tracking the movement of the object.}
}
@article{SHEFFIELD2024100333,
title = {Understanding Cognitive Behavioral Therapy for Psychosis Through the Predictive Coding Framework},
journal = {Biological Psychiatry Global Open Science},
volume = {4},
number = {4},
pages = {100333},
year = {2024},
issn = {2667-1743},
doi = {https://doi.org/10.1016/j.bpsgos.2024.100333},
url = {https://www.sciencedirect.com/science/article/pii/S2667174324000466},
author = {Julia M. Sheffield and Aaron P. Brinen and Brandee Feola and Stephan Heckers and Philip R. Corlett},
keywords = {Belief updating, CBTp, Persecutory delusions, Predictive coding, Psychotherapy, Volatility},
abstract = {Psychological treatments for persecutory delusions, particularly cognitive behavioral therapy for psychosis, are efficacious; however, mechanistic theories explaining why they work rarely bridge to the level of cognitive neuroscience. Predictive coding, a general brain processing theory rooted in cognitive and computational neuroscience, has increasing experimental support for explaining symptoms of psychosis, including the formation and maintenance of delusions. Here, we describe recent advances in cognitive behavioral therapy for psychosis–based psychotherapy for persecutory delusions, which targets specific psychological processes at the computational level of information processing. We outline how Bayesian learning models employed in predictive coding are superior to simple associative learning models for understanding the impact of cognitive behavioral interventions at the algorithmic level. We review hierarchical predictive coding as an account of belief updating rooted in prediction error signaling. We examine how this process is abnormal in psychotic disorders, garnering noisy sensory data that is made sense of through the development of overly strong delusional priors. We argue that effective cognitive behavioral therapy for psychosis systematically targets the way sensory data are selected, experienced, and interpreted, thus allowing for the strengthening of alternative beliefs. Finally, future directions based on these arguments are discussed.}
}
@incollection{BUJA2005391,
title = {14 - Computational Methods for High-Dimensional Rotations in Data Visualization},
editor = {C.R. Rao and E.J. Wegman and J.L. Solka},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {24},
pages = {391-413},
year = {2005},
booktitle = {Data Mining and Data Visualization},
issn = {0169-7161},
doi = {https://doi.org/10.1016/S0169-7161(04)24014-7},
url = {https://www.sciencedirect.com/science/article/pii/S0169716104240147},
author = {Andreas Buja and Dianne Cook and Daniel Asimov and Catherine Hurley},
abstract = {There exist many methods for visualizing complex relations among variables of a multivariate dataset. For pairs of quantitative variables, the method of choice is the scatterplot. For triples of quantitative variables, the method of choice is 3D data rotations. Such rotations let us perceive structure among three variables as shape of point scatters in virtual 3D space. Although not obvious, three-dimensional data rotations can be extended to higher dimensions. The mathematical construction of high-dimensional data rotations, however, is not an intuitive generalization. Whereas three-dimensional data rotations are thought of as rotations of an object in space, a proper framework for their high-dimensional extension is better based on rotations of a low-dimensional projection in high-dimensional space. The term “data rotations” is therefore a misnomer, and something along the lines of “high-to-low dimensional data projections” would be technically more accurate. To be useful, virtual rotations need to be under interactive user control, and they need to be animated. We therefore require projections not as static pictures but as movies under user control. Movies, however, are mathematically speaking one-parameter families of pictures. This article is therefore about one-parameter families of low-dimensional projections in high-dimensional data spaces. We describe several algorithms for dynamic projections, all based on the idea of smoothly interpolating a discrete sequence of projections. The algorithms lend themselves to the implementation of interactive visual exploration tools of high-dimensional data, such as so-called grand tours, guided tours and manual tours.}
}
@incollection{MARWALA202185,
title = {Chapter 7 - Bounded rational counterfactuals},
editor = {Tshilidzi Marwala},
booktitle = {Rational Machines and Artificial Intelligence},
publisher = {Academic Press},
pages = {85-96},
year = {2021},
isbn = {978-0-12-820676-8},
doi = {https://doi.org/10.1016/B978-0-12-820676-8.00012-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128206768000120},
author = {Tshilidzi Marwala},
keywords = {Rational counterfactuals, Bounded rationality, Optimization, Artificial intelligence},
abstract = {The rational counterfactual is identified from the factual and the knowledge of the laws that govern the relationships between the antecedent and the consequent of the factual, which maximizes the attainment of the desired consequent. However, the attainment of the desired consequent is not perfect and is, in fact, limited, which makes these counterfactuals bounded rational counterfactuals. In counterfactual thinking, factual statements such as “The COVID-19 afflicted the world, and the world economy contracted by 3%,” has the counterfactual “The COVID-19 did not afflict the world, and the world economy grew by 3%.” In this chapter, we use intelligent machines that use AI to build bounded rational counterfactuals. It is observed that intelligent machines can achieve bounded rational counterfactual better than human agents. In general, quantifiable factual easily has bounded rational counterfactuals when compared to qualitative counterfactuals.}
}
@article{SCHREIBER20142544,
title = {A few bad ideas on the way to the triumph of parallel computing},
journal = {Journal of Parallel and Distributed Computing},
volume = {74},
number = {7},
pages = {2544-2547},
year = {2014},
note = {Special Issue on Perspectives on Parallel and Distributed Processing},
issn = {0743-7315},
doi = {https://doi.org/10.1016/j.jpdc.2013.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0743731513002177},
author = {Robert Schreiber},
keywords = {Parallelism, Amdahl, Automatic parallelization, Accelerators, Exascale},
abstract = {Parallelism has become mainstream, in the multicore chip, the GPU, and the internet datacenter running MapReduce. In my field, large-scale scientific computing, parallelism now reigns triumphant. It was no simple, direct route that led to this triumph. Along the way, we were confused by ideas that, in retrospect, turned out to be distractions and errors. The thinking behind them was reasonable, but wrong. One can learn from a dissection of mistakes, so I will retell part of the story here.}
}
@article{BEATY2017189,
title = {Creative constraints: Brain activity and network dynamics underlying semantic interference during idea production},
journal = {NeuroImage},
volume = {148},
pages = {189-196},
year = {2017},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2017.01.012},
url = {https://www.sciencedirect.com/science/article/pii/S1053811917300125},
author = {Roger E. Beaty and Alexander P. Christensen and Mathias Benedek and Paul J. Silvia and Daniel L. Schacter},
keywords = {Creativity, Divergent thinking, Cognitive control, Functional connectivity, Default network, Executive control network},
abstract = {Functional neuroimaging research has recently revealed brain network interactions during performance on creative thinking tasks—particularly among regions of the default and executive control networks—but the cognitive mechanisms related to these interactions remain poorly understood. Here we test the hypothesis that the executive control network can interact with the default network to inhibit salient conceptual knowledge (i.e., pre-potent responses) elicited from memory during creative idea production. Participants studied common noun-verb pairs and were given a cued-recall test with corrective feedback to strengthen the paired association in memory. They then completed a verb generation task that presented either a previously studied noun (high-constraint) or an unstudied noun (low-constraint), and were asked to “think creatively” while searching for a novel verb to relate to the presented noun. Latent Semantic Analysis of verbal responses showed decreased semantic distance values in the high-constraint (i.e., interference) condition, which corresponded to increased neural activity within regions of the default (posterior cingulate cortex and bilateral angular gyri), salience (right anterior insula), and executive control (left dorsolateral prefrontal cortex) networks. Independent component analysis of intrinsic functional connectivity networks extended this finding by revealing differential interactions among these large-scale networks across the task conditions. The results suggest that interactions between the default and executive control networks underlie response inhibition during constrained idea production, providing insight into specific neurocognitive mechanisms supporting creative cognition.}
}
@article{SENVAR20161140,
title = {Hospital Site Selection via Hesitant Fuzzy TOPSIS},
journal = {IFAC-PapersOnLine},
volume = {49},
number = {12},
pages = {1140-1145},
year = {2016},
note = {8th IFAC Conference on Manufacturing Modelling, Management and Control MIM 2016},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2016.07.656},
url = {https://www.sciencedirect.com/science/article/pii/S2405896316309296},
author = {Ozlem Senvar and Irem Otay and Eda Bolturk},
keywords = {Facility Layout, Location Selection, Multi criteria decision making (MCDM), TOPSIS, Hesitant fuzzy set (HFS)},
abstract = {This study handles the problem of establishing a well-organized and distributed network of a hospital that delivers its services to the target population. We propose a new multi criteria decision making (MCDM) process that integrates hesitant fuzzy sets (HFSs) to Technique for Order Preference by Similarity to Ideal Solution (TOPSIS). The MCDM process defined under uncertainties are perfectly defined by HFSs reflecting comprehensively hesitant thinking of decision makers. Our proposed methodology is implemented to select the optimum site for a new hospital in Istanbul.}
}
@article{BAUSO201776,
title = {Consensus via multi-population robust mean-field games},
journal = {Systems & Control Letters},
volume = {107},
pages = {76-83},
year = {2017},
issn = {0167-6911},
doi = {https://doi.org/10.1016/j.sysconle.2017.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167691117301287},
author = {D. Bauso},
keywords = {Synchronization, Consensus, Mean-field games},
abstract = {In less prescriptive environments where individuals are told ‘what to do’ but not ‘how to do’, synchronization can be a byproduct of strategic thinking, prediction, and local interactions. We prove this in the context of multi-population robust mean-field games. The model sheds light on a multi-scale phenomenon involving fast synchronization within the same population and slow inter-cluster oscillation between different populations.}
}
@article{BERNUS201583,
title = {Enterprise architecture: Twenty years of the GERAM framework},
journal = {Annual Reviews in Control},
volume = {39},
pages = {83-93},
year = {2015},
issn = {1367-5788},
doi = {https://doi.org/10.1016/j.arcontrol.2015.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S1367578815000097},
author = {Peter Bernus and Ovidiu Noran and Arturo Molina},
abstract = {Apart from the 20-year anniversary in 2014 of the first publication of the GERAM (‘Generalised Enterprise Reference Architecture and Methodology’) Enterprise Architecture Framework, the timeliness of this paper lies in the new interest in the use of systems theory in enterprise architecture (EA), and consequently, ‘light-weight’ architecture frameworks (AFs). Thus, this paper is about the use of systems thinking and systems theory in EA and about how it is possible to reconcile and understand, based on a single overarching framework, the interplay of two major enterprise change endeavours: on one hand enterprise engineering (i.e. deliberate change) and on the other hand evolutionary, organic change. The paper also demonstrates how such change processes can be illustrated by employing systems thinking to construct dynamic business models; the evolution of these concepts is exemplified using past applications in networked enterprise building, and more recent proposals in environmental-, disaster- and healthcare management. Finally, the paper attempts to plot the way GERAM, as a framework to think about the creation and evolution of complex socio-technical systems, will continue to contribute to the society in the context of future challenges and emerging opportunities.}
}
@article{ZHANG2022104545,
title = {Watching a hands-on activity improves students’ understanding of randomness},
journal = {Computers & Education},
volume = {186},
pages = {104545},
year = {2022},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2022.104545},
url = {https://www.sciencedirect.com/science/article/pii/S0360131522001166},
author = {Icy (Yunyi) Zhang and Mary C. Tucker and James W. Stigler},
keywords = {Hands-on demonstration, Computer simulation, Statistics education, Multimedia learning, Online instruction, Instructional sequence, Embodied cognition},
abstract = {Introductory statistics students struggle to understand randomness as a data generating process, and especially its application to the practice of data analysis. Although modern computational techniques for data analysis such as simulation, randomization, and bootstrapping have the potential to make the idea of randomness more concrete, representing such random processes with R code is not as easy for students to understand as is something like a coin-flip, which is both concrete and embodied. In this study, in the context of multimedia learning, we designed and tested the efficacy of an instructional sequence that preceded computational simulations with embodied demonstrations. We investigated the role that embodied hands-on movement might play in facilitating students’ understanding of the shuffle function in R. Our findings showed that students who watched a video of hands shuffling data written on pieces of paper learned more from a subsequent live-coding demonstration of randomization using R than did students only introduced to the concept using R. Although others have found an advantage of students themselves engaging in hands-on activities, this study showed that merely watching someone else engage can benefit learning. Implications for online and remote instruction are discussed.}
}
@article{BLOTE2000221,
title = {Mental computation and conceptual understanding},
journal = {Learning and Instruction},
volume = {10},
number = {3},
pages = {221-247},
year = {2000},
issn = {0959-4752},
doi = {https://doi.org/10.1016/S0959-4752(99)00028-6},
url = {https://www.sciencedirect.com/science/article/pii/S0959475299000286},
author = {Anke W. Blöte and Anton S. Klein and Meindert Beishuizen},
keywords = {Arithmetic, Procedural flexibility, Conceptual understanding},
abstract = {The goal of this study was to assess the strategic flexibility of students in mental arithmetic up to the number 100. Sixty Dutch second-graders who took part in an experimental ‘realistic arithmetic’ program participated in the study. Results showed that students' preference for certain mathematical procedures depended on the number characteristics of the problems. This indicates that the students had a good conceptual understanding of numbers and procedures. Their actual use of these procedures, however, was somewhat limited. Most problems were solved within a sequential structure. A completely different procedure was used for solving subtraction problems that had a very small difference between the two numbers. Furthermore, it was found that a substantial increase in the students' use of a base-ten procedure occurred after the introduction of this procedure in the mathematics curriculum. Students' preference for this procedure also increased, although to a lesser extent. Another finding of the study was that students exhibited more flexible strategic behaviour with context problems than with numerical-expression problems.}
}
@article{SZNYCER2025106652,
title = {Emotional tears: What they are and how they work},
journal = {Evolution and Human Behavior},
volume = {46},
number = {1},
pages = {106652},
year = {2025},
issn = {1090-5138},
doi = {https://doi.org/10.1016/j.evolhumbehav.2025.106652},
url = {https://www.sciencedirect.com/science/article/pii/S1090513825000017},
author = {Daniel Sznycer and Asmir Gračanin and Debra Lieberman},
keywords = {Tears, Value, Emotion, Motivation, Signaling},
abstract = {Although much is known about emotional tears from the perspectives of neurobiology and behavior, the production of emotional tears and the responses they elicit depend sensitively on a rich set of computations—one which has received little attention to date. This review article aims to close this gap. Emotional tearing occurs during negative events (e.g., injuries) and positive events (e.g., achievements). Episodes of tearing appear to be united by tearers' subjective imputation of negative or positive value to certain internal or external phenomena. Knowing the degree to which objects, organisms, events, and states of affairs enhance or diminish one's prospects—the value of things—is a pressing matter for humans and other organisms. Value information is produced for internal consumption, to be used by behavior-regulating mechanisms in the focal individual. But some evaluations are made available, in addition, to other people, through tearing and other forms of verbal and non-verbal communication. Tearing may function as an implicit plea for receivers (the tear targets) to minimize the costs imposed on the tearer by nature, by third-parties, or by the tear targets themselves—common when the tearer has lower formidability or wherewithal than tear targets do. In addition, tearing may exhort tear targets to infer and register which things the tearer values, positively or negatively. Here, we characterize tears, describe the game-theoretic logic of bargaining from a position of weakness, outline the computational systems that regulate the production of and responses to emotional tears, and review findings about emotional tearing that are relevant to the signaling hypothesis.}
}
@article{ELAZZAOUI2023119595,
title = {A digital twin-based edge intelligence framework for decentralized decision in IoV system},
journal = {Information Sciences},
volume = {649},
pages = {119595},
year = {2023},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2023.119595},
url = {https://www.sciencedirect.com/science/article/pii/S0020025523011805},
author = {Abir {El Azzaoui} and Sekione Reward Jeremiah and Neal N. Xiong and Jong Hyuk Park},
keywords = {IoV, Digital twin, Smart contracts, Smart transportation},
abstract = {The Internet of Vehicle (IoV) is an emerging technology for the development of future smart cities. With the fast and exponential growing rate of Internet of Things (IoT), the smart transportation field is ushering in a revolutionary advancement. Smart transportation systems facilitate better informed, more coordinated, and smarter use of transport networks, with the use of advanced information and communication technologies applied to vehicles to help improve traffic management, minimize congestion, improve safety, and ultimately provide a more intelligent use of transport networks. Smart transportation is an integral part of modern-day smart city projects. However, the world climate institution has reported that carbon emissions from the overall transportation system accounts for one-fifth of global carbon dioxide with a sum of around 24% of energy. Electric vehicles (EV) represent a solution for this issue, yet, it is not sustainable. The communication between EVs and a roadside unit (RSU), and the continuous computational power required to support an IoV system also requires a reliance on energy harvesting. With this in mind, in this paper, we propose a decentralized trust management solution for IoV systems to reduce both carbon footprint and offload the computation power required. Our solution resides in developing the digital twin of vehicles on an intelligent edge environment to simulate the physical vehicle and handle the required data processing. Also, we implement a smart contract model for fast, secure, and sustainable on-road battery recharging between EVs.}
}
@article{GAO2022509,
title = {An integrated simulation method for PVSS parametric design using multi-objective optimization},
journal = {Frontiers of Architectural Research},
volume = {11},
number = {3},
pages = {509-526},
year = {2022},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2021.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S209526352100087X},
author = {Qing Gao and Ying Yang and Qian Wang},
keywords = {Integrated simulation, PV shading System, Parametric design, Multi-objective optimization, Thermal-daylighting balance},
abstract = {An adequate strategy for achieving energy efficiency when designing a photovoltaic shading system (PVSS) shall find an equilibrium between sunlight heat gain and daylight transmittances through effective analysis tools in a building's early design phases. However, traditional simulation methods are either time-consuming or lacking architectonical thinking. This paper proposes a new method for architects to integrate thermal and daylighting performance by using parametric script modelling and optimize their balance with multi-objective optimization (MOO) algorithm in PVSS design. A case study was conducted to demonstrate the workflow of proposed integrated simulation method in PVSS design, and further compared the results with that of three single-objective optimizations under the same design requirement. The findings show that the integrated framework is a feasible method for PVSS design and can be extended into the design of other advance shading system or building integrated photovoltaic.}
}
@incollection{TEZDUYAR199521,
title = { - Massively parallel finite element computation of 3d flows - mesh update strategies in computation of moving boundaries and interfaces°†},
editor = {A. Ecer and J. Hauser and P. Leca and J. Periaux},
booktitle = {Parallel Computational Fluid Dynamics 1993},
publisher = {North-Holland},
address = {Amsterdam},
pages = {21-30},
year = {1995},
isbn = {978-0-444-81999-4},
doi = {https://doi.org/10.1016/B978-044481999-4/50131-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780444819994501316},
author = {T. Tezduyar and S. Aliabadi and M. Behr and A. Johnson and S. Mittal},
abstract = {Publisher Summary
This chapter describes the parallel implicit finite element computations of compressible and incompressible flows with the Connection Machine (CM)—CM-200 and CM-5. The parallel implementations are based on the assumption that the mesh is unstructured. The computations of flow problems involving moving boundaries and interfaces are achieved by using the deformable-spatial-domain or stabilized space-time method. In this method, with special mesh update strategies, the frequency of remeshing is minimized. This avoids the projection errors generated by remeshing and also avoids the cost associated with repeated mesh generation and parallelization setup. This method and its implementation on the massively parallel supercomputers provide a new capability to solve a large class of practical problems involving free surfaces, two-liquid interfaces, and fluid-structure interactions. Now 3D incompressible flow computations can be carried out at sustained speeds of up to 7.0 GigaFLOPS on the CM-5. The 3D compressible flow computations are carried out at sustained speeds of up to 12.2 GigaFLOPS on the CM-5. This parallel performance is significant in the sense that now there is a new level of computational capability in finite element solution of 3D flow problems. Several 3D flow problems are solved using these parallel and update mesh strategies. The chapter discusses the computation of incompressible flow occurring between two concentric cylinders, sloshing in a liquid-filled container subjected to vertical vibrations, and supersonic flow past a delta-wing.}
}
@article{CIULLO2021100349,
title = {A framework for building climate storylines based on downward counterfactuals: The case of the European Union Solidarity fund},
journal = {Climate Risk Management},
volume = {33},
pages = {100349},
year = {2021},
issn = {2212-0963},
doi = {https://doi.org/10.1016/j.crm.2021.100349},
url = {https://www.sciencedirect.com/science/article/pii/S2212096321000784},
author = {Alessio Ciullo and Olivia Martius and Eric Strobl and David N. Bresch},
keywords = {Climate storylines, Downward counterfactuals, European Union Solidarity Fund},
abstract = {Recent research introduced the concept of climate storylines as an alternative approach to estimate climate impact and better deal with uncertainties. A climate storyline is an event-based approach which aims at building “physically self-consistent unfolding of past events, or of plausible future events or pathways”. As such, climate storylines may profit from downward counterfactual thinking, which aims at analyzing how past events could have been worse. Notwithstanding the various applications of downward counterfactual thinking in the natural risk management literature, no study relates this with the climate storyline approach. The main goal of this paper is thus to introduce a framework that supports the development of climate storylines from downward counterfactuals. The framework is event-oriented, it focuses on impact, and it is designed to be applied in a participatory fashion. As a proof-of-concept application, we study the impact of tropical cyclones on the European Union Solidarity Fund (EUSF) without conducting a participatory analysis. Tropical cyclones represent a serious threat for the European outermost regions, and their impact to the EUSF capital availability has never been studied. We find that payouts due to tropical cyclones can hamper a recovery of the fund if large payouts concurrently occur in mainland Europe. To avoid this also considering future changes, an increase in capitalization up to 90 % percent may be required.}
}
@article{KUAI2021150,
title = {Multi-source brain computing with systematic fusion for smart health},
journal = {Information Fusion},
volume = {75},
pages = {150-167},
year = {2021},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2021.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S1566253521000646},
author = {Hongzhi Kuai and Ning Zhong and Jianhui Chen and Yang Yang and Xiaofei Zhang and Peipeng Liang and Kazuyuki Imamura and Lianfang Ma and Haiyuan Wang},
keywords = {Brain computing, Brain informatics, Data-Brain, Smart health, Systematic fusion, Intelligence system},
abstract = {With the progress of artificial intelligence, big data and functional neuroimaging technologies, brain computing has rapidly advanced our understanding of brain intelligence and brain disorders. We argue that existing data analytical methods have become insufficient for brain computing when dealing with multiple brain big data sources, because such methods mainly focus on flattening strategies and fail to work well for systematic understanding of the constituent elements of cognition, emotion and disease, as well as the intra- and inter-relations within and among themselves. To address this problem, we present in this paper a novel multi-source brain computing platform by Data-Brain driven systematic fusion. First, we formalize a series of behaviors surrounding the Brain Informatics-based investigation process, and present a conceptual model to systematically represent content and context of functional neuroimaging data. Then, we propose the systematic brain computing framework with multi-aspect fusion and inference to understand brain specificity and give uncertainty quantification, as well as its inspiration and applications for translational studies on brain health. In particular, a graph matching-based task search algorithm is introduced to help systematic experimental design and data sampling with multiple cognitive tasks. The study increases the interpretability and transparency of brain computing findings by inferring and testing multiple hypotheses taking into consideration the effect of evidence combination. Finally, multiple sources of knowledge (K), information (I) and data (D) are driven by a KID loop as the thinking space to inspire never-ending learning and multi-dimensional interactions in the connected social–cyber–physical spaces. Experimental results have demonstrated the efficacy of the proposed brain computing method with systematic fusion.}
}
@article{SUYOTO2015328,
title = {Parametric Approach as a Tool for Decision-making in Planning and Design Process. Case study: Office Tower in Kebayoran Lama},
journal = {Procedia - Social and Behavioral Sciences},
volume = {184},
pages = {328-337},
year = {2015},
note = {REFLECTIONS ON CREATIVITY: PUBLIC ENGAGEMENT AND THE MAKING OF PLACE},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.05.098},
url = {https://www.sciencedirect.com/science/article/pii/S1877042815033479},
author = {William Suyoto and Aswin Indraprastha and Heru W. Purbo},
keywords = {parametric design, discrete method, office tower, building modeling},
abstract = {This study offers discrete method in parametric design to solve problems during design process (programming, site planning, massing, structure planning, and facade planning). This study is applied in the design of office tower in Kebayoran Lama, Jakarta. The objective of the study is to explore the uses of parametric design method, yet, maintains its time feasibility. The result of the study is a method for planning and design that is more advantageous than the conventional ones in terms of simultaneous, coordinated and accountable. This method enables designer to do many iterations and monitor changes during the design process. However, the method needs a higher skill in logical thinking during the process, which demands time.}
}
@incollection{CAPPAI2024804,
title = {Molecular Dynamics Simulations of Thermal Transport in Solid State Systems},
editor = {Manuel Yáñez and Russell J. Boyd},
booktitle = {Comprehensive Computational Chemistry (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {804-820},
year = {2024},
isbn = {978-0-12-823256-9},
doi = {https://doi.org/10.1016/B978-0-12-821978-2.00095-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128219782000957},
author = {Antonio Cappai and Claudio Melis and Luciano Colombo and Riccardo Dettori},
keywords = {Atomistic simulations, Classical molecular dynamics, Computational methods, Harmonic and anharmonic vibrational properties, Lattice thermal conductivity, Nanoscale thermal transport, Non-equilibrium thermodynamics, Thermal management, Thermal sciences, Thermoelectricity},
abstract = {In this chapter, we provide a synoptic review of the theoretical/computational approaches currently used to characterize thermal transport at the nanoscale, a topic of paramount importance for several applications and technological thermal management requirements. We focus in particular on the description of the atomistic techniques based on equilibrium (EMD), non-equilibrium (NEMD), and approach to equilibrium (AEMD) molecular dynamics (MD), which allow to efficiently describe relatively large and structurally complex systems with a reduced computational cost as compared to fully "ab-initio" techniques. We describe the theoretical background for each simulation strategy, as well as their implementation in state-of-the-art MD codes by underlying their intrinsic limitations and providing strategies to control some of them. We finally perform a series of benchmark calculations on bulk crystalline silicon by showing that the estimated thermal conductivity is weakly dependent on the specific strategy actually employed, while the overall computational cost is largely dependent on it.}
}
@article{GUNNING2021169,
title = {Brain-based mechanisms of late-life depression: Implications for novel interventions},
journal = {Seminars in Cell & Developmental Biology},
volume = {116},
pages = {169-179},
year = {2021},
note = {Special Issue: Myelin edited by Gonçalo Castelo-Branco and Roman Chrast / Special issue: Aging in the nervous system edited by Mara Mather},
issn = {1084-9521},
doi = {https://doi.org/10.1016/j.semcdb.2021.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1084952121001117},
author = {Faith M. Gunning and Lauren E. Oberlin and Maddy Schier and Lindsay W. Victoria},
keywords = {Aging, Depression, Functional connectivity, White matter, Apathy, Executive function},
abstract = {Late-life depression (LLD) is a particularly debilitating illness. Older adults suffering from depression commonly experience poor outcomes in response to antidepressant treatments, medical comorbidities, and declines in daily functioning. This review aims to further our understanding of the brain network dysfunctions underlying LLD that contribute to disrupted cognitive and affective processes and corresponding clinical manifestations. We provide an overview of a network model of LLD that integrates the salience network, the default mode network (DMN) and the executive control network (ECN). We discuss the brain-based structural and functional mechanisms of LLD with an emphasis on their link to clinical subtypes that often fail to respond to available treatments. Understanding the brain networks that underlie these disrupted processes can inform the development of targeted interventions for LLD. We propose behavioral, cognitive, or computational approaches to identifying novel, personalized interventions that may more effectively target the key cognitive and affective symptoms of LLD.}
}
@incollection{NA2025,
title = {Quantum computing research: An in-depth exploration},
series = {Advances in Computers},
publisher = {Elsevier},
year = {2025},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2025.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0065245825000294},
author = {Natraj N.A. and Pethuru Raj C},
keywords = {Quantum, Cryptography, Machine learning, Applications, Quantum Mechanics, Qubits, Gates, Drug discovery, chemistry},
abstract = {The area of computation has undergone a paradigm change with the introduction of quantum computing, which makes use of the complexity of quantum mechanics to successfully tackle problems that were previously considered to be impossible for traditional computers to address. This chapter will give a comprehensive study of quantum computing by delving into its core principles, important algorithms, and future applications. The goal of this chapter is to provide a comprehensive analysis of quantum computing. Quantum computing encompasses not only fundamental concepts like qubits, superposition, entanglement, quantum gates, and algorithms, but it also expands its scope to include quantum hardware, software, programming languages, simulation, modelling, and the intersection of quantum computing and artificial intelligence. Quantum computing is a relatively new field that has been showing significant promise in recent years. The transformative potential of quantum computing reverberates throughout a broad variety of sectors, including materials science and chemistry, as well as finance, optimisation, drug discovery, healthcare, and national security. Quantum computing has the potential to drastically alter these fields. Researchers and enthusiasts interested in acquiring a more in-depth understanding of this dynamic and continuously growing subject matter will find this chapter to be an indispensable resource. In addition to providing a detailed understanding of the theoretical underpinnings of quantum computing, it also details the implications that this technology has in the real world.}
}
@article{TWORZYDLO199387,
title = {Towards an automated environment in computational mechanics},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {104},
number = {1},
pages = {87-143},
year = {1993},
issn = {0045-7825},
doi = {https://doi.org/10.1016/0045-7825(93)90208-F},
url = {https://www.sciencedirect.com/science/article/pii/004578259390208F},
author = {W.W. Tworzydlo and J.T. Oden},
abstract = {Effective methods leading to an automated, computer-based solution of complex engineering design problems are studied in this paper. In particular, methods of automation of the finite element analyses are of primary interest here. This includes algorithmic approaches, based on error estimation. adaptivity and smart algorithms, as well as heuristic approaches based on methods of knowledge engineering. A computational environment, which interactively couples h-p adaptive finite element methods with object oriented programming and expert system tools, is presented. Several examples illustrate the merit and potential of the approaches studied here and confirm the feasibility of developing fully automated design environments.}
}
@article{CALEFFI2024110672,
title = {Distributed quantum computing: A survey},
journal = {Computer Networks},
volume = {254},
pages = {110672},
year = {2024},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2024.110672},
url = {https://www.sciencedirect.com/science/article/pii/S1389128624005048},
author = {Marcello Caleffi and Michele Amoretti and Davide Ferrari and Jessica Illiano and Antonio Manzalini and Angela Sara Cacciapuoti},
keywords = {Quantum internet, Quantum networks, Quantum communications, Quantum computing, Quantum computation, Distributed quantum computing, Quantum algorithms, Quantum compiler, Quantum compiling, Simulator},
abstract = {Nowadays, quantum computing has reached the engineering phase, with fully-functional quantum processors integrating hundreds of noisy qubits. Yet – to fully unveil the potential of quantum computing out of the labs into the business reality – the challenge ahead is to substantially scale the qubit number, reaching orders of magnitude exceeding thousands of fault-tolerant qubits. To this aim, the distributed quantum computing paradigm is recognized as the key solution for scaling the number of qubits. Indeed, accordingly to such a paradigm, multiple small-to-moderate-scale quantum processors communicate and cooperate for executing computational tasks exceeding the computational power of single processing devices. The aim of this survey is to provide the reader with an overview about the main challenges and open problems arising with distributed quantum computing from a computer and communications engineering perspective. Furthermore, this survey provides an easy access and guide towards the relevant literature and the prominent results in the field.}
}
@article{HAMALAINEN199619,
title = {Accelerating genetic algorithm computation in tree shaped parallel computer},
journal = {Journal of Systems Architecture},
volume = {42},
number = {1},
pages = {19-36},
year = {1996},
issn = {1383-7621},
doi = {https://doi.org/10.1016/1383-7621(96)00009-4},
url = {https://www.sciencedirect.com/science/article/pii/1383762196000094},
author = {Timo Hämäläinen and Harri Klapuri and Jukka Saarinen and Pekka Ojala and Kimmo Kaski},
keywords = {Parallel genetic algorithms, Parallel implementation, Parallel computing, Tree shape architecture},
abstract = {Realizations of genetic algorithms (GAs) in a tree shape parallel computer architecture are presented using different levels of parallelism. In addition, basic models for parallel GAs are considered. The tree shape parallel computer system, GAPA (Genetic Algorithm Parallel Accelerator) with special hardware for GA computation, is described in detail. Also mappings for centralized and distributed GA models are given and their performance has been measured for different population sizes.}
}
@article{TRAGER20241555,
title = {The human touch: Utilizing AlphaFold 3 to analyze structures of endogenous metabolons},
journal = {Structure},
volume = {32},
number = {10},
pages = {1555-1562},
year = {2024},
issn = {0969-2126},
doi = {https://doi.org/10.1016/j.str.2024.08.018},
url = {https://www.sciencedirect.com/science/article/pii/S0969212624003356},
author = {Toni K. Träger and Christian Tüting and Panagiotis L. Kastritis},
abstract = {Summary
Computational structural biology aims to accurately predict biomolecular complexes with AlphaFold 3 spearheading the field. However, challenges loom for structural analysis, especially when complex assemblies such as the pyruvate dehydrogenase complex (PDHc), which catalyzes the link reaction in cellular respiration, are studied. PDHc subcomplexes are challenging to predict, particularly interactions involving weaker, lower-affinity subcomplexes. Supervised modeling, i.e., integrative structural biology, will continue to play a role in fine-tuning this type of prediction (e.g., removing clashes, rebuilding loops/disordered regions, and redocking interfaces). 3D analysis of endogenous metabolic complexes continues to require, in addition to AI, precise and multi-faceted interrogation methods.}
}
@article{JIANG2024122157,
title = {Explicit potential function and fast algorithm for computing potentials in α×β conic surface resistor network},
journal = {Expert Systems with Applications},
volume = {238},
pages = {122157},
year = {2024},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.122157},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423026593},
author = {Xiaoyu Jiang and Gaojun Zhang and Yanpeng Zheng and Zhaolin Jiang},
keywords = {Resistor network, Chebyshev polynomials, DST-IV, Fast algorithm},
abstract = {Resistor network research is of great importance, yet many resistor networks and their large-scale fast computations have not received sufficient attention. This paper proposes a new resistor network with idiosyncratic shape, i.e., a α×β conic surface (CS) resistor network that resembles the upper part of a three-dimensional Dirac function. Utilizing the Recursion Transform (RT-V) method of Tan, a recursive matrix equation model is constructed based on Kirchhoff’s law and nodal voltages, which contains the modified tridiagonal Toeplitz matrix. By using the orthogonal matrix transformation, the eigenvalues and eigenvectors of the modified tridiagonal Toeplitz are obtained. The discrete sine transform of the fourth type (DST−IV) is utilized to solve node voltages, while the explicit potential function is represented by the Chebyshev polynomials of the second kind. In addition, explicit potential functions for some special cases are provided, and the potential distribution is illustrated using dynamic three-dimensional graph. To achieve a rapid calculation of the potential, a fast algorithm based on the multiplication of DST-IV with a vector is proposed. In the end, analysis of computational efficiency for the explicit potential function and the fast algorithm are shown.}
}
@article{WESSMANENZINGER2019105,
title = {Grade 5 children’s drawings for integer addition and subtraction open number sentences},
journal = {The Journal of Mathematical Behavior},
volume = {53},
pages = {105-128},
year = {2019},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2018.03.010},
url = {https://www.sciencedirect.com/science/article/pii/S0732312317300731},
author = {Nicole M. Wessman-Enzinger},
keywords = {Integers, Integer addition and subtraction, Learner-generated drawings, Number line, Models, Instructional models, Student thinking},
abstract = {Three Grade 5 children participated in a microgenetic study embedded in 12-week teaching experiment on integer addition and subtraction. They solved open number sentences in four individual sessions across the 12-weeks and produced drawings. Through the lens of learner-generated drawings and qualitative analysis, these drawings provide perspective into the children’s thinking about integer addition and subtraction. The following categories are described: Single and Double Set of Objects, Number Sequences, Empty Number Lines, Number Lines, Number Sentences, Sign Emphasis, and Answer in Box Only. One student drew sets of objects frequently and the other students drew number lines more. Descriptions of how use of their drawings changed over time are provided. Implications point to a re-examination of integer instructional models and insight into potential learning progressions.}
}
@article{DELEERSNYDER2024105602,
title = {A multidimensional AI-trained correction to the 1D approximate model for Airborne TDEM sensing},
journal = {Computers & Geosciences},
volume = {188},
pages = {105602},
year = {2024},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2024.105602},
url = {https://www.sciencedirect.com/science/article/pii/S0098300424000852},
author = {Wouter Deleersnyder and David Dudal and Thomas Hermans},
keywords = {Forward modelling, Machine Learning, Surrogate modelling, Electromagnetics, Airborne},
abstract = {The computational resources required to solve the full 3D inversion of time-domain electromagnetic data are immense. To overcome the time-consuming 3D simulations, we construct a surrogate model, more precisely, a data-driven statistical model to replace the 3D simulations. It is trained on 3D data and predicts the approximate output much faster. We construct a surrogate model that predicts the discrepancy between a 1D subsurface model and a deviation of the 1D assumption. The latter response is fastly computable with a semi-analytical 1D forward model. We exemplify the approach on a two-layered case. The results are encouraging even with few training samples. Given the computational cost related to the 3D simulations, there are limitations in the number of training samples that can be generated. In addition, certain applications require a wide range of parameters to be sampled, such as the electrical conductivity parameters in a saltwater intrusion case. The challenge of this work is achieving the best possible accuracy with only a few thousand samples. We propose to view the performance in terms of learning gain, representing the gain from the surrogate model whilst still acknowledging a residual discrepancy. Our works open new avenues for effectively simulating 3D TDEM data.}
}
@article{HEGG201856,
title = {Preservice teacher proficiency with transformations-based congruence proofs after a college proof-based geometry class},
journal = {The Journal of Mathematical Behavior},
volume = {51},
pages = {56-70},
year = {2018},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2018.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S0732312317301153},
author = {Meredith Hegg and Dimitri Papadopoulos and Brian Katz and Timothy Fukawa-Connelly},
keywords = {Mathematics teacher education, Transformational geometry, Proof},
abstract = {This report explores pre-service teachers’ proficiency with concepts of transformational geometry at the end of a semester-long advanced geometry course. In the course, the instructor incorporated transformational geometry content, including congruence proofs, in an attempt to prepare the pre-service teachers to teach high school geometry in alignment with the Common Core State Standards for Mathematics. At the conclusion of the course, students expressed a preference for using traditional triangle congruence criteria (SAS, ASA, SSS, and AAS) over using transformations to complete proofs, but were nevertheless generally successful in completing proofs using transformations. Similarly, while the students often described thinking of transformations in terms of analytic forms, they were successfully able to prove triangle congruences in synthetic contexts. Finally, some evidence indicates that students may have motion or process conceptions of transformations, but not map or object conceptions, but this evidence is not conclusive.}
}
@incollection{TONDEUR202424,
title = {Chapter 3 - Quality improvement movements},
editor = {Yves Tondeur},
booktitle = {Sustainable Quality Improvements for Isotope Dilution in Molecular Ultratrace Analyses},
publisher = {Elsevier},
pages = {24-70},
year = {2024},
isbn = {978-0-443-29034-3},
doi = {https://doi.org/10.1016/B978-0-443-29034-3.00022-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780443290343000223},
author = {Yves Tondeur},
keywords = {Accreditation & technology-in-use, Commitment-based approach, Empirical vs. rational methods, Isomer selectivity, Isotope dilution, Known & documented quality, Legislating competition, Methods innovation rule, Performance assessment, Precision & trueness, Purpose of quality control samples, Quick fixes vs. fundamental solution, Structural conflicts},
abstract = {Emerging over recent years is the notion that quality improvements are hard to come by, when in fact, countless opportunities to integrate new developments are overlooked primarily because of the restrictive ways in which analytical protocols have been written and enforced, or the misconceptions about their function. A point of instability was reached. The manifestation of a quality malaise can be seen through the efforts by many to enhance quality by attempting to transfer the responsibility for quality back to those doing the work and to those who need the work products. The testing industry is now forced into abandoning old formal structures, mental models, and behaviors. Realigning our thinking, discovering the limits, and identifying the structural conflicts are essential if one wants to improve quality. This chapter makes clear that doing the same thing than before better is not enough, or to solely implement quick fixes can be wasteful; somehow, we need to ensure the application of the method coevolves with its environment. As chemists, what can we do?}
}
@article{IONESCU2014275,
title = {Embodied Cognition: Challenges for Psychology and Education},
journal = {Procedia - Social and Behavioral Sciences},
volume = {128},
pages = {275-280},
year = {2014},
note = {International Conference: EDUCATION AND PSYCHOLOGY CHALLENGES - TEACHERS FOR THE KNOWLEDGE SOCIETY – 2nd EDITION EPC – TKS 2013},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2014.03.156},
url = {https://www.sciencedirect.com/science/article/pii/S1877042814022472},
author = {Thea Ionescu and Dermina Vasc},
keywords = {embodiment, cognition, sensory-motor processes, action, education.},
abstract = {Embodied cognition considers that human cognition is fundamentally grounded in sensory-motor processes and in our body's morphology and internal states. In this paper, we discuss some of the features of this post-cognitivist approach and the challenges that follow for psychology and education. These challenges point to the need to reconsider cognition and the way we pursue education today. If we want to have an efficient educational system we have to look at fundamental research in cognitive science to have an accurate description of what cognition is. Only then can we design optimal educational settings for the development of thinking.}
}
@incollection{BATCHELDER2015808,
title = {Mathematical Psychology: History},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {808-815},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.43059-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008097086843059X},
author = {William H. Batchelder},
keywords = {Bayesian theory, Choice, Correlation coefficient, Decision making, Differential psychology, Experimental psychology, Factor analysis, Game theory, Information science, Learning theory, Marginal utility, Measurement theory, Memory, Paired-comparison scaling, Psychometrics, Psychophysics, Response time, Scaling, Signal detection theory, Stochastic processes, Theory of grammar, Utility theory},
abstract = {The application of mathematics to certain problems within the field of psychology dates back to at least the seventeenth century. This article reviews some of these early applications, most of which either involve theories for experimental phenomena or statistical methods for measuring individual differences. These later applications led to the field of psychometrics in the 1930s; and the former led to the field of mathematical psychology in the 1950s, and both fields are active today. Early mathematical psychology was characterized by testable, formal theories in the areas of learning and memory, perception and psychophysics, choice and decision making, language and thinking, and measurement and scaling; and these areas still characterize the field today.}
}
@article{FINGER2025101535,
title = {When kids juggle it all: Biliteracy instruction and the development of discourse connectedness in L1 and L2 writing},
journal = {Cognitive Development},
volume = {73},
pages = {101535},
year = {2025},
issn = {0885-2014},
doi = {https://doi.org/10.1016/j.cogdev.2024.101535},
url = {https://www.sciencedirect.com/science/article/pii/S0885201424001205},
author = {Ingrid Finger and Cristiane Ely Lemke and Larissa da Silva Cury and Natália Bezerra Mota and Janaina Weissheimer},
keywords = {Bilingual writing development, Discourse connectedness, Graph analysis, Narrative writing},
abstract = {The present longitudinal study explored how bilingual educational contexts shape children's cognitive and linguistic development. Its main goal was to investigate the development of discourse connectedness (measured by long-range connectedness - LSC) in written narratives in Portuguese (L1) and English (L2) by 78 children of a bilingual school in Brazil within a year span (from 2021 to 2022). Participants created a narrative in their L1 or L2 based on a sequence of five images, which were analyzed with the computational tool SpeechGraphs (Mota et al., 2014). Connectedness scores were expected to vary as a function of Language (L1, L2) and of Year of data collection (Time 1, Time 2), favoring, respectively, the L1 and Time 2. The results confirmed our hypotheses, with long-range recurrence (LSC) scores in the L1 narratives higher than in the L2 at both times of data collection. In addition, the longitudinal analysis revealed higher connectedness scores for narratives written in Time 2 in both languages. Overall, our findings indicate that the children's performance in terms of connectedness progressed in a parallel way in the two languages during the school years, with an expected advantage for the narratives written in their dominant language. In addition, they highlight the potential of using SpeechGraphs - a cost-effective, non-invasive computational tool - to analyze children's use of two prestige languages in a particular bilingual educational context.}
}
@article{RUTTEN20211,
title = {50 Years of Russian Literature: Mapping, Mixing, and Queering Slavic Literary Studies},
journal = {Russian Literature},
volume = {125-126},
pages = {1-8},
year = {2021},
note = {50 Years of Russian Literature & Teffi’s Theatrical & Cinematic Work},
issn = {0304-3479},
doi = {https://doi.org/10.1016/j.ruslit.2021.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0304347921000673},
author = {Ellen Rutten},
keywords = {Russian Literature, Editorial, Transdisciplinarity, Slavic Literary Studies, Transnational Academic Communication},
abstract = {Russian Literature turned fifty this year. In this editorial contribution, editor-in-chief Ellen Rutten reflects on the journal’s past, its current profile, and future editorial plans. As Rutten argues, Russian Literature has three distinguishing features. First, the journal has always generously invited other disciplines on board – and its transdisciplinary inclusivity has increased in recent years – while maintaining a steady gaze on Slavic literary studies. Second, the journal acts as a transnational and transcontinental scholarly contact zone – a status that cannot be isolated from our choice to publish both Anglophone and Russophone analyses. And third, Russian Literature brings together a range of scholarly voices and genres that is unusually broad for a scholarly periodical, through a strategy of active editorial outreach to young talents and leading experts in the field. Rutten concludes with a few words on upcoming volumes and plans, including new archival publications and volumes-in-the-making inspired by recent shifts in thinking about geopolitics, gender, and health and environment.}
}
@article{ORAN1992251,
title = {Reactive-flow computations on a massively parallel computer},
journal = {Fluid Dynamics Research},
volume = {10},
number = {4},
pages = {251-271},
year = {1992},
issn = {0169-5983},
doi = {https://doi.org/10.1016/0169-5983(92)90025-R},
url = {https://www.sciencedirect.com/science/article/pii/016959839290025R},
author = {Elaine S. Oran and Jay P. Boris and C.Richard DeVore},
abstract = {Results are described of recent research and model developments for performing large-scale multidimensional compressible reacting-flow computations on the Connection Machine, a very fine-grained, parallel computer capable of multigigaflop performance. We are interested both in having general-purpose computer programs for routine production computations and in evaluating the architecture of the computer for a wide range of computational fluid dynamics and reacting-flow applications. We describe the hurdles involved in rethinking the structure and the algorithms to best suit this kind of computer, provide some relative timings for different programs, and describe ways of dealing with special constraints (such as periodic boundary conditions) imposed by the architecture. Finally, representative results are presented for several reacting and nonreacting computations, including the development and propagation of a spark in a hydrogen-oxygen mixture, an imploding detonation, and the generation of beam-channel turbulence.}
}
@incollection{ZHUGE201655,
title = {4 - The think lens},
editor = {Hai Zhuge},
booktitle = {Multi-Dimensional Summarization in Cyber-Physical Society},
publisher = {Morgan Kaufmann},
pages = {55-65},
year = {2016},
series = {Computer Science Reviews and Trends},
isbn = {978-0-12-803455-2},
doi = {https://doi.org/10.1016/B978-0-12-803455-2.00004-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128034552000044},
author = {Hai Zhuge},
keywords = {Think lens, Semantic lens, models, semantic images, multi-dimensional, semantic link network},
abstract = {The nature of many research problems is about scale and dimension of observation and thinking. Whether the patterns and rules on one scale still hold on the other scale? Whether the patterns and rules on one dimension or some dimensions still hold on the other dimension or some other dimensions? Summarization is also about the scale and the dimension of motivation, representation and thinking. Human eyes can focus on not only a part of a representation but also the whole from a certain distance like the lens of camera. The think lens is a mechanism that can zoom in and out while observing, searching, mapping, analysing, planning, predicting, calculating, reasoning, imaging, and representing patterns through semantic computing on various representations according to some principles and rules. This section presents a concept model of the think lens for realising general summarisation in cyber-physical society.}
}
@article{BRAUND2013175,
title = {First steps in teaching argumentation: A South African study},
journal = {International Journal of Educational Development},
volume = {33},
number = {2},
pages = {175-184},
year = {2013},
issn = {0738-0593},
doi = {https://doi.org/10.1016/j.ijedudev.2012.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0738059312000417},
author = {Martin Braund and Zena Scholtz and Melanie Sadeck and Robert Koopman},
keywords = {Critical thinking, Argumentation, Student teachers, Science},
abstract = {South African student teachers were studied to see how they coped with requirements to teach science using argumentation. Lesson observations, plans, reflective logs, post-teaching interviews and assessment of pupils’ argumentation were used to compare student teachers’ preparedness and interactions with pupils. Two clusters of students were identified representing high preparedness and low interaction. A high degree of preparedness alone did not guarantee high levels of argumentation. Schools’ educational situations were independent of success in teaching argumentation. The outcomes and implications for further development of teaching critical thinking are discussed.}
}
@article{SCHULTZ2022104766,
title = {Animacy and the prediction of behaviour},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {140},
pages = {104766},
year = {2022},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2022.104766},
url = {https://www.sciencedirect.com/science/article/pii/S014976342200255X},
author = {Johannes Schultz and Chris D. Frith},
keywords = {Animacy, Action prediction, Goal-directed action, Mentalizing, Theory-of-Mind, Intentions, Economic games, Social cognition},
abstract = {To survive, all animals need to predict what other agents are going to do next. We review neural mechanisms involved in the steps required for this ability. The first step is to determine whether an object is an agent, and if so, how sophisticated it is. This involves brain regions carrying representations of animate agents. The movements of the agent can then be anticipated in the short term based solely on physical constraints. In the longer term, taking into account the agent’s goals and intentions is useful. Observing goal directed behaviour activates the neural action observation network, and predicting future goal directed behaviour is helped by the observer’s own action generating mechanisms. Intentions are critically important in determining actions when interacting with other agents, as several intentions can lie behind an action. Here, interpretation is helped by prior beliefs about the agent and the brain’s mentalising system is engaged. Biologically-constrained computational models of action recognition exist, but equivalent models for understanding intentional agents remain to be developed.}
}
@incollection{MONLEZUN2023159,
title = {Chapter 6 - AI+patient safety: adaptive, embedded, intelligent},
editor = {Dominique J. Monlezun},
booktitle = {The Thinking Healthcare System},
publisher = {Academic Press},
pages = {159-182},
year = {2023},
isbn = {978-0-443-18906-7},
doi = {https://doi.org/10.1016/B978-0-443-18906-7.00007-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780443189067000076},
author = {Dominique J. Monlezun},
keywords = {Bias, Blockchain, Clinical alarms, Clinical reports, Command center intelligence, Data privacy, Data security, Design thinking, Drug safety, Explainability, Patient safety, Reproducibility},
abstract = {This chapter discusses the AI transformation of patient safety within modern healthcare systems, particularly those digitally extended with telehealth. It carefully details the definitions and debates in patient safety (including the major actors and researchers in this community who share a general critique about the slow to absent sustained, substantive, and equally shared improvements in healthcare systems' safe delivery of patient care). The chapter provides the standard (including WHO) and innovative though still practical conceptualizations of patient safety and then progresses to recent more promising recent advances in human-centered, standardized, and AI-enabled patient safety (including safety as design thinking and system strategy). The chapter illustrates these developments with concrete use cases (in AI-enabled drug safety, clinical reports, and alarms) and augmentation with automation (including embedded, ambient, and command center safety intelligence). The chapter concludes by considering new and growing challenges in AI-driven patient safety (including data security, privacy, bias, and inconsistency) and emerging solutions (including blockchain, bias reduction, reproducibility, explainability, effectiveness, and safety in embedded design).}
}
@article{PALIOURA2025105186,
title = {“Storytelling and educational robotics: A scoping review (2004–2024)”},
journal = {Computers & Education},
volume = {225},
pages = {105186},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2024.105186},
url = {https://www.sciencedirect.com/science/article/pii/S0360131524002008},
author = {Maria Palioura and Theodosios Sapounidis},
keywords = {Educational robotics, Storytelling, Scoping review},
abstract = {Storytelling has been used for years in educational practice and Educational Robotics is a rapidly growing field worldwide. Accordingly, researchers have attempted to combine Storytelling and Robotics in education. However, no systematic record exists on this combination. Therefore, we conducted a scoping review of 82 papers out of 5272 articles published in 5 Databases in the last 20 years to map the conducted research so far. In detail: the educational levels and the school subjects in which storytelling and educational robotics are applied, the types of robots used, the duration, the sample size, participants' age and the skills that students may develop through this combination. Additionally, we analyzed, grouped, and presented the tools used for measuring the potential effects of storytelling and educational robotics. Finally, the students' role in the activities was sought. Based on our findings, most interventions mainly addressed preschool and primary school students, the robots used the most are humanoid, most interventions did not exceed 6 h, and the number of participants was less than 20 students. Besides, most interventions tried to develop students’ skills (communication, creativity, collaboration) and attitudes (engagement, motivation, participation) with qualitative tools borrowed from other domains (e.g. psychology, healthcare). This scoping demonstrates a gap in the use of storytelling and educational robotics in secondary and university education and subjects like history, geography, etc. Finally, this combination seems to have the potential to enhance the educational process, but more research is needed to shed light on all the aspects of the combination.}
}
@article{SENAPATI202449,
title = {Oxymoron: An Automatic Detection from the Corpus},
journal = {Procedia Computer Science},
volume = {244},
pages = {49-56},
year = {2024},
note = {6th International Conference on AI in Computational Linguistics},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.10.177},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924029788},
author = {Apurbalal Senapati},
keywords = {Oxymoron, Antonymy, Corpus, Computational linguistic, Natural Language Processing, Bengali language},
abstract = {An oxymoron is a linguistic phenomenon in which a pair of opposite or antonymous words are combined to convey a new meaning. Sometimes, it is used to express figurative, irony, or rhetoric within the text. This issue has received relatively less attention in the realms of linguistics and computational disciplines. Oxymorons play a significant role in various language-processing applications. This study represents a pioneering effort in the exploration of oxymorons in the Bengali language. A corpus-based study of oxymoron is a fundamental issue that has not been explored so far. A system has been proposed for the automated recognition of oxymorons from a given corpus. Frequency analysis, semantic similarity, and an antonym dictionary have been employed to discern oxymorons within the corpus. The system achieved promising results when tested on a Bengali corpus, and found 308 distinct oxymorons. A corpus-based descriptive statistics is measured in two different corpora. The most common oxymorons are ranked based on their frequency. Their notable presence underscores the importance of the Bengali language. This study aimed to explore fundamental questions concerning oxymorons, such as the automated detection of oxymorons within a corpus, descriptive statistics regarding oxymorons across languages, and the process of their construction and creation. Additionally, efforts were made to extract oxymorons from large language models using zero-shot prompts, but the results were not as promising compared to our proposed system.}
}
@article{CADART2025113107,
title = {An optimal penalty method for the joint stiffening in beam models of additively manufactured lattice structures},
journal = {International Journal of Solids and Structures},
volume = {306},
pages = {113107},
year = {2025},
issn = {0020-7683},
doi = {https://doi.org/10.1016/j.ijsolstr.2024.113107},
url = {https://www.sciencedirect.com/science/article/pii/S0020768324004669},
author = {T. Cadart and T. Hirschler and S. Bahi and S. Roth and F. Demoly and N. Lebaal},
keywords = {Lattice structure, Beam formulation, Penalty method, Joint stiffening, Optimization, Additive manufacturing, Material jetting},
abstract = {Additive manufacturing is revolutionizing structural design, with lattice structures becoming increasingly prominent due to their superior mechanical properties. However, simulating these structures quickly and accurately using the finite element method (FEM) remains challenging. Recent research has highlighted beam element simulation within FEM as a more efficient alternative to traditional solid FE simulations, achieving similar accuracy with reduced computational resources. However, a significant challenge is managing the lack of rigidity at nodes and the prevalence of low aspect ratio beams. While various methodologies have been proposed to address these issues, there is still a gap in the comprehensive evaluation of their limitations. An optimal node penalization methodology is required to expand the limited range of accurately represented lattice behavior. A preliminary study investigates lattice geometries through comparative analysis of solid and beam FE simulations. Built on this, we developed a methodology suitable to linear, dynamics and nonlinear beam FE simulations, contributing to enhanced computational speed and accuracy. Several lattice structures were printed using material jetting and quasi-static compressive tests were conducted to validate the methodology’s accuracy. The numerical results reveal a good accuracy between the proposed beam FE methodology and the experimental data, offering a better alternative to conventional FEM for energy absorption in terms of computing time.}
}
@article{LU2022100056,
title = {Nonlinear EEG signatures of mind wandering during breath focus meditation},
journal = {Current Research in Neurobiology},
volume = {3},
pages = {100056},
year = {2022},
issn = {2665-945X},
doi = {https://doi.org/10.1016/j.crneur.2022.100056},
url = {https://www.sciencedirect.com/science/article/pii/S2665945X22000298},
author = {Yiqing Lu and Julio Rodriguez-Larios},
keywords = {EEG, Mind wandering, Meditation, Complexity, Nonlinear analysis},
abstract = {In meditation practices that involve focused attention to a specific object, novice practitioners often experience moments of distraction (i.e., mind wandering). Previous studies have investigated the neural correlates of mind wandering during meditation practice through Electroencephalography (EEG) using linear metrics (e.g., oscillatory power). However, their results are not fully consistent. Since the brain is known to be a chaotic/nonlinear system, it is possible that linear metrics cannot fully capture complex dynamics present in the EEG signal. In this study, we assess whether nonlinear EEG signatures can be used to characterize mind wandering during breath focus meditation in novice practitioners. For that purpose, we adopted an experience sampling paradigm in which 25 participants were iteratively interrupted during meditation practice to report whether they were focusing on the breath or thinking about something else. We compared the complexity of EEG signals during mind wandering and breath focus states using three different algorithms: Higuchi's fractal dimension (HFD), Lempel-Ziv complexity (LZC), and Sample entropy (SampEn). Our results showed that EEG complexity was generally reduced during mind wandering relative to breath focus states. We conclude that EEG complexity metrics are appropriate to disentangle mind wandering from breath focus states in novice meditation practitioners, and therefore, they could be used in future EEG neurofeedback protocols to facilitate meditation practice.}
}
@article{LIMONGELLI199289,
title = {Abstract specification of structures and methods in symbolic mathematical computation},
journal = {Theoretical Computer Science},
volume = {104},
number = {1},
pages = {89-107},
year = {1992},
issn = {0304-3975},
doi = {https://doi.org/10.1016/0304-3975(92)90167-E},
url = {https://www.sciencedirect.com/science/article/pii/030439759290167E},
author = {C. Limongelli and M. Temperini},
abstract = {This paper describes a methodology based on the object-oriented programming paradigm, to support the design and implementation of a symbolic computation system. The requirements of the system are related to the specification and treatment of mathematical structures. This treatment is considered from both the numerical and the symbolic points of view. The resulting programming system should be able to support the formal definition of mathematical data structures and methods at their highest level of abstraction, to perform computations on instances created from such definitions, and to handle abstract data structures through the manipulation of their logical properties. Particular consideration is given to the correctness aspects. Some examples of convenient application of the proposed design methodology are presented.}
}
@article{ISLAM2022100280,
title = {Industry 4.0: Skill set for employability},
journal = {Social Sciences & Humanities Open},
volume = {6},
number = {1},
pages = {100280},
year = {2022},
issn = {2590-2911},
doi = {https://doi.org/10.1016/j.ssaho.2022.100280},
url = {https://www.sciencedirect.com/science/article/pii/S2590291122000341},
author = {Md. Aminul Islam},
keywords = {Industry 4.0, Skills, Competencies, Graduates, A lower middle-income country, Bangladesh},
abstract = {This paper aims at finding whether students are ready to perform in the modern competitive business job arena. Most importantly, if they have the required skills and competencies to catch the opportunity offered by companies at the fourth industrial revolution where we notice the trend of automation and data exchange and IoT, cloud computing and cognitive computing have taken the lead. Our target participants in the survey include students from public and private universities in Bangladesh who will perform in the job market and who are already in the market. This is how we can bridge the gap between employers' expectations and students' perceptions of skills and competencies they acquire before entering the job market. After surveying and analyzing data collected from 361 undergraduate and graduate-level students, we found that both business and technology impact employment. Students are aware of the changing job market scenario, and they are trying to have those skills which will make them competent compared to the early years, but they are not prepared enough to accept the challenges faced in industry 4.0. This paper will be helpful for both the academicians to be aware of the future trend of the market so that they can prepare students to fight the challenges and do future research on them. At the same time, employers can get some ideas how students are thinking right now and how much training and development opportunity they should arrange for the newly recruited graduates who have lack expertise but if they are trained up, can be a source of strength for the companies.}
}
@article{RAHARINIRINA2021100332,
title = {Inferring gene regulatory networks from single-cell RNA-seq temporal snapshot data requires higher-order moments},
journal = {Patterns},
volume = {2},
number = {9},
pages = {100332},
year = {2021},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2021.100332},
url = {https://www.sciencedirect.com/science/article/pii/S266638992100180X},
author = {N. Alexia Raharinirina and Felix Peppert and Max {von Kleist} and Christof Schütte and Vikram Sunkara},
keywords = {single cell, RNA sequencing, time-course snapshots, Markov chains, chemical master equation, moment equations},
abstract = {Summary
Single-cell RNA sequencing (scRNA-seq) has become ubiquitous in biology. Recently, there has been a push for using scRNA-seq snapshot data to infer the underlying gene regulatory networks (GRNs) steering cellular function. To date, this aspiration remains unrealized due to technical and computational challenges. In this work we focus on the latter, which is under-represented in the literature. We took a systemic approach by subdividing the GRN inference into three fundamental components: data pre-processing, feature extraction, and inference. We observed that the regulatory signature is captured in the statistical moments of scRNA-seq data and requires computationally intensive minimization solvers to extract it. Furthermore, current data pre-processing might not conserve these statistical moments. Although our moment-based approach is a didactic tool for understanding the different compartments of GRN inference, this line of thinking—finding computationally feasible multi-dimensional statistics of data—is imperative for designing GRN inference methods.}
}
@article{ENGLAND2008163,
title = {Rattling the cage: computational models of chaperonin-mediated protein folding},
journal = {Current Opinion in Structural Biology},
volume = {18},
number = {2},
pages = {163-169},
year = {2008},
note = {Theory and simulation / Macromolecular assemblages},
issn = {0959-440X},
doi = {https://doi.org/10.1016/j.sbi.2007.12.013},
url = {https://www.sciencedirect.com/science/article/pii/S0959440X08000067},
author = {Jeremy England and Del Lucent and Vijay Pande},
abstract = {Chaperonins are known to maintain the stability of the proteome by facilitating the productive folding of numerous misfolded or aggregation-prone proteins and are thus essential for cell viability. Despite their established importance, the mechanism by which chaperonins facilitate protein folding remains unknown. Computer simulation techniques are now being employed to complement experimental ones in order to shed light on this mystery. Here we review previous computational models of chaperonin-mediated protein folding in the context of the two main hypotheses for chaperonin function: iterative annealing and landscape modulation. We then discuss new results pointing to the importance of solvent (a previously neglected factor) in chaperonin activity. We conclude with our views on the future role of simulation in studying chaperonin activity as well as protein folding in other biologically relevant confined contexts.}
}
@article{NA2023105139,
title = {Towards a neurocomputational account of social controllability: From models to mental health},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {148},
pages = {105139},
year = {2023},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2023.105139},
url = {https://www.sciencedirect.com/science/article/pii/S0149763423001082},
author = {Soojung Na and Shawn A. Rhoads and Alessandra N.C. Yu and Vincenzo G. Fiore and Xiaosi Gu},
keywords = {Social controllability, Computational psychiatry, Reinforcement learning, Model-based learning, Model-free learning, Cognitive map},
abstract = {Controllability, or the influence one has over their surroundings, is crucial for decision-making and mental health. Traditionally, controllability is operationalized in sensorimotor terms as one’s ability to exercise their actions to achieve an intended outcome (also termed “agency”). However, recent social neuroscience research suggests that humans also assess if and how they can exert influence over other people (i.e., their actions, outcomes, beliefs) to achieve desired outcomes ("social controllability”). In this review, we will synthesize empirical findings and neurocomputational frameworks related to social controllability. We first introduce the concepts of contextual and perceived controllability and their respective relevance for decision-making. Then, we outline neurocomputational frameworks that can be used to model social controllability, with a focus on behavioral economic paradigms and reinforcement learning approaches. Finally, we discuss the implications of social controllability for computational psychiatry research, using delusion and obsession-compulsion as examples. Taken together, we propose that social controllability could be a key area of investigation in future social neuroscience and computational psychiatry research.}
}
@article{BROWN201511,
title = {On unifiers, diversifiers, and the nature of pattern recognition},
journal = {Pattern Recognition Letters},
volume = {64},
pages = {11-20},
year = {2015},
note = {Philosophical Aspects of Pattern Recognition},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2015.04.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167865515001312},
author = {Gavin Brown},
keywords = {Nature of pattern recognition, Unifying, Diversifying, Dyson},
abstract = {We study a dichotomy of scientific styles, unifying and diversifying, as proposed by Freeman J. Dyson. We discuss the extent to which the dichotomy transfers from the natural sciences (where Dyson proposed it) to the field of Pattern Recognition. To address this we must firstly ask what it means to be a “unifier” or “diversifier” in a field, and what are the relative merits of each style of thinking. Secondly, given that Dyson applied this to the sciences, does it also apply in a field known to be a blend of science and engineering? Parallels are drawn to Platonic/Aristotelian views, and to Cartesian/Baconian science, and questions are asked on what drives the Kuhnian paradigm shifts of our field. This article is intended not to marginalise individuals into categories (unifier/diversifier) but instead to demonstrate the utility of philosophical reflection on our field, showing the depth and complexities a seemingly simple idea can unearth.}
}
@article{NEVES2009834,
title = {Structuring an MCDA model using SSM: A case study in energy efficiency},
journal = {European Journal of Operational Research},
volume = {199},
number = {3},
pages = {834-845},
year = {2009},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2009.01.053},
url = {https://www.sciencedirect.com/science/article/pii/S0377221709002033},
author = {L.P. Neves and L.C. Dias and C.H. Antunes and A.G. Martins},
keywords = {Problem structuring methods, Multiple criteria analysis, SSM, Value Focused Thinking, Energy efficiency},
abstract = {This work presents the use of a problem structuring method, Soft Systems Methodology (SSM), to structure a Multi-Criteria Decision Analysis (MCDA) model, aimed at appraising energy efficiency initiatives. SSM was useful to help defining clearly the decision problem context and the main actors involved, as well as to unveil the relevant objectives for each stakeholder. Keeney’s Value Focused Thinking approach was then used to refine and structure the list of objectives according to the perspective of the main evaluators identified. In addition to describing this particular case study, this paper aims at providing some general guidelines on how SSM may facilitate the emergence of objectives for MCDA models.}
}
@incollection{KRAAK2009468,
title = {Geovisualization},
editor = {Rob Kitchin and Nigel Thrift},
booktitle = {International Encyclopedia of Human Geography},
publisher = {Elsevier},
address = {Oxford},
pages = {468-480},
year = {2009},
isbn = {978-0-08-044910-4},
doi = {https://doi.org/10.1016/B978-008044910-4.00033-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008044910400033X},
author = {M.-J. Kraak},
keywords = {Alternative visualization, Cartography, Cognition, Coordinated-multiple-views, Geocomputation, Geoservices, Geovisualization, Information visualization, Interfaces, Maps, Representation, Spatiotemporal data, Usability, Visual exploration, Visual representation, Visual thinking},
abstract = {Recent developments in information and communication technology (ICT) have introduced many new opportunities, and have influenced many scientific disciplines in application of their methods and techniques. From a mapping perspective, this includes cartography and related disciplines like scientific visualization, image analysis and remote sensing, information visualization, exploratory data analysis, visual analytics, and GI Science. Interactivity and dynamics are prominent keywords and allow one not only to apply maps and diagrams to present-known facts but also to analyze and explore unknown data. The environment in which the maps and diagrams are used has also changed and often includes coordinated multiple views display via the Internet. This allows for simultaneous alternative views of the data and stimulates visual thinking, resulting in geovisualization.}
}
@article{ASGARI20251631,
title = {Uncovering the role of big data analytics on the resilience of agri-food supply chains: a systematic literature review},
journal = {Procedia Computer Science},
volume = {253},
pages = {1631-1639},
year = {2025},
note = {6th International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2025.01.225},
url = {https://www.sciencedirect.com/science/article/pii/S1877050925002339},
author = {Alireza Asgari},
keywords = {Big Data Analytics, Resilience, Agri-food Supply Chain, systematic literature review},
abstract = {In an era marked by frequent disruptions, ensuring the resilience of agri-food supply chains is critical for maintaining food availability and accessibility. The significant increase in generation and complexity of data due to the advancement of computational power and relational databases, has made the discovery of new information through the process of data collection and manipulation possible. Big data analytics can contribute significantly to attaining agri-food supply chain and has many implications inside the food sector. This systematic literature review investigates the role of big data analytics in bolstering agri-food supply chain’s ability to anticipate, respond to, and recover from disruptions. The findings reveal a positive influence of predictive and prescriptive analytics on readiness and response phases of resilience, particularly on capabilities such as risk management, flexibility, and agility. Based on these findings, a comprehensive framework is proposed which maps the current state-of-the-art in addition to identifying gaps in the current literature, offering valuable insights for practitioners and guiding future research.}
}
@article{DEMSZKY2025105183,
title = {Automated feedback improves teachers’ questioning quality in brick-and-mortar classrooms: Opportunities for further enhancement},
journal = {Computers & Education},
volume = {227},
pages = {105183},
year = {2025},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2024.105183},
url = {https://www.sciencedirect.com/science/article/pii/S0360131524001970},
author = {Dorottya Demszky and Jing Liu and Heather C. Hill and Shyamoli Sanghi and Ariel Chung},
keywords = {Computer-assisted instruction, Brick-and-mortar classroom, Natural language processing, Automated teacher feedback, Randomized controlled trial, Focusing questions, Mixed-methods study, K-12 instruction},
abstract = {AI-powered professional learning tools that provide teachers with individualized feedback on their instruction have proven effective at improving instruction and student engagement in virtual learning contexts. Despite the need for consistent, personalized professional learning in K-12 settings, the effectiveness of automated feedback tools in traditional classrooms remains unexplored. We present results from 224 Utah mathematics and science teachers who engaged in a pre-registered randomized controlled trial, conducted in partnership with TeachFX, to assess the impact of automated feedback in K-12 classrooms. This feedback targeted “focusing questions” — questions that probe students’ thinking by pressing for explanations and reflection. We find that teachers opened emails containing the automated feedback about 53–65% of the time, and the feedback increased their use of focusing questions by 20% (p < 0.01) compared to the control group. The feedback did not impact other teaching practices. Qualitative interviews with 13 teachers revealed mixed perceptions of the automated feedback. Some teachers appreciated the reflective insights, while others faced barriers such as skepticism about accuracy, data privacy concerns, and time constraints. Our findings highlight the promises and areas of improvement for implementing effective and teacher-friendly automated professional learning tools in brick-and-mortar classrooms.}
}
@article{NASSIRI201329,
title = {Computational modelling of long bone fractures fixed with locking plates – How can the risk of implant failure be reduced?},
journal = {Journal of Orthopaedics},
volume = {10},
number = {1},
pages = {29-37},
year = {2013},
issn = {0972-978X},
doi = {https://doi.org/10.1016/j.jor.2013.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0972978X13000020},
author = {M. Nassiri and B. MacDonald and J.M. O'Byrne},
keywords = {Modeling, Fracture, Locking, Plate, Failure},
abstract = {Background and purpose
The Locking Compression Plate (LCP) is part of a new plate generation requiring an adapted surgical technique and new thinking about commonly used concepts of internal fixation using plates. Knowledge of the fixation stability provided by these new plates is very limited and clarification is still necessary to determine how the mechanical stability and the risk of implant failure can best be controlled.
Methods
Upon validation, a finite element model of an LCP attached to a cylinder was developed to simulate and analyse the biomechanics of a transverse long bone fracture fixed with a locking plate. Of special interest were the factors influencing the mechanical conditions at the fracture site, the control of interfragmentary movement and implant failure.
Results
Several factors were shown to influence stability in compression. Increasing translation and/or fracture angle post fixation reduced construct stability. Axial stiffness was also influenced by the working length and plate-bone distance. The fracture gap had no effect on the construct stability when no bone contact occurred during loading. Stress analysis of the LCP demonstrated that the maximum Von Mises stresses were found in the innermost screws at the screw-head junction.
Interpretation
For the clinical use of the LCP as a locked internal fixator in fractures with an interfragmentary gap of 1 mm, at least two to four plate holes near the fracture gap should be omitted to allow fracture motion and bone contact to occur. This will also achieve a larger area of stress distribution on the plate and reduce the likelihood of fatigue failure due to cyclic loading.}
}
@article{LU2024103920,
title = {The integrated multi-performance fast optimization strategy for battery thermal management system},
journal = {Case Studies in Thermal Engineering},
volume = {54},
pages = {103920},
year = {2024},
issn = {2214-157X},
doi = {https://doi.org/10.1016/j.csite.2023.103920},
url = {https://www.sciencedirect.com/science/article/pii/S2214157X23012261},
author = {Hao Lu and Xiaole Tang and Hongchang Li and Wenjun Zhao and Xiqiang Chang and Weifang Lin},
keywords = {Short-cut computation, Computational fluid dynamics, Weighted average, Optimization algorithm},
abstract = {Increased battery energy density is required to boost electric vehicle endurance; however, this also raises the possibility of thermal runaway and power battery explosion. Improving the cooling system performance requires optimization and enhancement of classical systems. Traditional design approaches struggle to simultaneously enhance multiple aspects of performance, while an optimization based on Computational Fluid Dynamics (CFD) methods is often inefficient. Therefore, by integrating a flow resistance network model (FRNM) with a weighted average optimization algorithm (INFO), an efficient optimization for the comprehensive performance of the system can be achieved. Five optimized systems under different airflow rates were obtained through optimization. A comparison with two existing systems validated the effectiveness of the optimized system. The results demonstrate that, compared to the two reference systems, the optimized system decreases the maximum temperature difference by 65.51 % and 39.07 %, respectively. Furthermore, the improvement in temperature uniformity is more significant, increasing by 63.76 % and 34.40 %, respectively.}
}
@article{HERTENSTEIN20191213,
title = {Modulation of creativity by transcranial direct current stimulation},
journal = {Brain Stimulation},
volume = {12},
number = {5},
pages = {1213-1221},
year = {2019},
issn = {1935-861X},
doi = {https://doi.org/10.1016/j.brs.2019.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1935861X19302293},
author = {Elisabeth Hertenstein and Elena Waibel and Lukas Frase and Dieter Riemann and Bernd Feige and Michael A. Nitsche and Christoph P. Kaller and Christoph Nissen},
keywords = {Creativity, Flexibility, Transcranial direct current stimulation, Frontal cortex, Electroencephalography},
abstract = {Background
Creativity is the use of original ideas to accomplish something innovative. Previous research supports the notion that creativity is facilitated by an activation of the right and/or a deactivation of the left prefrontal cortex. In contrast, recent brain imaging studies suggest that creativity improves with left frontal activation.
Objective
The present study was designed to further elucidate the neural basis of and ways to modulate creativity, based on the modulation of prefrontal cortical activity through the non-invasive brain stimulation technique transcranial direct current stimulation (tDCS).
Methods
Ninety healthy University students performed three tasks on major aspects of creativity: conceptual expansion (Alternate Uses Task, AUT), associative thinking (Compound Remote Associate Task, CRA), and set shifting ability (Wisconsin Card Sorting Task, WCST). Simultaneously, they received cathodal stimulation of the left and anodal stimulation of the right inferior frontal gyrus (IFG), the reverse protocol, or sham stimulation.
Results
The main pattern of results was a superior performance with bilateral left cathodal/right anodal stimulation, and an inferior performance in the reversed protocol compared to sham stimulation. As a potential underlying physiological mechanism, resting state EEG beta power, indicative of enhanced cortical activity, in the right frontal area increased with anodal stimulation and was associated with better performance.
Conclusion
The findings provide new insights into ways of modulating creativity, whereby a deactivation of the left and an activation of the right prefrontal cortex with tDCS is associated with increased creativity. Potential future applications might include tDCS for patients with mental disorders and for healthy individuals in creative professions.}
}
@article{PIVIK2012548,
title = {Eating breakfast enhances the efficiency of neural networks engaged during mental arithmetic in school-aged children},
journal = {Physiology & Behavior},
volume = {106},
number = {4},
pages = {548-555},
year = {2012},
issn = {0031-9384},
doi = {https://doi.org/10.1016/j.physbeh.2012.03.034},
url = {https://www.sciencedirect.com/science/article/pii/S0031938412001394},
author = {R.T. Pivik and Kevin B. Tennal and Stephen D. Chapman and Yuyuan Gu},
keywords = {Morning nutrition, Mental arithmetic, Preadolescents, Time–frequency analysis},
abstract = {To determine the influence of a morning meal on complex mental functions in children (8–11y), time–frequency analyses were applied to electroencephalographic (EEG) activity recorded while children solved simple addition problems after an overnight fast and again after having either eaten or skipped breakfast. Power of low frequency EEG activity [2Hertz (Hz) bands in the 2–12Hz range] was determined from recordings over frontal and parietal brain regions associated with mathematical thinking during mental calculation of correctly answered problems. Analyses were adjusted for background variables known to influence or reflect the development of mathematical skills, i.e., age and measures of math competence and math fluency. Relative to fed children, those who continued to fast showed greater power increases in upper theta (6–8Hz) and both alpha bands (8–10Hz; 10–12Hz) across sites. Increased theta suggests greater demands on working memory. Increased alpha may facilitate task-essential activity by suppressing non-task-essential activity. Fasting children also had greater delta (2–4Hz) and greater lower-theta (4–6Hz) power in left frontal recordings—indicating a region-specific emphasis on both working memory for mental calculation (theta) and activation of processes that suppress interfering activity (delta). Fed children also showed a significant increase in correct responses while children who continued to fast did not. Taken together the findings suggest that neural network activity involved in processing numerical information is functionally enhanced and performance is improved in children who have eaten breakfast, whereas greater mental effort is required for this mathematical thinking in children who skip breakfast.}
}
@article{REZAPOUR2024108003,
title = {Learning experience assessment through players chat content in multiplayer online games},
journal = {Computers in Human Behavior},
volume = {151},
pages = {108003},
year = {2024},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2023.108003},
url = {https://www.sciencedirect.com/science/article/pii/S0747563223003540},
author = {Mohammad Mahdi Rezapour and Afsaneh Fatemi and Mohammad Ali Nematbakhsh},
keywords = {Learning experience assessment, Multiplayer online game, Natural language processing, BERT, Stealth assessment, Game-based learning},
abstract = {Assessing players’ learning experiences in a proper manner is a fundamental aspect of successful game-based learning programs. One notable characteristic of these programs is stealth assessment, which involves integrating formative assessment into the learning environment without disrupting the learning process. In multiplayer online games (MOGs), the in-game online chat system is a commonly used tool that enables players to communicate through text or voice messages during gameplay. However, there is a lack of specific research on incorporating players’ in-game chat content for computational learning experience assessment, which could enhance the validity of stealth assessment. This study proposes a stealth assessment method based on natural language processing to highlight the significance of players’ in-game chat data in estimating learners’ skills in MOGs. A natural language processing model is developed using a distilled version of the Google BERT pre-trained model. The evaluations demonstrate that the proposed method accurately estimates a player’s skill level by analyzing a few chat messages from the player. This method has the potential to make a profound impact on the field of game-based learning by enabling more precise assessment and supporting the design of tailored interventions and adaptive learning systems. This study pioneers computational skill assessment through chats in MOGs, opening up new opportunities for future investigations in skill assessment and having the potential to transform the field of game-based learning.}
}
@article{AWD2023107403,
title = {A review on the enhancement of failure mechanisms modeling in additively manufactured structures by machine learning},
journal = {Engineering Failure Analysis},
volume = {151},
pages = {107403},
year = {2023},
issn = {1350-6307},
doi = {https://doi.org/10.1016/j.engfailanal.2023.107403},
url = {https://www.sciencedirect.com/science/article/pii/S1350630723003576},
author = {Mustafa Awd and Lobna Saeed and Frank Walther},
keywords = {Modeling, Simulation, Finite Element Analysis, Analytical Models, Multi-Physics, Data-Driven Modeling, Additive Manufacturing, Failure Mechanisms},
abstract = {This review discusses the feasibility of using microstructure- and defect-sensitive models to predict the fatigue behavior of additively generated materials through a non-exclusive qualitative assessment of the current literature on the structural integrity of additively manufactured structures. The time it takes to implement additively manufactured structures is reduced if a computational model can predict and enhance their mechanical performance. Computational modeling techniques can express nonlinear, multimodal functions in failure analysis of engineering materials, reducing environmental waste and providing sustainable technology. Machine learning is used in manufacturing and industrial sectors to optimize process parameters and model data-driven correlations between processes, structures, and properties. Machine learning and artificial intelligence can be combined to enable atomistic-scale damage tolerance design, which can be customized using computer programming. The main advantage of machine learning is that it can be well integrated with finite element, analytical, or empirical modeling with a significant increase in the yield of the model being used.}
}
@article{RAMAMURTHY20124247,
title = {Design for sustainability: The role of CAD},
journal = {Renewable and Sustainable Energy Reviews},
volume = {16},
number = {6},
pages = {4247-4256},
year = {2012},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2012.03.009},
url = {https://www.sciencedirect.com/science/article/pii/S1364032112001918},
author = {Sudhir {Rama Murthy} and Monto Mani},
keywords = {Design technology, Sustainability, Design process, Design tools, Creativity, Computer Aided Design},
abstract = {The term design in this paper particularly refers to the process (verb) and less to the outcome or product. Design comprises a complex set of activities today involving both man and machine. Sustainability is a fundamental paradigm and carries significance in any process, natural or manmade, and its outcome. In simple terms, sustainability implies a state of sustainable living, viz. health and continuity, nurtured by diversity and evolution (innovations) in an ever-changing world. Design, in a similar line, has been comprehensively investigated and its current manifestations including design-aids (Computer Aided Design) have been evaluated in terms of sustainability. The paper investigates the rationale of sustainability to design as a whole – its purpose, its adoption in the natural world, its relevance to humankind and the technologies involved. Throughout its history, technology has been used to aid design. But in the current context of advanced algorithms and computational capacity, design no longer remains an exclusively animate faculty. Given this scenario, investigating sustainability in the light of advanced design aids such as CAD becomes pertinent. Considering that technology plays a part in design activities, the paper explores where technology must play a part and to what degree amongst the various activities that comprise design. The study includes an examination of the morphology of design and the development of a systems-thinking integrated forecasting model to evaluate the implications of CAD tools in design and sustainability. The results of the study along with a broad range of recommendations have been presented.}
}
@article{LIANG2019341,
title = {Is ecoregional scale precise enough for lake nutrient criteria? Insights from a novel relationship-based clustering approach},
journal = {Ecological Indicators},
volume = {97},
pages = {341-349},
year = {2019},
issn = {1470-160X},
doi = {https://doi.org/10.1016/j.ecolind.2018.10.034},
url = {https://www.sciencedirect.com/science/article/pii/S1470160X18308094},
author = {Zhongyao Liang and Yong Liu and Huili Chen and Yao Ji},
keywords = {Spatial scale, Nutrient criteria, Relationship-based clustering approach, Relationship mapping, Hierarchical clustering, Leave-one-out cross-validation},
abstract = {While the ecoregional lake nutrient criteria have been widely used in the past two decades, the overconfidence on their applicability may mislead the pollution management decisions, considering the spatial heterogeneity within the ecoregion. The exploration of applicability is thereby important, but is hindered by the difficulty in recognizing reliable relationship patterns between the nutrient and management endpoint. We propose a novel relationship-based clustering approach (RCA) to explore whether the ecoregional scale is precise enough for nutrient criteria. The approach (a) simulates relationships using Bayesian Linear Models, (b) clusters lakes according to relationship similarities via relationship mapping and hierarchical clustering, and (c) identifies reliable relationship patterns based on the leave-one-out cross-validation. The RCA is then employed to explore Chlorophyll a-total phosphorus relationships of 34 lakes in four Ecological Drainage Units (EDUs) in the U.S. Long-term water quality data is from a newly established database (LAGOS-NE). The results show that multiple relationship patterns exist in all the EDUs. The ecoregional relationships misestimate the nutrient effect in over a half of lakes. Therefore, we determine that the ecoregional scale is not precise enough for nutrient criteria and the sub-ecoregional scale is then recommended. Besides, the RCA provides a backward thinking for determining the spatial scale and can be used in some other fields where relationship-based clustering is needed.}
}