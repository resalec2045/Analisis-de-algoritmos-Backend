@article{QIN2025105181,
title = {Deconstructing customer satisfaction recipes: A dynamic configurational framework leveraging the power of online reviews in tourism contexts},
journal = {Tourism Management},
volume = {110},
pages = {105181},
year = {2025},
issn = {0261-5177},
doi = {https://doi.org/10.1016/j.tourman.2025.105181},
url = {https://www.sciencedirect.com/science/article/pii/S0261517725000512},
author = {Yong Qin and Chaoguang Luo and Eric W.T. Ngai},
keywords = {Customer satisfaction, Online reviews, Panel fsQCA, Three-factor theory, Tourism contexts},
abstract = {The complex relationship between product or service attribute performance and customer satisfaction in online review environments has been widely discussed. However, attribute configuration for enhancing customer satisfaction under holistic thinking, particularly considering temporal interactions among attributes, remains underexplored. To address this gap, this study develops a dynamic configurational framework based on complexity and three-factor theories to deconstruct attribute recipes and their temporal evolution for improving customer satisfaction in tourism contexts. By extracting key service attributes and affects directly from tourist reviews, it identifies patterns that consistently achieve high satisfaction over time. This is the first study to integrate online review mining with panel fuzzy-set qualitative comparative analysis from a customer experience perspective. Findings reveal the existence of multiple equivalent causal pathways, with distinct satisfaction pathways for different tourist segments. Practically, normative causal recipes assist policymakers in optimizing tourism resource allocation and providing strategic insights to dynamically respond to tourist demands.}
}
@incollection{ERICSSON200112256,
title = {Protocol Analysis in Psychology},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {12256-12262},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/01598-9},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767015989},
author = {K.A. Ericsson},
abstract = {Protocol analysis is the name for the methodology for eliciting, transcribing, and encoding verbal reports of thoughts into objective data for evaluating and testing theories of thinking. Philosophers since Aristotle have introspected on their own thinking as a means to analyze the structure of their thought processes. However, introspective analysis of one's thoughts and behavior was found to be reactive. In response to these criticisms a general theoretical framework was developed for how participants could verbalize their thinking without influencing the course of their thinking. Instructions to elicit such immediate reports were developed and shown to uncover thinking without the reactive effects due to explanations and descriptions of their thinking. Rigorous methods for analyzing verbal reports have been developed based on a formal analysis of the tasks. Short segments of verbal reports are coded into formal categories and the resulting data show similar reliability and validity as other forms of data on cognitive processes, such as reaction times and eye fixations. This general framework for collecting and analyzing verbal reports of thinking has been applied to laboratory studies of memory, problem solving, and decision making, and to everyday life in the study of expert performance and text comprehension.}
}
@incollection{DIX2003381,
title = {CHAPTER 14 - Upside-Down ∀s and Algorithms—Computational Formalisms and Theory},
editor = {John M. Carroll},
booktitle = {HCI Models, Theories, and Frameworks},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {381-429},
year = {2003},
series = {Interactive Technologies},
isbn = {978-1-55860-808-5},
doi = {https://doi.org/10.1016/B978-155860808-5/50014-9},
url = {https://www.sciencedirect.com/science/article/pii/B9781558608085500149},
author = {Alan Dix}
}
@article{CAI2004135,
title = {Why do U.S. and Chinese students think differently in mathematical problem solving?: Impact of early algebra learning and teachers’ beliefs},
journal = {The Journal of Mathematical Behavior},
volume = {23},
number = {2},
pages = {135-167},
year = {2004},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2004.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732312304000124},
author = {Jinfa Cai},
abstract = {This paper reports two studies that examined the impact of early algebra learning and teachers’ beliefs on U.S. and Chinese students’ thinking. The first study examined the extent to which U.S. and Chinese students’ selection of solution strategies and representations is related to their opportunity to learn algebra. The second study examined the impact of teachers’ beliefs on their students’ thinking through analyzing U.S. and Chinese teachers’ scoring of student responses. The results of the first study showed that, for the U.S. sample, students who have formally learned algebraic concepts are as likely to use visual representations as those who have not formally learned algebraic concepts in their problem solving. For the Chinese sample, students rarely used visual representations whether or not they had formally learned algebraic concepts. The findings of the second study clearly showed that U.S. and Chinese teachers view students’ responses involving concrete strategies and visual representations differently. Moreover, although both U.S. and Chinese teachers value responses involving more generalized strategies and symbolic representations equally high, Chinese teachers expect 6th graders to use the generalized strategies to solve problems while U.S. teachers do not. The research reported in this paper contributed to our understanding of the differences between U.S. and Chinese students’ mathematical thinking. This research also established the feasibility of using teachers’ scoring of student responses as an alternative and effective way of examining teachers’ beliefs.}
}
@article{BERNALMANRIQUE202086,
title = {Effect of acceptance and commitment therapy in improving interpersonal skills in adolescents: A randomized waitlist control trial},
journal = {Journal of Contextual Behavioral Science},
volume = {17},
pages = {86-94},
year = {2020},
issn = {2212-1447},
doi = {https://doi.org/10.1016/j.jcbs.2020.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S2212144720301551},
author = {Koryn N. Bernal-Manrique and María B. García-Martín and Francisco J. Ruiz},
keywords = {Acceptance and commitment therapy, Interpersonal skills, Emotional disorders, Psychological flexibility, Repetitive negative thinking},
abstract = {This parallel randomized controlled trial evaluated the effect of acceptance and commitment therapy (ACT) focused on repetitive negative thinking (RNT) versus a waitlist control (WLC) in improving interpersonal skills in adolescents with problems of social and school adaptation. Forty-two adolescents (11–17 years) agreed to participate. Participants were allocated through simple randomization to the intervention condition or the waitlist control condition. The intervention was a 3-session, group-based, RNT-focused ACT protocol. The primary outcome was the performance on a test of interpersonal skills (Interpersonal Conflict Resolution Assessment, ESCI). At posttreatment, repeated measures ANOVA showed that the intervention was efficacious in increasing overall interpersonal skills (d = 2.62), progress in values (d = 1.23), and reducing emotional symptoms (d = 0.98). No adverse events were found. A brief RNT-focused ACT intervention was highly efficacious in improving interpersonal skills and reducing emotional symptoms in adolescents.}
}
@article{GILL2018733,
title = {Data to Decision and Judgment Making – a Question of Wisdom},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {30},
pages = {733-738},
year = {2018},
note = {18th IFAC Conference on Technology, Culture and International Stability TECIS 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.11.205},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318328702},
author = {Karamjit S Gill},
keywords = {algorithms, artificial intelligence, big data, calculation, decision, judgment, wisdom},
abstract = {The technological waves of super artificial intelligence, big data, algorithms, and machine learning continue to impact our thinking and actions, thereby affecting the ways individuals, professions and institutions make judgments. On the one hand, there is an argument that more data and knowledge together with the cyber physical system of industry4.0 will automatically push society along some track toward a better world for all. On the other hand, we hear worrying voices of the imponderable downsides of powerful new cyber-, bio-, Nano-technologies, and synthetic biology. In the age of uncertainties, big data and the algorithm, how is the decision and judgment making process being affected?}
}
@article{TURNQUIST2024112726,
title = {Adaptive mesh methods on compact manifolds via Optimal Transport and Optimal Information Transport},
journal = {Journal of Computational Physics},
volume = {500},
pages = {112726},
year = {2024},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2023.112726},
url = {https://www.sciencedirect.com/science/article/pii/S0021999123008215},
author = {Axel G.R. Turnquist},
keywords = {Optimal transport, Optimal information transport, Diffeomorphic density matching, Moving mesh methods, Convergent numerical methods, Compact manifolds},
abstract = {Moving mesh methods were devised to redistribute a mesh in a smooth way, while keeping the number of vertices of the mesh and their connectivity unchanged. A fruitful theoretical point-of-view is to take such moving mesh methods and think of them as an application of the diffeomorphic density matching problem. Given two probability measures μ0 and μ1, the diffeomorphic density matching problem consists of finding a diffeomorphic pushforward map T such that T#μ0=μ1. Moving mesh methods are seen to be an instance of the diffeomorphic density matching problem by treating the probability density as the local density of nodes in the mesh. It is preferable that the restructuring of the mesh be done in a smooth way that avoids tangling the connections between nodes, which would lead to numerical instability when the mesh is used in computational applications. This then suggests that a diffeomorphic map T is desirable to avoid tangling. The first tool employed to solve the moving mesh problem between source and target probability densities on the sphere was Optimal Transport (OT). Recently Optimal Information Transport (OIT) was rigorously derived and developed allowing for the computation of a diffeomorphic mapping by simply solving a Poisson equation. Not only is the equation simpler to solve numerically in OIT, but with Optimal Transport there is no guarantee that the mapping between probability density functions defines a diffeomorphism for general 2D compact manifolds. In this manuscript, we perform a side-by-side comparison of using Optimal Transport and Optimal Information Transport on the sphere for adaptive mesh problems. We choose to perform this comparison with recently developed provably convergent solvers, but these are, of course, not the only numerical methods that may be used. We believe that Optimal Information Transport is preferable in computations due to the fact that the partial differential equation (PDE) solve step is simply a Poisson equation. For more general surfaces M, we show how the Optimal Transport and Optimal Information Transport problems can be reduced to solving on the sphere, provided that there exists a diffeomorphic mapping Φ:M→S2. This implies that the Optimal Transport problem on M with a special cost function can be solved with regularity guarantees, while computations for the problem are performed on the unit sphere.}
}
@article{AMADEI2020120149,
title = {Revisiting positive peace using systems tools},
journal = {Technological Forecasting and Social Change},
volume = {158},
pages = {120149},
year = {2020},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2020.120149},
url = {https://www.sciencedirect.com/science/article/pii/S0040162520309756},
author = {Bernard Amadei},
keywords = {Complex systems, Systems thinking, System dynamics, Cross-impact analysis, Network analysis, Positive peace, Peace geometry},
abstract = {This paper looks at peace with an integrated perspective. As a state, peace cannot be measured directly and requires the use of proxies and indicators. This paper revisits the positive peace index (PPI) introduced by the Institute for Economy and Peace (IEP) through the lens of systems thinking and modeling. Three sets of systems tools (cross-impact analysis, network analysis, and system dynamics) are proposed to explicitly account for the different levels of influence and dependence among the eight domains used to determine the PPI at the country level. Although more comprehensive than the original IEP formulation, the integrated approach proposed herein requires decisionmakers to be systems thinkers and able to conduct a detailed analysis of how the eight domains influence (impact) or depend on (sensitive to) each other. The proposed approach allows decisionmakers to capture the multidimensional and cross-disciplinary nature of positive peace better. This paper also shows that the three components of peace (positive, negative, and cultural) initially proposed by Johan Galtung can be represented using three-dimensional geometric features.}
}
@article{HEILMAN20041,
title = {Computational models of epileptiform activity in single neurons},
journal = {Biosystems},
volume = {78},
number = {1},
pages = {1-21},
year = {2004},
issn = {0303-2647},
doi = {https://doi.org/10.1016/j.biosystems.2004.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0303264704000978},
author = {Avram D. Heilman and James Quattrochi},
keywords = {Paroxysmal depolarizing shifts (PDS), Sustained depolarizations (SD), Hippocampus, Autaptic CA1/CA3 pyramidal neuron, Voltage-gated Ca channels, Ca-dependent K channels},
abstract = {A series of original computational models written in NEURON of increasing physiological and morphological complexity were developed to determine the dominant causes of epileptiform behavior. Current injections to a model hippocampal pyramidal neuron consisting of three compartments produced the sustained depolarizations (SD) and simple paroxysmal depolarizing shifts (PDS) characteristic of ictal and interictal behavior in a cell, respectively. Our results indicate that SDs are the result of the semi-saturation of Na+, Ca2+ and K+ active channels, particularly the CaN, with regular Na+/K+ spikes riding atop a saturated depolarization; PDS rides on a similar semi-saturated depolarization whose shape depends more heavily on interactions between low-threshold voltage-gated Ca2+ channels (CaT) and Ca2+-dependent K+ channels. Our results reflect and predict recent physiological data, and we report here a cellular basis of epilepsy whose mechanisms reside mainly in the membrane channels, and not in specific morphology or network interactions, advancing a possible resolution to the cellular/network debate over the etiology of epileptiform activity.}
}
@article{GREGORY198254,
title = {Current design thinking: 24 papers from Design 79, I Chem E Midlands Branch (available from (ChemE Rugby) 336 pp, £15},
journal = {Design Studies},
volume = {3},
number = {1},
pages = {54},
year = {1982},
issn = {0142-694X},
doi = {https://doi.org/10.1016/0142-694X(82)90084-9},
url = {https://www.sciencedirect.com/science/article/pii/0142694X82900849},
author = {Sydney Gregory}
}
@article{LI202231,
title = {Application Analysis of Artificial Intelligent Neural Network Based on Intelligent Diagnosis},
journal = {Procedia Computer Science},
volume = {208},
pages = {31-35},
year = {2022},
note = {7th International Conference on Intelligent, Interactive Systems and Applications},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922014478},
author = {Yukun Li},
keywords = {Intelligent diagnosis, artificial intelligence network, automobile fault diagnosis, the neural network},
abstract = {In recent years, the continuous development of computer science and AI technology makes the application prospect of artificial intelligence in fault diagnosis emerge. As a simulation technology of human thinking pattern, intelligent diagnosis technology can check and manage the monitoring target in real time to ensure the accuracy of data information. This paper introduces the basic principles of key artificial intelligence technologies in the field of sports, such as convolutional neural network, object detection, object tracking and action recognition. Then it analyzes the application status of intelligent diagnosis technology and artificial intelligence network under intelligent diagnosis, and puts forward the application of artificial intelligence neural network in automobile fault diagnosis based on examples. In the construction of the neural network system, the real-time collection of vehicle operation data can be analyzed, once the fault is found, the driver can be notified in time to avoid safety accidents. The author summarizes the existing research results on the application of artificial intelligence algorithm in intelligent diagnosis, in order to provide help for the subsequent research.}
}
@incollection{MILLER2023125,
title = {Chapter 7 - Graduate and postgraduate education at a crossroads},
editor = {Susan Miller and Walter Moos and Barbara Munk and Stephen Munk and Charles Hart and David Spellmeyer},
booktitle = {Managing the Drug Discovery Process (Second Edition)},
publisher = {Woodhead Publishing},
edition = {Second Edition},
pages = {125-155},
year = {2023},
series = {Woodhead Publishing Series in Biomedicine},
isbn = {978-0-12-824304-6},
doi = {https://doi.org/10.1016/B978-0-12-824304-6.00009-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780128243046000092},
author = {Susan Miller and Walter Moos and Barbara Munk and Stephen Munk and Charles Hart and David Spellmeyer},
keywords = {Academia, Career, Critical thinking, Diversity, Education, Graduate school, Immigration, Industry, Jobs, Learn by doing, Medicinal chemistry, Online education, Organic chemistry, Pharmaceutical, Pharmacology, Postdoctoral, Postgraduate, Master’s degree, Doctorate},
abstract = {In this chapter, we introduce a proverbial crossroads in graduate and postgraduate education and jobs. We use medicinal chemistry as a core example for many topics, representative of what could also be said about pharmacology and other critical disciplines involved in drug discovery. Many factors are at play today for drug hunters, including an explosion of information, available now, at your fingertips, a move away from memorization toward critical thinking, the importance of learning by doing, and what has in the past been called “the gathering storm.” Core drug discovery disciplines are discussed, along with the importance of diversity and interdisciplinary skills and the value of academia-industry symbiosis. Challenges in making sure we continue to educate and engage the best and the brightest to tackle important biomedical problems are considered, especially in the context of personalized medicine and its interfaces with big data, bioinformatics, pharmacogenomics, and more. Finally, we scratch the surface on how to navigate graduate school, postdocs, employers, and careers.}
}
@article{KONG20241462,
title = {On locality of quantum information in the Heisenberg picture for arbitrary states},
journal = {Chinese Journal of Physics},
volume = {89},
pages = {1462-1473},
year = {2024},
issn = {0577-9073},
doi = {https://doi.org/10.1016/j.cjph.2024.04.028},
url = {https://www.sciencedirect.com/science/article/pii/S0577907324001655},
author = {Otto C.W. Kong},
keywords = {Quantum information, Quantum locality, Deutsch–Hayden descriptors, Noncommutative values of observables},
abstract = {The locality issue of quantum mechanics is a key issue to a proper understanding of quantum physics and beyond. What has been commonly emphasized as quantum nonlocality has received an inspiring examination through the notion of the Heisenberg picture of quantum information. Deutsch and Hayden established a local description of quantum information in a setting of quantum information flow in a system of qubits. With the introduction of a slightly modified version of what we call the Deutsch–Hayden matrix values of observables, together with our recently introduced parallel notion of the noncommutative values from a more fundamental perspective, we clarify all the locality issues based on such values as quantum information carried by local observables in any given arbitrary state of a generic composite system. Quantum information as the ‘quantum’ values of observables gives a transparent conceptual picture of all that. Spatial locality for a projective measurement is also discussed. The pressing question is if and how such information for an entangled system can be retrieved through local processes which can only be addressed with new experimental thinking.}
}
@article{BADIA2024101049,
title = {Analysing the radiation reliability, performance and energy consumption of low-power SoC through heterogeneous parallelism},
journal = {Sustainable Computing: Informatics and Systems},
volume = {44},
pages = {101049},
year = {2024},
issn = {2210-5379},
doi = {https://doi.org/10.1016/j.suscom.2024.101049},
url = {https://www.sciencedirect.com/science/article/pii/S2210537924000945},
author = {Jose M. Badia and German Leon and Mario Garcia-Valderas and Jose A. Belloch and Almudena Lindoso and Luis Entrena},
keywords = {Heterogeneous parallelism, System-on-Chip, Fault tolerance, Energy consumption, Neutron irradiation},
abstract = {This study focuses on the low-power Tegra X1 System-on-Chip (SoC) from the Jetson Nano Developer Kit, which is increasingly used in various environments and tasks. As these SoCs grow in prevalence, it becomes crucial to analyse their computational performance, energy consumption, and reliability, especially for safety-critical applications. A key factor examined in this paper is the SoC’s neutron radiation tolerance. This is explored by subjecting a parallel version of matrix multiplication, which has been offloaded to various hardware components via OpenMP, to neutron irradiation. Through this approach, this researcher establishes a correlation between the SoC’s reliability and its computational and energy performance. The analysis enables the identification of an optimal workload distribution strategy, considering factors such as execution time, energy efficiency, and system reliability. Experimental results reveal that, while the GPU executes matrix multiplication tasks more rapidly and efficiently than the CPU, using both components only marginally reduces execution time. Interestingly, GPU usage significantly increases the SoC’s critical section, leading to an escalated error rate for both Detected Unrecoverable Errors (DUE) and Silent Data Corruptions (SDC), with the CPU showing a higher average number of affected elements per SDC.}
}
@article{M2023120604,
title = {Design of a Cognitive Knowledge Representation Model to Assess the Reasoning Levels of Primary School Children},
journal = {Expert Systems with Applications},
volume = {231},
pages = {120604},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120604},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423011065},
author = {Srivani M. and Abirami Murugappan},
keywords = {Customized AI based teaching, Cognitive performance test, Reasoning coefficient, Cognition level, Knowledge representation, Cognitive metrics},
abstract = {Background and aim:
In recent days, the research on student’s intelligence level modelling is a challenging Artificial Intelligence (AI) task, which gains more attraction because it provides actionable insights to the tutor by analysing the intelligence level of the learners. Each learner’s knowledge, comprehension, and intellectual capacities are unique. It is critical to identify these capacities and provide learners, particularly slow learners, with the necessary knowledge. Cognitive Performance Test (CPT) is an essential component for assessing the knowledge level of students. The reasoning level or coefficient deals with the analysis of the thinking capability in a logical way. It also reflects the child’s learning potential. The main aim of the proposed system is to design a Cognitive Knowledge Representation Model (CKRM), which fuses Cognitive Performance Metrics (CPM) calculation and Reasoning Coefficient Calculation (RCC) algorithms to assess the student’s intelligence level. The result of the proposed system is stratification of students to three different ranges of reasoning coefficient.
Methods:
The CKRM consists of the following phases: data collection, statistical Exploratory Data Analysis (EDA), model building and analysis, which involve the assessment of the knowledge level using CPT and calculation of reasoning coefficient using First Order Logic (FOL), and finally model evaluation using cognitive evaluation metrics. CPM and RCC algorithms have been proposed in this paper to calculate the student’s reasoning coefficient by using the forward chaining FOL inference engine. The dataset is a real time data which consists of the academic and cognitive performance details of school students from classes 1 to 6 for the year 2019 to 2020. The academic data are collected from the Educational Management Information System (EMIS) maintained by the school. The cognitive performance data are collected by conducting the tests for the students using the memory training application called Lumosity.
Results:
The proposed system’s performance is evaluated using ten Machine Learning (ML) algorithms in which the Quadratic Discriminant Analysis achieved an accuracy of 0.97 for classes 1, 2, and 3. For classes 4, 5, and 6, nearly twelve ML algorithms are evaluated in which Random Forest (RF) Classifier achieved an accuracy of 0.98. Six math expert committee teachers concluded that the reasoning coefficient value was acceptable with an average accuracy of 0.92 for classes 1, 2, 3 and 0.9 for classes 4, 5, 6. In comparison to the pre-existing models employed in the prior research, it was determined that the created CKRM (academic and cognitive) was superior. The cognitive metrics such as taskability, Response Time (RT), knowledge capacity and utilization has also been evaluated. The average values of taskability, RT, knowledge capacity and knowledge utilization are 0.85, 0.81, 0.55, and 0.44.
Conclusion:
The ultimate goal is to make customized teaching easier; hence, this article involves determining a student’s cognitive level by estimating their reasoning coefficient. The suggested approach analyses and categorizes students’ cognitive abilities, such as memory, reasoning, problem solving, thinking, and logical reasoning, using three different reasoning coefficients. This approach assists teachers in determining the degree of intelligence of their students.}
}
@article{PETERSON20223586,
title = {Physical computing for materials acceleration platforms},
journal = {Matter},
volume = {5},
number = {11},
pages = {3586-3596},
year = {2022},
issn = {2590-2385},
doi = {https://doi.org/10.1016/j.matt.2022.09.022},
url = {https://www.sciencedirect.com/science/article/pii/S2590238522005409},
author = {Erik Peterson and Alexander Lavin},
keywords = {materials acceleration platforms, AI-driven science, simulation intelligence, physical computing, self-driving labs, inverse design, computational metamaterials},
abstract = {Summary
A “technology lottery” describes a research idea or technology succeeding over others because it is suited to available software/hardware and not necessarily because it is superior. The nascent field of self-driving laboratories, particularly materials acceleration platforms (MAPs), is at risk: while it is logical and opportunistic to inject existing lab equipment and workflows with artificial intelligence (AI) and automation, such MAPs can constrain research by proliferating existing biases in science, mechatronics, and general-purpose computing. Rather than conformity, MAPs present opportunity to pursue new vectors of engineering physics with advances in cyber-physical learning and closed-loop, self-optimizing systems. We outline a simulation-based MAP program to design computers that use physics to solve optimization problems: the physical computing (PC)-MAP can mitigate hardware-software-substrate-user information losses present in all other MAP classes and eliminate lotteries by perfecting alignment between computing problems and media. We describe early PC advances and research pursuits toward optimal design of new materials and computing media.}
}
@article{LIU2024105391,
title = {Exploring three pillars of construction robotics via dual-track quantitative analysis},
journal = {Automation in Construction},
volume = {162},
pages = {105391},
year = {2024},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2024.105391},
url = {https://www.sciencedirect.com/science/article/pii/S0926580524001274},
author = {Yuming Liu and Aidi Hizami Bin Alias and Nuzul Azam Haron and Nabilah Abu Bakar and Hao Wang},
keywords = {Construction robotics, BERTopic model, BIM, Human–robot collaboration, Deep reinforcement learning, Dual-track quantitative analysis},
abstract = {Construction robotics has emerged as a leading technology in the construction industry. This paper conducts an innovative dual-track quantitative comprehensive method to analyze the current literature and assess future trends. First, a bibliometric review of 955 journal articles published between 1974 and 2023 was performed, exploring keywords, journals, countries, and clusters. Furthermore, a neural topic model based on BERTopic addresses topic modeling repetition issues. The study identifies building information modeling (BIM), human–robot collaboration (HRC), and deep reinforcement learning (DRL) as “three pillars” in the field. Additionally, we systematically reviewed the relevant literature and nested symbiotic relationships. The outcome of this study is twofold: first, the findings provide quantitative and qualitative scientific guidance for future research on trends; second, the innovative dual-track quantitative analysis research methodology simultaneously stimulates critical thinking about the modeling of other similarly trending topics characterized to avoid high degree of homogeneity and corpus overlap.}
}
@article{LI2021104369,
title = {Elementary effects analysis of factors controlling COVID-19 infections in computational simulation reveals the importance of social distancing and mask usage},
journal = {Computers in Biology and Medicine},
volume = {134},
pages = {104369},
year = {2021},
issn = {0010-4825},
doi = {https://doi.org/10.1016/j.compbiomed.2021.104369},
url = {https://www.sciencedirect.com/science/article/pii/S0010482521001633},
author = {Kelvin K.F. Li and Stephen A. Jarvis and Fayyaz Minhas},
keywords = {COVID-19, Agent-based modelling, Coronavirus, Simulation, SARS-COV-2, netlogo, Python, Epidemiology, Survival, Infectious diseases, VIRUS, Stochastic processes, Stochasticity, Social distancing, Masks, Isolation, Lockdown},
abstract = {COVID-19 was declared a pandemic by the World Health Organisation (WHO) on March 11th, 2020. With half of the world's countries in lockdown as of April due to this pandemic, monitoring and understanding the spread of the virus and infection rates and how these factors relate to behavioural and societal parameters is crucial for developing control strategies. This paper aims to investigate the effectiveness of masks, social distancing, lockdown and self-isolation for reducing the spread of SARS-CoV-2 infections. Our findings from an agent-based simulation modelling showed that whilst requiring a lockdown is widely believed to be the most efficient method to quickly reduce infection numbers, the practice of social distancing and the usage of surgical masks can potentially be more effective than requiring a lockdown. Our multivariate analysis of simulation results using the Morris Elementary Effects Method suggests that if a sufficient proportion of the population uses surgical masks and follows social distancing regulations, then SARS-CoV-2 infections can be controlled without requiring a lockdown.}
}
@article{AMACHER2020100022,
title = {Specificity in PDZ-peptide interaction networks: Computational analysis and review},
journal = {Journal of Structural Biology: X},
volume = {4},
pages = {100022},
year = {2020},
issn = {2590-1524},
doi = {https://doi.org/10.1016/j.yjsbx.2020.100022},
url = {https://www.sciencedirect.com/science/article/pii/S2590152420300040},
author = {Jeanine F. Amacher and Lionel Brooks and Thomas H. Hampton and Dean R. Madden},
keywords = {Protein-protein interactions, PDZ, Peptide-binding domains, Therapeutic targets},
abstract = {Globular PDZ domains typically serve as protein–protein interaction modules that regulate a wide variety of cellular functions via recognition of short linear motifs (SLiMs). Often, PDZ mediated-interactions are essential components of macromolecular complexes, and disruption affects the entire scaffold. Due to their roles as linchpins in trafficking and signaling pathways, PDZ domains are attractive targets: both for controlling viral pathogens, which bind PDZ domains and hijack cellular machinery, as well as for developing therapies to combat human disease. However, successful therapeutic interventions that avoid off-target effects are a challenge, because each PDZ domain interacts with a number of cellular targets, and specific binding preferences can be difficult to decipher. Over twenty-five years of research has produced a wealth of data on the stereochemical preferences of individual PDZ proteins and their binding partners. Currently the field lacks a central repository for this information. Here, we provide this important resource and provide a manually curated, comprehensive list of the 271 human PDZ domains. We use individual domain, as well as recent genomic and proteomic, data in order to gain a holistic view of PDZ domains and interaction networks, arguing this knowledge is critical to optimize targeting selectivity and to benefit human health.}
}
@incollection{ALISEDA2007431,
title = { - Logical, Historical and Computational Approaches},
editor = {Theo A.F. Kuipers},
booktitle = {General Philosophy of Science},
publisher = {North-Holland},
address = {Amsterdam},
pages = {431-513},
year = {2007},
series = {Handbook of the Philosophy of Science},
issn = {18789846},
doi = {https://doi.org/10.1016/B978-044451548-3/50010-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780444515483500100},
author = {Atocha Aliseda and Donald Gillies},
abstract = {Publisher Summary
This chapter focuses on the logical, historical and computational approaches to the philosophy of science. It discusses how the logical approach to philosophy of science was introduced by the Vienna Circle, and developed by them and their followers and associates. The logical approach to philosophy of science remained the dominant subject throughout the 1950s; but, from the early 1960s, it was challenged by a striking development of the historical approach. The historical approach was not introduced for the ﬁrst time in the 1960s. On the contrary, it had been developed by Mach and Duhem much earlier. Although, Mach and Duhem are cited by the Vienna Circle as important inﬂuences on their philosophy, the Vienna Circle did not adopt the historical features of these two thinkers. In the excitement generated by the new logic of Frege and Russell, history of science seems to have been temporarily forgotten. The general idea of the historical approach is not new in the 1960s, however, that decade saw striking developments in this approach. After Kuhn, the analysis of scientiﬁc revolutions became a major problem for philosophy of science, while Lakatos applied the historical approach to mathematics for the ﬁrst time.}
}
@article{SEVERENGIZ2018429,
title = {Influence of Gaming Elements on Summative Assessment in Engineering Education for Sustainable Manufacturing},
journal = {Procedia Manufacturing},
volume = {21},
pages = {429-437},
year = {2018},
note = {15th Global Conference on Sustainable Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.02.141},
url = {https://www.sciencedirect.com/science/article/pii/S235197891830180X},
author = {Mustafa Severengiz and Ina Roeder and Kristina Schindler and Günther Seliger},
keywords = {summative assessment, gamification, higher education, engineering education},
abstract = {Regarding the massive sustainability challenge mankind is currently facing, there is an indisputable need to implement sustainability as the key reference point into higher engineering education in order to prepare the stakeholders of tomorrow. This requires networked thinking on the part of the learner and increases the learning goals’ complexity dramatically. The actual achieved learning outcomes are often evaluated by assessing factual knowledge in higher education. However, it has been shown many times that students choose the examination format for orientation when studying. Thus, the authors propose a gamified summative assessment approach that requires networked thinking to direct students’ learning efforts towards broad competency building. In a study with 25 students of a master engineering course, the effects of a gamified examination design are investigated.}
}
@article{BONCHEKDOKOW201444,
title = {Towards computational models of intention detection and intention prediction},
journal = {Cognitive Systems Research},
volume = {28},
pages = {44-79},
year = {2014},
note = {Special Issue on Mindreading},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2013.07.004},
url = {https://www.sciencedirect.com/science/article/pii/S1389041713000399},
author = {Elisheva Bonchek-Dokow and Gal A. Kaminka},
keywords = {Intention recognition, Intention prediction, Cognitive modeling},
abstract = {Intention recognition is one of the core components of mindreading, an important process in social cognition. Human beings, from age of 18months, have been shown to be able to extrapolate intentions from observed actions, even when the performer failed at achieving the goal. Existing accounts of intention recognition emphasize the use of an intent (plan) library, which is matched against observed actions for recognition. These therefore cannot account for recognition of failed sequences of actions, nor novel actions. In this paper, we begin to tackle these open questions by examining computational models for components of human intention recognition, which emphasize the ability of humans to detect and identify intentions in a sequence of observed actions, based solely on the rationality of movement (its efficiency). We provide a high-level overview of intention recognition as a whole, and then elaborate on two components of the model, which we believe to be at its core, namely, those of intention detection and intention prediction. By intention detection we mean the ability to discern whether a sequence of actions has any underlying intention at all, or whether it was performed in an arbitrary manner with no goal in mind. By intention prediction we mean the ability to extend an incomplete sequence of actions to its most likely intended goal. We evaluate the model, and these two components, in context of existing literature, and in a number of experiments with more than 140 human subjects. For intention detection, our model was able to attribute high levels of intention to those traces perceived by humans as intentional, and vice versa. For intention prediction as well, our model performed in a way that closely matched that of humans. The work highlights the intimate relationship between the ability to generate plans, and the ability to recognize intentions.}
}
@article{COELHOLOPES2023103555,
title = {The structure of a strategic crisis management model: The context and characteristics of a brazilian community college},
journal = {International Journal of Disaster Risk Reduction},
volume = {87},
pages = {103555},
year = {2023},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2023.103555},
url = {https://www.sciencedirect.com/science/article/pii/S2212420923000353},
author = {Gisele Silveira {Coelho Lopes} and Carlos Ricardo Rossetto and Micheline {Ramos de Oliveira} and Jorge Oneide Sausen and Rudimar {Antunes da Rocha}},
keywords = {Crisis management, Organizational strategy, Community university},
abstract = {This study presents the structure of a strategic crisis management model, considering the context and characteristics of a Brazilian community college. Strategic crisis management associated with coordination and control mechanisms theoretically underpinned the problem under study. It is a single case study, with grounded theory as the strategy for data treatment and content analysis to interpret and present the findings. In the model, the strategic and tactical stages systematized the dynamics of crisis management in a coordinated manner. Strategic crisis management was considered a continuous process rather than a strictly punctual one. The dynamism of the model's operationalization considered some premises that guided the behavior of the leadership and the crisis management team: i) pragmatic strategic thinking shaped by rationality; ii) quick responses in facing the crisis; iii) simplicity in actions; iv) reversible decisions susceptible to flexibilization; v) creativity and boldness to innovate and set new standards; v) collaboration for common causes.}
}
@article{LI2025e42756,
title = {Classifying breast intraductal proliferative lesions via a knowledge distillation framework using convolutional neural network-based nuclei-segmentation-assisted classification (KDCNN-NSAC)},
journal = {Heliyon},
pages = {e42756},
year = {2025},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2025.e42756},
url = {https://www.sciencedirect.com/science/article/pii/S2405844025011375},
author = {Xiangmin Li and Jiamei Chen and Bo Luo and Minyan Xia and Xu Zhang and Hangjia Zhu and Yutian Zhang-Cai and Yongshun Chen and Yang Yang and Yaofeng Wen},
keywords = {Breast intraductal proliferative lesions, Breast cancer, Knowledge distillation, Convolutional neural network, Nuclei segmentation, Classification},
abstract = {ABSTRACT
Background and objective
Diagnosis of breast intraductal proliferative lesions (BIDPLs) in hematoxylin-eosin (HE) images remains a time-consuming and intractable topic because of subjective processes and subtle morphological differences. Convolutional neural networks (CNNs) show great potential for providing objective analysis strategies for HE images. In this study, we proposed a novel knowledge distillation (KD) framework using CNN-based nuclei segmentation-assisted classification (KDCNN-NSAC).
Methods
The diagnosis of BIDPLs is treated as multiple class classification tasks in the BReAst Carcinoma Subtyping dataset. The KDCNN-NSAC fully leveraged the epithelial and stromal nuclei-level features in training phases and performed region-of-interest (ROI)-level classifications in predicting phases. Then, the whole slide image (WSI) was diagnosed based on the risk ratings of the ROIs within it, instead of processing a WSI.
Results
The principal results showed that in ROI-level classifications, KDCNN-NSAC outperformed the state-of-the-art methods for 7-class classification with an average F1 score of 63.26% and achieves F1 score of 98.36% and 94.21%, respectively, in distinguishing BIDPLs from invasive cancer and normal tissue. The WSI-level predictions obtained a high degree of consistency with the pathologists’ annotation (kappa value of 0.88). Ablation experiments showed that nuclei segmentation and classification components improve the performance of the baseline model in KDCNN-NSAC by 3%.
Conclusions
The KDCNN-NSAC makes the model focus on important cellular information and predicts the WSI in accordance with the pathologists’ diagnostic thinking, thus improving model explainability. Moreover, the introduce of KDCNN-NSAC will help achieve superior performance in diagnosing BIDPLs.}
}
@article{PARTTO2012442,
title = {Explaining failures in innovative thought processes in engineering design},
journal = {Procedia - Social and Behavioral Sciences},
volume = {41},
pages = {442-449},
year = {2012},
note = {The First International Conference on Leadership, Technology and Innovation Management},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2012.04.053},
url = {https://www.sciencedirect.com/science/article/pii/S1877042812009330},
author = {Minna Pärttö and Pertti Saariluoma},
keywords = {Microinnovation processes, engineering design, thought errors, thought failures},
abstract = {The aim of this study is to explore factors causing failures in innovative thought processes in engineering design. An innovation process is here understood as a complex and multi-phased thinking and problem solving process generating new and mostly unforeseeable solutions. The phases are partly overlapping and simultaneous. This complicated nature of innovation process demands a lot from innovation management, and thus it is not unusual that innovation processes fail. Identifying problems and shortcomings is important because it helps organizations to eliminate them in the future. This study focus on thought processes of individual participants in an innovation process, which is referred by us as microinnovation approach. This approach understands innovations as being based on human thinking.This study shows that factors related to knowledge, management and interaction are causing failures in engineering design. We found haste to be the most common reason for failures. Other contributing factors were lack of long-term thinking and inability to understand others’ perspective.}
}
@article{KALBANDE2023138474,
title = {Machine learning based quantification of VOC contribution in surface ozone prediction},
journal = {Chemosphere},
volume = {326},
pages = {138474},
year = {2023},
issn = {0045-6535},
doi = {https://doi.org/10.1016/j.chemosphere.2023.138474},
url = {https://www.sciencedirect.com/science/article/pii/S0045653523007415},
author = {Ritesh Kalbande and Bipin Kumar and Sujit Maji and Ravi Yadav and Kaustubh Atey and Devendra Singh Rathore and Gufran Beig},
keywords = {Ozone, VOCs, Machine learning, Meteorology, Isoprene},
abstract = {The prediction of surface ozone is essential attributing to its impact on human and environmental health. Volatile organic compounds (VOCs) are crucial in driving ozone concentration; particularly in urban areas where VOC limited regimes are prominent. The limited measurements of VOCs, however, hinder assessing the VOC-ozone relationship. This work applies machine learning (ML) algorithms for temporal forecasting of surface ozone over a metropolitan city in India. The availability of continuous VOCs measurement data along with meteorology and other pollutants during 2014–2016 makes it possible to deduce the influence of various input parameters on surface ozone prediction. After evaluating the best ML model for ozone prediction, simulations were carried out using varied input combinations. The combination with isoprene, meteorology, NOx, and CO (Isop + MNC) was the best with RMSE 4.41 ppbv and MAPE 6.77%. A season-wise comparison of simulations having all data, only meteorological data and Isop + MNC as input showed that Isop + MNC simulation gives the best results during the summer season (RMSE: 5.86 ppbv, MAPE: 7.05%). This shows the increased ability of the model to capture ozone peaks (high ozone during summer) relatively better when isoprene data is used. The overall results highlight that using all available data doesn't necessarily give best prediction results; also critical thinking is essential when evaluating the model results.}
}
@article{MIASNIKOVA202126,
title = {Cross-frequency phase coupling of brain oscillations and relevance attribution as saliency detection in abstract reasoning},
journal = {Neuroscience Research},
volume = {166},
pages = {26-33},
year = {2021},
issn = {0168-0102},
doi = {https://doi.org/10.1016/j.neures.2020.05.012},
url = {https://www.sciencedirect.com/science/article/pii/S0168010219305772},
author = {Aleksandra Miasnikova and Gleb Perevoznyuk and Olga Martynova and Mikhail Baklushev},
keywords = {Abstract reasoning, Salience, Phase synchronization, Cross-frequency coupling, Phase-to-phase coupling, EEG},
abstract = {Abstract reasoning is associated with the ability to detect relations among objects, ideas, events. It underlies the understanding of other individuals’ thoughts and intentions. In natural settings, individuals have to infer relevant associations that have proven to be reliable or precise predictors. Salience theory suggests that the attribution of meaning to stimulus depends on their contingency, saliency, and relevance to adaptation. So far, subjective estimates of relevance have mostly been explored in motivation and implicit learning. Mechanisms underlying formation of associations in abstract thinking with regard to their subjective relevance, or salience, are not clear. Applying novel computational methods, we investigated relevance detection in categorization tasks in 17 healthy individuals. Two models of relevance detection were developed: a conventional one with nouns from the same semantic category, an aberrant one based on an insignificant common feature. Control condition introduced non-related words. The participants were to detect either a relevant principle or an insignificant feature to group presented words. In control condition they inferred that the stimuli were irrelevant to any grouping idea. Cross-frequency phase coupling analysis revealed statistically distinct patterns of synchronization representing search and decision in the models of normal and aberrant relevance detection. Significantly distinct frontotemporal functional networks with central and parietal components in the theta and alpha frequency bands may reflect differences in relevance detection.}
}
@article{AIGBAVBOA20173003,
title = {Sustainable Construction Practices: “A Lazy View” of Construction Professionals in the South Africa Construction Industry},
journal = {Energy Procedia},
volume = {105},
pages = {3003-3010},
year = {2017},
note = {8th International Conference on Applied Energy, ICAE2016, 8-11 October 2016, Beijing, China},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2017.03.743},
url = {https://www.sciencedirect.com/science/article/pii/S1876610217308068},
author = {Clinton Aigbavboa and Ifije Ohiomah and Thulisile Zwane},
keywords = {Climate change, sustainable thinking, sustainable construction practices, South Africa},
abstract = {The construction industry has been found to cause damaging effects to the environment by means of waste generation, energy and water depletion and several other forms of damage to the environment. This damage has led to experts and environmentalist calling for a sustainable way of carrying out construction activities. Thus, this study addresses the challenges hindering the adoption of sustainable construction practices in the South Africa construction industry. The data used in this research were sourced from both primary and secondary sources. The primary data was collected through a questionnaire aimed at practicing construction professional in the South African construction industry. Indicative Findings from the questionnaire survey revealed that the foremost challenges faced by South African construction industry towards the adoption of sustainable construction practices is the assumption (a lazy view) of additional cost to building projects, followed by limited understanding of the benefits of sustainable construction amongst others. The study contributes to sustainability thinking in the South African construction industry; and it is recommended that strategies and actions should be pursued actively to speed up the process in creating a sustainable-oriented construction industry, which is paramount towards building a sustainable future.}
}
@article{GAN2024102786,
title = {Large models for intelligent transportation systems and autonomous vehicles: A survey},
journal = {Advanced Engineering Informatics},
volume = {62},
pages = {102786},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102786},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624004348},
author = {Lu Gan and Wenbo Chu and Guofa Li and Xiaolin Tang and Keqiang Li},
keywords = {Intelligent transportation systems, Autonomous vehicles, Survey, Large models, Efficient deployment techniques},
abstract = {Large models are widely used in intelligent transportation systems (ITS) and autonomous vehicles (AV) due to their excellent new capabilities such as intelligence emergence, domain application adaptive, and multi-task learning. By integrating massive amounts of data, vehicles and transportation systems based on large models can understand the real-world environment, simulate the reasoning process of human drivers, optimize traffic flow, and improve driving safety and efficiency. However, in the practical implementation of large models, there are three key research questions: (1) How can model reasoning be consistent with the target task? (2) How to improve the redundancy of structures and weights caused by complex models? (3) How to solve the problems of supercomputing power, high latency, and large throughput of large models in practical deployment? These challenges have stimulated the development of deployment techniques for large models. Although existing review articles discuss large model technologies from singular or partial perspectives, there remains a lack of comprehensive systematic investigations into the application and deployment of large models for ITS and AV. Therefore, this paper conducted a quantitative analysis of scientific literature, demonstrating the necessity and significance of studying large models for ITS and AV. Subsequently, it outlined the concept and characteristics of large models and provided a detailed summary of the frontier progress of large models for ITS and AV. Moreover, to bridge the gap between large model inference and target tasks, address structural and weight redundancies, and tackle challenges in practical deployment such as high computational power, latency, and throughput, it explored efficient deployment techniques to accelerate the rapid deployment of large models. Finally, it discussed the challenges and future trends. This paper aims to provide researchers and engineers with an understanding of the forefront advancements and future trends of large models to facilitate the rapid implementation of large models and accelerate their development in ITS and AV.}
}
@article{GAO20241233,
title = {Hetero-Bäcklund transformation, bilinear forms and multi-solitons for a (2＋1)-dimensional generalized modified dispersive water-wave system for the shallow water},
journal = {Chinese Journal of Physics},
volume = {92},
pages = {1233-1239},
year = {2024},
issn = {0577-9073},
doi = {https://doi.org/10.1016/j.cjph.2024.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0577907324003940},
author = {Xin-Yi Gao},
keywords = {Shallow water, Nonlinear and dispersive long gravity waves, (2＋1)-dimensional generalized modified dispersive water-wave system, Hetero-Bäcklund transformation, Bilinear form, Soliton, Symbolic computation},
abstract = {This shallow-water-directed paper plans to consider a (2＋1)-dimensional generalized modified dispersive water-wave (2DGMDWW) system, which describes the nonlinear and dispersive long gravity waves travelling along two horizontal directions in the shallow water of uniform depth. With symbolic computation, (1) a hetero-Bäcklund transformation is constructed, coupling the solutions as for the 2DGMDWW system with the solutions as for a known (2＋1)-dimensional Boiti-Leon-Pempinelli system describing the water waves in an infinitely narrow channel of constant depth, with that hetero-Bäcklund transformation dependent on the shallow-water coefficients in the 2DGMDWW system, with the former solutions indicating certain shallow-water-wave patterns for the height of the water surface and the horizontal velocity of the water wave, while with the latter solutions related to the horizontal velocity and elevation of the water wave; (2) two sets of the bilinear forms are obtained, each set of which is shown to depend on the shallow-water coefficients in the 2DGMDWW system and to be linked to certain shallow-water-wave patterns for the height of the water surface and the horizontal velocity of the water wave; and (3) two sets of the N-soliton solutions are also worked out, each set of which is seen to rely on the shallow-water coefficients in the 2DGMDWW system and to represent the existence of N-solitonic shallow-water-wave patterns with respect to the height of the water surface and the horizontal velocity of the water wave, with N as a positive integer.}
}
@incollection{PAUL2005431,
title = {Chapter 15 - Models of Computation for Systems-on-Chips},
editor = {Ahmed Amine Jerraya and Wayne Wolf},
booktitle = {Multiprocessor Systems-on-Chips},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {431-463},
year = {2005},
series = {Systems on Silicon},
issn = {18759661},
doi = {https://doi.org/10.1016/B978-012385251-9/50031-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780123852519500311},
author = {JoAnn M. Paul and Donald E. Thomas},
abstract = {Publisher Summary
This chapter describes system modeling and its relationship to models of computation. It compares several different models of computation and evaluates their usefulness at various stages in system design. It also describes the modeling environment for software and hardware (MESH) environment for hardware and software modeling. Models of computation (MoCs) are abstract representations of computing systems. Computer modeling can be separated into three areas—formal MoCs, computer artifacts, and computer design tools. A formal MoC is generally considered to be one with a mathematical basis. Simulations of formal models may be more efficient for large systems; however, the properties of formal models permit the representation of the system to be manipulated purely mathematically. Computer artifacts are the objects of computer architects. They include software, hardware, or both. Design tools are computer programs that are used to assist the construction of instances of computers as well as the conceptualization of computer artifacts. Design tools may be considered synonymous with design artifacts because they are objects, or entities, used to facilitate the design process. They may have a formal mathematical basis.}
}
@article{CHEN1998475,
title = {A computationally attractive nonlinear predictive control scheme with guaranteed stability for stable systems},
journal = {Journal of Process Control},
volume = {8},
number = {5},
pages = {475-485},
year = {1998},
note = {ADCHEM '97 IFAC Symposium: Advanced Control of Chemical Processes},
issn = {0959-1524},
doi = {https://doi.org/10.1016/S0959-1524(98)00021-3},
url = {https://www.sciencedirect.com/science/article/pii/S0959152498000213},
author = {H. Chen and F. Allgöwer},
keywords = {nonlinear predictive control, constraints, stability, terminal conditions},
abstract = {We introduce in this paper a nonlinear model predictive control scheme for open-loop stable systems subject to input and state constraints. Closed-loop stability is guaranteed by an appropriate choice of the finite prediction horizon, independent of the specification of the desired control performance. In addition, this control scheme is likely to allow ‘real time’ implementation, because of its computational attractiveness. The theoretical results are demonstrated and discussed with a CSTR control application.}
}
@article{CASTELLO202354,
title = {Towards competency-based education in the chemical engineering undergraduate program in Uruguay: Three examples of integrating essential skills},
journal = {Education for Chemical Engineers},
volume = {44},
pages = {54-62},
year = {2023},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2023.05.004},
url = {https://www.sciencedirect.com/science/article/pii/S1749772823000210},
author = {E. Castelló and C. Santiviago and J. Ferreira and R. Coniglio and E. Budelli and V. Larnaudie and M. Passeggi and I. López},
keywords = {Engineering education, Professional skills, Unconventional laboratory practice, Industrial Internships, Autonomous learning},
abstract = {In 2021, Universidad de la República in Uruguay approved a new Chemical Engineering undergraduate program that incorporates novel conceptual definitions such as competency-based education. This paper describes the process of defining the new curriculum plan and presents the program's structure, as well as specific and cross-disciplinary competencies. These competencies are then compared to the learning outcomes established in the guide for programs accreditation of the Institution of Chemical Engineers. To provide practical examples of how the competency-based approach was incorporated into the program, three specific cases are presented. The first case focuses on the implementation of the internship and industry project. The second case illustrates the incorporation of computational tools as an essential part of different courses throughout the degree program. Finally, the third case describes a new design for the fluid mechanics laboratory that emphasizes hands-on learning and helps students develop several competencies.}
}
@article{CROSSLEY2024100865,
title = {A large-scale corpus for assessing written argumentation: PERSUADE 2.0},
journal = {Assessing Writing},
volume = {61},
pages = {100865},
year = {2024},
issn = {1075-2935},
doi = {https://doi.org/10.1016/j.asw.2024.100865},
url = {https://www.sciencedirect.com/science/article/pii/S1075293524000588},
author = {S.A. Crossley and Y. Tian and P. Baffour and A. Franklin and M. Benner and U. Boser},
keywords = {Corpus linguistics, Writing assessment, Argumentation, Individual differences},
abstract = {This research methods article introduces the open source PERSUADE 2.0 corpus. The PERSUADE 2.0 corpus comprises over 25,000 argumentative essays produced by 6th-12th grade students in the United States for 15 prompts on two writing tasks: independent and source-based writing. The PERSUADE 2.0 corpus also provides detailed individual and demographic information for each writer. The goal of the PERSUADE 2.0 corpus is to advance research into relationships between discourse elements, their effectiveness, writing quality, writing tasks and prompts, and demographic and individual differences.}
}
@article{ZHOU20241018,
title = {A 21st Century View of Allowed and Forbidden Electrocyclic Reactions},
journal = {The Journal of Organic Chemistry},
volume = {89},
number = {2},
pages = {1018-1034},
year = {2024},
issn = {0022-3263},
doi = {https://doi.org/10.1021/acs.joc.3c02103},
url = {https://www.sciencedirect.com/science/article/pii/S0022326324000720},
author = {Qingyang Zhou and Garrett Kukier and Igor Gordiy and Roald Hoffmann and Jeffrey I. Seeman and K. N. Houk},
abstract = {In 1965, Woodward and Hoffmann proposed a theory to predict the stereochemistry of electrocyclic reactions, which, after expansion and generalization, became known as the Woodward–Hoffmann Rules. Subsequently, Longuet-Higgins and Abrahamson used correlation diagrams to propose that the stereoselectivity of electrocyclizations could be explained by the correlation of reactant and product orbitals with the same symmetry. Immediately thereafter, Hoffmann and Woodward applied correlation diagrams to explain the mechanism of cycloadditions. We describe these discoveries and their evolution. We now report an investigation of various electrocyclic reactions using DFT and CASSCF. We track the frontier molecular orbitals along the intrinsic reaction coordinate and modeled trajectories and examine the correlation between HOMO and LUMO for thermally forbidden systems. We also investigate the electrocyclizations of several highly polarized systems for which the Houk group had predicted that donor–acceptor substitution can induce zwitterionic character, thereby providing low-energy pathways for formally forbidden reactions. We conclude with perspectives on the field of pericyclic reactions, including a refinement as the meaning of Woodward and Hoffmann’s “Violations. There are none!” Lastly, we comment on the burgeoning influence of computations on all fields of chemistry.}
}
@incollection{RUBIN2023125,
title = {Chapter 9 - Unlocking creative tensions with a paradox approach},
editor = {Roni Reiter-Palmon and Sam Hunter},
booktitle = {Handbook of Organizational Creativity (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
pages = {125-145},
year = {2023},
isbn = {978-0-323-91840-4},
doi = {https://doi.org/10.1016/B978-0-323-91840-4.00006-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323918404000062},
author = {Matthew Rubin and Ella Miron-Spektor and Joshua Keller},
keywords = {Creativity, Innovation, Paradox, Mindset, Paradoxical leadership, Culture},
abstract = {Leaders and their employees must navigate competing yet interrelated demands and processes when developing and implementing creative ideas. They have to engage in divergent and convergent thinking, challenge existing assumptions and accept them, plan and persist while remaining spontaneous and adaptive. We explore how, why, and when adopting a paradox approach to navigating such tensions enhances creativity and innovation. Rather than seeking to eliminate the discomfort associated with tensions by prioritizing one demand or process over the other, the paradox approach sees tensions as an opportunity for growth and learning. When adopting a paradox approach, people feel comfortable with the discomfort as tensions arise and recognize that by engaging in one process they enable the seemingly opposing process. We review research on paradoxical frames, mindset, and leadership, and offer a comprehensive theoretical model that delineates the related cognitive, affective, motivational, and social pathways, as well as contextual and cultural boundary conditions. We conclude by identifying promising future directions for research.}
}
@incollection{JAIN2015181,
title = {Chapter Seven - Computational Methods for RNA Structure Validation and Improvement},
editor = {Sarah A. Woodson and Frédéric H.T. Allain},
series = {Methods in Enzymology},
publisher = {Academic Press},
volume = {558},
pages = {181-212},
year = {2015},
booktitle = {Structures of Large RNA Molecules and Their Complexes},
issn = {0076-6879},
doi = {https://doi.org/10.1016/bs.mie.2015.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0076687915000208},
author = {Swati Jain and David C. Richardson and Jane S. Richardson},
keywords = {RNA crystallography, RNA backbone conformers, Ribose pucker, Clash correction, MolProbity, PHENIX, ERRASER, wwPDB validation},
abstract = {With increasing recognition of the roles RNA molecules and RNA/protein complexes play in an unexpected variety of biological processes, understanding of RNA structure–function relationships is of high current importance. To make clean biological interpretations from three-dimensional structures, it is imperative to have high-quality, accurate RNA crystal structures available, and the community has thoroughly embraced that goal. However, due to the many degrees of freedom inherent in RNA structure (especially for the backbone), it is a significant challenge to succeed in building accurate experimental models for RNA structures. This chapter describes the tools and techniques our research group and our collaborators have developed over the years to help RNA structural biologists both evaluate and achieve better accuracy. Expert analysis of large, high-resolution, quality-conscious RNA datasets provides the fundamental information that enables automated methods for robust and efficient error diagnosis in validating RNA structures at all resolutions. The even more crucial goal of correcting the diagnosed outliers has steadily developed toward highly effective, computationally based techniques. Automation enables solving complex issues in large RNA structures, but cannot circumvent the need for thoughtful examination of local details, and so we also provide some guidance for interpreting and acting on the results of current structure validation for RNA.}
}
@article{VANHOOIJDONK2022101044,
title = {Creativity and change of context: The influence of object-context (in)congruency on cognitive flexibility},
journal = {Thinking Skills and Creativity},
volume = {45},
pages = {101044},
year = {2022},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2022.101044},
url = {https://www.sciencedirect.com/science/article/pii/S1871187122000475},
author = {Mare {van Hooijdonk} and Simone M. Ritter and Marcel Linka and Evelyn Kroesbergen},
keywords = {Creativity, Spatial context, Object congruence, Cognitive flexibility, Stimulating creativity},
abstract = {Specific environmental features, such as natural settings or spatial design, can foster creativity. The effect of object-context congruency on creativity has not yet been investigated. While congruence between an object and its visual context provides meaning to the object, it may hamper creativity due to mental fixation effects. In the current study, virtual reality technology (VR) was employed to examine the hypothesis that people display more cognitive flexibility - a key element of creativity, representing the ability to overcome mental fixation - when thinking about an object while being in an incongruent than in a congruent environment. Participants (N = 184) performed an Alternative Uses Task, in which they had to name as many uses for a book as possible, while being immersed in a virtual environment that was either object-context congruent (i.e., places where you would expect a book; e.g., a library or a living room; n = 91) or object-context incongruent (i.e., places where a book is not expected; e.g., a clothing store or a car workshop; n = 93). The effect of object (in)congruency was also assessed for three other indices of creativity: fluency (i.e., the number of ideas generated), originality and usefulness. In line with our hypothesis, participants scored higher on pure cognitive flexibility in the object-context incongruent than in the object-context congruent environment. Moreover, participants in the object-context incongruent environment condition generated more original ideas. The theoretical and practical implications of the current findings are discussed.}
}
@article{JUDD1997907,
title = {Computational economics and economic theory: Substitutes or complements?},
journal = {Journal of Economic Dynamics and Control},
volume = {21},
number = {6},
pages = {907-942},
year = {1997},
note = {Society of Computational Economics Conference},
issn = {0165-1889},
doi = {https://doi.org/10.1016/S0165-1889(97)00010-9},
url = {https://www.sciencedirect.com/science/article/pii/S0165188997000109},
author = {Kenneth L. Judd},
keywords = {Computational approach, Theoretical analysis},
abstract = {This essay examines the idea and potential of a ‘computational approach to theory’, discusses methodological issues raised by such computational methods, and outlines the problems associated with the dissemination of computational methods and the exposition of computational results. We argue that the study of a theory need not be confined to proving theorems, that current and future computer technologies create new possibilities for theoretical analysis, and that by resolving these issues we will create an intellectual atmosphere in which computational methods can make substantial contributions to economic analysis.}
}
@article{CHEN202321,
title = {Varieties of specification: Redefining over- and under-specification},
journal = {Journal of Pragmatics},
volume = {216},
pages = {21-42},
year = {2023},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2023.07.019},
url = {https://www.sciencedirect.com/science/article/pii/S037821662300200X},
author = {Guanyi Chen and Kees {van Deemter}},
keywords = {Referring expressions, Over-specification, Under-specification},
abstract = {A long tradition of research in theoretical, experimental and computational pragmatics has investigated over-specification and under-specification in referring expressions. Along broadly Gricean lines, these studies compare the amount of information expressed by a referring expression against the amount of information that is required. Often, however, these studies offer no formal definition of what “required” means, and how the comparison should be performed. In this paper, we use a simple set-theoretic perspective to define some communicatively important types of over-/under-specification. We argue that our perspective enables an enhanced understanding of reference phenomena that can pay important dividends for the analysis of reference in corpora and for the evaluation of computational models of referring. To illustrate and substantiate our claims, we analyse two corpora, containing Chinese and English referring expressions respectively, using the new perspective. The results show that interesting new monolingual and cross-linguistic insights can be obtained from our perspective.}
}
@article{JORAJURIA2022664,
title = {Oscillatory Source Tensor Discriminant Analysis (OSTDA): A regularized tensor pipeline for SSVEP-based BCI systems},
journal = {Neurocomputing},
volume = {492},
pages = {664-675},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2021.07.103},
url = {https://www.sciencedirect.com/science/article/pii/S0925231221018956},
author = {Tania Jorajuría and Mina {Jamshidi Idaji} and Zafer İşcan and Marisol Gómez and Vadim V. Nikulin and Carmen Vidaurre},
keywords = {Brain-computer interface, Steady-state visual evoked potential, Spatio-spectral decomposition, Higher order discriminant analysis, Analytical regularization, Tensor-based feature reduction},
abstract = {Periodic signals called Steady-State Visual Evoked Potentials (SSVEP) are elicited in the brain by flickering stimuli. They are usually detected by means of regression techniques that need relatively long trial lengths to provide feedback and/or sufficient number of calibration trials to be reliably estimated in the context of brain-computer interface (BCI). Thus, for BCI systems designed to operate with SSVEP signals, reliability is achieved at the expense of speed or extra recording time. Furthermore, regardless of the trial length, calibration free regression-based methods have been shown to suffer from significant performance drops when cognitive perturbations are present affecting the attention to the flickering stimuli. In this study we present a novel technique called Oscillatory Source Tensor Discriminant Analysis (OSTDA) that extracts oscillatory sources and classifies them using the newly developed tensor-based discriminant analysis with shrinkage. The proposed approach is robust for small sample size settings where only a few calibration trials are available. Besides, it works well with both low- and high-number-of-channel settings, using trials as short as one second. OSTDA performs similarly or significantly better than other three benchmarked state-of-the-art techniques under different experimental settings, including those with cognitive disturbances (i.e. four datasets with control, listening, speaking and thinking conditions). Overall, in this paper we show that OSTDA is the only pipeline among all the studied ones that can achieve optimal results in all analyzed conditions.}
}
@article{CHAN2022102109,
title = {Slow down to speed up: Longer pause time before solving problems relates to higher strategy efficiency},
journal = {Learning and Individual Differences},
volume = {93},
pages = {102109},
year = {2022},
issn = {1041-6080},
doi = {https://doi.org/10.1016/j.lindif.2021.102109},
url = {https://www.sciencedirect.com/science/article/pii/S1041608021001461},
author = {Jenny Yun-Chen Chan and Erin R. Ottmar and Ji-Eun Lee},
keywords = {Pause time, Strategy efficiency, Algebra problem-solving, Online learning environment, Metacognitive skills},
abstract = {We examined the influences of pre-solving pause time, algebraic knowledge, mathematics self-efficacy, and mathematics anxiety on middle-schoolers' strategy efficiency in an algebra learning game. We measured strategy efficiency using (a) the number of steps taken to complete a problem, (b) the proportion of problems completed on the initial attempt, and (c) the number of resets prior to completing the problems. Using the log data from the game, we found that longer pre-solving pause time was associated with more efficient strategies, as indicated by fewer solution steps, higher initial completion rate, and fewer resets. Higher algebraic knowledge was associated with higher initial completion rate and fewer resets. Mathematics self-efficacy and mathematics anxiety was not associated with any measures of strategy efficiency. The results suggest that pause time may be an indicator of student thinking before problem-solving, and provide insights into using data from online learning platforms to examine students' problem-solving processes.}
}
@article{CARLISLE1996248,
title = {Software Caching and Computation Migration in Olden},
journal = {Journal of Parallel and Distributed Computing},
volume = {38},
number = {2},
pages = {248-255},
year = {1996},
issn = {0743-7315},
doi = {https://doi.org/10.1006/jpdc.1996.0145},
url = {https://www.sciencedirect.com/science/article/pii/S0743731596901458},
author = {Martin C. Carlisle and Anne Rogers},
abstract = {Software caching and computation migration are mechanisms that satisfy remote references by either bringing a copy of the data to the computation or moving the computation to the data. We evaluate these mechanisms usingOlden, a system that, with minimal programmer annotations, provides parallelism for C programs that use recursively defined structures, such as trees, lists, and DAGs. We demonstrate that providing both software caching and computation migration can improve the performance of these programs, and provide a compile-time heuristic that selects between them for each pointer dereference. We have implemented the heuristic in Olden on the Thinking Machines CM-5. We describe our implementation and report on experiments with eleven benchmarks.}
}
@article{FAELENS2021106510,
title = {Social media use and well-being: A prospective experience-sampling study},
journal = {Computers in Human Behavior},
volume = {114},
pages = {106510},
year = {2021},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2020.106510},
url = {https://www.sciencedirect.com/science/article/pii/S0747563220302624},
author = {Lien Faelens and Kristof Hoorelbeke and Bart Soenens and Kyle {Van Gaeveren} and Lieven {De Marez} and Rudi {De Raedt} and Ernst H.W. Koster},
keywords = {Social media, Social comparison, Self-esteem, Repetitive negative thinking, Negative affect},
abstract = {Facebook and Instagram are currently the most popular Social Network Sites (SNS) for young adults. A large amount of research examined the relationship between these SNS and well-being, and possible intermediate constructs such as social comparison, self-esteem, and repetitive negative thinking (RNT). However, most of these studies have cross-sectional designs and use self-report indicators of SNS use. Therefore, their conclusions should be interpreted cautiously. Consequently, the goal of the current experience sampling study was to examine the temporal dynamics between objective indicators of SNS use, and self-reports of social comparison, RNT, and daily fluctuations in negative affect. More specifically, we assessed 98 participants 6 times per day during 14 days to examine reciprocal relationships between SNS use, negative affect, emotion regulation, and key psychological constructs. Results indicate that (1) both Facebook and Instagram use predicted reduced well-being, and (2) self-esteem and RNT appear to be important intermediate constructs in these relationships. Future longitudinal and experimental studies are needed to further support and extend the current research findings.}
}
@article{EDELMAN1997296,
title = {Computational theories of object recognition},
journal = {Trends in Cognitive Sciences},
volume = {1},
number = {8},
pages = {296-304},
year = {1997},
issn = {1364-6613},
doi = {https://doi.org/10.1016/S1364-6613(97)01090-5},
url = {https://www.sciencedirect.com/science/article/pii/S1364661397010905},
author = {S. Edelman},
abstract = {This paper examines four current theoretical approaches to the representation and recognition of visual objects: structural descriptions, geometric constraints, multidimensional feature spaces and shape-space approximation. The strengths and weaknesses of the four theories are considered, with a special focus on their approach to categorization — a computationally challenging task which is not widely addressed in computer vision, where the stress is rather on the generalization of recognition across changes of viewpoint.}
}
@article{KING1980313,
title = {Thinking: Readings in cognitive science: P.N. Johnson-Laird and P.C. Wason Cambridge University Press, 1977},
journal = {Artificial Intelligence},
volume = {13},
number = {3},
pages = {313-322},
year = {1980},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(80)90005-3},
url = {https://www.sciencedirect.com/science/article/pii/0004370280900053},
author = {Margaret King}
}
@article{ZORAN2025101135,
title = {Digital gastronomy 2.0: A 15-year transformative journey in culinary-tech evolution and interaction},
journal = {International Journal of Gastronomy and Food Science},
volume = {39},
pages = {101135},
year = {2025},
issn = {1878-450X},
doi = {https://doi.org/10.1016/j.ijgfs.2025.101135},
url = {https://www.sciencedirect.com/science/article/pii/S1878450X25000368},
author = {Amit Raphael Zoran},
abstract = {This paper reviews 15 years of exploration and development in Digital Gastronomy (DG), tracing its progression from foundational frameworks to AI-integrated culinary systems. The journey begins with integrating computational tools like laser cooking, 3D printing, CNC milling, and modular molds, which expand the possibilities of creativity and precision in the kitchen. Building on these technologies, the Meta-Recipe (MR) framework introduces a structured approach to recipe design, allowing chefs to adapt dishes dynamically while maintaining culinary coherence. The concept of “Digital Alchemy” extends this foundation, blending AI-driven methods with traditional healing and sustainable practices to emphasize well-being and environmental consciousness. These advancements culminate in the vision of an AI-augmented kitchen, conceptualized as a collaborative and adaptive space that bridges culinary artistry with algorithmic precision. This research highlights DG's potential as an evolving interdisciplinary field, offering new gastronomy, creativity, and sustainability directions.}
}
@article{HAGSTROM1998385,
title = {Experiments with approximate radiation boundary conditions for computational aeroacoustics},
journal = {Applied Numerical Mathematics},
volume = {27},
number = {4},
pages = {385-402},
year = {1998},
note = {Special Issue on Absorbing Boundary Conditions},
issn = {0168-9274},
doi = {https://doi.org/10.1016/S0168-9274(98)00021-X},
url = {https://www.sciencedirect.com/science/article/pii/S016892749800021X},
author = {Thomas Hagstrom and John Goodrich},
keywords = {Nonreflecting Boundary Conditions, Aeroacoustics},
abstract = {We present a series of numerical experiments on the accuracy of approximate radiation boundary conditions for computational aeroacoustics based on Padé approximants. Our test problem is described by an infinite periodic array of pressure pulses, for which we can independently evaluate the exact solution by numerical quadrature. It is demonstrated that acceptable long time accuracy can be achieved, but only if conditions of high order are employed. As predicted by theory, the order required for a given accuracy is proportional to the time of the simulation.}
}
@article{KAZANINA2023996,
title = {The neural ingredients for a language of thought are available},
journal = {Trends in Cognitive Sciences},
volume = {27},
number = {11},
pages = {996-1007},
year = {2023},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2023.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S1364661323001936},
author = {Nina Kazanina and David Poeppel},
keywords = {language-of-thought, symbolic representation, computational theory of mind, spatial navigation, compositionality},
abstract = {The classical notion of a ‘language of thought’ (LoT), advanced prominently by the philosopher Jerry Fodor, is an influential position in cognitive science whereby the mental representations underpinning thought are considered to be compositional and productive, enabling the construction of new complex thoughts from more primitive symbolic concepts. LoT theory has been challenged because a neural implementation has been deemed implausible. We disagree. Examples of critical computational ingredients needed for a neural implementation of a LoT have in fact been demonstrated, in particular in the hippocampal spatial navigation system of rodents. Here, we show that cell types found in spatial navigation (border cells, object cells, head-direction cells, etc.) provide key types of representation and computation required for the LoT, underscoring its neurobiological viability.}
}
@article{TARMIZI2010384,
title = {Effects of Problem-based Learning Approach in Learning of Statistics among University Students},
journal = {Procedia - Social and Behavioral Sciences},
volume = {8},
pages = {384-392},
year = {2010},
note = {International Conference on Mathematics Education Research 2010 (ICMER 2010)},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2010.12.054},
url = {https://www.sciencedirect.com/science/article/pii/S1877042810021592},
author = {Rohani Ahmad Tarmizi and Sahar Bayat},
keywords = {Problem-based learning, E-learning, Statistic learning performance, Mental load},
abstract = {Current Mathematics Curriculum concerns are focused on students’ needs to think mathematically rather than just mathematical computation. Students should be able to develop more complex, abstract, and powerful mathematical structures. This can dramatically enable them to solve a broad variety of meaningful problems. Furthermore, students ought to become autonomous and self-motivated in their mathematical activities such as acquiring mathematical concepts, skills and problem solving; meta-cognitively aware of their mathematical thinking; highly motivated in mathematics learning and develop positive attitudes towards mathematical task. To achieve this learning goal, an investigation into efficient learning mode, the problem-based learning (PBL) was undertaken. PBL has been successfully applied in medical, engineering, economics, and accounting field but lack of evidence in mathematics field. This study examined possible outcomes of PBL among postgraduate students who were taking Educational Statistic course. Three statistic tests were employed to assess the students’ performance in statistic learning. The Meta-cognitive Awareness Inventory (MAI), which comprises of 52 items was used to assess the students’ meta-cognitive strategy in solving Educational Statistics problems. Students’ motivation towards the PBL learning was measured by Keller's Motivational Design Questionnaire with 36 items. Comparison of students’ performance based on three tests showed that there is significant diffrence between mean performance (F [2,28]=5.571, p<0.05). In addition, results indicated that there is significant positive effects on students meta-cognitive awareness (t [30]=3.358, p<0.05) and on students motivation level (t [30]=2.484, p<0.05) after undergoing PBL intervention.}
}
@article{BAI2011364,
title = {Prediction of human voluntary movement before it occurs},
journal = {Clinical Neurophysiology},
volume = {122},
number = {2},
pages = {364-372},
year = {2011},
issn = {1388-2457},
doi = {https://doi.org/10.1016/j.clinph.2010.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S1388245710005699},
author = {Ou Bai and Varun Rathi and Peter Lin and Dandan Huang and Harsha Battapady and Ding-Yu Fei and Logan Schneider and Elise Houdayer and Xuedong Chen and Mark Hallett},
keywords = {Human intention, Voluntary movement, Prediction, Movement-related cortical potentials (MRCP), Event-related desynchronization (ERD), Electroencephalography (EEG), Brain–computer interface (BCI), Consciousness},
abstract = {Objective
Human voluntary movement is associated with two changes in electroencephalography (EEG) that can be observed as early as 1.5s prior to movement: slow DC potentials and frequency power shifts in the alpha and beta bands. Our goal was to determine whether and when we can reliably predict human natural movement BEFORE it occurs from EEG signals ONLINE IN REAL-TIME.
Methods
We developed a computational algorithm to support online prediction. Seven healthy volunteers participated in this study and performed wrist extensions at their own pace.
Results
The average online prediction time was 0.62±0.25s before actual movement monitored by EMG signals. There were also predictions that occurred without subsequent actual movements, where subjects often reported that they were thinking about making a movement.
Conclusion
Human voluntary movement can be predicted before movement occurs.
Significance
The successful prediction of human movement intention will provide further insight into how the brain prepares for movement, as well as the potential for direct cortical control of a device which may be faster than normal physical control.}
}
@article{KOTIR2024140042,
title = {Field experiences and lessons learned from applying participatory system dynamics modelling to sustainable water and agri-food systems},
journal = {Journal of Cleaner Production},
volume = {434},
pages = {140042},
year = {2024},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2023.140042},
url = {https://www.sciencedirect.com/science/article/pii/S0959652623042002},
author = {Julius H. Kotir and Renata Jagustovic and George Papachristos and Robert B. Zougmore and Aad Kessler and Martin Reynolds and Mathieu Ouedraogo and Coen J. Ritsema and Ammar Abdul Aziz and Ron Johnstone},
keywords = {Africa, Group model building, Systems thinking, Stakeholder engagement, System modelling, Sustainable development},
abstract = {Achieving the objectives of sustainable development in water and agri-food systems requires the utilisation of decision-support tools in stakeholder-driven processes to construct and simulate various scenarios and evaluate the outcomes of associated policy interventions. While it is common practice to involve stakeholders in participatory modelling processes, their comprehensive documentation and the lessons learned remain scarce. In this paper, we share our experience of engaging stakeholders throughout the entire system dynamics modelling process. We draw on two projects implemented in the Volta River Basin, West Africa, to understand the dynamics of water and agri-food systems under changing environmental and socioeconomic conditions. We outline eight key insights and lessons as practical guides derived from each stage of the participatory modelling process, including the pre-workshop stage, problem definition, model conceptualization, simulation model formulation, model testing and verification, and policy design and evaluation. Our findings demonstrate that stakeholders can actively contribute to all phases of the system dynamics modelling process, including parameter estimation, sensitivity analysis, and numerical simulation experiments. However, we encountered notable challenges, including the time-intensive nature of the process, the struggle to reach a consensus on the modelled problem, and the difficulty of translating the conceptual model into a simulation model using stock and flow diagrams – all of which were addressed through a structured facilitation process. While the projects were anchored in the specific context of West Africa, the key lessons and insights highlighted have broader significance, particularly for researchers employing PSDM in regions characterised by multifaceted human-environmental systems and where stakeholder involvement is crucial for holistic understanding and effective policy interventions. This paper contributes practical guidance for future efforts with participatory modelling, particularly in regions worldwide grappling with sustainable development challenges in water and agri-food systems, and where stakeholder involvement is crucial for holistic understanding of the multiple challenges and for designing effective policy interventions.}
}
@incollection{ADRIAANS2008133,
title = {LEARNING AND THE COOPERATIVE COMPUTATIONAL UNIVERSE},
editor = {Pieter Adriaans and Johan {van Benthem}},
booktitle = {Philosophy of Information},
publisher = {North-Holland},
address = {Amsterdam},
pages = {133-167},
year = {2008},
series = {Handbook of the Philosophy of Science},
issn = {18789846},
doi = {https://doi.org/10.1016/B978-0-444-51726-5.50010-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780444517265500108},
author = {Pieter Adriaans}
}
@article{DALLAQUA2021422,
title = {ForestEyes Project: Conception, enhancements, and challenges},
journal = {Future Generation Computer Systems},
volume = {124},
pages = {422-435},
year = {2021},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2021.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X21001965},
author = {Fernanda B.J.R. Dallaqua and Álvaro L. Fazenda and Fabio A. Faria},
keywords = {Citizen Science, Deforestation area detection, Rainforest, Tropical forest, Volunteered Thinking},
abstract = {Rainforests play an important role in the global ecosystem. However, significant regions of them are facing deforestation and degradation due to several reasons. Diverse government and private initiatives were created to monitor and alert for deforestation increases from remote sensing images, using different ways to deal with the notable amount of generated data. Citizen Science projects can also be used to reach the same goal. Citizen Science consists of scientific research involving nonprofessional volunteers for analyzing, collecting data, and using their computational resources to outcome advancements in science and to increase the public’s understanding of problems in specific knowledge areas such as astronomy, chemistry, mathematics, and physics. In this sense, this work presents a Citizen Science project called ForestEyes, which uses volunteer’s answers through the analysis and classification of remote sensing images to monitor deforestation regions in rainforests. To evaluate the quality of those answers, different campaigns/workflows were launched using remote sensing images from Brazilian Legal Amazon and their results were compared to an official groundtruth from the Amazon Deforestation Monitoring Project PRODES. In this work, the first two workflows that enclose the State of Rondônia in the years 2013 and 2016 received more than 35,000 answers from 383 volunteers in the 2,050 created tasks in only two and a half weeks after their launch. For the other four workflows, even enclosing the same area (Rondônia) and different setups (e.g., image segmentation method, image spatial resolution, and detection target), they received 51,035 volunteers’ answers gathered from 281 volunteers in 3,358 tasks. In the performed experiments, it was possible to observe that the volunteers achieved satisfactory overall accuracy, higher than 75%, in the classification of forestation and non-forestation areas using the ForestEyes project. Furthermore, considering an efficient segmentation and a better image spatial resolution, they achieved almost 66% accuracy in the classification of recent deforestation, which is a great challenge to overcome. Therefore, these results show that Citizen Science might be a powerful tool in monitoring deforestation regions in rainforests as well as in obtaining high-quality labeled data.}
}
@incollection{ASHBY2024255,
title = {Chapter 10 - Circular Materials Economics},
editor = {Michael F. Ashby},
booktitle = {Materials and Sustainable Development (Second Edition)},
publisher = {Butterworth-Heinemann},
edition = {Second Edition},
pages = {255-295},
year = {2024},
isbn = {978-0-323-98361-7},
doi = {https://doi.org/10.1016/B978-0-323-98361-7.00010-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323983617000105},
author = {Michael F. Ashby},
keywords = {Circularity, Material efficiency, Linear materials economy, Circular materials economy, Reuse, Repair, Recycling, Take-back legislation, Recycling targets, Increased product life, Urban mining, Business models, Measuring circularity, Modelling circularity, Limits to circularity},
abstract = {When products come to the end of their lives, the materials they contain are still there. Repurposing, repair or recycling can return them to active use, creating a technological cycle that, in some ways, parallels the carbon, nitrogen and hydrological cycles of the biosphere. In developed nations they lost urgency as the cost of materials fell and that of labour rose, making it cheaper to make new products than to fix old ones, leading to a materials economy that is largely linear, characterized by the sequence “take – make – use – dispose”. Increasing population and affluence, and the limited capacity for the planet to provide resources and absorb waste direct thinking towards a more circular way of using materials. Governments have sought to reduce waste by imposing take-back regulations, setting mandatory recycling targets and requiring minimum service lives. These, and the efficiency movement – eco-efficiency, material-efficiency, energy efficiency – seek to allow business as usual with reduced drain on natural resources without any real change of behaviour. The ‘circularity’ concept is a way of thinking that looks not just for efficiencies but also for new ways to provide functions. The idea of deploying rather than consuming materials, of using them not once but many times has economic as well as environmental appeal. This chapter examines the background, the successes, the difficulties, and the ultimate limits of implementing a circular materials economy.}
}
@article{DECARO200758,
title = {Methodologies for examining problem solving success and failure},
journal = {Methods},
volume = {42},
number = {1},
pages = {58-67},
year = {2007},
note = {Neurocognitive Mechanisms of Creativity: A Toolkit},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2006.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S1046202306002982},
author = {Marci S. DeCaro and Mareike Wieth and Sian L. Beilock},
keywords = {Working memory, Performance, Pressure, Individual differences, Problem solving, Creativity, Short term memory, Stress, Math},
abstract = {When designing research to examine the variables underlying creative thinking and problem solving success, one must not only consider (a) the demands of the task being performed, but (b) the characteristics of the individual performing the task and (c) the constraints of the skill execution environment. In the current paper we describe methodologies that allow one to effectively study creative thinking by capturing interactions among the individual, task, and problem solving situation. In doing so, we demonstrate that the relation between executive functioning and problem solving success is not always as straightforward as one might initially believe.}
}
@article{PELAEZ2025109363,
title = {Universally Adaptable Multiscale Molecular Dynamics (UAMMD). A native-GPU software ecosystem for complex fluids, soft matter, and beyond},
journal = {Computer Physics Communications},
volume = {306},
pages = {109363},
year = {2025},
issn = {0010-4655},
doi = {https://doi.org/10.1016/j.cpc.2024.109363},
url = {https://www.sciencedirect.com/science/article/pii/S0010465524002868},
author = {Raúl P. Peláez and Pablo Ibáñez-Freire and Pablo Palacios-Alonso and Aleksandar Donev and Rafael Delgado-Buscalioni},
keywords = {Molecular dynamics, Hydrodynamics, C++, CUDA, Soft matter},
abstract = {We introduce UAMMD (Universally Adaptable Multiscale Molecular Dynamics), a novel software infrastructure tailored for mesoscale complex fluid simulations on GPUs. The UAMMD library encompasses a comprehensive range of computational schemes optimized for the GPU, spanning from molecular dynamics to immersed boundary fluctuating-hydrodynamics. Developed in CUDA/C++14, this header-only open-source software serves both as a simulation engine and as a library with a modular architecture, offering a vast array of independent modules, categorized as interactors (neighbor search, bonded, non-bonded and electrostatic interactions, etc.) and integrators (molecular dynamics, dissipative particle dynamics, smooth particle hydrodynamics, Brownian hydrodynamics and a rather complete array of Immersed Boundary -IB- schemes). UAMMD excels in schemes that couple particle-based elastic structures with continuum fields in different regions of the mesoscale. To that end, thermal fluctuations can be added in physically consistent ways, and fast modes can be eliminated to adapt UAMMD to different regimes (compressible or incompressible flow, inertial or Stokesian dynamics, etc.). Thus, UAMMD is extremely useful for coarse-grained simulations of nanoparticles, and soft and biological matter (from proteins to viruses and micro-swimmers). Importantly, all UAMMD developments are hand-to-hand validated against experimental techniques, and it has proven to quantitatively reproduce experimental signals from quartz-crystal microbalance, atomic force microscopy, magnetic sensors, optic-matter interaction and ultrasound.
Program summary
Program Title: UAMMD CPC Library link to program files: https://doi.org/10.17632/srrt2y5s4m.1 Developer's repository link: https://github.com/RaulPPelaez/UAMMD/ Licensing provisions: GPLv3 Programming language: C++/CUDA Nature of problem: The key problem addressed in computational physics is simulating the behavior of matter at various scales, encompassing both discrete (particle-based) and continuum (field-based) approaches. The challenge lies in accurately and efficiently modeling interactions at different spatio-temporal scales, ranging from atomic (microscopic) to fluid dynamics (macroscopic). This complexity is further amplified in mesoscale regions, where different physics domains intersect, necessitating advanced computational techniques to capture the nuanced dynamics of systems such as colloids, polymers, and biological structures. Solution method: The present solution consists in the creation of UAMMD (Universally Adaptable Multiscale Molecular Dynamics), a CUDA/C++14 library designed for GPU-accelerated complex fluid simulations. UAMMD offers a flexible platform that integrates discrete particle dynamics with continuum fluid dynamics. It supports a variety of computational schemes, each tailored for specific spatio-temporal regimes. The library's modular architecture allows for the seamless introduction of new algorithms and easy integration into existing codebases. Additional comments including restrictions and unusual features: UAMMD's design emphasizes modularity and GPU-native architecture, optimizing computational efficiency and flexibility. However, its focus on GPU acceleration and low level nature means it requires compatible hardware and familiarity with CUDA programming. While UAMMD is versatile in handling various physical regimes, it currently lacks certain standard force field potentials and multi-GPU support. Nonetheless, its ongoing development and open-source nature promise continual enhancements.}
}
@article{RYDER2022100703,
title = {Rethinking reflective practice: John Boyd’s OODA loop as an alternative to Kolb},
journal = {The International Journal of Management Education},
volume = {20},
number = {3},
pages = {100703},
year = {2022},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2022.100703},
url = {https://www.sciencedirect.com/science/article/pii/S1472811722001057},
author = {Mike Ryder and Carolyn Downs},
keywords = {OODA, Reflective practice, Experiential learning, Work-based learning, Employability, Business education},
abstract = {The world is changing and business schools are struggling to keep up. Theories of reflective practice developed by the likes of Schon (1983), Gibbs (1988), Driscoll (1994, 2007) and Kolb (1984, 2015) are outdated and unfit for current purposes. Problems include the chronology of events, the orientation of the observer, the impact of external inputs, and the fact that neither education nor the workplace follow a structured, linear path. In response to these challenges, we propose a new ‘solution’: John Boyd's OODA loop. We argue that OODA loops offer the chance to reshape reflective practice and work-based learning for a world in which individuals must cope with ‘an unfolding evolving reality that is uncertain, ever changing and unpredictable’ (Boyd, 1995, slide 1). By embracing the philosophy of John Boyd and his OODA loop theory, business schools can develop greater resilience and employability in graduates, preparing them to embrace change while also embedding the concept of life-long learning to make them better equipped to face the uncertainty that the modern world brings.}
}
@article{FERREIRA20131446,
title = {Fostering the Creative Development of Computer Science Students in Programming and Interaction Design},
journal = {Procedia Computer Science},
volume = {18},
pages = {1446-1455},
year = {2013},
note = {2013 International Conference on Computational Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2013.05.312},
url = {https://www.sciencedirect.com/science/article/pii/S1877050913004559},
author = {Deller James Ferreira},
keywords = {Creativity, Programming, Interaction design},
abstract = {This study explores the enhancement of creativity in undergraduate students studying computer science. We assume that everybody has creative potential. As a teacher, we can explicitly encourage creative thinking, providing space to let students collaboratively discover and explore their creativity. This paper presents a dialogical framework to help the teacher fostering creativity among students of computer science in programming and interaction design. The framework presented here involves underlying dialogic processes from seven collaborative and creative dimensions that allow students to develop creativity. The use of the pedagogical framework makes it possible to teachers create significant interaction design and computer programming experiences to students, motivating them to activate mental processes underlying creativity. Students can simultaneously activate two or more ideas, images, or thoughts and have them interact, prompt thought experiments, change cognitive perspectives, raise new points of view, and risk category mistakes.}
}
@article{TIAN2018104,
title = {The association between visual creativity and cortical thickness in healthy adults},
journal = {Neuroscience Letters},
volume = {683},
pages = {104-110},
year = {2018},
issn = {0304-3940},
doi = {https://doi.org/10.1016/j.neulet.2018.06.036},
url = {https://www.sciencedirect.com/science/article/pii/S0304394018304397},
author = {Fang Tian and Qunlin Chen and Wenfeng Zhu and Yongming Wang and Wenjing Yang and Xingxing Zhu and Xue Tian and Qinglin Zhang and Guikang Cao and Jiang Qiu},
keywords = {Visual creativity, Cortical thickness, Prefrontal cortex, Supplementary motor cortex, Insula},
abstract = {Creativity is necessary to human survival, human prosperity, civilization and well-being. Visual creativity is an important part of creativity and is the ability to create products of novel and useful visual forms, playing important role in many fields such as art, painting and sculpture. There have been several neuroimaging studies exploring the neural basis of visual creativity. However, to date, little is known about the relationship between cortical structure and visual creativity as measured by the Torrance Tests of Creative Thinking. Here, we investigated the association between cortical thickness and visual creativity in a large sample of 310 healthy adults. We used multiple regression to analyze the correlation between cortical thickness and visual creativity, adjusting for gender, age and general intelligence. The results showed that visual creativity was significantly negatively correlated with cortical thickness in the left middle frontal gyrus (MFG), right inferior frontal gyrus (IFG), right supplementary motor cortex (SMA) and the left insula. These observations have implications for understanding that a thinner prefrontal cortex (PFC) (e.g. IFG, MFG), SMA and insula correspond to higher visual creative performance, presumably due to their role in executive attention, cognitive control, motor planning and dynamic switching.}
}
@article{BLOSS2016,
title = {Reimagining Human Research Protections for 21st Century Science},
journal = {Journal of Medical Internet Research},
volume = {18},
number = {12},
year = {2016},
issn = {1438-8871},
doi = {https://doi.org/10.2196/jmir.6634},
url = {https://www.sciencedirect.com/science/article/pii/S1438887116003204},
author = {Cinnamon Bloss and Camille Nebeker and Matthew Bietz and Deborah Bae and Barbara Bigby and Mary Devereaux and James Fowler and Ann Waldo and Nadir Weibel and Kevin Patrick and Scott Klemmer and Lori Melichar},
keywords = {ethics committees, research, biomedical research, telemedicine, informed consent, behavioral research},
abstract = {Background
Evolving research practices and new forms of research enabled by technological advances require a redesigned research oversight system that respects and protects human research participants.
Objective
Our objective was to generate creative ideas for redesigning our current human research oversight system.
Methods
A total of 11 researchers and institutional review board (IRB) professionals participated in a January 2015 design thinking workshop to develop ideas for redesigning the IRB system.
Results
Ideas in 5 major domains were generated. The areas of focus were (1) improving the consent form and process, (2) empowering researchers to protect their participants, (3) creating a system to learn from mistakes, (4) improving IRB efficiency, and (5) facilitating review of research that leverages technological advances.
Conclusions
We describe the impetus for and results of a design thinking workshop to reimagine a human research protections system that is responsive to 21st century science.}
}
@article{NIRMALADEVI2025126553,
title = {DCNN-SBiL: EEG signal based mild cognitive impairment classification using compact convolutional network},
journal = {Expert Systems with Applications},
volume = {273},
pages = {126553},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.126553},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425001757},
author = {A. {Nirmala Devi} and M. Latha},
keywords = {Mild cognitive impairment, Deep learning, Compact convolutional neural network, EEG signal, Dual attention, Alzheimer’s disease, Improved tuneable Q wavelet transform},
abstract = {Mild cognitive impairment (MCI) is a state that falls between the more severe decline of dementia and the typical aging-related loss of memory and thinking. MCI must be diagnosed earlier to avoid complete memory loss. Several Machine Learning (ML) and Deep Learning (DL) models employ standard feature extraction approaches to achieve effective MCI categorization. However, it has some drawbacks, including lower accuracy, longer time consumption, less feature learning, and increased model complexity. The proposed method introduces a novel deep learning model to address the limitations of existing MCI classification approaches. Initially, the Electroencephalography (EEG) signal is pre-processed using the Sequential Savitzky-Golay filtering model (SEQ-SG), which improves the signal’s quality and removes unnecessary noise. The Improved Tuneable Q Wavelet Transform (ITQWT) feature extraction model is used to extract relevant features. The Coati Stochastic Optimization (CSO) algorithm selects the most optimal channel features from the EEG signal. Finally, the proposed deep learning model, Dual Attention Assisted Compact Convolutional Network with Stacked Bi-LSTM (DCCN-SBiL), is used to classify EEG signals into three categories: Alzheimer’s disease, MCI, and normal. The proposed model is optimized using the Gazelle Optimization Algorithm (GOA), which tunes the classification model’s hyperparameters. The proposed classification model is evaluated using the Mendeley Dataset, which contains EEG signals from Alzheimer's disease, MCI and Normal. The proposed model has shown great performance in many performance parameters, including 97.25% accuracy, 95.94% recall, 96.03% precision, and 94.65% specificity in MCI classification.}
}
@article{DELLACQUA20241727,
title = {Empathy-Aware Behavior Trees for Social Care Decision Systems},
journal = {Procedia Computer Science},
volume = {239},
pages = {1727-1735},
year = {2024},
note = {CENTERIS – International Conference on ENTERprise Information Systems / ProjMAN - International Conference on Project MANagement / HCist - International Conference on Health and Social Care Information Systems and Technologies 2023},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.06.351},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924015953},
author = {Pierangelo Dell’Acqua and Stefania Costantini and Abeer Dyoub and Giovanni De Gasperis and Andrea Monaldini and Andrea Rafanelli},
keywords = {Behavior Trees, Affective Computing, Decision Making},
abstract = {There is growing attention on the importance of building intelligent systems where humans and Artificial Intelligence-based systems (AIs) form teams exploiting the potentially synergistic relationships between humans and automation. In the last decade, the computational modeling of empathy has gained increasing attention. Empowering interactive agents with empathic capabilities leads, on the human’s side, to more trust, increases engagement, and thus interaction length, helps cope with stress. These findings suggest that agents endowed with empathy may enhance social interaction in educational applications, artificial companions, medical assistants, and gaming applications. This article focuses on modeling the empathic behavior of virtual agents interacting with humans. We propose a formal model that enables virtual agents to exhibit empathic, emotional behavior. Specifically, we extend the modeling of empathy via behavior trees with a new type of node allowing the specification of various kinds of empathy. Using the proposed extension, we show how different agents’ reactive behavior can be modeled.}
}
@incollection{DRYGAS20201,
title = {1 - Introduction to computational methods and theory of composites},
editor = {Piotr Drygaś and Simon Gluzman and Vladimir Mityushev and Wojciech Nawalaniec},
booktitle = {Applied Analysis of Composite Media},
publisher = {Woodhead Publishing},
pages = {1-56},
year = {2020},
series = {Woodhead Publishing Series in Composites Science and Engineering},
isbn = {978-0-08-102670-0},
doi = {https://doi.org/10.1016/B978-0-08-102670-0.00010-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008102670000010X},
author = {Piotr Drygaś and Simon Gluzman and Vladimir Mityushev and Wojciech Nawalaniec},
keywords = {Self-consistent approximation, structural sum, statistical mechanics methods, self-Similarity and renormalization-group},
abstract = {Overview of traditional approaches based on self-consistent approximations in composite materials is presented. Their restrictions are underlined. Neoclassical approach previously introduced in the Preface, is illustrated and compared to methods applied in statistical mechanics. The structural sums, the key construction of the neoclassical approach, are outlined. Method of series and asymptotic method of approximants, Padé approximants, DLog Padé approximants, Factor, Root, Additive approximants are briefly discussed. Notion of Self-Similarity and renormalization-group is introduced. Minimal difference and minimal derivative methods of calculation for short series are discussed in detail. Critical Index is calculated from various short series. DLog root approximants are introduced and illustrate by several examples, where the DLog Padé approximants fail. DLog additive approximants are introduced and presented iteratively. Multiple examples are presented in the chapter and in the appendix. Method of Log Padé approximants is suggested.}
}
@article{WANG2025102570,
title = {ST-TNet: An spatio-temporal joint transformer network for CSI feedback in FDD-MIMO systems},
journal = {Physical Communication},
volume = {68},
pages = {102570},
year = {2025},
issn = {1874-4907},
doi = {https://doi.org/10.1016/j.phycom.2024.102570},
url = {https://www.sciencedirect.com/science/article/pii/S187449072400288X},
author = {Linyu Wang and Yize Cao and Jianhong Xiang},
keywords = {Massive MIMO, CSI feedback, Deep learning, Lightweighting, Transformer, Attention},
abstract = {In recent years, deep learning methods have been shown to have strong potential and superiority in reducing channel state information (CSI) feedback overhead and further improving feedback accuracy to maximize the performance benefits of massive Multiple-Input Multiple-Output (MIMO) in frequency division duplex (FDD) mode. As the CSI matrices are transformed into sequences for input to the Transformer model, the rearrangement leads to the loss of the original physical location relationships. Based on this problem, this paper proposes a transformer decoder based on spatio-temporal joint (ST-T). We employ a spatial attention mechanism to compensate for this information loss and focus on key spatial features more accurately, further exploiting the potential of single- and two-layer transformers in reconstructing CSI matrices. The results are validated by simulations based on DCRNet and CLNet encoders, which show that higher performance can be achieved with lower computational load compared to other lightweight models.}
}
@article{MAHMUD2025111321,
title = {RSPCA: Random Sample Partition and Clustering Approximation for ensemble learning of big data},
journal = {Pattern Recognition},
volume = {161},
pages = {111321},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111321},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010720},
author = {Mohammad Sultan Mahmud and Hua Zheng and Diego Garcia-Gil and Salvador García and Joshua Zhexue Huang},
keywords = {Clustering approximation, Ensemble clustering, Incremental clustering, Ensemble learning},
abstract = {Large-scale data clustering needs an approximate approach for improving computation efficiency and data scalability. In this paper, we propose a novel method for ensemble clustering of large-scale datasets that uses the Random Sample Partition and Clustering Approximation (RSPCA) to tackle the problems of big data computing in cluster analysis. In the RSPCA computing framework, a big dataset is first partitioned into a set of disjoint random samples, called RSP data blocks that remain distributions consistent with that of the original big dataset. In ensemble clustering, a few RSP data blocks are randomly selected, and a clustering operation is performed independently on each data block to generate the clustering result of the data block. All clustering results of selected data blocks are aggregated to the ensemble result as an approximate result of the entire big dataset. To improve the robustness of the ensemble result, the ensemble clustering process can be conducted incrementally using multiple batches of selected RSP data blocks. To improve computation efficiency, we use the I-niceDP algorithm to automatically find the number of clusters in RSP data blocks and the k-means algorithm to determine more accurate cluster centroids in RSP data blocks as inputs to the ensemble process. Spectral and correlation clustering methods are used as the consensus functions to handle irregular clusters. Comprehensive experiment results on both real and synthetic datasets demonstrate that the ensemble of clustering results on a few RSP data blocks is sufficient for a good global discovery of the entire big dataset, and the new approach is computationally efficient and scalable to big data.}
}
@article{SYCHEV2024101261,
title = {Educational models for cognition: Methodology of modeling intellectual skills for intelligent tutoring systems},
journal = {Cognitive Systems Research},
volume = {87},
pages = {101261},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101261},
url = {https://www.sciencedirect.com/science/article/pii/S138904172400055X},
author = {Oleg Sychev},
keywords = {Reasoning modeling, Constraint-based modeling, Intelligent tutoring systems},
abstract = {Automation of teaching people new skills requires modeling of human reasoning because human cognition involves active reasoning over the new subject domain to acquire skills that will later become automatic. The article presents Thought Process Trees — a language for modeling human reasoning that was created to facilitate the development of intelligent tutoring systems, which can perform the same reasoning that is expected of a student and find deficiencies in their line of thinking, providing explanatory messages and allowing them to learn from performance errors. The methodology of building trees which better reflect human learning is discussed, with examples of design choices during the modeling process and their consequences. The characteristics of educational modeling that impact building subject-domain models for intelligent tutoring systems are discussed. The trees were formalized and served as a basis for developing a framework for constructing intelligent tutoring systems. This significantly lowered the time required to build and debug a constraint-based subject-domain model. The framework has already been used to develop five intelligent tutoring systems and their prototypes and is being used to develop more of them.}
}
@article{SUDDENDORF201826,
title = {Prospection and natural selection},
journal = {Current Opinion in Behavioral Sciences},
volume = {24},
pages = {26-31},
year = {2018},
note = {Survival circuits},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2018.01.019},
url = {https://www.sciencedirect.com/science/article/pii/S2352154617302449},
author = {T Suddendorf and A Bulley and B Miloyan},
abstract = {Prospection refers to thinking about the future, a capacity that has become the subject of increasing research in recent years. Here we first distinguish basic prospection, such as associative learning, from more complex prospection commonly observed in humans, such as episodic foresight, the ability to imagine diverse future situations and organize current actions accordingly. We review recent studies on complex prospection in various contexts, such as decision-making, planning, deliberate practice, information gathering, and social coordination. Prospection appears to play many important roles in human survival and reproduction. Foreseeing threats and opportunities before they arise, for instance, drives attempts at avoiding future harm and obtaining future benefits, and recognizing the future utility of a solution turns it into an innovation, motivating refinement and dissemination. Although we do not know about the original contexts in which complex prospection evolved, it is increasingly clear through research on the emergence of these capacities in childhood and on related disorders in various clinical conditions, that limitations in prospection can have profound functional consequences.}
}
@article{BAYRAKTARSARI2024110835,
title = {Architectural spatial layout design for hospitals: A review},
journal = {Journal of Building Engineering},
volume = {97},
pages = {110835},
year = {2024},
issn = {2352-7102},
doi = {https://doi.org/10.1016/j.jobe.2024.110835},
url = {https://www.sciencedirect.com/science/article/pii/S2352710224024033},
author = {Aysegul Ozlem {Bayraktar Sari} and Wassim Jabi},
keywords = {Architectural spatial layout design, Hospital spatial layout design, Computational design, Facility layout planning, Machine learning (ML) driven layout design, Systematic review},
abstract = {The design of hospital spatial layouts is a critical aspect of healthcare architecture, directly influencing patient outcomes, staff efficiency, and the overall quality of care. A well-designed hospital layout is essential for ensuring smooth operations, minimizing errors, and improving both patient and staff experiences. This paper reviews the significant advances in the field, particularly focusing on the transition from traditional design methods to the integration of computational techniques and machine learning (ML) in hospital layout planning. Despite these technological advancements, there remains a notable gap in the full adoption and optimization of these methods to effectively address the inherent complexities of healthcare environments. This review identifies that while computational methods and machine learning-driven approaches have brought precision and innovation to hospital design, the challenge lies in balancing these technologies with the expertise and insights of human designers. Moreover, the need for interdisciplinary collaboration between architects, healthcare professionals, and engineers is emphasized as crucial for the successful implementation of advanced design strategies. Insights from this review highlight the potential of future research to bridge the existing gaps, proposing directions for the continuous integration of technology in hospital layout design.}
}
@article{DEVOE2012466,
title = {Time, money, and happiness: How does putting a price on time affect our ability to smell the roses?},
journal = {Journal of Experimental Social Psychology},
volume = {48},
number = {2},
pages = {466-474},
year = {2012},
issn = {0022-1031},
doi = {https://doi.org/10.1016/j.jesp.2011.11.012},
url = {https://www.sciencedirect.com/science/article/pii/S0022103111002897},
author = {Sanford E. DeVoe and Julian House},
keywords = {Time, Money, Impatience, Happiness},
abstract = {In this paper, we investigate how the impatience that results from placing a price on time impairs individuals' ability to derive happiness from pleasurable experiences. Experiment 1 demonstrated that thinking about one's income as an hourly wage reduced the happiness that participants derived from leisure time on the internet. Experiment 2 revealed that a similar manipulation decreased participants' state of happiness after listening to a pleasant song and that this effect was fully mediated by the degree of impatience experienced during the music. Finally, Experiment 3 showed that the deleterious effect on happiness caused by impatience was attenuated by offering participants monetary compensation in exchange for time spent listening to music, suggesting that a sensation of unprofitably wasted time underlay the induced impatience. Together these experiments establish that thinking about time in terms of money can influence how people experience pleasurable events by instigating greater impatience during unpaid time.}
}
@article{EGIDI2020155,
title = {Desertification risk, economic resilience and social issues: From theory to practice},
journal = {Chinese Journal of Population, Resources and Environment},
volume = {18},
number = {2},
pages = {155-163},
year = {2020},
issn = {2325-4262},
doi = {https://doi.org/10.1016/j.cjpre.2021.04.022},
url = {https://www.sciencedirect.com/science/article/pii/S2325426221000310},
author = {Gianluca Egidi and Luca Salvati},
keywords = {Population dynamics, Ecosystem functioning, Socio-ecological resilience, Complex adaptive systems, Interpretative framework},
abstract = {Land degradation and early forms of desertification in both advanced economies and emerging countries reflect complex socio-environmental processes driven by multiple interactions between biophysical and socioeconomic forces across different spatial scales. The present study investigates desertification risk, land degradation, and socio-demographic dynamics through the lens of “resilience,” adopting complex adaptive systems (CAS) thinking. The resilience of socio-environmental systems exposed to land degradation is defined as the capacity of a regional economy to respond to crises and reorganize by making changes to preserve functions, structure, and feedback, and to promote future development options. By reviewing the socioeconomic resilience of local socio-ecological systems exposed to land degradation, this study achieves a better comprehension of the multifaceted processes that lead to a higher risk of desertification and the intimate relationship with underlying population trends and demographic dynamics. A comprehensive approach based on resilience thinking was formulated to review both environmental and socio-demographic issues at the landscape scale, and provide a suitable foundation for sustainability science and regional development policies.}
}
@article{SAVIN2024108324,
title = {Reviewing studies of degrowth: Are claims matched by data, methods and policy analysis?},
journal = {Ecological Economics},
volume = {226},
pages = {108324},
year = {2024},
issn = {0921-8009},
doi = {https://doi.org/10.1016/j.ecolecon.2024.108324},
url = {https://www.sciencedirect.com/science/article/pii/S0921800924002210},
author = {Ivan Savin and Jeroen {van den Bergh}},
keywords = {Economic growth, Environmental policy, GDP, Political feasibility, Post-growth},
abstract = {In the last decade many publications have appeared on degrowth as a strategy to confront environmental and social problems. We undertake a systematic review of their content, data and methods. This involves the use of computational linguistics to identify main topics investigated. Based on a sample of 561 studies we conclude that: (1) content covers 11 main topics; (2) the large majority (almost 90%) of studies are opinions rather than analysis; (3) few studies use quantitative or qualitative data, and even fewer ones use formal modelling; (4) the first and second type tend to include small samples or focus on non-representative cases; (5) most studies offer ad hoc and subjective policy advice, lacking policy evaluation and integration with insights from the literature on environmental/climate policies; (6) of the few studies on public support, a majority concludes that degrowth strategies and policies are socially-politically infeasible; (7) various studies represent a “reverse causality” confusion, i.e. use the term degrowth not for a deliberate strategy but to denote economic decline (in GDP terms) resulting from exogenous factors or public policies; (8) few studies adopt a system-wide perspective – instead most focus on small, local cases without a clear implication for the economy as a whole. We illustrate each of these findings for concrete studies.}
}
@article{KEARNS2024543,
title = {The Application of Knowledge Engineering via the Use of a Biomimetic Digital Twin Ecosystem, Phenotype-Driven Variant Analysis, and Exome Sequencing to Understand the Molecular Mechanisms of Disease},
journal = {The Journal of Molecular Diagnostics},
volume = {26},
number = {7},
pages = {543-551},
year = {2024},
issn = {1525-1578},
doi = {https://doi.org/10.1016/j.jmoldx.2024.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S152515782400062X},
author = {William G. Kearns and Georgios Stamoulis and Joseph Glick and Lawrence Baisch and Andrew Benner and Dalton Brough and Luke Du and Bradford Wilson and Laura Kearns and Nicholas Ng and Maya Seshan and Raymond Anchan},
abstract = {Applied artificial intelligence, particularly large language models, in biomedical research is accelerating, but effective discovery and validation requires a toolset without limitations or bias. On January 30, 2023, the National Academies of Sciences, Engineering, and Medicine (NAS) appointed an ad hoc committee to identify the needs and opportunities to advance the mathematical, statistical, and computational foundations of digital twins in applications across science, medicine, engineering, and society. On December 15, 2023, the NAS released a 164-page report, “Foundational Research Gaps and Future Directions for Digital Twins.” This report described the importance of using digital twins in biomedical research. The current study was designed to develop an innovative method that incorporated phenotype-ranking algorithms with knowledge engineering via a biomimetic digital twin ecosystem. This ecosystem applied real-world reasoning principles to nonnormalized, raw data to identify hidden or "dark" data. Clinical exome sequencing study on patients with endometriosis indicated four variants of unknown clinical significance potentially associated with endometriosis-related disorders in nearly all patients analyzed. One variant of unknown clinical significance was identified in all patient samples and could be a biomarker for diagnostics. To the best of our knowledge, this is the first study to incorporate the recommendations of the NAS to biomedical research. This method can be used to understand the mechanisms of any disease, for virtual clinical trials, and to identify effective new therapies.}
}
@article{LUNGU2008255,
title = {Partial current information and signal extraction in a rational expectations macroeconomic model: A computational solution},
journal = {Economic Modelling},
volume = {25},
number = {2},
pages = {255-273},
year = {2008},
issn = {0264-9993},
doi = {https://doi.org/10.1016/j.econmod.2007.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0264999307000818},
author = {L. Lungu and K.G.P. Matthews and A.P.L. Minford},
keywords = {Rational expectations, Partial current information, Signal extraction, Macroeconomic modelling},
abstract = {Previous attempts at modelling current observed endogenous financial variables in a macroeconomic model have concentrated on only one variable — the short-term rate of interest. This paper applies a general search algorithm to a macroeconomic model with an observed interest rate and exchange rate to solve the signal extraction problem. Firstly, the algorithm is tested against a linear model with a known analytical solution. Then, the algorithm is applied to all the observed current endogenous variables in a non-linear rational expectations model of the UK. The informational advantage of applying the signal extraction algorithm is evaluated in terms of the forecasting efficiency of the model.}
}
@article{RUTHERFORD2023102255,
title = {“Don't [ruminate], be happy”: A cognitive perspective linking depression and anhedonia},
journal = {Clinical Psychology Review},
volume = {101},
pages = {102255},
year = {2023},
issn = {0272-7358},
doi = {https://doi.org/10.1016/j.cpr.2023.102255},
url = {https://www.sciencedirect.com/science/article/pii/S0272735823000132},
author = {Ashleigh V. Rutherford and Samuel D. McDougle and Jutta Joormann},
keywords = {Rumination, Emotion regulation, Working memory, Reinforcement learning, Depression},
abstract = {Anhedonia, a lack of pleasure in things an individual once enjoyed, and rumination, the process of perseverative and repetitive attention to specific thoughts, are hallmark features of depression. Though these both contribute to the same debilitating disorder, they have often been studied independently and through different theoretical lenses (e.g., biological vs. cognitive). Cognitive theories and research on rumination have largely focused on understanding negative affect in depression with much less focus on the etiology and maintenance of anhedonia. In this paper, we argue that by examining the relation between cognitive constructs and deficits in positive affect, we may better understand anhedonia in depression thereby improving prevention and intervention efforts. We review the extant literature on cognitive deficits in depression and discuss how these dysfunctions may not only lead to sustained negative affect but, importantly, interfere with an ability to attend to social and environmental cues that could restore positive affect. Specifically, we discuss how rumination is associated to deficits in working memory and propose that these deficits in working memory may contribute to anhedonia in depression. We further argue that analytical approaches such as computational modeling are needed to study these questions and, finally, discuss implications for treatment.}
}
@article{POLZER2022100181,
title = {The rise of people analytics and the future of organizational research},
journal = {Research in Organizational Behavior},
volume = {42},
pages = {100181},
year = {2022},
issn = {0191-3085},
doi = {https://doi.org/10.1016/j.riob.2023.100181},
url = {https://www.sciencedirect.com/science/article/pii/S0191308523000011},
author = {Jeffrey T. Polzer},
keywords = {People analytics, Algorithms, Decision-making, Networks, Teams, Meetings, Culture, Monitoring, Computational social science, Organizational behavior},
abstract = {Organizations are transforming as they adopt new technologies and use new sources of data, changing the experiences of employees and pushing organizational researchers to respond. As employees perform their daily activities, they generate vast digital data. These data, when combined with established methods and new analytic techniques, create unprecedented opportunities for studying human behavior at work and have fueled the rise of people analytics as a new institutional field of practice. In this chapter, I describe the emerging field of people analytics and new organizational phenomena that accompany the use of data and algorithms. These practices are affecting how individuals, groups, and organizations function, ranging from decision-making processes and work procedures, to communication and collaboration, to attempts to monitor and control employees. In each of these domains, I describe recent research and propose new research directions. Many of these domains intersect with the emerging field of Computational Social Science, in which disciplinary scholars are applying computational methods to an expanding array of digitized data, pursuing interests that extend far into the organizational domain. Organizational scholars are well-positioned to bridge organizational and disciplinary advances to stay at the forefront of research on the future of work.}
}
@article{KULIK20242338,
title = {Reaction: The challenge of open-shell transition metal catalysis in “systems chemistry”},
journal = {Chem},
volume = {10},
number = {8},
pages = {2338-2339},
year = {2024},
issn = {2451-9294},
doi = {https://doi.org/10.1016/j.chempr.2024.06.026},
url = {https://www.sciencedirect.com/science/article/pii/S245192942400305X},
author = {Heather J. Kulik},
abstract = {Professor Heather J. Kulik is a professor in chemical engineering and chemistry at MIT. She received her BE in chemical engineering from the Cooper Union in 2004 and her PhD from the Department of Materials Science and Engineering at MIT in 2009. She completed postdocs at Lawrence Livermore and Stanford prior to joining MIT as a faculty member in 2013. Her research in computational inorganic chemistry has been recognized by an ONR YIP, a DARPA Director’s fellowship, an NSF CAREER Award, a Sloan Fellowship, an AIChE CoMSEF Impact Award, and a Hans Fischer Senior Fellowship from TU Munich, among others.}
}
@article{GONG2023105530,
title = {Continuous time causal structure induction with prevention and generation},
journal = {Cognition},
volume = {240},
pages = {105530},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2023.105530},
url = {https://www.sciencedirect.com/science/article/pii/S0010027723001646},
author = {Tianwei Gong and Neil R. Bramley},
keywords = {Causal learning, Time, Prevention, Structure induction, Summary statistics},
abstract = {Most research into causal learning has focused on atemporal contingency data settings while fewer studies have examined learning and reasoning about systems exhibiting events that unfold in continuous time. Of these, none have yet explored learning about preventative causal influences. How do people use temporal information to infer which components of a causal system are generating or preventing activity of other components? In what ways do generative and preventative causes interact in shaping the behavior of causal mechanisms and their learnability? We explore human causal structure learning within a space of hypotheses that combine generative and preventative causal relationships. Participants observe the behavior of causal devices as they are perturbed by fixed interventions and subject to either regular or irregular spontaneous activations. We find that participants are capable learners in this setting, successfully identifying the large majority of generative, preventative and non-causal relationships but making certain attribution errors. We lay out a computational-level framework for normative inference in this setting and propose a family of more cognitively plausible algorithmic approximations. We find that participants’ judgment patterns can be both qualitatively and quantitatively captured by a model that approximates normative inference via a simulation and summary statistics scheme based on structurally local computation using temporally local evidence.}
}
@article{FLINT2025100948,
title = {Expansion of analytical methods in auditing education},
journal = {Journal of Accounting Education},
volume = {70},
pages = {100948},
year = {2025},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2024.100948},
url = {https://www.sciencedirect.com/science/article/pii/S0748575124000642},
author = {Michele S. Flint},
keywords = {Auditing education, Analytical procedures, Data analytics, Beneish M−score, Altman Z-score, Sloan Accrual},
abstract = {Data analytics is changing the audit environment and carries significant implications for auditing education. Both international auditing education (International Accounting Education Standards Board (IAESB), 2019a; IAESB, 2019b) and U.S.-based regulatory bodies (American Institute of Certified Public Accountants (AICPA), 2021c; AICPA & National Association of State Boards of Accountancy (NASBA), 2021) have made efforts to address the growing expectations for auditing education, citing fraud risk and going concern risk. While auditing courses have progressed to include some computerized audit software for case studies, the study of analytical procedures has been limited to the application of basic financial ratios, trend analyses and common-size financial statements. Demands for advanced analytics place most emphasis on computerized query and computational methods; however, several advanced analytical models, namely the Altman Z-score, Beneish M−score and the Sloan Accrual formula provide opportunities for greater insight on specific audit risks and do not require advanced computer-based skills. The ability to link audit procedures, specifically analytical procedures to the audit objectives of financial risk and going concern risk strengthens the rationale for introduction of these advanced models within the context of auditing education. This paper discusses the inherent value in these analytical models, links them to audit objectives, proposes the inclusion of these three analytical models as a component of auditing education, and suggests that future study be undertaken to assess implementation and student learning. In addition, we recommend future study of other analytical models that may provide further insight for auditing students.}
}
@article{SEWALL2020,
title = {Fiber Force: A Fiber Diet Intervention in an Advanced Course-Based Undergraduate Research Experience (CURE) Course},
journal = {Journal of Microbiology & Biology Education},
volume = {21},
number = {1},
year = {2020},
issn = {1935-7877},
doi = {https://doi.org/10.1128/jmbe.v21i1.1991},
url = {https://www.sciencedirect.com/science/article/pii/S1935787720000660},
author = {Julia Massimelli Sewall and Andrew Oliver and Kameryn Denaro and Alexander B. Chase and Claudia Weihe and Mi Lay and Jennifer B. H. Martiny and Katrine Whiteson},
abstract = {Course-based undergraduate research experiences (CUREs) are an effective way to introduce students to contemporary scientific research. Research experiences have been shown to promote critical thinking, improve understanding and proper use of the scientific method, and help students learn practical skills including writing and oral communication. We aimed to improve scientific training by engaging students enrolled in an upper division elective course in a human microbiome CURE. The “Fiber Force” course is aimed at studying the effect of a wholesome high-fiber diet (40 to 50 g/day for two weeks) on the students’ gut microbiomes. Enrolled students participated in a noninvasive diet intervention, designed health surveys, tested hypotheses on the effect of a diet intervention on the gut microbiome, and analyzed their own samples (as anonymized aggregates). The course involved learning laboratory techniques (e.g., DNA extraction, PCR, and 16S sequencing) and the incorporation of computational techniques to analyze microbiome data with QIIME2 and within the R software environment. In addition, the learning objectives focused on effective student performance in writing, data analysis, and oral communication. Enrolled students showed high performance grades on writing, data analysis and oral communication assignments. Pre- and post-course surveys indicate that the students found the experience favorable, increased their interest in science, and heightened awareness of their diet habits. Fiber Force constitutes a validated case of a research experience on microbiology with the capacity to improve research training and promote healthy dietary habits.}
}
@article{ZHAO2025128946,
title = {The effect of the head number for multi-head self-attention in remaining useful life prediction of rolling bearing and interpretability},
journal = {Neurocomputing},
volume = {616},
pages = {128946},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128946},
url = {https://www.sciencedirect.com/science/article/pii/S092523122401717X},
author = {Qiwu Zhao and Xiaoli Zhang and Fangzhen Wang and Panfeng Fan and Erick Mbeka},
keywords = {Remaining useful life prediction, Machine learning, Multi-head self-attention mechanism, Interpretability, Graph theory, Functional networks},
abstract = {As one of the machine learning (ML) models, the multi-head self-attention mechanism (MSM) is competent in encoding high-level feature representations, providing computing superiorities, and systematically processing sequences bypassing the recurrent neural networks (RNN) models. However, the model performance and computational results are affected by head number, and the lack of impact interpretability has become a primary obstacle due to the complex internal working mechanisms. Therefore, the effects of the head number of the MSM on the accuracy of the result, the robustness of the model, and computation efficiency are investigated in the remaining useful life (RUL) prediction of rolling bearings. The results show that the accuracy of prediction results will be reduced caused by large or few head numbers. In addition, the more heads are selected, the more robust and higher the predictive efficiency of the model is achieved. The above effects are explained relying on the visualization of the attention weight distribution and functional networks, which are constructed and solved by the equivalent fully connected layer and graph theory analysis, respectively. The model's attention coefficient distribution during training and prediction shows that the representative information will be captured inadequately if fewer heads are selected, which causes MSM to neglect to assign large attention coefficients to degraded information. On the contrary, representational degradation information and redundant information will be acquired by models with too many heads. MSM will be disturbed by this redundant information in the attention weight distribution, resulting in incorrect allocation of attention. Both of these cases will reduce the accuracy of the prediction results. In addition, the selection rules of the head number are established based on the feature complexity that is measured by the sample entropy (SamEn). The local range for head selection is also found based on the relationship between head number and feature complexity; The effects of the head number of the MSM on the robustness of the model and computation efficiency are explained by the changes in the three parameters (average of the clustering coefficients, global efficiency, and of the average shortest path length) of the graph, which is constructed after solving the function network. The research provides a reference for rolling bearing prediction with high computational accuracy, calculation efficiency, and strong robustness using MSM.}
}
@article{TWORZYDLO1995759,
title = {Knowledge-based methods and smart algorithms in computational mechanics},
journal = {Engineering Fracture Mechanics},
volume = {50},
number = {5},
pages = {759-800},
year = {1995},
issn = {0013-7944},
doi = {https://doi.org/10.1016/0013-7944(94)E0060-T},
url = {https://www.sciencedirect.com/science/article/pii/0013794494E0060T},
author = {W.W. Tworzydlo and J.T. Oden},
abstract = {Effective methods leading to automated, computer-based solution of complex engineering design problems are studied in this paper. In particular, methods of automation of the finite element analyses are of primary interest here. These include algorithmic approaches, based on error estimation, adaptivity and smart algorithms, as well as heuristic approaches based on methods of knowledge engineering. A computational environment, which interactively couples h-p adaptive finite element methods with object-oriented programming and expert system tools, is presented. Several examples illustrate the merit and potential of the approaches studied here.}
}
@article{FARAJ2021100337,
title = {Unto the breach: What the COVID-19 pandemic exposes about digitalization},
journal = {Information and Organization},
volume = {31},
number = {1},
pages = {100337},
year = {2021},
issn = {1471-7727},
doi = {https://doi.org/10.1016/j.infoandorg.2021.100337},
url = {https://www.sciencedirect.com/science/article/pii/S1471772721000038},
author = {Samer Faraj and Wadih Renno and Anand Bhardwaj},
keywords = {COVID, Digitalization, Technology, Organizing, Breaching experiment},
abstract = {Much recent scholarly investigation has been focused on the promise of digitalization and the new ways of working and organizing it makes possible. In this paper, we analyze how the COVID-19 pandemic has acted as a natural breaching experiment that has challenged taken-for-granted expectations about digitalization and revealed four important issues: uneven access to digital infrastructures, the persistence of the analog in digitalization, the brittleness of unchecked digitalization, and panoptical surveillance. The sudden shift to digital work has exposed taken-for-granted assumptions about the universality of digital access. The crisis has also revealed that many highly digitalized processes still rely on analog elements. The pandemic has also exposed that many algorithms used in digitalized inter-organizational processes are brittle due to overreliance on historic patterns. Finally, the pandemic has breached fundamental expectations of privacy when organizational surveillance was extended into private and public spaces. Thus, the pandemic has laid bare fundamental challenges in digitalization and has exposed the limits of rose‑tinted thinking about the relation between technology and organizing.}
}
@article{BARKER2023100569,
title = {An Inflection Point in Cancer Protein Biomarkers: What was and What's Next},
journal = {Molecular & Cellular Proteomics},
volume = {22},
number = {7},
pages = {100569},
year = {2023},
issn = {1535-9476},
doi = {https://doi.org/10.1016/j.mcpro.2023.100569},
url = {https://www.sciencedirect.com/science/article/pii/S1535947623000804},
author = {Anna D. Barker and Mario M. Alba and Parag Mallick and David B. Agus and Jerry S.H. Lee},
keywords = {proteomics, cancer biomarkers, protein biomarkers, complex adaptive systems, clinical proteomics},
abstract = {Biomarkers remain the highest value proposition in cancer medicine today—especially protein biomarkers. Despite decades of evolving regulatory frameworks to facilitate the review of emerging technologies, biomarkers have been mostly about promise with very little to show for improvements in human health. Cancer is an emergent property of a complex system, and deconvoluting the integrative and dynamic nature of the overall system through biomarkers is a daunting proposition. The last 2 decades have seen an explosion of multiomics profiling and a range of advanced technologies for precision medicine, including the emergence of liquid biopsy, exciting advances in single-cell analysis, artificial intelligence (machine and deep learning) for data analysis, and many other advanced technologies that promise to transform biomarker discovery. Combining multiple omics modalities to acquire a more comprehensive landscape of the disease state, we are increasingly developing biomarkers to support therapy selection and patient monitoring. Furthering precision medicine, especially in oncology, necessitates moving away from the lens of reductionist thinking toward viewing and understanding that complex diseases are, in fact, complex adaptive systems. As such, we believe it is necessary to redefine biomarkers as representations of biological system states at different hierarchical levels of biological order. This definition could include traditional molecular, histologic, radiographic, or physiological characteristics, as well as emerging classes of digital markers and complex algorithms. To succeed in the future, we must move past purely observational individual studies and instead start building a mechanistic framework to enable integrative analysis of new studies within the context of prior studies. Identifying information in complex systems and applying theoretical constructs, such as information theory, to study cancer as a disease of dysregulated communication could prove to be “game changing” for the clinical outcome of cancer patients.}
}
@article{MUSGRAVE2017137,
title = {Understanding and advancing graduate teaching assistants’ mathematical knowledge for teaching},
journal = {The Journal of Mathematical Behavior},
volume = {45},
pages = {137-149},
year = {2017},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2016.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S0732312316302012},
author = {Stacy Musgrave and Marilyn P. Carlson},
keywords = {Graduate student teaching assistant, Mathematical meanings, Average rate of change, Precalculus},
abstract = {Graduate student teaching assistants (GTAs) usually teach introductory level courses at the undergraduate level. Since GTAs constitute the majority of future mathematics faculty, their image of effective teaching and preparedness to lead instructional improvements will impact future directions in undergraduate mathematics curriculum and instruction. In this paper, we argue for the need to support GTAs in improving their mathematical meanings of foundational ideas and their ability to support productive student thinking. By investigating GTAs’ meanings for average rate of change, a key content area in precalculus and calculus, we found evidence that even mathematically sophisticated GTAs possess impoverished meanings of this key idea. We argue for the need, and highlight one approach, for supporting GTAs to improve their understanding of foundational mathematical ideas and how these ideas are learned.}
}
@article{IVANOV2023108938,
title = {Intelligent digital twin (iDT) for supply chain stress-testing, resilience, and viability},
journal = {International Journal of Production Economics},
volume = {263},
pages = {108938},
year = {2023},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2023.108938},
url = {https://www.sciencedirect.com/science/article/pii/S0925527323001706},
author = {Dmitry Ivanov},
keywords = {Supply chain resilience, Intelligent digital twin, Data analytics, Stress-test, Ripple effect, anyLogistix},
abstract = {A large variety of models have been developed in the last two decades aiming at supply chain (SC) stress-testing and resilience. New digital and artificial intelligence (AI) technologies allow to develop novel approaches and tools in this area for the transition from standalone models to intelligent decision-support systems (DSSs). However, the literature lacks concepts and guidelines for the design of such systems. In this paper, we offer a generalized decision-making framework for using digital twins in SC stress-testing and resilience analysis as well as delineate how digital twins can contribute to theory development in SC resilience and viability. We position our proposed approach as an intelligent digital twin (iDT) – a human–AI system which visualizes physical SCs in digital form, collects and processes data for modelling using analytics methods, mimics human decision-making rules, and creates new knowledge and decision-making algorithms through human–AI collaboration. We conclude that the iDT supports monitoring, disruption prediction (early signals), event-driven responses, learning, and proactive thinking, integrating proactive and reactive approaches to SC resilience. The iDT helps to make the unknown known and so contributes to the development of a proactive, adaptation-based view on SC resilience and viability. This research can be used to solve existing problems in the industry, and it develops new methods and infrastructures for solutions to future problems.}
}
@article{SAIKIA2021129664,
title = {Study of interacting mechanism of amino acid and Alzheimer's drug using vibrational techniques and computational method},
journal = {Journal of Molecular Structure},
volume = {1227},
pages = {129664},
year = {2021},
issn = {0022-2860},
doi = {https://doi.org/10.1016/j.molstruc.2020.129664},
url = {https://www.sciencedirect.com/science/article/pii/S0022286020319773},
author = {Jyotshna Saikia and Bhargab Borah and Th. Gomti Devi},
keywords = {DL-Alanine, Memantine, Raman, FTIR, DFT},
abstract = {The present work is undertaken to investigate the molecular interaction between Memantine (Alzheimer's drug) and DL-Alanine (amino acid) using DFT and vibrational spectroscopic methods, in particular, Fourier Transform Infrared Spectroscopy (FTIR) and Raman techniques. The DFT calculations of these molecules are carried out using the B3LYP/6–311 ++ G (d, p) level of theory. The experimental FTIR and Raman spectra of the molecules are compared to the respective DFT computed wavenumbers. A satisfactory agreement is obtained between experimental and computed wavenumbers. Further, HOMO-LUMO energy gap, Natural Bond Orbital (NBO) analysis, total energy, zero-point vibrational energy, Molecular Electrostatic Potential (MEP), chemical potential, hardness, ionization energy, global electrophilicity index, dipole moments, and first-order hyperpolarizabilities of the interacting state are reported and compared to the respective parameters of the individual states. The NBO analysis of the molecules indicates the transfer of charge between DL-Alanine and Memantine through NH•••O intermolecular hydrogen bonds. The molecular docking studies of the molecules are  performed to investigate the binding affinity of the ligand with the 6DG7 receptor.}
}
@article{LUO202571,
title = {HybProm: An attention-assisted hybrid CNN-BiLSTM model for the interpretable prediction of DNA promoter},
journal = {Methods},
volume = {235},
pages = {71-80},
year = {2025},
issn = {1046-2023},
doi = {https://doi.org/10.1016/j.ymeth.2025.02.001},
url = {https://www.sciencedirect.com/science/article/pii/S1046202325000349},
author = {Rentao Luo and Jiawei Liu and Lixin Guan and Mengshan Li},
keywords = {Promoter, Deep learning, Attention, Gene sequences, Bioinformatics},
abstract = {Promoter prediction is essential for analyzing gene structures, understanding regulatory networks, transcription mechanisms, and precisely controlling gene expression. Recently, computational and deep learning methods for promoter prediction have gained attention. However, there is still room to improve their accuracy. To address this, we propose the HybProm model, which uses DNA2Vec to transform DNA sequences into low-dimensional vectors, followed by a CNN-BiLSTM-Attention architecture to extract features and predict promoters across species, including E. coli, humans, mice, and plants. Experiments show that HybProm consistently achieves high accuracy (90%-99%) and offers good interpretability by identifying key sequence patterns and positions that drive predictions.}
}
@article{TALWAR2021102341,
title = {Has financial attitude impacted the trading activity of retail investors during the COVID-19 pandemic?},
journal = {Journal of Retailing and Consumer Services},
volume = {58},
pages = {102341},
year = {2021},
issn = {0969-6989},
doi = {https://doi.org/10.1016/j.jretconser.2020.102341},
url = {https://www.sciencedirect.com/science/article/pii/S0969698920313497},
author = {Manish Talwar and Shalini Talwar and Puneet Kaur and Naliniprava Tripathy and Amandeep Dhir},
keywords = {Artificial neural network, COVID-19, Financial behavior, Financial attitude, Financial anxiety, Pandemic},
abstract = {Financial attitude influences the financial behavior of retail investors. Although the extant research has acknowledged and examined this relationship, the measures of financial attitude and behavior still vary widely and are generally posed as a series of questions rather than statements. In addition to this, there is insufficient knowledge regarding retail investors' behavior in the face of a health crisis, such as the current COVID-19 pandemic. This study addresses these gaps in the prior literature by examining the relative influence of six dimensions of financial attitude, namely, financial anxiety, optimism, financial security, deliberative thinking, interest in financial issues, and needs for precautionary savings, on the trading activity of retail investors during the pandemic. Data were collected from 404 respondents and analyzed using the artificial neural network (ANN) method. The results revealed that all six dimensions had a positive influence on trading activity, with interest in financial issues exerting the strongest influence, followed by deliberative thinking. The study thus contributes important inferences for researchers and managers.}
}
@article{FALBEN2023105386,
title = {The power of the unexpected: Prediction errors enhance stereotype-based learning},
journal = {Cognition},
volume = {235},
pages = {105386},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2023.105386},
url = {https://www.sciencedirect.com/science/article/pii/S0010027723000203},
author = {Johanna K. Falbén and Marius Golubickis and Dimitra Tsamadi and Linn M. Persson and C. Neil Macrae},
keywords = {Stereotyping, Person perception, Reinforcement learning, Prediction errors, Drift diffusion model},
abstract = {Stereotyping is a ubiquitous feature of social cognition, yet surprisingly little is known about how group-related beliefs influence the acquisition of person knowledge. Accordingly, in combination with computational modeling (i.e., Reinforcement Learning Drift Diffusion Model analysis), here we used a probabilistic selection task to explore the extent to which gender stereotypes impact instrumental learning. Several theoretically interesting effects were observed. First, reflecting the impact of cultural socialization on person construal, an expectancy-based preference for stereotype-consistent (vs. stereotype-inconsistent) responses was observed. Second, underscoring the potency of unexpected information, learning rates were faster for counter-stereotypic compared to stereotypic individuals, both for negative and positive prediction errors. Collectively, these findings are consistent with predictive accounts of social perception and have implications for the conditions under which stereotyping can potentially be reduced.}
}
@article{DAVIS20111046,
title = {Homogeneous steady deformation: A review of computational techniques},
journal = {Journal of Structural Geology},
volume = {33},
number = {6},
pages = {1046-1062},
year = {2011},
issn = {0191-8141},
doi = {https://doi.org/10.1016/j.jsg.2011.03.001},
url = {https://www.sciencedirect.com/science/article/pii/S0191814111000447},
author = {Joshua R. Davis and Sarah J. Titus},
keywords = {Kinematic model, Homogeneous deformation, Velocity gradient, Transpression, Vorticity},
abstract = {Homogeneous steady models are frequently used in the structural geology community to describe rock deformation. We review the literature on these models in a streamlined, coordinate-free framework based on matrix exponentials and logarithms. These mathematical tools allow us to compute progressive and simultaneous deformations easily. As an application, we develop transpression with triclinic symmetry in two ways. The tools let us integrate field data related to position and velocity in computing best-fit models with many degrees of freedom. As an application, we reanalyze a published study to demonstrate the extent to which kinematic vorticity is sensitive to modeling assumptions. The tools also open the door to an increased role for the mathematics of Lie groups (spaces of deformations) in structural geology. We suggest two topics for further study: numerical methods for non-steady deformations, and statistics of deformation tensors.}
}
@article{STOLOWY2022101334,
title = {Competing for narrative authority in capital markets: Activist short sellers vs. financial analysts},
journal = {Accounting, Organizations and Society},
volume = {100},
pages = {101334},
year = {2022},
issn = {0361-3682},
doi = {https://doi.org/10.1016/j.aos.2022.101334},
url = {https://www.sciencedirect.com/science/article/pii/S0361368222000010},
author = {Hervé Stolowy and Luc Paugam and Yves Gendron},
keywords = {Activist short sellers, Expertise, Financial analysts, Framing, Narrative authority},
abstract = {Activist short sellers (AShSs) and financial analysts are information intermediaries who analyze firm disclosures as well as produce and disseminate influential investment narratives. This study aims to better understand narrative challenges surrounding the legitimate expertise of financial analysts. Specifically, we examine how AShSs challenge sell-side financial analysts' narrative authority (i.e., the perception that they produce expert knowledge) in interpreting firms' performance and future prospects. We investigate how analysts respond (or do not respond) to this challenge. We use 442 AShS reports, 12 interviews with AShSs and analysts, and analysts' stock recommendations and target prices. In their criticisms of analysts (found in one-third of reports), AShSs frequently frame analysts as lacking market expertise and critical thinking – two core dimensions of analysts' narrative authority. Sixty-six percent of analysts, although explicitly criticized in AShS reports, do not engage in written responses in their equity research reports because they reportedly either adopt a renunciation attitude to the challenge or they engage in off-the-record discussions with certain market participants. However, 34% of analysts respond overtly by counter-framing AShSs as lacking market expertise and objectivity. After the dissemination of AShS reports, analysts, on average, do not revise their highly visible stock recommendations but they revise target prices downward. Theoretically, this study extends our understanding of the construction of narrative authority in capital markets as we examine a challenge to the expertise of influential information intermediaries.}
}
@article{BIELZA2000725,
title = {Structural, elicitation and computational issues faced when solving complex decision making problems with influence diagrams},
journal = {Computers & Operations Research},
volume = {27},
number = {7},
pages = {725-740},
year = {2000},
issn = {0305-0548},
doi = {https://doi.org/10.1016/S0305-0548(99)00113-6},
url = {https://www.sciencedirect.com/science/article/pii/S0305054899001136},
author = {C. Bielza and M. Gómez and S. Rı́os-Insua and J.A.Fernández {del Pozo}},
keywords = {Decision analysis, Influence diagrams, Implementation issues, Medical decision making, Neonatal jaundice},
abstract = {Influence diagrams have become a popular tool for representing and solving decision making problems under uncertainty (Shachter, Operations Research 1986;34:871–82). We show here some practical difficulties when using them to construct a medical decision support system. Specifically, it is hard to tackle issues related to the problem structuring, like the existence of constraints on the sequence of decisions, and the time evolution modeling; related to the knowledge-acquisition, like probability and utility assignment; and related to computational limitations, in memory storage and evaluation phases, as well as the explanation of results. We have recently developed a complex decision support system for neonatal jaundice management — a very common medical problem — , encountering all these difficulties. In this paper, we describe them and how they have been undertaken, providing insights into the community involved in the design and solution of decision models by means of influence diagrams.
Scope and purpose
Decision Analysis is a very well-known discipline that deals with the practice of Decision Theory (Clemen, Making hard decisions: an introduction to decision analysis, 2nd ed. Pacific Grove, CA: Duxbury, 1996). It comprises various steps usually implemented in a decision support system: definition of the alternatives and objectives, modelization of the structure of the decision problem, as well as the beliefs and preferences of the decision maker. The recommended alternative is the one with maximum expected utility, once all the assignments have been refined via sensitivity analyses. However, there are a number of difficulties faced in practice when solving large problems, that require an attentive study.}
}
@article{NOWACK2024459,
title = {Science and reflections: With some thoughts to young applied scientists and engineers},
journal = {Earthquake Science},
volume = {37},
number = {5},
pages = {459-493},
year = {2024},
issn = {1674-4519},
doi = {https://doi.org/10.1016/j.eqs.2024.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1674451924000648},
author = {Robert L. Nowack},
keywords = {geophysics, computational and data science, applied science and engineering},
abstract = {I provide some science and reflections from my experiences working in geophysics, along with connections to computational and data sciences, including recent developments in machine learning. I highlight several individuals and groups who have influenced me, both through direct collaborations as well as from ideas and insights that I have learned from. While my reflections are rooted in geophysics, they should also be relevant to other computational scientific and engineering fields. I also provide some thoughts for young, applied scientists and engineers.}
}
@article{KLOOSTER2024110771,
title = {A systematic review on eHealth technology personalization approaches},
journal = {iScience},
volume = {27},
number = {9},
pages = {110771},
year = {2024},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2024.110771},
url = {https://www.sciencedirect.com/science/article/pii/S2589004224019965},
author = {Iris ten Klooster and Hanneke Kip and Lisette {van Gemert-Pijnen} and Rik Crutzen and Saskia Kelders},
keywords = {Health sciences, Health technology},
abstract = {Summary
Despite the widespread use of personalization of eHealth technologies, there is a lack of comprehensive understanding regarding its application. This systematic review aims to bridge this gap by identifying and clustering different personalization approaches based on the type of variables used for user segmentation and the adaptations to the eHealth technology and examining the role of computational methods in the literature. From the 412 included reports, we identified 13 clusters of personalization approaches, such as behavior + channeling and environment + recommendations. Within these clusters, 10 computational methods were utilized to match segments with technology adaptations, such as classification-based methods and reinforcement learning. Several gaps were identified in the literature, such as the limited exploration of technology-related variables, the limited focus on user interaction reminders, and a frequent reliance on a single type of variable for personalization. Future research should explore leveraging technology-specific features to attain individualistic segmentation approaches.}
}
@article{GOLTZ2021103417,
title = {Do you listen to music while studying? A portrait of how people use music to optimize their cognitive performance},
journal = {Acta Psychologica},
volume = {220},
pages = {103417},
year = {2021},
issn = {0001-6918},
doi = {https://doi.org/10.1016/j.actpsy.2021.103417},
url = {https://www.sciencedirect.com/science/article/pii/S0001691821001670},
author = {Franziska Goltz and Makiko Sadakata},
keywords = {Background music, Cognitive performance, Music perception},
abstract = {The effect of background music (BGM) on cognitive task performance is a popular topic. However, the evidence is not converging: experimental studies show mixed results depending on the task, the type of music used and individual characteristics. Here, we explored how people use BGM while optimally performing various cognitive tasks in everyday life, such as reading, writing, memorizing, and critical thinking. Specifically, the frequency of BGM usage, preferred music types, beliefs about the scientific evidence on BGM, and individual characteristics, such as age, extraversion and musical background were investigated. Although the results confirmed highly diverse strategies among individuals regarding when, how often, why and what type of BGM is used, we found several general tendencies: people tend to use less BGM when engaged in more difficult tasks, they become less critical about the type of BGM when engaged in easier tasks, and there is a negative correlation between the frequency of BGM and age, indicating that younger generations tend to use more BGM than older adults. The current and previous evidence are discussed in light of existing theories. Altogether, this study identifies essential variables to consider in future research and further forwards a theory-driven perspective in the field.}
}
@article{MERRITT2024103670,
title = {Igniting kid power: The impact of environmental service-learning on elementary students' awareness of energy problems and solutions},
journal = {Energy Research & Social Science},
volume = {116},
pages = {103670},
year = {2024},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2024.103670},
url = {https://www.sciencedirect.com/science/article/pii/S2214629624002615},
author = {Eileen G. Merritt and Andrea E. Weinberg and Candace Lapan and Sara E. Rimm-Kaufman},
keywords = {Environmental service-learning, Energy literacy, Elementary students, Science education, Randomized controlled trial},
abstract = {Energy concepts are taught in many schools, but children rarely have an opportunity to grapple with energy problems and work on their own solutions. This study explores the impacts of Connect Science, a service-learning (SL) program developed to enhance elementary students' energy literacy in the United States. Program impacts were explored within the context of a randomized controlled trial. Teachers in the SL intervention group were provided with professional development, coaching and curricular materials. Each fourth grade class chose an energy problem to address, and designed projects to test out a solution. Teachers in a waitlist control group taught their typical energy unit. Upon completion of the unit, students were asked to write about a problem related to energy production or use and propose a potential solution. Inductive content analysis was used to code 703 student responses (377 from control group and 326 from SL group). The majority of students expressed concerns about wasting or using too much electricity or the use of nonrenewable energy sources. Solutions focused on energy conservation and the use of renewable or clean resources were mentioned most frequently overall. Students in the SL group were significantly more likely to mention environmental impacts of various energy sources and to suggest energy conservation solutions or educating others. Conversely, the control group student responses more often focused on electric circuits or electrical safety. Results from this study suggest the promise of environmental SL programs to advance energy literacy and promote critical thinking about how to address energy problems.}
}
@incollection{RUNCO20141,
title = {Chapter 1 - Cognition and Creativity},
editor = {Mark A. Runco},
booktitle = {Creativity (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {1-38},
year = {2014},
isbn = {978-0-12-410512-6},
doi = {https://doi.org/10.1016/B978-0-12-410512-6.00001-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780124105126000011},
author = {Mark A. Runco},
keywords = {Threshold theory, IQ, Structure of intellect, Associative theory, Problem solving, Problem finding, Incubation, Insight, Intuition, Meta-cognition, Mindfulness, Overinclusive thinking},
abstract = {This chapter discusses various aspects of cognition and creativity. Cognitive theories focus on thinking skills and intellectual processes. The approaches to creative cognition are extremely varied. There are bridges between basic cognitive processes and creative problem solving, as well as connections with intelligence, problem solving, language, and other indications of individual differences. The basic processes are generally nomothetic, meaning that they represent universals. Divergent thinking is employed when an individual is faced with an open-ended task. From this perspective divergent thinking is a kind of problem solving. Divergent thinking is not synonymous with creative thinking, but it does tell something about the cognitive processes that may lead to original ideas and solutions. Many theories of creative cognition look to associative processes. Associative theories focus on how ideas are generated and chained together. Cognitive theories of creativity often focus specifically on the problem-solving process. A problem can be defined as a situation with a goal and an obstacle. The stage models of creative cognition are also elaborated.}
}
@article{XIA2025108857,
title = {LSDNet: Lightweight strip-steel surface defect detection networks for edge device environment},
journal = {Optics and Lasers in Engineering},
volume = {186},
pages = {108857},
year = {2025},
issn = {0143-8166},
doi = {https://doi.org/10.1016/j.optlaseng.2025.108857},
url = {https://www.sciencedirect.com/science/article/pii/S0143816625000442},
author = {Xuhui Xia and Jiale Guo and Zelin Zhang and Lei Wang and Yuyao Guo},
keywords = {Cold-rolled strip steel, Defect classification, Lightweight network, Feature extraction},
abstract = {Online recognizing defects of the strip-steel surface on resource-constrained embedded devices is a difficult problem. The traditional deep learning model with deep network layers and large parameter counts cannot balance the efficiency and the accuracy. This paper proposes a specialized lightweight deep learning detection model (LSDNet) for strip-steel surface defects. Moreover, LSDNet effectively classifies and recognizes these defects with fewer model parameters. LSDNet adopts Mobilenetv2 as the basic framework and constructs a new feature extraction module. The SPD-Conv module enhances the feature learning capacity for small targets and reduces model redundancy, while the ECANet module improves feature extraction capabilities. Additionally, the parameter-free attention mechanism (SimAM) is incorporated after the initial and final convolutional layers to boost recognition accuracy. Computational efficiency is achieved by substituting fully connected layers with a spatially invariant global average pooling layer, thereby preserving essential depth information. Dropout layers are deployed to enhance generalization, and dynamic learning rate adjustments optimize the training process. Experimental results demonstrate that the proposed LSDNet achieves a classification accuracy of 98.60 %, an F1−score of 98.57 %, with only 0.76 million parameters and 0.095 billion FLOPs for strip-steel surface defects. Compared to Mobilenetv2, LSDNet reduces the parameter count by 2.749 million and improves the classification accuracy by 1.69 %. This method performs better than other classification models in balancing recognition efficiency and accuracy.}
}
@incollection{MOHAN2025541,
title = {Chapter 51 - Exploring the exciting potential and challenges of brain computer interfaces},
editor = {M.A. Ansari and R.S. Anand and Pragati Tripathi and Rajat Mehrotra and Md Belal Bin Heyat},
booktitle = {Artificial Intelligence in Biomedical and Modern Healthcare Informatics},
publisher = {Academic Press},
pages = {541-550},
year = {2025},
isbn = {978-0-443-21870-5},
doi = {https://doi.org/10.1016/B978-0-443-21870-5.00051-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780443218705000510},
author = {Anand Mohan and R.S. Anand},
keywords = {Brain–computer interface (BCI), EEG, Machine learning, Motor imagery, PSD},
abstract = {Electroencephalogram (EEG) signals contain various information about the cognitive thinking, emotion, and thoughts of a person. Verbal communication is the normal form of interaction method used, but various kinds of physically disabled people who are not in the condition to express themselves can be assisted using the EEG signal rehabilitation technique. EEG signals can be used effectively in rehabilitation by using brain–computer interfaces (BCIs). BCI is a technology that allows interaction between the brain and a computer. This kind of technique can be used to treat patients with paralyzed muscles and locked in syndromes by helping them interact with others using their EEG signals. The application of BCI can be in medical field, education, and security. In this chapter, all aspects of BCIs are discussed in great detail and also have worked on motor imaginary-based dataset and have used linear discriminant analysis (LDA) algorithm as the classifier, which showed 91% accuracy.}
}