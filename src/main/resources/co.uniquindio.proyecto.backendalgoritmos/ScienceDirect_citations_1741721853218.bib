@article{BUTZ2025105948,
title = {Contextualizing predictive minds},
journal = {Neuroscience & Biobehavioral Reviews},
volume = {168},
pages = {105948},
year = {2025},
issn = {0149-7634},
doi = {https://doi.org/10.1016/j.neubiorev.2024.105948},
url = {https://www.sciencedirect.com/science/article/pii/S0149763424004172},
author = {Martin V. Butz and Maximilian Mittenbühler and Sarah Schwöbel and Asya Achimova and Christian Gumbsch and Sebastian Otte and Stefan Kiebel},
keywords = {Prediction, Cognitive modeling, Events, Active inference, Learning, Behavior, Free energy, Abstraction, Context inference, Deep learning},
abstract = {The structure of human memory seems to be optimized for efficient prediction, planning, and behavior. We propose that these capacities rely on a tripartite structure of memory that includes concepts, events, and contexts—three layers that constitute the mental world model. We suggest that the mechanism that critically increases adaptivity and flexibility is the tendency to contextualize. This tendency promotes local, context-encoding abstractions, which focus event- and concept-based planning and inference processes on the task and situation at hand. As a result, cognitive contextualization offers a solution to the frame problem—the need to select relevant features of the environment from the rich stream of sensorimotor signals. We draw evidence for our proposal from developmental psychology and neuroscience. Adopting a computational stance, we present evidence from cognitive modeling research which suggests that context sensitivity is a feature that is critical for maximizing the efficiency of cognitive processes. Finally, we turn to recent deep-learning architectures which independently demonstrate how context-sensitive memory can emerge in a self-organized learning system constrained by cognitively-inspired inductive biases.}
}
@incollection{SAWLEY1995181,
title = { - A serial data-parallel multi-block method for compressible flow computations},
editor = {A. Ecer and J. Hauser and P. Leca and J. Periaux},
booktitle = {Parallel Computational Fluid Dynamics 1993},
publisher = {North-Holland},
address = {Amsterdam},
pages = {181-188},
year = {1995},
isbn = {978-0-444-81999-4},
doi = {https://doi.org/10.1016/B978-044481999-4/50148-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780444819994501481},
author = {M.L. Sawley and J.K. Tegnér and C.M. Bergman},
abstract = {Publisher Summary
Block structured meshes not only provide the possibility to compute flows in complex geometries but also lend themselves in a natural way to coarse-grain parallel processing via the distribution of different blocks to different processors. Nevertheless, for some flow computations, a fine-grain data parallel implementation may be more appropriate. This chapter presents a study of such an implementation, which utilizes the simplicity of the data parallel approach. Particular attention is placed on a dynamic block management strategy that allows computations to be undertaken only in blocks where useful work is to be performed. The question of code portability among four different parallel computer systems is addressed in the chapter. This chapter concludes that the serial data-parallel multi-block method provides a number of advantages: (1) it retains the simplicity of the above-mentioned data parallel methods, because each block is treated individually in the same manner as for a single block computation; (2) it does not impose any parallelization constraints on the mesh generation procedure, in principle, any number of blocks of unequal size can be employed; the transfer of data between two blocks (block connectivity) is performed in a transparent manner via globally addressable memory contrasting with the explicit data transfer required by message passing implementations; (3) because individual blocks are treated sequentially, a simple dynamic block management algorithm can be applied to avoid performing unnecessary operations; and (4) the use of standard Fortran 90 facilitates code portability among different platforms supporting the data parallel programming method.}
}
@article{NAZIDIZAJI2015318,
title = {Does the smartest designer design better? Effect of intelligence quotient on students’ design skills in architectural design studio},
journal = {Frontiers of Architectural Research},
volume = {4},
number = {4},
pages = {318-329},
year = {2015},
issn = {2095-2635},
doi = {https://doi.org/10.1016/j.foar.2015.08.002},
url = {https://www.sciencedirect.com/science/article/pii/S2095263515000394},
author = {Sajjad Nazidizaji and Ana Tomé and Francisco Regateiro},
keywords = {Architectural design studio, Intelligence quotient (IQ), Design education, Human factors, Design thinking},
abstract = {Understanding the cognitive processes of the human mind is necessary to further learn about design thinking processes. Cognitive studies are also significant in the research about design studio. The aim of this study is to examine the effect of designers intelligence quotient (IQ) on their designs. The statistical population in this study consisted of all Deylaman Institute of Higher Education architecture graduate students enrolled in 2011. Sixty of these students were selected via simple random sampling based on the finite population sample size calculation formula. The students’ IQ was measured using Raven’s Progressive Matrices. The students’ scores in Architecture Design Studio (ADS) courses from first grade (ADS-1) to fifth grade (ADS-5) and the mean scores of the design courses were used in determining the students’ design ability. Inferential statistics, as well as correlation analysis and mean comparison test for independent samples with SPSS, were also employed to analyze the research data. Results indicated that the students’ IQ, ADS-1 to ADS-4 scores, and the mean scores of the students’ design courses were not significantly correlated. By contrast, the students’ IQ and ADS-5 scores were significantly correlated. As the complexity of the design problem and designers’ experience increased, the effect of IQ on design seemingly intensified.}
}
@article{NIKZAINAL2024101739,
title = {Prof. Serena Nik-Zainal},
journal = {Cell Reports Medicine},
volume = {5},
number = {9},
pages = {101739},
year = {2024},
issn = {2666-3791},
doi = {https://doi.org/10.1016/j.xcrm.2024.101739},
url = {https://www.sciencedirect.com/science/article/pii/S2666379124004695},
author = {Serena Nik-Zainal},
abstract = {Serena Nik-Zainal, MD, PhD, is professor of genomic medicine and bioinformatics and an honorary consultant in clinical genetics at the University of Cambridge. Prof. Nik-Zainal has dedicated her career to studying the physiology of cancer mutagenesis via a combination of computational and experimental work, as well as validation with clinical data. Among the many awards she has earned for her work, she has recently received the 2024 ESMO Award for Translational Research, for the research in the field of mutational signatures and her efforts in translating their use into clinics.}
}
@article{ZHU2023126915,
title = {SPAR: An efficient self-attention network using Switching Partition Strategy for skeleton-based action recognition},
journal = {Neurocomputing},
volume = {562},
pages = {126915},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2023.126915},
url = {https://www.sciencedirect.com/science/article/pii/S092523122301038X},
author = {ZiJie Zhu and RenDong Ying and Fei Wen and PeiLin Liu},
keywords = {Action recognition, Self-attention, 3D-skeleton, Graph convolutional networks},
abstract = {Graph convolutional networks (GCN) have become the mainstream in skeleton-based action recognition. For further performance improvement, existing methods propose to utilize self-attention to model long-range features of joints. However, these methods cannot balance accuracy with computational efficiency. In this paper, we propose the Switching Partition Strategy (SPAR) Network that uses the self-attention mechanism for the simultaneous and efficient extraction of spatial–temporal long-range information from the skeleton. We design two partition strategies that reduce the computational cost and improve the efficiency of the computation of self-attention. Extensive experiments are conducted on two large-scale datasets, i.e. NTU RGB+D 60 and NTU RGB+D 120, to evaluate the performance of the proposed SPAR network. The results demonstrate that our method outperforms the state-of-the-art on accuracy as well as computational cost.}
}
@article{FLATER2018144,
title = {Architecture for software-assisted quantity calculus},
journal = {Computer Standards & Interfaces},
volume = {56},
pages = {144-147},
year = {2018},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2017.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0920548917303069},
author = {David Flater},
keywords = {SI, Quantity, Unit, Uncertainty, Value, Unit 1},
abstract = {A quantity value, such as 5 kg, consists of a number and a reference (often an International System of Units (SI) unit) that together express the magnitude of a quantity. Many software libraries, packages, and ontologies that implement “quantities and units” functions are available. Although all of them begin with SI and associated practices, they differ in how they address issues such as ad hoc counting units, ratios of two quantities of the same kind, and uncertainty. This short article describes an architecture that addresses the complete set of functions in a simple and consistent fashion. Its goal is to encourage more convergent thinking about the functions and the underlying concepts so that the many disparate implementations, present and future, will become more consistent with one another.}
}
@article{SEVERENGIZ2018429,
title = {Influence of Gaming Elements on Summative Assessment in Engineering Education for Sustainable Manufacturing},
journal = {Procedia Manufacturing},
volume = {21},
pages = {429-437},
year = {2018},
note = {15th Global Conference on Sustainable Manufacturing},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2018.02.141},
url = {https://www.sciencedirect.com/science/article/pii/S235197891830180X},
author = {Mustafa Severengiz and Ina Roeder and Kristina Schindler and Günther Seliger},
keywords = {summative assessment, gamification, higher education, engineering education},
abstract = {Regarding the massive sustainability challenge mankind is currently facing, there is an indisputable need to implement sustainability as the key reference point into higher engineering education in order to prepare the stakeholders of tomorrow. This requires networked thinking on the part of the learner and increases the learning goals’ complexity dramatically. The actual achieved learning outcomes are often evaluated by assessing factual knowledge in higher education. However, it has been shown many times that students choose the examination format for orientation when studying. Thus, the authors propose a gamified summative assessment approach that requires networked thinking to direct students’ learning efforts towards broad competency building. In a study with 25 students of a master engineering course, the effects of a gamified examination design are investigated.}
}
@incollection{DUNBAR200113746,
title = {Scientific Reasoning and Discovery, Cognitive Psychology of},
editor = {Neil J. Smelser and Paul B. Baltes},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences},
publisher = {Pergamon},
address = {Oxford},
pages = {13746-13749},
year = {2001},
isbn = {978-0-08-043076-8},
doi = {https://doi.org/10.1016/B0-08-043076-7/01602-8},
url = {https://www.sciencedirect.com/science/article/pii/B0080430767016028},
author = {K. Dunbar},
abstract = {The cognitive mechanisms underlying scientific thinking and discovery have been investigated using approaches from cognitive psychology, cognitive science, and artificial intelligence. In this article, six overlapping approaches are discussed. First, historical analyses and interviews have provided important information on the types of thinking involved in particular discoveries or used by individual scientists. Second, scientific reasoning has been thought of as a form of inductive thinking, and as a form of problem solving. Researchers using this approach have delineated some of the problem solving and inductive reasoning strategies used in science. Third, much research on errors in scientific reasoning, particularly on the topic of ‘confirmation bias’ has revealed some of the circumstances under which science can go awry. Fourth, many researchers have investigated how children's thinking is similar to, or different from, that of scientists. A fifth approach has been to investigate scientists reasoning live or ‘in vivo’ in their own labs. This work has shown how processes such as analogy, distributed cognition, and specific types of inductive and deductive reasoning strategies are used together by scientists. Finally, the incorporation of cognitive mechanisms into computer programs that make discoveries is seen as an important development in the cognitive psychology of scientific thinking.}
}
@article{LYU2023366,
title = {Application of Deep Learning in the Search and a Certain Celestial Body},
journal = {Procedia Computer Science},
volume = {228},
pages = {366-372},
year = {2023},
note = {3rd International Conference on Machine Learning and Big Data Analytics for IoT Security and Privacy},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.11.042},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923018665},
author = {Yang Lyu and Donglin Su},
keywords = {Deep Learning, Search Analysis, Research Applications, a Certain Celestial Body},
abstract = {With the development of the times, celestial body search has become increasingly common, and people want to enhance their understanding of the universe through further search of celestial bodies. Nowadays, many software and hardware have been invented to assist in celestial body search, but the computational efficiency and efficiency of current software are still insufficient. So this article focuses on the research and application of Deep Learning (DL) in the search and analysis of "a certain celestial body", aiming to improve celestial search through DL. Through experiments, this article uses DL to achieve a maximum computational rate of 78% and a minimum of 70% for celestial search. The computational efficiency of celestial search without DL can reach up to 68% and 57%, respectively. After using DL, the efficiency of celestial search reaches up to 82% and 70%, while before using DL, the efficiency of celestial search reaches up to 62% and 50%, respectively. From this data, it can be seen that DL can achieve good results in celestial search.}
}
@article{XU2024473,
title = {Review of the continuous catalytic ortho-para hydrogen conversion technology for hydrogen liquefaction},
journal = {International Journal of Hydrogen Energy},
volume = {62},
pages = {473-487},
year = {2024},
issn = {0360-3199},
doi = {https://doi.org/10.1016/j.ijhydene.2024.03.085},
url = {https://www.sciencedirect.com/science/article/pii/S0360319924009170},
author = {Pan Xu and Jian Wen and Ke Li and Simin Wang and Yanzhong Li},
keywords = {Hydrogen liquefaction, Continuous catalytic ortho-para hydrogen conversion technology, Ortho-para hydrogen conversion catalyst, Packing layer, Plate-fin heat exchanger},
abstract = {In order to meet rapidly growing demand of liquid hydrogen in the future hydrogen industry and energy structure, the continuous catalytic ortho-para hydrogen conversion technology (CCOPHCT) has been once again proposed and has become an important choice to improve the hydrogen liquefaction units (HLUs). The origin, concept, and research progress of the CCOPHCT are systematically reviewed for the first time in this paper. However, the research depth and breadth of the CCOPHCT are insufficient to support its current application. To solve it, for the continuous catalytic ortho-para hydrogen conversion plate-fin heat exchanger (CCOPHC-PFHE) with better comprehensive performances, this paper comprehensively summarizes the research achievements in the related fields from the perspective of the unit analysis, including the ortho-para hydrogen conversion (OPHC), packing layer and plate-fin heat exchanger (PFHE), which to provide further thinking for the development of the CCOPHC-PFHE. Further, some suggestions for the CCOPHCT are proposed based on the existed research foundations, including preparing the effective ortho-para hydrogen conversion catalyst (OPHCC), developing the accurate OPHC dynamical model, revealing the coupling mechanism in the packing layer filled with the OPHCC and establishing an effective design method and standard of the CCOPHC-PFHE. In addition, considering the special conditions on the CCOPHC-PFHE, the importance of the experimental research is emphasized. And based on the established hydrogen experimental platform with the comprehensive supporting implementations, the experimental device of the CCOPHC-PFHE has been completed and a series of experimental tests are currently in progressing.}
}
@article{KRONICK2011435,
title = {Compensatory beliefs and intentions contribute to the prediction of caloric intake in dieters},
journal = {Appetite},
volume = {57},
number = {2},
pages = {435-438},
year = {2011},
issn = {0195-6663},
doi = {https://doi.org/10.1016/j.appet.2011.05.306},
url = {https://www.sciencedirect.com/science/article/pii/S0195666311004636},
author = {Ilana Kronick and Randy P. Auerbach and Christine Stich and Bärbel Knäuper},
keywords = {Compensatory beliefs, Compensatory intentions, Restraint, Disinhibition, Caloric intake, Experience sampling methodology},
abstract = {One cognitive process that impacts dieters’ decision to indulge is the activation of compensatory beliefs. Compensatory beliefs (CBs) are convictions that the consequences of engaging in an indulgent behaviour (eating cake) can be neutralized by the effects of another behaviour (skipping dinner). Using experience sampling methodology, this study hypothesized that, in addition to the cognitive processes associated with restraint and disinhibition, compensatory thinking contributes to the prediction of caloric intake. Results indicated that higher scores on CB, CI and TFEQ-D predicted a greater number of portions eaten signifying that, along with disinhibition, compensatory thinking predicts caloric intake in dieters.}
}
@article{CAO2024101200,
title = {Explanatory models in neuroscience, Part 2: Functional intelligibility and the contravariance principle},
journal = {Cognitive Systems Research},
volume = {85},
pages = {101200},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2023.101200},
url = {https://www.sciencedirect.com/science/article/pii/S1389041723001341},
author = {Rosa Cao and Daniel Yamins},
keywords = {Evolution, Contravariance, Intelligibility, Function, Optimization, No-miracles, Instrumentalism, Realism, Philosophy, Constraints, Evolutionary landscape, Models, Explanation, Evo-devo, Development, Learning, Deep learning, Abstraction},
abstract = {Computational modeling plays an increasingly important role in neuroscience, highlighting the philosophical question of how computational models explain. In the particular case of neural network models, concerns have been raised about their intelligibility, and how these models relate (if at all) to what is found in the brain. We claim that what makes a system intelligible is an understanding of the dependencies between its behavior and the factors that are responsible for that behavior. In biology, many of these dependencies are naturally “top-down”, as ethological imperatives interact with evolutionary and developmental constraints under natural selection to produce systems with capabilities and behaviors appropriate to their evolutionary needs. We describe how the optimization techniques used to construct neural network models capture some key aspects of these dependencies, and thus help explain why brain systems are as they are — because when a challenging ecologically-relevant goal is shared by a neural network and the brain, it places constraints on the possible mechanisms exhibited in both kinds of systems. The presence and strength of these constraints explain why some outcomes are more likely than others. By combining two familiar modes of explanation — one based on bottom-up mechanistic description (whose relation to neural network models we address in a companion paper) and the other based on top-down constraints, these models have the potential to illuminate brain function.}
}
@article{DIAS2022140,
title = {Utilization of the Arena simulation software and Lean improvements in the management of metal surface treatment processes},
journal = {Procedia Computer Science},
volume = {204},
pages = {140-147},
year = {2022},
note = {International Conference on Industry Sciences and Computer Science Innovation},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.08.017},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922007554},
author = {A.S.M.E. Dias and R.M.G. Antunes and A. Abreu and V. Anes and H.V.G. Navas and T. Morgado and J.M.F. Calado},
keywords = {Process management, Arena software, Lean tools, Case study, Metal surface treatments},
abstract = {For companies to stand out in increasingly competitive, dynamic and global markets, they must have customer satisfaction goals, create value through their processes, products and services and also aim for innovation. In this context, computer sciences combined with engineering processes constitutes a powerful way for companies to be able to improve process management, to interact with such markets in an efficient and effective way. The main objective of this article is to use Arena simulation software, to quantitatively predict the impact of improvements applied in metal surface treatment processes, based on tools to support Lean thinking. A case study in a Portuguese company in the metalworking sector is presented, in which it is verified that the proposed improvements in terms of the factory layout and resource management, suggested by the comparison between simulations of the current state of the company and the improved one, streamline the processes of finishing in metals, namely zinc coating and lacquering which prevent the occurrence of oxidation and the consequent corrosion of the base metals, by adding other metals and materials to their surface, which adhere and protect it. Through the results obtained, it is concluded that the reduction of waiting times and transport of stocks without production and of work-in-progress, as well as the increase of the productive capacity, make the company more able to guarantee the satisfaction of the requirements of its customers and improve its positioning in the market compared to its competitors.}
}
@article{SUDHESHWAR2024108305,
title = {Learning from Safe-by-Design for Safe-and-Sustainable-by-Design: Mapping the current landscape of Safe-by-Design reviews, case studies, and frameworks},
journal = {Environment International},
volume = {183},
pages = {108305},
year = {2024},
issn = {0160-4120},
doi = {https://doi.org/10.1016/j.envint.2023.108305},
url = {https://www.sciencedirect.com/science/article/pii/S0160412023005780},
author = {Akshat Sudheshwar and Christina Apel and Klaus Kümmerer and Zhanyun Wang and Lya G. Soeteman-Hernández and Eugenia Valsami-Jones and Claudia Som and Bernd Nowack},
keywords = {Safe-by-Design (SbD), Safe and Sustainable-by-Design (SSbD), Literature mapping, SSbD implementation},
abstract = {With the introduction of the European Commission's “Safe and Sustainable-by-Design” (SSbD) framework, the interest in understanding the implications of safety and sustainability assessments of chemicals, materials, and processes at early-innovation stages has skyrocketed. Our study focuses on the “Safe-by-Design” (SbD) approach from the nanomaterials sector, which predates the SSbD framework. In this assessment, SbD studies have been compiled and categorized into reviews, case studies, and frameworks. Reviews of SbD tools have been further classified as quantitative, qualitative, or toolboxes and repositories. We assessed the SbD case studies and classified them into three categories: safe(r)-by-modeling, safe(r)-by-selection, or safe(r)-by-redesign. This classification enabled us to understand past SbD work and subsequently use it to define future SSbD work so as to avoid confusion and possibilities of “SSbD-washing” (similar to greenwashing). Finally, the preexisting SbD frameworks have been studied and contextualized against the SSbD framework. Several key recommendations for SSbD based on our analysis can be made. Knowledge gained from existing approaches such as SbD, green and sustainable chemistry, and benign-by-design approaches needs to be preserved and effectively transferred to SSbD. Better incorporation of chemical and material functionality into the SSbD framework is required. The concept of lifecycle thinking and the stage-gate innovation model need to be reconciled for SSbD. The development of high-throughput screening models is critical for the operationalization of SSbD. We conclude that the rapid pace of both SbD and SSbD development necessitates a regular mapping of the newly published literature that is relevant to this field.}
}
@incollection{MILLER2023169,
title = {Chapter 9 - Doctoral and professional programs},
editor = {Susan Miller and Walter Moos and Barbara Munk and Stephen Munk and Charles Hart and David Spellmeyer},
booktitle = {Managing the Drug Discovery Process (Second Edition)},
publisher = {Woodhead Publishing},
edition = {Second Edition},
pages = {169-196},
year = {2023},
series = {Woodhead Publishing Series in Biomedicine},
isbn = {978-0-12-824304-6},
doi = {https://doi.org/10.1016/B978-0-12-824304-6.00013-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128243046000134},
author = {Susan Miller and Walter Moos and Barbara Munk and Stephen Munk and Charles Hart and David Spellmeyer},
keywords = {Basic/applied/clinical, Career/job, Collaboration/teams, Critical thinking, -index, PhD/PharmD, Postdoc/postdoctoral, Problem identification, Research design, Writing/publishing},
abstract = {In this chapter on graduate and professional education, we explore doctoral and professional programs, posing a number of key questions you should ask yourself. Where to apply to graduate school or a postdoc, and why? With whom should you work? A PhD or a PharmD? What must you do to be successful? Moreover, we touch on traits important to becoming an independent researcher and ask whether success in graduate school or a postdoctoral fellowship requires different skills than undergraduate degrees. Critical thinking habits underpin this discussion. We outline possible career choices—jobs!—touching on the knowledge and expertise used by drug hunters, and also ask what might be of most value to potential employers. Each of us is different, and what’s best for you is something you will have to decipher, but hopefully you will consult with family, friends, and advisors or mentors before making a final decision. Regardless, “the big leap” is coming, so get ready.}
}
@article{JAYAPRAKASAM2015229,
title = {PSOGSA-Explore: A new hybrid metaheuristic approach for beampattern optimization in collaborative beamforming},
journal = {Applied Soft Computing},
volume = {30},
pages = {229-237},
year = {2015},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2015.01.024},
url = {https://www.sciencedirect.com/science/article/pii/S1568494615000435},
author = {S. Jayaprakasam and S.K.A. Rahim and Chee Yen Leow},
keywords = {Collaborative beamforming, Random array, Sidelobe suppression, Particle swarm optimization (PSO), Gravitational search algorithm (GSA)},
abstract = {A conventional collaborative beamforming (CB) system suffers from high sidelobes due to the random positioning of the nodes. This paper introduces a hybrid metaheuristic optimization algorithm called the Particle Swarm Optimization and Gravitational Search Algorithm-Explore (PSOGSA-E) to suppress the peak sidelobe level (PSL) in CB, by the means of finding the best weight for each node. The proposed algorithm combines the local search ability of the gravitational search algorithm (GSA) with the social thinking skills of the legacy particle swarm optimization (PSO) and allows exploration to avoid premature convergence. The proposed algorithm also simplifies the cost of variable parameter tuning compared to the legacy optimization algorithms. Simulations show that the proposed PSOGSA-E outperforms the conventional, the legacy PSO, GSA and PSOGSA optimized collaborative beamformer by obtaining better results faster, producing up to 100% improvement in PSL reduction when the disk size is small.}
}
@article{KALBANDE2023138474,
title = {Machine learning based quantification of VOC contribution in surface ozone prediction},
journal = {Chemosphere},
volume = {326},
pages = {138474},
year = {2023},
issn = {0045-6535},
doi = {https://doi.org/10.1016/j.chemosphere.2023.138474},
url = {https://www.sciencedirect.com/science/article/pii/S0045653523007415},
author = {Ritesh Kalbande and Bipin Kumar and Sujit Maji and Ravi Yadav and Kaustubh Atey and Devendra Singh Rathore and Gufran Beig},
keywords = {Ozone, VOCs, Machine learning, Meteorology, Isoprene},
abstract = {The prediction of surface ozone is essential attributing to its impact on human and environmental health. Volatile organic compounds (VOCs) are crucial in driving ozone concentration; particularly in urban areas where VOC limited regimes are prominent. The limited measurements of VOCs, however, hinder assessing the VOC-ozone relationship. This work applies machine learning (ML) algorithms for temporal forecasting of surface ozone over a metropolitan city in India. The availability of continuous VOCs measurement data along with meteorology and other pollutants during 2014–2016 makes it possible to deduce the influence of various input parameters on surface ozone prediction. After evaluating the best ML model for ozone prediction, simulations were carried out using varied input combinations. The combination with isoprene, meteorology, NOx, and CO (Isop + MNC) was the best with RMSE 4.41 ppbv and MAPE 6.77%. A season-wise comparison of simulations having all data, only meteorological data and Isop + MNC as input showed that Isop + MNC simulation gives the best results during the summer season (RMSE: 5.86 ppbv, MAPE: 7.05%). This shows the increased ability of the model to capture ozone peaks (high ozone during summer) relatively better when isoprene data is used. The overall results highlight that using all available data doesn't necessarily give best prediction results; also critical thinking is essential when evaluating the model results.}
}
@article{GOERTZEL199595,
title = {Self-reference, computation, and mind},
journal = {Journal of Social and Evolutionary Systems},
volume = {18},
number = {1},
pages = {95-101},
year = {1995},
issn = {1061-7361},
doi = {https://doi.org/10.1016/1061-7361(95)90018-7},
url = {https://www.sciencedirect.com/science/article/pii/1061736195900187},
author = {Ben Goertzel and Harold Bowman}
}
@article{JOHNSTON2003325,
title = {Biological computation of image motion from flows over boundaries},
journal = {Journal of Physiology-Paris},
volume = {97},
number = {2},
pages = {325-334},
year = {2003},
note = {Neurogeometry and visual perception},
issn = {0928-4257},
doi = {https://doi.org/10.1016/j.jphysparis.2003.09.016},
url = {https://www.sciencedirect.com/science/article/pii/S0928425703000664},
author = {A. Johnston and P.W. McOwan and C.P. Benton},
keywords = {Optic flow, Cortex, Differential forms, Vision, Motion},
abstract = {A theory of early motion processing in the human and primate visual system is presented which is based on the idea that spatio-temporal retinal image data is represented in primary visual cortex by a truncated 3D Taylor expansion that we refer to as a jet vector. This representation allows all the concepts of differential geometry to be applied to the analysis of visual information processing. We show in particular how the generalised Stokes theorem can be used to move from the calculation of derivatives of image brightness at a point to the calculation of image brightness differences on the boundary of a volume in space–time and how this can be generalised to apply to integrals of products of derivatives. We also provide novel interpretations of the roles of direction selective, bi-directional and pan-directional cells and of type I and type II cells in V5/MT.}
}
@incollection{SALTZER20091,
title = {Chapter 1 - Systems},
editor = {Jerome H. Saltzer and M. Frans Kaashoek},
booktitle = {Principles of Computer System Design},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {1-42},
year = {2009},
isbn = {978-0-12-374957-4},
doi = {https://doi.org/10.1016/B978-0-12-374957-4.00010-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780123749574000104},
author = {Jerome H. Saltzer and M. Frans Kaashoek},
abstract = {Publisher Summary
This chapter introduces some of the vocabulary and concepts used in designing computer systems. It also introduces the “systems perspective,” a way of thinking about systems that is global and encompassing rather than focused on particular issues. The usual course of study of computer science and engineering begins with linguistic constructs for describing computations (software) and physical constructs for realizing computations (hardware). To develop applications that have these requirements, the designer must look beyond the software and hardware and view the computer system as a whole. In doing so, the designer encounters many new problems—so many that the limit on the scope of computer systems generally arises neither from laws of physics nor from theoretical impossibility, but rather from limitations of human understanding.}
}
@incollection{GISZTER2007323,
title = {Primitives, premotor drives, and pattern generation: a combined computational and neuroethological perspective},
editor = {Paul Cisek and Trevor Drew and John F. Kalaska},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {165},
pages = {323-346},
year = {2007},
booktitle = {Computational Neuroscience: Theoretical Insights into Brain Function},
issn = {0079-6123},
doi = {https://doi.org/10.1016/S0079-6123(06)65020-6},
url = {https://www.sciencedirect.com/science/article/pii/S0079612306650206},
author = {Simon Giszter and Vidyangi Patil and Corey Hart},
keywords = {primitives, motor synergies, force-fields, modularity, feedback, motor pattern analysis, decomposition, rhythm generation, pattern shaping},
abstract = {A modular motor organization may be needed to solve the degrees of freedom problem in biological motor control. Reflex elements, kinematic primitives, muscle synergies, force-field primitives and/or pattern generators all have experimental support as modular elements. We discuss the possible relations of force-field primitives, spinal feedback systems, and pattern generation and shaping systems in detail, and review methods for examining underlying motor pattern structure in intact or semi-intact behaving animals. The divisions of systems into primitives, synergies, and rhythmic elements or oscillators suggest specific functions and methods of construction of movement. We briefly discuss the limitations and caveats needed in these interpretations given current knowledge, together with some of the hypotheses arising from these frameworks.}
}
@article{LONGIN2022103280,
title = {Augmenting perception: How artificial intelligence transforms sensory substitution},
journal = {Consciousness and Cognition},
volume = {99},
pages = {103280},
year = {2022},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2022.103280},
url = {https://www.sciencedirect.com/science/article/pii/S1053810022000125},
author = {Louis Longin and Ophelia Deroy},
keywords = {Sensory substitution, Sensory extension, Intelligent sensory augmentation, Information quality, Senses, Artificial intelligence},
abstract = {What happens when artificial sensors are coupled with the human senses? Using technology to extend the senses is an old human dream, on which sensory substitution and other augmentation technologies have already delivered. Laser tactile canes, corneal implants and magnetic belts can correct or extend what individuals could otherwise perceive. Here we show why accommodating intelligent sensory augmentation devices not just improves but also changes the way of thinking and classifying former sensory augmentation devices. We review the benefits in terms of signal processing and show why non-linear transformation is more than a mere improvement compared to classical linear transformation.}
}
@article{ANDERSON2023102027,
title = {Nurse scholars of the Robert Wood Johnson Foundation Harold Amos Medical Faculty Development Program},
journal = {Nursing Outlook},
volume = {71},
number = {5},
pages = {102027},
year = {2023},
issn = {0029-6554},
doi = {https://doi.org/10.1016/j.outlook.2023.102027},
url = {https://www.sciencedirect.com/science/article/pii/S002965542300132X},
author = {Cindy M. Anderson and Nina Ardery and Daniel Pesut and Carmen Alvarez and Tamryn F. Gray and Karen M. Rose and Jasmine L. Travers and Janiece Taylor and Kathy D. Wright},
keywords = {Health professions diversity, Equity, Inclusive excellence, Robert Wood Johnson Foundation, Nurse faculty scholars, Harold Amos, Medical faculty development, Legacy leadership},
abstract = {Background
The challenge to increase the diversity, inclusivity, and equity of nurse scientists is a critical issue to enhance nursing knowledge development, health care, health equity, and health outcomes in the United States.
Purpose
The purpose of this paper is to highlight the current nurse scholars in the Robert Wood Johnson Foundation (RWJF) Harold Amos Medical Faculty Development Program (AMFDP).
Discussion
Profiles and the programs of research and scholarship of the current AMFDP nurse scholars are described and discussed. Scholars share lessons learned, and how the AMFDP program has influenced their thinking and commitments to future action in service of nursing science, diversity efforts, legacy leadership, issues of health equity.
Conclusion
RWJF has a history of supporting the development of nursing scholars. AMFDP is an example of legacy leadership program that contributes to a culture of health and the development of next-generation nursing science scholars.}
}
@article{SONI2024100016,
title = {Advancements in MXene-based electrocatalysts for hydrogen evolution reaction processes: A comprehensive review},
journal = {Journal of Alloys and Compounds Communications},
volume = {3},
pages = {100016},
year = {2024},
issn = {2950-2845},
doi = {https://doi.org/10.1016/j.jacomc.2024.100016},
url = {https://www.sciencedirect.com/science/article/pii/S295028452400016X},
author = {Kunjal Soni and Rakesh Kumar Ameta},
keywords = {MXenes, 2D materials, Two-electron transfer process, Hydrogen evolution process, Electrocatalysts},
abstract = {MXenes are a newly emerging family of two-dimensional (2D) materials that include carbonitrides, nitrides, and carbides of transition metals. They have attracted much interest from scientists and researchers due to their potential use in electrocatalysts, where a two-electron transfer process is applied. Their remarkable properties, such as strong chemical and structural stability, high electrical conductivity, and large active surface area, make them effective for their potential in advanced hydrogen evolution reactions (HER). This thorough analysis starts by carefully outlining the forward-thinking advances in MXene synthesis and development. It then explores the theoretical and empirical aspects of MXene-based HER electrocatalysts. This review paper presents methods for improving the HER catalytic activity of MXene, including terminal modification, metal-atom doping, and the creation of various nanostructures to increase the density of active sites. The study clarifies current issues and new opportunities and provides a valuable framework for the future development of effective MXene-based electrocatalysts for HERs.}
}
@article{OMORI19991157,
title = {Emergence of symbolic behavior from brain like memory with dynamic attention},
journal = {Neural Networks},
volume = {12},
number = {7},
pages = {1157-1172},
year = {1999},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(99)00054-4},
url = {https://www.sciencedirect.com/science/article/pii/S0893608099000544},
author = {T. Omori and A. Mochizuki and K. Mizutani and M. Nishizaki},
keywords = {Symbolic behavior, Associative memory, Attention, PATON, Inference, Hippocampus, Model, Computational theory},
abstract = {An important feature of human intelligence is the use of symbols. This is seen in our daily use of language and logical thinking. However, the use of symbols is not limited to humans. We observe planned action sequences in primate behavior and prediction-based action in higher mammals. For the representation and operation of symbols by the brain neural circuit, no specific construction principle or computational theory is known so far. In this paper, we regard the brain as a complex of associative memory and dynamic attentional system, and starting from two hypotheses on information representation and operation in the brain, we propose a model of primitive symbolic behavior emergence that is consistent with the conventional symbolic processing model. We also describe a computational theory of the symbolic processing model in associative memory. Through computer simulation studies on a language-like memory search and map learning by a moving robot, we discuss the validity of the model.}
}
@article{TALAMI2025115242,
title = {Evaluating the effectiveness, reliability and efficiency of a multi-objective sequential optimization approach for building performance design},
journal = {Energy and Buildings},
volume = {329},
pages = {115242},
year = {2025},
issn = {0378-7788},
doi = {https://doi.org/10.1016/j.enbuild.2024.115242},
url = {https://www.sciencedirect.com/science/article/pii/S0378778824013586},
author = {Riccardo Talami and Jonathan Wright and Bianca Howard},
keywords = {Building optimization, Building performance, Algorithm, Building simulation, Building design},
abstract = {The complexity of performance-based building design stems from the evaluation of numerous candidate design options, driven by the plethora of variables, objectives, and constraints inherent in multi-disciplinary projects. This necessitates optimization approaches to support the identification of well performing designs while reducing the computational time of performance evaluation. In response, this paper proposes and evaluates a sequential approach for multi-objective design optimization of building geometry, fabric, HVAC system and controls for building performance. This approach involves sequential optimizations with optimal solutions from previous stages passed to the next. The performance of the sequential approach is benchmarked against a full factorial search, assessing its effectiveness in finding global optima, solution quality, reliability to scale and variations of problem formulations, and computational efficiency compared to the NSGA-II algorithm. 24 configurations of the sequential approach are tested on a multi-scale case study, simulating 874 to 4,147,200 design options for an office building, aiming to minimize energy demand while maintaining thermal comfort. A two-stage sequential process-(building geometry + fabric) and (HVAC system + controls) identified the same Pareto-optimal solutions as the full factorial search across all four scales and variations of problem formulations, demonstrating 100 % effectiveness and reliability. This approach required 100,700 function evaluations, representing a 91.2 % reduction in computational effort compared to the full factorial search. In contrast, NSGA-II achieved only 73.5 % of the global optima with the same number of function evaluations. This research indicates that a sequential optimization approach is a highly efficient and robust alternative to the standard NSGA-II algorithm.}
}
@article{PUPUNWIWAT2011827,
title = {Conceptual Selective RFID Anti-Collision Technique Management},
journal = {Procedia Computer Science},
volume = {5},
pages = {827-834},
year = {2011},
note = {The 2nd International Conference on Ambient Systems, Networks and Technologies (ANT-2011) / The 8th International Conference on Mobile Web Information Systems (MobiWIS 2011)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2011.07.114},
url = {https://www.sciencedirect.com/science/article/pii/S187705091100439X},
author = {Prapassara Pupunwiwat and Peter Darcy and Bela Stantic},
keywords = {Radio Frequency Identification (RFID), Anti-Collision, Decision Tree, Six Thinking Hats},
abstract = {Radio Frequency Identification (RFID) uses wireless radio frequency technology to automatically identify tagged objects. Despite the extensive development of RFID technology, tag collisions still remains a major drawback. The collision issue can be solved by using anti-collision techniques. While existing research has focused on improving anti-collision methods alone, it is also essential that a suitable type of anti-collision algorithm is selected for the specific circumstance. In this work, we evaluate anti-collision techniques and perform a comparative analysis in order to find the advantages and disadvantages of each approach. To identify the best anti-collision selection method in various scenarios, we have proposed two strategies for selective anti-collision technique management: a “Novel Decision Tree Strategy” and a “Six Thinking Hats Strategy”. We have shown that the selection of the correct technique for specific scenarios improve the quality of the data collection which, in turn, will increase the integrity of the data after being transformed, aggregated, and used for event processing.}
}
@article{JUDD1997907,
title = {Computational economics and economic theory: Substitutes or complements?},
journal = {Journal of Economic Dynamics and Control},
volume = {21},
number = {6},
pages = {907-942},
year = {1997},
note = {Society of Computational Economics Conference},
issn = {0165-1889},
doi = {https://doi.org/10.1016/S0165-1889(97)00010-9},
url = {https://www.sciencedirect.com/science/article/pii/S0165188997000109},
author = {Kenneth L. Judd},
keywords = {Computational approach, Theoretical analysis},
abstract = {This essay examines the idea and potential of a ‘computational approach to theory’, discusses methodological issues raised by such computational methods, and outlines the problems associated with the dissemination of computational methods and the exposition of computational results. We argue that the study of a theory need not be confined to proving theorems, that current and future computer technologies create new possibilities for theoretical analysis, and that by resolving these issues we will create an intellectual atmosphere in which computational methods can make substantial contributions to economic analysis.}
}
@article{ZHAO2024109027,
title = {Towards the definition of spatial granules},
journal = {Fuzzy Sets and Systems},
volume = {490},
pages = {109027},
year = {2024},
issn = {0165-0114},
doi = {https://doi.org/10.1016/j.fss.2024.109027},
url = {https://www.sciencedirect.com/science/article/pii/S0165011424001738},
author = {Liquan Zhao and Yiyu Yao},
keywords = {Granularity, Fineness, Subsethood, Coarse-fine relation, Quotient join, Quotient meet, Granular space, Spatial granular computing},
abstract = {Three basic issues of granular computing are construction or definition of granules, measures of granules, and computation or reasoning with granules. This paper reviews the main theories of granular computing and introduces the definition of spatial granules. A granule is composed of one or more atomic granules. The rationality of this definition is explained from the four aspects: simplicity, applicability, measurability and visualization. A one-to-one correspondence is established between the granules and the points in the unit hypercube, and the coarsening and refining of the granules are the descending and ascending dimensions of the points, respectively. The weak fuzzy tolerance relation and weak fuzzy equivalence relation are defined so as to study on all fuzzy binary relations. The notion of layer granularity/fineness is introduced and each granule can be easily denoted by two numbers, which can be used to pre-process macro knowledge space and greatly improve the search speed. This paper also discusses the main properties of granules including the necessary and sufficient conditions of coarse-fine relation and the main principles of granular space.}
}
@article{AMEL2023104187,
title = {Toward an automatic detection of cardiac structures in short and long axis views},
journal = {Biomedical Signal Processing and Control},
volume = {79},
pages = {104187},
year = {2023},
issn = {1746-8094},
doi = {https://doi.org/10.1016/j.bspc.2022.104187},
url = {https://www.sciencedirect.com/science/article/pii/S1746809422006413},
author = {Laidi Amel and Mohammed Ammar and Mostafa {El Habib Daho} and Said Mahmoudi},
keywords = {Cardiac MRI Segmentation, Shape Descriptors, Particle Swarm Optimization, Residual Network, Interpretability},
abstract = {Objective
This work aims to create an automatic detection process of cardiac structures in both short-axis and long-axis views. A workflow inspired by human thinking process, for better explainability.
Methods
we began by separating the images into two classes: long axis and short axis, using a Residual Network model. Then, we used Particle Swarm Optimization for general segmentation. After segmentation, a characterization step based on shape descriptors calculated from bounding box and ANOVA for features selection were applied on the binary images to detect the location of each region of interest: lung, left and right ventricle in the short-axis view, the aorta, the left heart (left atrium and ventricle), and the right heart (right atrium and ventricle) in the long axis view.
Results
we achieved a 90% accuracy on view separation. We have selected: Elongation, Compactness, Circularity, Type Factor, for short axis identification; and:Area, Centre of Mass Y, Moment of Inertia XY, Moment of Inertia YY, for long axis identification.
Conclusion
a successful separation of long axis and short axis views allows for a better characterization and detection of segmented cardiac structures. After that, any method can be applied for segmentation, attribute selection, and classification.
Significance
an attempt to introduce explainability into cardiac image segmentation, we tried to mimic the human workflow while computerizing each step. The process seems to be valid and added clarity and interpretability to the detection.}
}
@article{BEAR2020104057,
title = {What comes to mind?},
journal = {Cognition},
volume = {194},
pages = {104057},
year = {2020},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2019.104057},
url = {https://www.sciencedirect.com/science/article/pii/S0010027719302306},
author = {Adam Bear and Samantha Bensinger and Julian Jara-Ettinger and Joshua Knobe and Fiery Cushman},
keywords = {Sampling, Decision-making, Consciousness, Computation},
abstract = {When solving problems, like making predictions or choices, people often “sample” possibilities into mind. Here, we consider whether there is structure to the kinds of thoughts people sample by default—that is, without an explicit goal. Across three experiments we found that what comes to mind by default are samples from a probability distribution that combines what people think is likely and what they think is good. Experiment 1 found that the first quantities that come to mind for everyday behaviors and events are quantities that combine what is average and ideal. Experiment 2 found, in a manipulated context, that the distribution of numbers that come to mind resemble the mathematical product of the presented statistical distribution and a (softmax-transformed) prescriptive distribution. Experiment 3 replicated these findings in a visual domain. These results provide insight into the process generating people’s conscious thoughts and invite new questions about the value of thinking about things that are both likely and good.}
}
@article{FILIPPOU2016892,
title = {Modelling the impact of study behaviours on academic performance to inform the design of a persuasive system},
journal = {Information & Management},
volume = {53},
number = {7},
pages = {892-903},
year = {2016},
note = {Special Issue on Papers Presented at Pacis 2015},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2016.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0378720616300507},
author = {Justin Filippou and Christopher Cheong and France Cheong},
keywords = {Study behaviour, Persuasive systems, Linear modelling, Higher education},
abstract = {Information technology is deeply ingrained in most aspects of everyday life and can be designed to influence users to behave in a certain way. Influencing students to improve their study behaviour would be a useful application of this technology. As a preamble to the design of a persuasive system for learning, we collected data to identify the study behaviours of students and recent alumni. We then developed two models to measure which behaviours have the most significant impact on learning performance. Current students reported more foundational behaviours whereas alumni demonstrated more higher-order thinking traits.}
}
@article{FAELENS2021106510,
title = {Social media use and well-being: A prospective experience-sampling study},
journal = {Computers in Human Behavior},
volume = {114},
pages = {106510},
year = {2021},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2020.106510},
url = {https://www.sciencedirect.com/science/article/pii/S0747563220302624},
author = {Lien Faelens and Kristof Hoorelbeke and Bart Soenens and Kyle {Van Gaeveren} and Lieven {De Marez} and Rudi {De Raedt} and Ernst H.W. Koster},
keywords = {Social media, Social comparison, Self-esteem, Repetitive negative thinking, Negative affect},
abstract = {Facebook and Instagram are currently the most popular Social Network Sites (SNS) for young adults. A large amount of research examined the relationship between these SNS and well-being, and possible intermediate constructs such as social comparison, self-esteem, and repetitive negative thinking (RNT). However, most of these studies have cross-sectional designs and use self-report indicators of SNS use. Therefore, their conclusions should be interpreted cautiously. Consequently, the goal of the current experience sampling study was to examine the temporal dynamics between objective indicators of SNS use, and self-reports of social comparison, RNT, and daily fluctuations in negative affect. More specifically, we assessed 98 participants 6 times per day during 14 days to examine reciprocal relationships between SNS use, negative affect, emotion regulation, and key psychological constructs. Results indicate that (1) both Facebook and Instagram use predicted reduced well-being, and (2) self-esteem and RNT appear to be important intermediate constructs in these relationships. Future longitudinal and experimental studies are needed to further support and extend the current research findings.}
}
@article{VERNON2019122,
title = {Internal simulation in embodied cognitive systems: Comment on “Muscleless motor synergies and actions without movements: From motor neuroscience to cognitive robotics” by Vishwanathan Mohan et al.},
journal = {Physics of Life Reviews},
volume = {30},
pages = {122-125},
year = {2019},
issn = {1571-0645},
doi = {https://doi.org/10.1016/j.plrev.2019.02.012},
url = {https://www.sciencedirect.com/science/article/pii/S1571064519300429},
author = {David Vernon},
keywords = {Internal simulation, Embodied cognition, Cognitive robotics, Episodic future thinking}
}
@article{EDELMAN1997296,
title = {Computational theories of object recognition},
journal = {Trends in Cognitive Sciences},
volume = {1},
number = {8},
pages = {296-304},
year = {1997},
issn = {1364-6613},
doi = {https://doi.org/10.1016/S1364-6613(97)01090-5},
url = {https://www.sciencedirect.com/science/article/pii/S1364661397010905},
author = {S. Edelman},
abstract = {This paper examines four current theoretical approaches to the representation and recognition of visual objects: structural descriptions, geometric constraints, multidimensional feature spaces and shape-space approximation. The strengths and weaknesses of the four theories are considered, with a special focus on their approach to categorization — a computationally challenging task which is not widely addressed in computer vision, where the stress is rather on the generalization of recognition across changes of viewpoint.}
}
@article{JIANG2024109208,
title = {Surrogate-based Shape Optimization Design for the Stable Descent of Mars Parachutes},
journal = {Aerospace Science and Technology},
volume = {150},
pages = {109208},
year = {2024},
issn = {1270-9638},
doi = {https://doi.org/10.1016/j.ast.2024.109208},
url = {https://www.sciencedirect.com/science/article/pii/S1270963824003390},
author = {Lulu Jiang and Guanhua Chen and Xiaopeng Xue and Xin Pan and Gang Chen},
keywords = {Supersonic Parachute, Mars atmosphere, Aerodynamic Optimization, Shape design, Wide Speed Range},
abstract = {The crucial role that the supersonic parachute plays in space exploration missions has been widely recognized, as it directly impacts the safe landing of probes. However, parachute models with optimization on different aerodynamic performances often involve design conflicts with each other. Additionally, the parachute design focusing on a single point cannot fully adapt to different speed ranges during stable descent. This complexity makes it challenging to use traditional shape design methods, which rely on empirical knowledge, to address these coupled design issues. Faced with the design challenges of Mars parachutes, this study, inspired by aircraft aerodynamic optimization principles, establishes a shape design method specifically for the stable descent phase of Mars parachutes. The method combines numerical simulation and surrogate-based optimization strategies, aiming to enhance the overall performance during stable descent and meet various demands of different exploration missions. Meanwhile, by providing a rapid estimate of the shape during the design phase, the method significantly improves computational efficiency. The optimal models effectively balance comprehensive performance in the supersonic-transonic-subsonic speed domain by conducting shape optimization research on the disk-gap-band parachute using the surrogate-based optimization strategy. Also, it exhibits better deceleration and stability across the entire speed range compared to the base model, even when deviating from the design Mach number. Importantly, the advantages of canopy-only optimization for drag performance extend to the capsule-canopy two-body system, enhancing the drag performance of the canopy in the two-body system. This strategic approach reduces the transient calculation time for the two-body system, further improving computational efficiency. The method provides a practical and forward-thinking solution for the design of Mars parachutes.}
}
@article{ZHANG2025111031,
title = {Constrained multi-scale dense connections for biomedical image segmentation},
journal = {Pattern Recognition},
volume = {158},
pages = {111031},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111031},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324007829},
author = {Jiawei Zhang and Yanchun Zhang and Hailong Qiu and Tianchen Wang and Xiaomeng Li and Shanfeng Zhu and Meiping Huang and Jian Zhuang and Yiyu Shi and Xiaowei Xu},
keywords = {Multi-scale dense connections, Image segmentation, Network architecture search, Feature fusion},
abstract = {Multi-scale dense connection has been widely used in the biomedical image community to enhance the segmentation performance. In this way, features from all or most scales are aggregated or iteratively fused. However, by analyzing the details, we discover that some connections involving distant scales may not contribute to, or even harm, the performance, while they always introduce a noticeable increase in computational cost. In this paper, we propose constrained multi-scale dense connections (CMDC) for biomedical image segmentation. In contrast to current general lightweight approaches, we first introduce two methods, a naive method and a network architecture search (NAS)-based method, to remove redundant connections and verify the optimal connection configuration, thereby improving overall efficiency and accuracy. The results demonstrate that the two approaches obtain a similar optimal configuration in which most features at the adjacent scales are connected. Then, we applied the optimal configuration to various backbone networks to build constrained multi-scale dense networks (CMD-Net). Experimental results evaluated on eight image segmentation datasets covering biomedical images and natural images demonstrate the effectiveness of CMD-Net across a variety of backbone networks (FCN, U-Net, DeepLabV3, SegNet, FCNsa, ConvUNeXt) with a much lower increase in computational cost. Furthermore, CMD-Net achieves state-of-the-art performance on four publicly available datasets. We believe that the CMDC method can offer valuable insight for ways to engage in dense connectivity at multiple scales within communities. The source code has been made available at https://github.com/JerRuy/CMD-Net.}
}
@article{NUNEZV2024101173,
title = {Recommendation system using bio-inspired algorithms for urban orchards},
journal = {Internet of Things},
volume = {26},
pages = {101173},
year = {2024},
issn = {2542-6605},
doi = {https://doi.org/10.1016/j.iot.2024.101173},
url = {https://www.sciencedirect.com/science/article/pii/S2542660524001148},
author = {Juan M. {Núñez V.} and Juan M. Corchado and Diana M. Giraldo and Sara Rodríguez-González and Fernando {De la Prieta}},
keywords = {Internet of Things, Bio-inspired algorithms, Urban orchards, Lettuce crops, Social design},
abstract = {According to the Food and Agriculture Organization of the United Nations (FAO), climate change is exponentially affecting agricultural production worldwide, with food prices expected to increase by up to 90 percent by 2030 and hunger and malnutrition rates to rise by 2050. This paper presents the development of a platform based on the Internet of Things (IoT) for monitoring urban gardens as a strategy to mitigate hunger, promote food sovereignty and circular economy in areas of food shortage. To this end, an Internet of Things (IoT) architecture is proposed and implemented that involves a social design layer that allows an effective transfer of knowledge to communities and a recommendation system based on evolutionary computation to optimize and maximize the productivity of urban orchards, and thus contribute to the 2030 agenda of the Sustainable Development Goals (SDGs). Finally, three experiments in urban gardens are shown to validate evolutionary computation and artificial intelligence models, such as multiple linear regression, genetic algorithms, ant colony algorithms and spatial estimation and inference algorithms such as the Kriging algorithm. The productivity of urban lettuce orchards is increased between 25 and 45%.}
}
@incollection{KISS2019109,
title = {Process Systems Engineering from an industrial and academic perspective},
editor = {Anton A. Kiss and Edwin Zondervan and Richard Lakerveld and Leyla Özkan},
series = {Computer Aided Chemical Engineering},
publisher = {Elsevier},
volume = {46},
pages = {109-114},
year = {2019},
booktitle = {29th European Symposium on Computer Aided Process Engineering},
issn = {1570-7946},
doi = {https://doi.org/10.1016/B978-0-12-818634-3.50019-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186343500199},
author = {Anton A. Kiss and Johan Grievink},
keywords = {PSE, industry, education, research, interface, perspectives},
abstract = {Process Systems Engineering (PSE) deals with decision-making, at all levels and scales, by understanding complex process systems using a holistic view. Computer Aided Process Engineering (CAPE) is a complementary field that focuses on developing methods and providing solution through systematic computer aided techniques for problems related to the design, control and operation of chemical systems. The ‘PSE’ term suffers from a branding issue to the point that PSE does not get the recognition it deserves. This work aims to provide an informative industrial and academic perspective on PSE, arguing that the ‘systems thinking’ and ‘systems problem solving’ have to be prioritized ahead of just applications of computational problem solving methods. A multi-level view of the PSE field is provided within the academic and industrial context, and enhancements for PSE are suggested at their industrial and academic interfaces.}
}
@article{DIACOPOULOS2020103911,
title = {A systematic review of mobile learning in social studies},
journal = {Computers & Education},
volume = {154},
pages = {103911},
year = {2020},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2020.103911},
url = {https://www.sciencedirect.com/science/article/pii/S036013152030110X},
author = {Mark Michael Diacopoulos and Helen Crompton}
}
@article{KAZANINA2023996,
title = {The neural ingredients for a language of thought are available},
journal = {Trends in Cognitive Sciences},
volume = {27},
number = {11},
pages = {996-1007},
year = {2023},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2023.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S1364661323001936},
author = {Nina Kazanina and David Poeppel},
keywords = {language-of-thought, symbolic representation, computational theory of mind, spatial navigation, compositionality},
abstract = {The classical notion of a ‘language of thought’ (LoT), advanced prominently by the philosopher Jerry Fodor, is an influential position in cognitive science whereby the mental representations underpinning thought are considered to be compositional and productive, enabling the construction of new complex thoughts from more primitive symbolic concepts. LoT theory has been challenged because a neural implementation has been deemed implausible. We disagree. Examples of critical computational ingredients needed for a neural implementation of a LoT have in fact been demonstrated, in particular in the hippocampal spatial navigation system of rodents. Here, we show that cell types found in spatial navigation (border cells, object cells, head-direction cells, etc.) provide key types of representation and computation required for the LoT, underscoring its neurobiological viability.}
}
@incollection{KIHLSTROM2018,
title = {Cognitive Psychology: Overview☆},
booktitle = {Reference Module in Neuroscience and Biobehavioral Psychology},
publisher = {Elsevier},
year = {2018},
isbn = {978-0-12-809324-5},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.21702-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093245217021},
author = {John F. Kihlstrom and Lillian Park},
keywords = {Cognition, Sensation, Perception, Attention, Memory, Categorization, Learning, Language, Reasoning, Judgment, Decision-making, Choice, Cognitive development, Cognitive neuroscience, Cognitive sociology},
abstract = {Cognitive psychology seeks to understand how we acquire knowledge about ourselves and the world, how this knowledge is represented in the mind and brain, and how we use knowledge to guide behavior. Major topics in cognitive psychology include sensation and perception, attention, memory, categorization, learning, language and communication, and thinking, reasoning, judgment, and decision-making. Cognitive development is discussed from both an ontogenetic and phylogenetic point of view. Cognitive neuroscience explores the neural substrates of cognitive processes. The cognitive point of view has been extended to personality, social, and clinical psychology, as well as to sociology, anthropology, and other social-science disciplines.}
}
@article{RYLE1953189,
title = {Thinking},
journal = {Acta Psychologica},
volume = {9},
pages = {189-196},
year = {1953},
issn = {0001-6918},
doi = {https://doi.org/10.1016/0001-6918(53)90012-2},
url = {https://www.sciencedirect.com/science/article/pii/0001691853900122},
author = {Gilbert Ryle}
}
@incollection{MAIDA201639,
title = {Chapter 2 - Cognitive Computing and Neural Networks: Reverse Engineering the Brain},
editor = {Venkat N. Gudivada and Vijay V. Raghavan and Venu Govindaraju and C.R. Rao},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {35},
pages = {39-78},
year = {2016},
booktitle = {Cognitive Computing: Theory and Applications},
issn = {0169-7161},
doi = {https://doi.org/10.1016/bs.host.2016.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0169716116300529},
author = {A.S. Maida},
keywords = {Brain simulation, Deep belief networks, Convolutional networks, Liquid computing, Biological neural networks, Neocortex},
abstract = {Cognitive computing seeks to build applications which model and mimic human thinking. One approach toward achieving this goal is to develop brain-inspired computational models. A prime example of such a model is the class of deep convolutional networks which is currently used in pattern recognition, machine vision, and machine learning. We offer a brief review of the mammalian neocortex, the minicolumn, and the ventral pathway. We provide descriptions of abstract neural circuits that have been used to model these areas of the brain. This include Poisson spiking networks, liquid computing networks, spiking models of feature discovery in the ventral pathway, spike-timing-dependent plasticity learning, restricted Boltzmann machines, deep belief networks, and deep convolutional networks. In summary, this chapter explores abstractions of neural networks found within the mammalian neocortex that support cognition and the beginnings of cognitive computation.}
}
@article{GROSBERG2009359,
title = {Computational models of heart pumping efficiencies based on contraction waves in spiral elastic bands},
journal = {Journal of Theoretical Biology},
volume = {257},
number = {3},
pages = {359-370},
year = {2009},
issn = {0022-5193},
doi = {https://doi.org/10.1016/j.jtbi.2008.11.022},
url = {https://www.sciencedirect.com/science/article/pii/S0022519308006103},
author = {Anna Grosberg and Morteza Gharib},
keywords = {Cardiac modeling, Left ventricular twist, Myocardium macro-structure, Finite element simulations},
abstract = {We present a framework for modeling biological pumping organs based on coupled spiral elastic band geometries and active wave-propagating excitation mechanisms. Two pumping mechanisms are considered in detail by way of example: one of a simple tube, which represents a embryonic fish heart and another more complicated structure with the potential to model the adult human heart. Through finite element modeling different elastic contractions are induced in the band. For each version the pumping efficiency is measured and the dynamics are evaluated. We show that by combining helical shapes of muscle bands with a contraction wave it is possible not only to achieve efficient pumping, but also to create desired dynamics of the structure. As a result we match the function of the model pumps and their dynamics to physiological observations.}
}
@incollection{ASHBY2024255,
title = {Chapter 10 - Circular Materials Economics},
editor = {Michael F. Ashby},
booktitle = {Materials and Sustainable Development (Second Edition)},
publisher = {Butterworth-Heinemann},
edition = {Second Edition},
pages = {255-295},
year = {2024},
isbn = {978-0-323-98361-7},
doi = {https://doi.org/10.1016/B978-0-323-98361-7.00010-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323983617000105},
author = {Michael F. Ashby},
keywords = {Circularity, Material efficiency, Linear materials economy, Circular materials economy, Reuse, Repair, Recycling, Take-back legislation, Recycling targets, Increased product life, Urban mining, Business models, Measuring circularity, Modelling circularity, Limits to circularity},
abstract = {When products come to the end of their lives, the materials they contain are still there. Repurposing, repair or recycling can return them to active use, creating a technological cycle that, in some ways, parallels the carbon, nitrogen and hydrological cycles of the biosphere. In developed nations they lost urgency as the cost of materials fell and that of labour rose, making it cheaper to make new products than to fix old ones, leading to a materials economy that is largely linear, characterized by the sequence “take – make – use – dispose”. Increasing population and affluence, and the limited capacity for the planet to provide resources and absorb waste direct thinking towards a more circular way of using materials. Governments have sought to reduce waste by imposing take-back regulations, setting mandatory recycling targets and requiring minimum service lives. These, and the efficiency movement – eco-efficiency, material-efficiency, energy efficiency – seek to allow business as usual with reduced drain on natural resources without any real change of behaviour. The ‘circularity’ concept is a way of thinking that looks not just for efficiencies but also for new ways to provide functions. The idea of deploying rather than consuming materials, of using them not once but many times has economic as well as environmental appeal. This chapter examines the background, the successes, the difficulties, and the ultimate limits of implementing a circular materials economy.}
}
@incollection{KOZA2002275,
title = {Chapter 10 - Genetic Programming: Biologically Inspired Computation That Exhibits Creativity in Producing Human-Competitive Results},
editor = {Peter J Bentley and David W. Corne},
booktitle = {Creative Evolutionary Systems},
publisher = {Morgan Kaufmann},
address = {San Francisco},
pages = {275-298},
year = {2002},
series = {The Morgan Kaufmann Series in Artificial Intelligence},
isbn = {978-1-55860-673-9},
doi = {https://doi.org/10.1016/B978-155860673-9/50048-3},
url = {https://www.sciencedirect.com/science/article/pii/B9781558606739500483},
author = {John R. Koza and Forrest H. Bennett and David Andre and Martin A. Keane},
abstract = {Publisher Summary
One of the central challenges of computer science is to get a computer to solve a problem without programming it explicitly. The challenge is to create an automatic system whose input is a high-level statement of a problem's requirements and whose output is a satisfactory solution to the given problem. This challenge is the common goal of such fields of research as artificial intelligence and machine learning. Paraphrasing Arthur Samuel, this challenge addresses the question: How can computers are made to do what needs to be done, without being told exactly how to do it? As Samuel further explained: “The aim is to get machines to exhibit behavior, which if done by humans, would be assumed to involve the use of intelligence.” This chapter provides an affirmative answer to the following two questions: Starting only with a high-level statement of the problem's requirements, can computers automatically discover the solution to nontrivial problems? And, can automatically created solutions be competitive with the products of human creativity and inventiveness? In answering these questions, this chapter focuses on a biologically inspired domain-independent problem-solving technique of evolutionary computation, called genetic programming. For each problem, genetic programming automatically creates entities that improve on previously patented inventions, or duplicate the functionality of previously patented inventions or duplicate the functionality of previously patented inventions. The chapter also discusses the importance of illogic in achieving creativity and inventiveness.}
}
@article{STATHOPOULOS20031565,
title = {Wind loads on low buildings: in the wake of Alan Davenport's contributions},
journal = {Journal of Wind Engineering and Industrial Aerodynamics},
volume = {91},
number = {12},
pages = {1565-1585},
year = {2003},
note = {ENGINEERING SYMPOSIUM To Honour ALAN G. DAVENPORT for his 40 Years of Contributions},
issn = {0167-6105},
doi = {https://doi.org/10.1016/j.jweia.2003.09.019},
url = {https://www.sciencedirect.com/science/article/pii/S0167610503001302},
author = {Ted Stathopoulos},
keywords = {Building, Code, Computational wind engineering, Design, Load, Pressure, Standard, Time series, Wind},
abstract = {The paper reviews the evolution of knowledge and its current state regarding the evaluation of wind loads on low buildings by placing particular emphasis on Alan Davenport's contributions. These contributions have paved the way to the current state-of-the-art and have influenced the thinking of not only Alan's closest collaborators but also of other researchers in this area around the world. The paper will provide a brief historical perspective, followed by some detailed description of the University of Western Ontario's research on wind loads on low buildings carried out in the 1970s. Visualizing the wake of Davenport's contributions in this area, the paper will refer to the influence of this knowledge in the formulation of design load provisions in contemporary wind standards and codes of practice. The paper will also discuss the status of computational wind engineering as well as the so-called computer-aided wind engineering in the evaluation of wind pressures on low buildings.}
}
@article{ALVARADO20043,
title = {Autonomous agents and computational intelligence: the future of AI application for petroleum industry},
journal = {Expert Systems with Applications},
volume = {26},
number = {1},
pages = {3-8},
year = {2004},
note = {Intelligent Computing in the Petroleum Industry, ICPI-02},
issn = {0957-4174},
doi = {https://doi.org/10.1016/S0957-4174(03)00103-9},
url = {https://www.sciencedirect.com/science/article/pii/S0957417403001039},
author = {Matı&#x0301;as Alvarado and Leonid Cheremetov and Francisco Cantú}
}
@article{SARTON195551,
title = {The astral religion of antiquity and the “thinking machines” of to-day},
journal = {Vistas in Astronomy},
volume = {1},
pages = {51-60},
year = {1955},
issn = {0083-6656},
doi = {https://doi.org/10.1016/0083-6656(55)90012-X},
url = {https://www.sciencedirect.com/science/article/pii/008366565590012X},
author = {George Sarton}
}
@article{GOLDBERG20007,
title = {The Design of Innovation: Lessons from Genetic Algorithms, Lessons for the Real World},
journal = {Technological Forecasting and Social Change},
volume = {64},
number = {1},
pages = {7-12},
year = {2000},
issn = {0040-1625},
doi = {https://doi.org/10.1016/S0040-1625(99)00079-7},
url = {https://www.sciencedirect.com/science/article/pii/S0040162599000797},
author = {David E Goldberg},
abstract = {This article considers some of the connections between genetic algorithms (GAs)—search procedures based on the mechanics of natural selection and natural genetics—and human innovation. Simply stated, innovation has been a source of inspiration for thinking about genetic algorithms, and as the algorithms have improved, GAs have become increasingly interesting computational models of the processes of innovation. The article reviews the basics of genetic algorithm operation and connects the basic mechanics to two processes of innovation: continual improvement and discontinuous change. Thereafter, some of the technical lessons of genetic algorithm processing are reviewed and their implications are briefly explored in the context of organizational change.}
}
@article{JU2022101062,
title = {Proposal for a STEAM education program for creativity exploring the roofline of a hanok using GeoGebra and 4Dframe},
journal = {Thinking Skills and Creativity},
volume = {45},
pages = {101062},
year = {2022},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2022.101062},
url = {https://www.sciencedirect.com/science/article/pii/S1871187122000657},
author = {Hyunshik Ju and Hogul Park and Eun Young Jung and Seoung-Hey Paik},
keywords = {STEAM education program, Creativity, Catenary curve, Korean traditional architecture, Modelling, GeoGebra, 4Dframe},
abstract = {This research was conducted to confirm the feasibility of a STEAM education program in which the mathematics, physics, and Korean traditional arts underlying the hanok roofline are investigated using educational tools of GeoGebra and 4Dframe. This paper contends that this program has the potential to engage students in knowledge restructuring regarding the perception of the hanok’s architectural beauty, Newton's concept of gravity, and mathematical functions. The Octagonal Pavilion in Tapgol Park in Seoul, South Korea, a representative hanok, was used as an educational resource. GeoGebra is used to determine that the roofline of the Octagonal Pavilion generally follows the formula of the catenary curve and then the roofline is modelled using 4Dframe. The catenary form of the roofline of the hanok is linked to the Korean sense of beauty in the pursuit of naturalness under the influence of gravity and organically harmonizes with the environment. The class described in this study, in which the curve of the roofline of the Octagonal Pavilion is explored using GeoGebra and 4Dframe, can help develop creative and critical thinking in students in the context of STEAM education. The findings of this study have the potential to expand the scope of STEAM education to include content for creative education.}
}
@article{GIRARD2005215,
title = {From brainstem to cortex: Computational models of saccade generation circuitry},
journal = {Progress in Neurobiology},
volume = {77},
number = {4},
pages = {215-251},
year = {2005},
issn = {0301-0082},
doi = {https://doi.org/10.1016/j.pneurobio.2005.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S030100820500153X},
author = {B. Girard and A. Berthoz},
keywords = {Saccade generation circuitry, Computational models, Brainstem, Superior colliculus, Cerebellum, Basal ganglia, Cortex},
abstract = {The brain circuitry of saccadic eye movements, from brainstem to cortex, has been extensively studied during the last 30 years. The wealth of data gathered allowed the conception of numerous computational models. These models proposed descriptions of the putative mechanisms generating this data, and, in turn, made predictions and helped to plan new experiments. In this article, we review the computational models of the five main brain regions involved in saccade generation: reticular formation saccadic burst generators, superior colliculus, cerebellum, basal ganglia and premotor cortical areas. We present the various topics these models are concerned with: location of the feedback loop, multimodal saccades, long-term adaptation, on the fly trajectory correction, strategy and metrics selection, short-term spatial memory, transformations between retinocentric and craniocentric reference frames, sequence learning, to name the principle ones. Our objective is to provide a global view of the whole system. Indeed, narrowing too much the modelled areas while trying to explain too much data is a recurrent problem that should be avoided. Moreover, beyond the multiple research topics remaining to be solved locally, questions regarding the operation of the whole structure can now be addressed by building on the existing models.}
}
@article{LEBARON2000679,
title = {Agent-based computational finance: Suggested readings and early research},
journal = {Journal of Economic Dynamics and Control},
volume = {24},
number = {5},
pages = {679-702},
year = {2000},
issn = {0165-1889},
doi = {https://doi.org/10.1016/S0165-1889(99)00022-6},
url = {https://www.sciencedirect.com/science/article/pii/S0165188999000226},
author = {Blake LeBaron},
keywords = {Agents, Heterogeneous information, Simulated markets, Learning, Evolution},
abstract = {The use of computer simulated markets with individual adaptive agents in finance is a new, but growing field. This paper explores some of the early works in the area concentrating on a set of some of the earliest papers. Six papers are summarized in detail, along with references to many other pieces of this wide ranging research area. It also covers many of the questions that new researchers will face when getting into the field, and hopefully can serve as a kind of minitutorial for those interested in getting started.}
}
@article{ALY2014206,
title = {Atmospheric boundary-layer simulation for the built environment: Past, present and future},
journal = {Building and Environment},
volume = {75},
pages = {206-221},
year = {2014},
issn = {0360-1323},
doi = {https://doi.org/10.1016/j.buildenv.2014.02.004},
url = {https://www.sciencedirect.com/science/article/pii/S0360132314000444},
author = {Aly Mousaad Aly},
keywords = {Aerodynamics, Aeroelasticity, Atmospheric boundary-layer, Built environment, Experimental/computational wind engineering},
abstract = {This paper summarizes the state-of-the-art techniques used to simulate hurricane winds in atmospheric boundary-layer (ABL) for wind engineering testing. The wind tunnel simulation concept is presented along with its potential applications, advantages and challenges. ABL simulation at open-jet simulators is presented along with an application example followed by a discussion on the advantages and challenges of testing at these facilities. Some of the challenges and advantages of using computational fluid dynamics (CFD) are presented with an application example. The paper show that the way the wind can be simulated is complex and matching one parameter at full-scale may lead to a mismatch of other parameters. For instance, while large-scale testing is expected to improve Reynolds number and hence approach the full-scale scenario, it is challenging to generate large-scale turbulence in an artificially created wind. New testing protocols for low-rise structures and small-size architectural features are presented as an answer to challenging questions associated with both wind tunnel and open-jet testing. Results show that it is the testing protocol that can be adapted to enhance the prediction of full-scale physics in nature. Thinking out of the box and accepting non-traditional ABL is necessary to compensate for Reynolds effects and to allow for convenient experimentation. New research directions with focus on wind, rain and waves as well as other types of non-synoptic winds are needed, in addition to a more focus on the flow physics in the lower part of the ABL, where the major part of the infrastructure exists.}
}
@article{KNAPP2017370,
title = {Energy-efficient Legionella control that mimics nature and an open-source computational model to aid system design},
journal = {Applied Thermal Engineering},
volume = {127},
pages = {370-377},
year = {2017},
issn = {1359-4311},
doi = {https://doi.org/10.1016/j.applthermaleng.2017.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S135943111633664X},
author = {Samuel Knapp and Bo Nordell},
keywords = {Thermal model, Heat exchanger, Pasteurization, Legionnaire’s disease, Microsoft Excel},
abstract = {Although there is no direct connection, the incidence of Legionnaire’s disease has increased concurrently with increased usage of energy efficient domestic hot water (DHW) systems, which serve as ideal growth environments for Legionella pneumophila, the bacteria responsible for Legionnaire’s disease. The Duck Foot Heat Exchange Model (DFHXM) was developed to aid design of energy efficient thermal pasteurization systems with Legionella control specifically in mind. The model simulates a system design imitating the countercurrent heat exchange in the feet of ducks, an evolutionary adaption reducing environmental heat losses in cold climates. Such systems use a heat exchanger to preheat fluids prior to pasteurization and cool the same fluid after pasteurization. Thus, the design requires minimal addition of heat to achieve pasteurization temperatures and to cover environmental heat losses. This article describes the underlying principles and use of the freely available Microsoft Excel model, as well as compares results from the DFHXM to measurements of an experimental pilot system. Simulation outputs agreed well with experimental results for transient and steady-state temperatures, the largest discrepancy in steady-state temperatures being 4.6%. Lastly, we discuss the flexibility of the DFHXM to simulate a wide variety of designs with special emphasis on Legionella control and solar-thermal water disinfection.}
}
@article{WEICHBROTH20223798,
title = {A note on the affective computing systems and machines: a classification and appraisal},
journal = {Procedia Computer Science},
volume = {207},
pages = {3798-3807},
year = {2022},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 26th International Conference KES2022},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.09.441},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922013345},
author = {Paweł Weichbroth and Wiktor Sroka},
keywords = {Affective Computing, Artificial Emotional Intelligence, Classification, System, Machine},
abstract = {Affective computing (AfC) is a continuously growing multidisciplinary field, spanning areas from artificial intelligence, throughout engineering, psychology, education, cognitive science, to sociology. Therefore, many studies have been devoted to the aim of addressing numerous issues, regarding different facets of AfC solutions. However, there is a lack of classification of the AfC systems. This study aims to fill this gap by reviewing and evaluating the state-of-the-art studies in a qualitative manner. In this line of thinking, we put forward a threefold classification that breaks down to desktop and mobile AfC systems, and AfC machines. Moreover, we identified four types of AfC systems, based on the features extracted. In our opinion, the results of this study can serve as a guide for future affect-related research and design, on the one hand, and provide a better understanding on the role of emotions and affect in human-computer interaction, on the other hand.}
}
@article{OLSON1995183,
title = {Emergent computation and the modeling and management of ecological systems},
journal = {Computers and Electronics in Agriculture},
volume = {12},
number = {3},
pages = {183-209},
year = {1995},
issn = {0168-1699},
doi = {https://doi.org/10.1016/0168-1699(94)00022-I},
url = {https://www.sciencedirect.com/science/article/pii/016816999400022I},
author = {Richard L. Olson and Ronaldo A. Sequeira},
keywords = {Emergent computation, Ecosystem dynamics, Ecosystem management},
abstract = {This paper introduces the emergent computational paradigm, discusses its applicability and potential in ecosystem management, and reviews the literature. Emergent computation is significantly different from the “classic” computational paradigm, where control is top-down and centralized. In emergent systems, overall system dynamics emerge from the local interactions of independent agents. In such systems, overall global control is minimized or eliminated altogether. Applications in ecosystem management include use of “artificial ecosystems” as surrogate experimental systems, and genetics-based machine learning systems to evolve management rule-sets for complex domains. Cellular automata, neural networks, genetic algorithms and classifier systems are discussed as examples of the emergent approach. Finally, an in-depth literature review of artificial ecosystems is provided.}
}
@article{YAP19973,
title = {Towards exact geometric computation},
journal = {Computational Geometry},
volume = {7},
number = {1},
pages = {3-23},
year = {1997},
issn = {0925-7721},
doi = {https://doi.org/10.1016/0925-7721(95)00040-2},
url = {https://www.sciencedirect.com/science/article/pii/0925772195000402},
author = {Chee-Keng Yap},
abstract = {Exact computation is assumed in most algorithms in computational geometry. In practice, implementors perform computation in some fixed-precision model, usually the machine floating-point arithmetic. Such implementations have many well-known problems, here informally called “robustness issues”. To reconcile theory and practice, authors have suggested that theoretical algorithms ought to be redesigned to become robust under fixed-precision arithmetic. We suggest that in many cases, implementors should make robustness a non-issue by computing exactly. The advantages of exact computation are too many to ignore. Many of the presumed difficulties of exact computation are partly surmountable and partly inherent with the robustness goal. This paper formulates the theoretical framework for exact computation based on algebraic numbers. We then examine the practical support needed to make the exact approach a viable alternative. It turns out that the exact computation paradigm encompasses a rich set of computational tactics. Our fundamental premise is that the traditional “BigNumber” package that forms the work-horse for exact computation must be reinvented to take advantage of many features found in geometric algorithms. Beyond this, we postulate several other packages to be built on top of the BigNumber package.}
}
@article{LI2024109502,
title = {Numerical study on heat transfer performance of printed circuit heat exchanger with anisotropic thermal conductivity},
journal = {International Journal of Heat and Fluid Flow},
volume = {109},
pages = {109502},
year = {2024},
issn = {0142-727X},
doi = {https://doi.org/10.1016/j.ijheatfluidflow.2024.109502},
url = {https://www.sciencedirect.com/science/article/pii/S0142727X24002273},
author = {Libo Li and Jiyuan Bi and Jingkai Ma and Xiaoxu Zhang and Qiuwang Wang and Ting Ma},
keywords = {Printed circuit heat exchanger, Anisotropic thermal conductivity, Numerical simulation, Thermal resistance, Heat exchanger efficiency},
abstract = {Printed Circuit Heat Exchangers are compact and efficient heat exchangers, widely used in nuclear engineering, very high-temperature reactors, and aerospace systems. This study investigates the heat transfer performance of a heat exchanger with anisotropic thermal conductivity, such as fiber reinforced composites. Numerical simulations were conducted to examine the synergistic effect of three-dimensional thermal resistance on heat exchanger performance. The most significant impact on performance is the z-direction thermal resistance, followed by the y-direction, while the x-direction has the least impact. Contrary to traditional design thinking, increasing the overall heat exchanger thermal resistance under the same thermal resistance ratio improves heat transfer efficiency at the studied conditions. The results suggest that it is necessary to design the lowest thermal conductivity direction as the z-direction and increase the y-direction thermal conductivity to enhance heat exchanger performance. In the numerical investigation presented in this study, the efficiency of the heat exchanger was improved by approximately 23 % under specific operating conditions by adjusting the thermal conductivity of anisotropic materials to control the thermal resistance in the x, y and z directions. It is evident that the manipulation of anisotropic material properties has a substantial influence on the performance of heat exchangers.}
}
@article{MAHMUD2025111321,
title = {RSPCA: Random Sample Partition and Clustering Approximation for ensemble learning of big data},
journal = {Pattern Recognition},
volume = {161},
pages = {111321},
year = {2025},
issn = {0031-3203},
doi = {https://doi.org/10.1016/j.patcog.2024.111321},
url = {https://www.sciencedirect.com/science/article/pii/S0031320324010720},
author = {Mohammad Sultan Mahmud and Hua Zheng and Diego Garcia-Gil and Salvador García and Joshua Zhexue Huang},
keywords = {Clustering approximation, Ensemble clustering, Incremental clustering, Ensemble learning},
abstract = {Large-scale data clustering needs an approximate approach for improving computation efficiency and data scalability. In this paper, we propose a novel method for ensemble clustering of large-scale datasets that uses the Random Sample Partition and Clustering Approximation (RSPCA) to tackle the problems of big data computing in cluster analysis. In the RSPCA computing framework, a big dataset is first partitioned into a set of disjoint random samples, called RSP data blocks that remain distributions consistent with that of the original big dataset. In ensemble clustering, a few RSP data blocks are randomly selected, and a clustering operation is performed independently on each data block to generate the clustering result of the data block. All clustering results of selected data blocks are aggregated to the ensemble result as an approximate result of the entire big dataset. To improve the robustness of the ensemble result, the ensemble clustering process can be conducted incrementally using multiple batches of selected RSP data blocks. To improve computation efficiency, we use the I-niceDP algorithm to automatically find the number of clusters in RSP data blocks and the k-means algorithm to determine more accurate cluster centroids in RSP data blocks as inputs to the ensemble process. Spectral and correlation clustering methods are used as the consensus functions to handle irregular clusters. Comprehensive experiment results on both real and synthetic datasets demonstrate that the ensemble of clustering results on a few RSP data blocks is sufficient for a good global discovery of the entire big dataset, and the new approach is computationally efficient and scalable to big data.}
}
@incollection{SCHOMMERS201991,
title = {Chapter 2 - Theoretical and Computational Methods},
editor = {Wolfram Schommers},
booktitle = {Basic Physics of Nanoscience},
publisher = {Elsevier},
pages = {91-202},
year = {2019},
isbn = {978-0-12-813718-5},
doi = {https://doi.org/10.1016/B978-0-12-813718-5.00002-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128137185000028},
author = {Wolfram Schommers},
keywords = {Simulation methods, interaction potentials, anharmonicities, temperature effects, molecular dynamics, nanosystems, structures, dynamics},
abstract = {It is underlined that typical nanosystems are adequately described only by the fundamental laws of theoretical physics. It is in particular argued that phenomenological models are in most cases not sophisticated enough. For the description of such nanosystems the theoretical and computational tools have to be selected carefully and have in particular to be improved in many cases. In this connection the interaction laws (potentials) between the atoms, forming a nanosystem, are critical functions because the structure and dynamics of such systems are very sensitive to small variations in the potentials. This point has been studied in detail. Various potential laws have been introduced and discussed in connection with applications. The most relevant simulations methods are quoted and their relevance for nanotechnology is discussed. In particular, the molecular dynamics method is described in detail. We give typical examples, which demonstrate the fact the molecular dynamics method is a powerful and reliable tool for the investigation of typical nanosystems with their large variety of structures and complex dynamical states. The examples deal with wear at the nanotechnological level and with metallic nanoclusters as building blocks.}
}
@article{VANDUN2023113880,
title = {ProcessGAN: Supporting the creation of business process improvement ideas through generative machine learning},
journal = {Decision Support Systems},
volume = {165},
pages = {113880},
year = {2023},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2022.113880},
url = {https://www.sciencedirect.com/science/article/pii/S0167923622001518},
author = {Christopher {van Dun} and Linda Moder and Wolfgang Kratsch and Maximilian Röglinger},
keywords = {Business process improvement, Business process redesign, Generative adversarial networks, Generative machine learning, Process mining},
abstract = {Business processes are a key driver of organizational success, which is why business process improvement (BPI) is a central activity of business process management. Despite an abundance of approaches, BPI as a creative task is time-consuming and labour-intensive. Most importantly, its level of computational support is low. The few computational BPI approaches hardly leverage the opportunities brought about by computational creativity, neglect process data, and rely on rather rigid improvement patterns. Given the increasing amount of process data in the form of event logs and the uptake of generative machine learning for automating creative tasks in various domains, there is huge potential for BPI. Hence, following the design science research paradigm, we specified, implemented, and evaluated ProcessGAN, a novel computational BPI approach based on generative adversarial networks that supports the creation of BPI ideas. Our evaluation shows that ProcessGAN improves the creativity of process designers, particularly the originality of BPI ideas, and shapes up useful in real-world settings. Moreover, ProcessGAN is the first approach to combine BPI and computational creativity.}
}
@article{SYCHEV2024101261,
title = {Educational models for cognition: Methodology of modeling intellectual skills for intelligent tutoring systems},
journal = {Cognitive Systems Research},
volume = {87},
pages = {101261},
year = {2024},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101261},
url = {https://www.sciencedirect.com/science/article/pii/S138904172400055X},
author = {Oleg Sychev},
keywords = {Reasoning modeling, Constraint-based modeling, Intelligent tutoring systems},
abstract = {Automation of teaching people new skills requires modeling of human reasoning because human cognition involves active reasoning over the new subject domain to acquire skills that will later become automatic. The article presents Thought Process Trees — a language for modeling human reasoning that was created to facilitate the development of intelligent tutoring systems, which can perform the same reasoning that is expected of a student and find deficiencies in their line of thinking, providing explanatory messages and allowing them to learn from performance errors. The methodology of building trees which better reflect human learning is discussed, with examples of design choices during the modeling process and their consequences. The characteristics of educational modeling that impact building subject-domain models for intelligent tutoring systems are discussed. The trees were formalized and served as a basis for developing a framework for constructing intelligent tutoring systems. This significantly lowered the time required to build and debug a constraint-based subject-domain model. The framework has already been used to develop five intelligent tutoring systems and their prototypes and is being used to develop more of them.}
}
@article{PORNSUWANCHAROEN20181034,
title = {Meditation mathematical formalism and Lorentz factor calculation based-on Mindfulness foundation},
journal = {Results in Physics},
volume = {11},
pages = {1034-1038},
year = {2018},
issn = {2211-3797},
doi = {https://doi.org/10.1016/j.rinp.2018.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S2211379718325294},
author = {N. Pornsuwancharoen and I.S. Amiri and J. Ali and P. Youplao and P. Yupapin},
keywords = {Meditation science, Mindfulness Foundation, Buddhism philosophy, Mathematics foundation, Natural science},
abstract = {Mindfulness foundation is an excellent method of the human spiritual development by the reasonable thinking and consideration, which was established by Lord Buddha a long time ago. There are four ways of thinking and consideration-(i) form (body), (ii) sensation, (iii) spiritual and (iv) Dhamma. In this paper, we propose the use of the form consideration for the spiritual development, in which the form can be considered thoroughly inside the body by the spiritual projection. By using the nonlinear microring resonator known as a Panda-ring resonator, the electromagnetic (EM) signals called polaritons can be generated by the coupling interaction between the intense EM fields and the ionic diploes within the almost closed system, where the dipoles can obtain from the coupling between the gold grating and the strong electromagnetic fields. In the manipulation, cells, tissues, and organs inside the human body can communicate with the spiritual (polaritonic) signals and investigation. The simulation results obtained have shown that the Lorentz factor of 0.99999959 is obtained. The successively filtering of the signal circulation within the body during the meditation can be formulated and the meditation behaviors modeled. The aura, the stopping, and the cold body states can be configured and explained.}
}
@article{KALAY199837,
title = {P3: Computational environment to support design collaboration},
journal = {Automation in Construction},
volume = {8},
number = {1},
pages = {37-48},
year = {1998},
issn = {0926-5805},
doi = {https://doi.org/10.1016/S0926-5805(98)00064-8},
url = {https://www.sciencedirect.com/science/article/pii/S0926580598000648},
author = {Yehuda E Kalay},
keywords = {Collaborative design, Design environment, Product model, Performance model, Process model},
abstract = {The work reported in this paper addresses the paradoxical state of the construction industry (also known as A/E/C, for Architecture, Engineering and Construction), where the design of highly integrated facilities is undertaken by severely fragmented teams, leading to diminished performance of both processes and products. The construction industry has been trying to overcome this problem by partitioning the design process hierarchically or temporally. While these methods are procedurally efficient, their piecemeal nature diminishes the overall performance of the project. Computational methods intended to facilitate collaboration in the construction industry have, so far, focused primarily on improving the flow of information among the participants. They have largely met their stated objective of improved communication, but have done little to improve joint decision-making, and therefore have not significantly improved the quality of the design project itself. We suggest that the main impediment to effective collaboration and joint decision-making in the A/E/C industry is the divergence of disciplinary `world-views', which are the product of educational and professional processes through which the individuals participating in the design process have been socialized into their respective disciplines. To maximize the performance of the overall project, these different world-views must be reconciled, possibly at the expense of individual goals. Such reconciliation can only be accomplished if the participants find the attainment of the overall goals of the project more compelling than their individual disciplinary goals. This will happen when the participants have become cognizant and appreciative of world-views other than their own, including the objectives and concerns of other participants. To achieve this state of knowledge, we propose to avail to the participants of the design team highly specific, contextualized information, reflecting each participant's valuation of the proposed design actions. P3 is a semantically-rich computational environment, which is intended to fulfill this mission. It consists of: (1) a shared representation of the evolving design project, connected (through the World Wide Web) to (2) individual experts and their discipline-specific knowledge repositories; and (3) a computational project manager makes the individual valuations visible to all the participants, and helps them deliberate and negotiate their respective positions for the purpose of improving the overall performance of the project. The paper discusses the theories on which the three components are founded, their function, and the principles of their implementation.}
}
@incollection{WU202257,
title = {Chapter 3 - CTDA methodology},
editor = {Jiaping Wu and Junyu He and George Christakos},
booktitle = {Quantitative Analysis and Modeling of Earth and Environmental Data},
publisher = {Elsevier},
pages = {57-100},
year = {2022},
isbn = {978-0-12-816341-2},
doi = {https://doi.org/10.1016/B978-0-12-816341-2.00010-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128163412000101},
author = {Jiaping Wu and Junyu He and George Christakos},
keywords = {Methodological chain, Knowledge bases, Big data, Scales, Visualization, Chronotopologic statistics},
abstract = {Abstract The methodological characteristics of the chronotopologic data analysis chain are discussed. Various kinds of knowledge are considered and properly classified, and several illustrative examples in applied sciences are presented. Big data and data-driven analyses are critically reviewed, and their implementation carefully assessed. Data scale types are classifications considered in property- and attribute-oriented settings. Classical statistics inadequacies are pointed out and the need of a chronotopology-dependent statistics is outlined. The chronotopologic visualization thinking mode and techniques are briefly reviewed.}
}
@incollection{RUNCO201469,
title = {Chapter 3 - Biological Perspectives on Creativity},
editor = {Mark A. Runco},
booktitle = {Creativity (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {69-108},
year = {2014},
isbn = {978-0-12-410512-6},
doi = {https://doi.org/10.1016/B978-0-12-410512-6.00003-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780124105126000035},
author = {Mark A. Runco},
keywords = {Split brain, Corpus callosum, Pre-frontal cortex, Cerebellum, Altered states of consciousness, Exercise, Stress, Dreams, Drugs, Genetics, Dopamine, Adoption studies, Genealogies},
abstract = {This chapter discusses various aspects of biological perspectives on creativity. Some of the research on creativity as of late involves the brain and biological correlates of originality, novelty, and insight. Handedness is sometimes used as an indication of hemispheric dominance or hemisphericity, with right-handed people being compared with left-handed people. There are several reports of left-handed persons outnumbering the right-handed in creative and eminent samples. Hemisphericity and other important brain structures and processes contributing to creative thinking and behavior have been studied with electroencephalogram (EEG), positron emission topography (PET), cerebral blood flow, and magnetic resonance imaging (MRI) techniques. Numerous EEG studies suggest that there are particular brain wave patterns and brain structures that are associated with creative problem solving, or at least specific phases within the problem solving process. EEGs suggest a complex kind of activity while individuals work on divergent thinking tasks. The complexity disappears when those same individuals work on convergent thinking tasks. It is found that the role of the prefrontal cortex in creative thinking and behavior comes from several sources and uses different methodologies.}
}
@incollection{SHI2021117,
title = {Chapter 4 - Mind model},
editor = {Zhongzhi Shi},
booktitle = {Intelligence Science},
publisher = {Elsevier},
pages = {117-149},
year = {2021},
isbn = {978-0-323-85380-4},
doi = {https://doi.org/10.1016/B978-0-323-85380-4.00004-X},
url = {https://www.sciencedirect.com/science/article/pii/B978032385380400004X},
author = {Zhongzhi Shi},
keywords = {Mind model, Turing machine, physical symbol system, SOAR model, ACT-R model, CAM model, cognitive cycle, PMJ model},
abstract = {The technology of building mind model is often called mind modeling, which aims to explore and study the human thinking mechanism.}
}
@article{COOPER19821,
title = {Energy conservation in buildings: Part 2-A commentary on British government thinking},
journal = {Applied Energy},
volume = {10},
number = {1},
pages = {1-45},
year = {1982},
issn = {0306-2619},
doi = {https://doi.org/10.1016/0306-2619(82)90058-7},
url = {https://www.sciencedirect.com/science/article/pii/0306261982900587},
author = {Ian Cooper},
abstract = {Like my previous paper in this journal this commentary is focused on government statements published during the period 1974 to 1979. It is intended as an introductory guide aimed at two overlapping audiences. First, it is addressed to those interested in the reasoning which lies behind the Government's technical arguments on energy conservation in buildings. Secondly, it is directed towards those who seek to understand the social implications and consequences of this area of government endeavour. Not all the statements examined in this commentary represent official expressions of government policy. Some, indeed, are prefaced in their originals by specific disclaimers to this effect. Rather, they should be read as examples of arguments voiced by a variety of individuals and groups who are capable of informing, influencing or making decisions that affect this field of government activity. It should not be supposed that the government statements brought together in this commentary are necessarily consistent or coherent. Instead, in some cases at least, they seem incompatible and may even be irreconcilable. But, given that the source material is drawn from a wide range of documents with a broad range of authors and was published over a number of years, the extent of their unanimity is remarkable. As an introductory guide, this commentary is not offered as exhaustive, as representative of all aspects or shades of government thinking on this subject. On the contrary, only statements published in documents emanating from, or associated with, the Department of Energy have, for the most part, been cited. For the sake of brevity, statements published by other government departments with responsibility for the conservation of energy in buildings—such as the Department of the Environment—have not been drawn upon.}
}
@article{LUNGU2008255,
title = {Partial current information and signal extraction in a rational expectations macroeconomic model: A computational solution},
journal = {Economic Modelling},
volume = {25},
number = {2},
pages = {255-273},
year = {2008},
issn = {0264-9993},
doi = {https://doi.org/10.1016/j.econmod.2007.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0264999307000818},
author = {L. Lungu and K.G.P. Matthews and A.P.L. Minford},
keywords = {Rational expectations, Partial current information, Signal extraction, Macroeconomic modelling},
abstract = {Previous attempts at modelling current observed endogenous financial variables in a macroeconomic model have concentrated on only one variable — the short-term rate of interest. This paper applies a general search algorithm to a macroeconomic model with an observed interest rate and exchange rate to solve the signal extraction problem. Firstly, the algorithm is tested against a linear model with a known analytical solution. Then, the algorithm is applied to all the observed current endogenous variables in a non-linear rational expectations model of the UK. The informational advantage of applying the signal extraction algorithm is evaluated in terms of the forecasting efficiency of the model.}
}
@article{INTRONE201479,
title = {Improving decision-making performance through argumentation: An argument-based decision support system to compute with evidence},
journal = {Decision Support Systems},
volume = {64},
pages = {79-89},
year = {2014},
issn = {0167-9236},
doi = {https://doi.org/10.1016/j.dss.2014.04.005},
url = {https://www.sciencedirect.com/science/article/pii/S0167923614001262},
author = {Joshua Introne and Luca Iandoli},
keywords = {Computer-supported argumentation, Evidence-based reasoning, Dempster–Shafer belief aggregation, Housing market prediction},
abstract = {While research has shown that argument based systems (ABSs) can be used to improve aspects of individual thinking and learning, relatively few studies have shown that ABSs improve decision performance in real world tasks. In this article, we strive to improve the value-proposition of ABSs for decision makers by showing that individuals can, with minimal training, use a novel ABS called Pendo to improve their ability to predict housing market trends. Pendo helps to weight and aggregate evidence through a computational engine to support evidence-based reasoning, a well-documented deficiency in human decision-making. It also supports individuals in the creation of knowledge artifacts that can be used to solve similar problems in the same domain. An unexpected finding and one of the major contributions of this work is that individual unaided decision-making performance was not predictive of an individual's performance with Pendo, even though the average performance of assisted individuals was higher. We infer that the skills activated when using the tool are substantially different than those enacted to solve the same problem without that tool. We discuss the implications this result has for the design and application of ABSs to decision-making, and possibly other decision support technologies.}
}
@article{RUTHERFORD2023102255,
title = {“Don't [ruminate], be happy”: A cognitive perspective linking depression and anhedonia},
journal = {Clinical Psychology Review},
volume = {101},
pages = {102255},
year = {2023},
issn = {0272-7358},
doi = {https://doi.org/10.1016/j.cpr.2023.102255},
url = {https://www.sciencedirect.com/science/article/pii/S0272735823000132},
author = {Ashleigh V. Rutherford and Samuel D. McDougle and Jutta Joormann},
keywords = {Rumination, Emotion regulation, Working memory, Reinforcement learning, Depression},
abstract = {Anhedonia, a lack of pleasure in things an individual once enjoyed, and rumination, the process of perseverative and repetitive attention to specific thoughts, are hallmark features of depression. Though these both contribute to the same debilitating disorder, they have often been studied independently and through different theoretical lenses (e.g., biological vs. cognitive). Cognitive theories and research on rumination have largely focused on understanding negative affect in depression with much less focus on the etiology and maintenance of anhedonia. In this paper, we argue that by examining the relation between cognitive constructs and deficits in positive affect, we may better understand anhedonia in depression thereby improving prevention and intervention efforts. We review the extant literature on cognitive deficits in depression and discuss how these dysfunctions may not only lead to sustained negative affect but, importantly, interfere with an ability to attend to social and environmental cues that could restore positive affect. Specifically, we discuss how rumination is associated to deficits in working memory and propose that these deficits in working memory may contribute to anhedonia in depression. We further argue that analytical approaches such as computational modeling are needed to study these questions and, finally, discuss implications for treatment.}
}
@article{SPINU2022100205,
title = {A matter of trust: Learning lessons about causality will make qAOPs credible},
journal = {Computational Toxicology},
volume = {21},
pages = {100205},
year = {2022},
issn = {2468-1113},
doi = {https://doi.org/10.1016/j.comtox.2021.100205},
url = {https://www.sciencedirect.com/science/article/pii/S2468111321000517},
author = {Nicoleta Spînu and Mark T.D. Cronin and Judith C. Madden and Andrew P. Worth},
keywords = {Model credibility, Adverse Outcome Pathway, qAOP, Causality, Next Generation Risk Assessment},
abstract = {Toxicology in the 21st Century has seen a shift from chemical risk assessment based on traditional animal tests, identifying apical endpoints and doses that are “safe”, to the prospect of Next Generation Risk Assessment based on non-animal methods. Increasingly, large and high throughput in vitro datasets are being generated and exploited to develop computational models. This is accompanied by an increased use of machine learning approaches in the model building process. A potential problem, however, is that such models, while robust and predictive, may still lack credibility from the perspective of the end-user. In this commentary, we argue that the science of causal inference and reasoning, as proposed by Judea Pearl, will facilitate the development, use and acceptance of quantitative AOP models. Our hope is that by importing established concepts of causality from outside the field of toxicology, we can be “constructively disruptive” to the current toxicological paradigm, using the “Causal Revolution” to bring about a “Toxicological Revolution” more rapidly.}
}
@article{CANADAS201687,
title = {Second graders articulating ideas about linear functional relationships},
journal = {The Journal of Mathematical Behavior},
volume = {41},
pages = {87-103},
year = {2016},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2015.10.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732312315300055},
author = {María C. Cañadas and Bárbara M. Brizuela and Maria Blanton},
keywords = {Quantities, Functional thinking, Early algebra, Elementary students},
abstract = {In this paper, we explore the ideas that second grade students articulate about functional relationships. We adopt a function-based approach to introduce elementary school children to algebraic content. We present results from a design-based research study carried out with 21 second-grade students (approximately 7 years of age). We focus on a lesson from our classroom teaching experiment in which the students were working on a problem that involved a linear functional relationship (y=2x). From the analysis of students’ written work and classroom video, we illustrate two different approaches that students adopt to express the relationship between two quantities. Students show fluency recontextualizing the problem posed, moving between extra-mathematical and intra-mathematical contexts.}
}
@article{BARELI2013472,
title = {Sketching profiles: Awareness to individual differences in sketching as a means of enhancing design solution development},
journal = {Design Studies},
volume = {34},
number = {4},
pages = {472-493},
year = {2013},
note = {Special Issue: Articulating Design Thinking},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2013.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X13000197},
author = {Shoshi Bar-Eli},
keywords = {design processes, design research, design behavior, problem solving, design tools},
abstract = {This paper focuses on the differences between interior design students' design processes as derived from an analysis of their sketching and design behavior. By implementing qualitative methodologies in the analysis of the sketches produced in the conceptual phase of the design process, the experiment allows identifying sketching characteristics and profiles. The motivation is to show that sketches can serve as a tool to differentiate between designers and recognize their personal approach and design strategies. The results point to three distinct sketching profiles that characterize designers' use of sketches as a tool for thinking and communicating ideas during their solution generation process. Awareness to differences between students' sketches and design behavior may support the development of pedagogical concepts, strategies and tools.}
}
@article{FLINT2025100948,
title = {Expansion of analytical methods in auditing education},
journal = {Journal of Accounting Education},
volume = {70},
pages = {100948},
year = {2025},
issn = {0748-5751},
doi = {https://doi.org/10.1016/j.jaccedu.2024.100948},
url = {https://www.sciencedirect.com/science/article/pii/S0748575124000642},
author = {Michele S. Flint},
keywords = {Auditing education, Analytical procedures, Data analytics, Beneish M−score, Altman Z-score, Sloan Accrual},
abstract = {Data analytics is changing the audit environment and carries significant implications for auditing education. Both international auditing education (International Accounting Education Standards Board (IAESB), 2019a; IAESB, 2019b) and U.S.-based regulatory bodies (American Institute of Certified Public Accountants (AICPA), 2021c; AICPA & National Association of State Boards of Accountancy (NASBA), 2021) have made efforts to address the growing expectations for auditing education, citing fraud risk and going concern risk. While auditing courses have progressed to include some computerized audit software for case studies, the study of analytical procedures has been limited to the application of basic financial ratios, trend analyses and common-size financial statements. Demands for advanced analytics place most emphasis on computerized query and computational methods; however, several advanced analytical models, namely the Altman Z-score, Beneish M−score and the Sloan Accrual formula provide opportunities for greater insight on specific audit risks and do not require advanced computer-based skills. The ability to link audit procedures, specifically analytical procedures to the audit objectives of financial risk and going concern risk strengthens the rationale for introduction of these advanced models within the context of auditing education. This paper discusses the inherent value in these analytical models, links them to audit objectives, proposes the inclusion of these three analytical models as a component of auditing education, and suggests that future study be undertaken to assess implementation and student learning. In addition, we recommend future study of other analytical models that may provide further insight for auditing students.}
}
@article{DASILVA2022402,
title = {A Predictive, Context-Dependent Stochastic Model for Engineering Applications},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {2},
pages = {402-407},
year = {2022},
note = {14th IFAC Workshop on Intelligent Manufacturing Systems IMS 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.04.227},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322002282},
author = {Márcio J. {da Silva} and Gustavo Künzel and Carlos E. Pereira},
keywords = {Data Mining, Predictive Situation, Context Testing, Industrial Alarm System, Recommendation Systems},
abstract = {This work explores the architecture of a context-dependent probabilistic model. We identify opportunities for providing reminders to operators in their environment as a means to address information overload. Hence, there is a need to represent a state of knowledge and help them stay vigilant during their jobs. Along with the architectural improvements, which further specialize information flows and develop a data-driven approach, continual learning techniques covered events in a probabilistic graphical model called Context-Dependent Recommendation Systems (CD-RS). We demonstrated, as a result, the use of statistical thinking and Design of Experiments (DoE), which are most clear in conducting a suitable experiment. Moreover, the validation of the model and experiments of the novel architecture based on the collected data from a real case study demonstrates the value of the proposed methods.}
}
@article{ELVEVAG2023115098,
title = {Reflections on measuring disordered thoughts as expressed via language},
journal = {Psychiatry Research},
volume = {322},
pages = {115098},
year = {2023},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2023.115098},
url = {https://www.sciencedirect.com/science/article/pii/S0165178123000513},
author = {Brita Elvevåg},
keywords = {Assessment, Language, Memory},
abstract = {Thought disorder, as inferred from disorganized and incoherent speech, is an important part of the clinical presentation in schizophrenia. Traditional measurement approaches essentially count occurrences of certain speech events which may have restricted their usefulness. Applying speech technologies in assessment can help automate traditional clinical rating tasks and thereby complement the process. Adopting these computational approaches affords clinical translational opportunities to enhance the traditional assessment by applying such methods remotely and scoring various parts of the assessment automatically. Further, digital measures of language may help detect subtle clinically significant signs and thus potentially disrupt the usual manner by which things are conducted. If proven beneficial to patient care, methods where patients’ voice are the primary data source could become core components of future clinical decision support systems that improve risk assessment. However, even if it is possible to measure thought disorder in a sensitive, reliable and efficient manner, there remain many challenges to then translate into a clinically implementable tool that can contribute towards providing better care. Indeed, embracing technology - notably artificial intelligence - requires vigorous standards for reporting underlying assumptions so as to ensure a trustworthy and ethical clinical science.}
}
@article{STORLIE20091735,
title = {Implementation and evaluation of nonparametric regression procedures for sensitivity analysis of computationally demanding models},
journal = {Reliability Engineering & System Safety},
volume = {94},
number = {11},
pages = {1735-1763},
year = {2009},
issn = {0951-8320},
doi = {https://doi.org/10.1016/j.ress.2009.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0951832009001112},
author = {Curtis B. Storlie and Laura P. Swiler and Jon C. Helton and Cedric J. Sallaberry},
keywords = {Bootstrap, Confidence intervals, Meta-model, Nonparametric regression, Sensitivity analysis, Surrogate model, Uncertainty analysis, Variance decomposition},
abstract = {The analysis of many physical and engineering problems involves running complex computational models (simulation models, computer codes). With problems of this type, it is important to understand the relationships between the input variables (whose values are often imprecisely known) and the output. The goal of sensitivity analysis (SA) is to study this relationship and identify the most significant factors or variables affecting the results of the model. In this presentation, an improvement on existing methods for SA of complex computer models is described for use when the model is too computationally expensive for a standard Monte-Carlo analysis. In these situations, a meta-model or surrogate model can be used to estimate the necessary sensitivity index for each input. A sensitivity index is a measure of the variance in the response that is due to the uncertainty in an input. Most existing approaches to this problem either do not work well with a large number of input variables and/or they ignore the error involved in estimating a sensitivity index. Here, a new approach to sensitivity index estimation using meta-models and bootstrap confidence intervals is described that provides solutions to these drawbacks. Further, an efficient yet effective approach to incorporate this methodology into an actual SA is presented. Several simulated and real examples illustrate the utility of this approach. This framework can be extended to uncertainty analysis as well.}
}
@article{ZHAO2025128946,
title = {The effect of the head number for multi-head self-attention in remaining useful life prediction of rolling bearing and interpretability},
journal = {Neurocomputing},
volume = {616},
pages = {128946},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128946},
url = {https://www.sciencedirect.com/science/article/pii/S092523122401717X},
author = {Qiwu Zhao and Xiaoli Zhang and Fangzhen Wang and Panfeng Fan and Erick Mbeka},
keywords = {Remaining useful life prediction, Machine learning, Multi-head self-attention mechanism, Interpretability, Graph theory, Functional networks},
abstract = {As one of the machine learning (ML) models, the multi-head self-attention mechanism (MSM) is competent in encoding high-level feature representations, providing computing superiorities, and systematically processing sequences bypassing the recurrent neural networks (RNN) models. However, the model performance and computational results are affected by head number, and the lack of impact interpretability has become a primary obstacle due to the complex internal working mechanisms. Therefore, the effects of the head number of the MSM on the accuracy of the result, the robustness of the model, and computation efficiency are investigated in the remaining useful life (RUL) prediction of rolling bearings. The results show that the accuracy of prediction results will be reduced caused by large or few head numbers. In addition, the more heads are selected, the more robust and higher the predictive efficiency of the model is achieved. The above effects are explained relying on the visualization of the attention weight distribution and functional networks, which are constructed and solved by the equivalent fully connected layer and graph theory analysis, respectively. The model's attention coefficient distribution during training and prediction shows that the representative information will be captured inadequately if fewer heads are selected, which causes MSM to neglect to assign large attention coefficients to degraded information. On the contrary, representational degradation information and redundant information will be acquired by models with too many heads. MSM will be disturbed by this redundant information in the attention weight distribution, resulting in incorrect allocation of attention. Both of these cases will reduce the accuracy of the prediction results. In addition, the selection rules of the head number are established based on the feature complexity that is measured by the sample entropy (SamEn). The local range for head selection is also found based on the relationship between head number and feature complexity; The effects of the head number of the MSM on the robustness of the model and computation efficiency are explained by the changes in the three parameters (average of the clustering coefficients, global efficiency, and of the average shortest path length) of the graph, which is constructed after solving the function network. The research provides a reference for rolling bearing prediction with high computational accuracy, calculation efficiency, and strong robustness using MSM.}
}
@article{OH2023100602,
title = {Making computing visible & tangible: A paper-based computing toolkit for codesigning inclusive computing education activities},
journal = {International Journal of Child-Computer Interaction},
volume = {38},
pages = {100602},
year = {2023},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2023.100602},
url = {https://www.sciencedirect.com/science/article/pii/S2212868923000399},
author = {HyunJoo Oh and Sherry Hsi and Noah Posner and Colin Dixon and Tymirra Smith and Tingyu Cheng},
keywords = {Paper-based computing, Codesign, Inclusive CS education, Learning through making},
abstract = {MCVT (Making Computing Visible and Tangible) Cards are a toolkit of paper-based computing cards intended for use in the codesign of inclusive computing education. Working with groups of teachers and students over multiple design sessions, we share our toolkit, design drivers and material considerations; and use cases drawn from a week-long codesign workshop where seven teachers made and adapted cards for their future classroom facilitation. Our findings suggest that teachers valued the MCVT toolkit as a resource for their own learning and perceived the cards to be useful for supporting new computational practices, specifically for learning through making and connecting to examples of everyday computing. Critically reviewed by teachers during codesign workshops, the toolkit however posed some implementation challenges and constraints for learning through making and troubleshooting circuitry. From teacher surveys, interviews, workshop video recordings, and teacher-constructed projects, we show how teachers codesigned new design prototypes and pedagogical activities while also adapting and extending paper-based computing materials so their students could take advantage of the unique technical and expressive affordances of MCVT Cards. Our design research contributes a new perspective on using interactive paper computing cards as a medium for instructional materials development to support more inclusive computing education.}
}
@article{RAHMAN20125541,
title = {Developing Mathematical Communication Skills of Engineering Students},
journal = {Procedia - Social and Behavioral Sciences},
volume = {46},
pages = {5541-5547},
year = {2012},
note = {4th WORLD CONFERENCE ON EDUCATIONAL SCIENCES (WCES-2012) 02-05 February 2012 Barcelona, Spain},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2012.06.472},
url = {https://www.sciencedirect.com/science/article/pii/S1877042812022082},
author = {Roselainy Abdul Rahman and Yudariah Mohammad Yusof and Hamidreza Kashefi and Sabariah Baharun},
keywords = {Communication, Mathematical Thinking, Multivariable Calculus, Student's Obstacles},
abstract = {In Malaysia and also elsewhere in the world the demands for graduates who have employability skills such as ability to think critically, solve problems and can communicate are highly sought in the workplace. In the early 2006, the development of such skills was recognized as integral goals of undergraduate education at Universiti Teknologi Malaysia. Since then rigorous efforts have been made to inculcate these skills amongst the undergraduates. In this paper, we will share some of our experiences in coping with the challenges of changing our teaching practices to accommodate this quest though focusing on communication. For mathematics learning to occur, we believed that students should participate actively in the knowledge construction and be able to take charge of their own learning. Taking these aspects into consideration, we had developed a framework of active learning and used it to guide our instruction in engineering mathematics at UTM. Here we will discuss the strategies that we had designed and employed in engaging students with the subject matter as well as to initiate and support student's thinking and communication in the language of mathematics. Some student's responses that gave indications of their struggle, progress and growth encountered in the research implementation will also be presented.}
}
@article{LI2023119775,
title = {Neural representations of self-generated thought during think-aloud fMRI},
journal = {NeuroImage},
volume = {265},
pages = {119775},
year = {2023},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2022.119775},
url = {https://www.sciencedirect.com/science/article/pii/S1053811922008965},
author = {Hui-Xian Li and Bin Lu and Yu-Wei Wang and Xue-Ying Li and Xiao Chen and Chao-Gan Yan},
keywords = {Self-generated thoughts, Think-aloud fMRI, Natural language processing, Representational similarity analysis},
abstract = {Is the brain at rest during the so-called resting state? Ongoing experiences in the resting state vary in unobserved and uncontrolled ways across time, individuals, and populations. However, the role of self-generated thoughts in resting-state fMRI remains largely unexplored. In this study, we collected real-time self-generated thoughts during “resting-state” fMRI scans via the think-aloud method (i.e., think-aloud fMRI), which required participants to report whatever they were currently thinking. We first investigated brain activation patterns during a think-aloud condition and found that significantly activated brain areas included all brain regions required for speech. We then calculated the relationship between divergence in thought content and brain activation during think-aloud and found that divergence in thought content was associated with many brain regions. Finally, we explored the neural representation of self-generated thoughts by performing representational similarity analysis (RSA) at three neural scales: a voxel-wise whole-brain searchlight level, a region-level whole-brain analysis using the Schaefer 400-parcels, and at the systems level using the Yeo seven-networks. We found that “resting-state” self-generated thoughts were distributed across a wide range of brain regions involving all seven Yeo networks. This study highlights the value of considering ongoing experiences during resting-state fMRI and providing preliminary methodological support for think-aloud fMRI.}
}
@article{LI2023106299,
title = {Constructing a link between multivariate titanium-based semiconductor band gaps and chemical formulae based on machine learning},
journal = {Materials Today Communications},
volume = {35},
pages = {106299},
year = {2023},
issn = {2352-4928},
doi = {https://doi.org/10.1016/j.mtcomm.2023.106299},
url = {https://www.sciencedirect.com/science/article/pii/S235249282300990X},
author = {Jiawei Li and Zhengxin Chen and Jiang Wu and Jia Lin and Ping He and Rui Zhu and Cheng Peng and Hai Zhang and Wenhao Li and Xu Fang and Hongtao Shen},
keywords = {Random forest model, Chemical formula, Components, Bandgap, Machine learning},
abstract = {Titanium-based semiconductors are wildly recognized as one of the most commonly used photocatalysts for photocatalysis. Energy band modulation is a key aspect of the catalytic activity of photocatalytic semiconductors, but the acquisition of semiconductor energy bands is still a complex and important task. In recent years, machine learning has played an important role in materials prediction, where the crystal structure of a material is usually used as input in energy band prediction. However, existing machine learning algorithms cannot accurately predict the energy bands of materials from structural components. Here, we convert the chemical formula into component descriptors after comparing first principles and ultraviolet-visible spectrophotometry (UV–vis) errors on bandgap values, and the component and bandgap values form a set of labeled data pairs. The chemical formula components are used as input to accurately predict the energy bands of the material. In our evaluation, the model outperforms existing machine learning methods in predicting energy bands, yielding mean absolute value errors of about 0.277 eV, and possesses a significant advantage in component prediction. In particular, this method of predicting the photocatalytic semiconductor energy band gap from chemical formula components provides a new way of thinking about photocatalyst selectivity.}
}
@incollection{VALLERO2021601,
title = {Chapter 14 - The Future},
editor = {Daniel A. Vallero},
booktitle = {Environmental Systems Science},
publisher = {Elsevier},
pages = {601-613},
year = {2021},
isbn = {978-0-12-821953-9},
doi = {https://doi.org/10.1016/B978-0-12-821953-9.00004-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128219539000040},
author = {Daniel A. Vallero},
keywords = {Precautionary principle, Evidence-based risk assessment, Exposome, Translational science, Scientific workflow, Ontologies, Resilience, Data-driven decision-making, Precision science, Life cycle risk assessment (LCRA)},
abstract = {Solving and preventing environmental problems will continue to rely on systems thinking that translates and combines data, information, knowledge, and wisdom from numerous scientific and other perspectives. The chapter provides insights into possible directions for environmental systems science, especially ways to address complexities at every scale from cellular to planetary. Environmental scientists and engineers will engage in precision science and customized approaches to reduce risks, improve reliability and resilience, and ensure sustainability.}
}
@article{GUSTAFSON199557,
title = {Theory and computation of periodic solutions of autonomous partial differential equation boundary value problems, with application to the driven cavity problem},
journal = {Mathematical and Computer Modelling},
volume = {22},
number = {9},
pages = {57-75},
year = {1995},
issn = {0895-7177},
doi = {https://doi.org/10.1016/0895-7177(95)00168-2},
url = {https://www.sciencedirect.com/science/article/pii/0895717795001682},
author = {K. Gustafson},
keywords = {Navier-Stokes equations, Driven cavity problem, Pressure boundary condition, Vortex shedding, Hopf bifurcation},
abstract = {In ordinary differential equations, one distinguishes two cases: autonomous and nonautonomous. Roughly speaking, the theory of the latter is built upon the theory of the former. The same distinction should be applied to partial differential equations, where much less is known. Here I will focus on the question of the generation of periodic solutions for autonomous partial differential equation boundary value problems. Specifically, I consider the incompressible Navier-Stokes equations, and the important driven cavity problem. For simplicity, attention is restricted to two bifurcation parameters, the Reynolds number and the Aspect ratio. Only Dirichlet velocity boundary conditions are considered. Both the known theory and known computational results for the driven cavity are surveyed. The importance of computationally adhering to the div u = 0 condition to accurately simulate unsteady flows which will be qualitatively correct for the incompressible Navier-Stokes equations is stressed. The dependence of sustained periodicity upon the existence of highly localized vortex shedding sequences somewhere along the boundary is pointed out. A new analysis of the pressure boundary condition, based upon a general regularity principle, is given. A conjectured Hopf bifurcation criticality curve is explained.}
}
@article{GALLISTEL2021104533,
title = {The physical basis of memory},
journal = {Cognition},
volume = {213},
pages = {104533},
year = {2021},
note = {Special Issue in Honour of Jacques Mehler, Cognition’s founding editor},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2020.104533},
url = {https://www.sciencedirect.com/science/article/pii/S0010027720303528},
author = {C.R. Gallistel},
keywords = {Engram, Communication channel, Plastic synapse, Molecules},
abstract = {Neuroscientists are searching for the engram within the conceptual framework established by John Locke's theory of mind. This framework was elaborated before the development of information theory, before the development of information processing machines and the science of computation, before the discovery that molecules carry hereditary information, before the discovery of the codon code and the molecular machinery for editing the messages written in this code and translating it into transcription factors that mark abstract features of organic structure such as anterior and distal. The search for the engram needs to abandon Locke's conceptual framework and work within a framework informed by these developments. The engram is the medium by which information extracted from past experience is transmitted to the computations that inform future behavior. The information-conveying symbols in the engram are rapidly generated in the course of computations, which implies that they are molecules.}
}
@article{ZOHDI20073927,
title = {Computation of strongly coupled multifield interaction in particle–fluid systems},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {196},
number = {37},
pages = {3927-3950},
year = {2007},
note = {Special Issue Honoring the 80th Birthday of Professor Ivo Babuška},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2006.10.040},
url = {https://www.sciencedirect.com/science/article/pii/S004578250700117X},
author = {T.I. Zohdi},
keywords = {Particle–fluid interaction, Multiple fields, Iterative methods},
abstract = {The present work develops a flexible and robust solution strategy to resolve coupled systems comprised of large numbers of flowing particles embedded within a fluid. A model problem, consisting of particles which may undergo inelastic collisions in the presence of near-field forces, is considered. The particles are surrounded by a continuous interstitial fluid which is assumed to obey the compressible Navier–Stokes equations. Thermal effects are also considered. Such particle/fluid systems are strongly coupled, due to the mechanical forces and heat transfer induced by the fluid onto the particles and vice-versa. Because the coupling of the various particle and fluid fields can dramatically change over the course of a flow process, a primary focus of this work is the development of a recursive “staggering” solution scheme, whereby the time-steps are adaptively adjusted to control the error associated with the incomplete resolution of the coupled interaction between the various solid particulate and continuum fluid fields. A central feature of the approach is the ability to account for the presence of particles within the fluid in a straightforward manner that can be easily incorporated within any standard computational fluid mechanics code based on finite difference, finite element or finite volume type discretization. A three dimensional example is provided to illustrate the overall approach.}
}
@article{XU2025121541,
title = {Adaptive sequential three-way decisions for dynamic time warping},
journal = {Information Sciences},
volume = {690},
pages = {121541},
year = {2025},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.121541},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524014555},
author = {Jianfeng Xu and Ruihua Wang and Yuanjian Zhang and Weiping Ding},
keywords = {Time-series data, Sequential three-way decisions, Dynamic time warping, Uncertainty},
abstract = {Dynamic time warping (DTW) algorithm is widely used in diversified applications due to its excellent anti-deformation and anti-interference in measuring time-series based similarity. However, the high time complexity of DTW restrains the applicability of real-time case. The existing DTW acceleration studies suffer from a loss of accuracy. How to accelerate computation while maintaining satisfying computational accuracy remains challenging. Motivated by sequential three-way decisions, this paper develops a novel model with adaptive sequential three-way decisions for dynamic time warping (AS3-DTW). Firstly, we systematically summarize distance differences under the context of adjacent tripartite search spaces for DTW, and propose five patterns of granularity adjustments of the search spaces. Furthermore, we present the corresponding calculation method of DTW adjacent tripartite search spaces distances difference. Finally, we construct a novel dynamism on adaptively adjusting time warping by combining sequence-based multi-granularity with sequential three-way decisions. Experimental results show that AS3-DTW effectively achieves promising trade-off between computational speed and accuracy on multiple UCR datasets when compared with the state-of-the-art algorithms.}
}
@article{KHAZAEI2025115420,
title = {Renewable energy portfolio in Mexico for Industry 5.0 and SDGs: Hydrogen, wind, or solar?},
journal = {Renewable and Sustainable Energy Reviews},
volume = {213},
pages = {115420},
year = {2025},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2025.115420},
url = {https://www.sciencedirect.com/science/article/pii/S1364032125000930},
author = {Moein Khazaei and Fatemeh Gholian-Jouybari and Mahdi {Davari Dolatabadi} and Aryan {Pourebrahimi Alamdari} and Hamidreza Eskandari and Mostafa Hajiaghaei-Keshteli},
keywords = {Renewable energies, Industry 5.0, Sustainable development goals, Portfolio selection, Nearshoring},
abstract = {Despite a surge in Foreign Direct Investment (FDI) in Mexico, like nearshoring, the slow growth in international investment in renewables challenges the country's progress in achieving Sustainable Development Goals (SDGs) related to clean energy. To the best of current knowledge, this research is one of the first to explore the integration of renewable energy (Green/Blue/Turquoise Hydrogen, Solar, and Wind plants) in Mexico, emphasizing a diverse portfolio of projects aligned with SDGs and Industry 5.0. While previous works have focused on the nexus between energy, Industry 4.0, and sustainability, the present study advances this discourse by incorporating Industry 5.0 principles and a comprehensive methodological approach. Through a comprehensive methodology involving Value-Focused Thinking (VFT), fuzzy Decision-Making Trial and Evaluation Laboratory (DEMATEL), and multi-objective mathematical programming, the study identifies key criteria encompassing social, economic, environmental, and technological dimensions. The resulting criteria form a robust framework for evaluating project sustainability. The fuzzy DEMATEL analysis reveals intricate interrelations among criteria, emphasizing the need for balanced considerations. Results highlighted job creation, income equality, and microfinance support as key social considerations, while energy-related criteria emphasized sustainable practices. The proposed multi-objective programming model and COmbined COmpromise SOlution (COCOSO) method facilitated the selection of eight projects, with one project as the top-ranked option across various scoring strategies. Overall, this research provides a nuanced roadmap for effective decision-making in renewable energy projects, offering insights into project strengths, weaknesses, and potential areas for improvement.}
}
@incollection{2004201,
title = {Chapter 6 Alternatives to purely Lagrangian computations},
editor = {Jonas A. Zukas},
series = {Studies in Applied Mechanics},
publisher = {Elsevier},
volume = {49},
pages = {201-250},
year = {2004},
booktitle = {Introduction to Hydrocodes},
issn = {0922-5382},
doi = {https://doi.org/10.1016/S0922-5382(04)80007-2},
url = {https://www.sciencedirect.com/science/article/pii/S0922538204800072},
abstract = {Publisher Summary
Lagrangian techniques deal with problems involving fast and transient loading. Lagrangian methods offer several advantages over the competition. Because Lagrangian codes cannot solve all the problems involving fast short-duration loading, other techniques shoulb be mentioned. The chapter describes the popular alternative methods, Euler codes, coupled Euler-Lagrange codes, arbitrary Lagrange-Euler (ALE) techniques, and meshless methods The advantages of Lagrange codes are offset by grid distortion. With large distortions, the time increment for advancing the computations is forced to approach zero, thus rendering the calculations uneconomical. The use of sliding interfaces and rezoning can extend the range of applicability of Lagrange codes to larger distortions. Similarly, the ability to handle large distortions in Euler codes is offset by the need to account for material transport. Pure Euler techniques are ideal for handling large distortions.}
}
@article{HEIDELBERGER1996627,
title = {Accelerating mean time to failure computations},
journal = {Performance Evaluation},
volume = {27-28},
pages = {627-645},
year = {1996},
issn = {0166-5316},
doi = {https://doi.org/10.1016/S0166-5316(96)90049-8},
url = {https://www.sciencedirect.com/science/article/pii/S0166531696900498},
author = {Philip Heidelberger and Jogesh K. Muppala and Kishor S. Trivedi},
keywords = {Markov chains, Mean time to failure, Numerical methods},
abstract = {In this paper we consider the problem of numerical computation of the mean time to failure (MTTF) in Markovian dependability and/or performance models. The problem can be cast as a system of linear equations which is solved using an iterative method preserving sparsity of the Markov chain matrix. For highly dependable systems, system failure is a rare event and the above system solution can take an extremely large number of iterations. We propose to solve the problem by dividing the computation in two parts. First, by making some of the high probability states absorbing, we compute the MTTF of the modified Markov chain. In a subsequent step, by solving another system of linear equations, we are able to compute the MTTF of the original model. We prove that for a class of highly dependable systems, the resulting method can speed up computation of the MTTF by orders of magnitude. Experimental results supporting this claim are presented. We also obtain bounds on the convergence rate for computing the mean entrance time of a rare set of states in a class of queueing models.}
}
@article{WOLFF2024893,
title = {The mediodorsal thalamus in executive control},
journal = {Neuron},
volume = {112},
number = {6},
pages = {893-908},
year = {2024},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2024.01.002},
url = {https://www.sciencedirect.com/science/article/pii/S0896627324000023},
author = {Mathieu Wolff and Michael M. Halassa},
keywords = {thalamus, mediodorsal, prefrontal cortex, cognition, cognitive flexibility, computation, neural architectures},
abstract = {Summary
Executive control, the ability to organize thoughts and action plans in real time, is a defining feature of higher cognition. Classical theories have emphasized cortical contributions to this process, but recent studies have reinvigorated interest in the role of the thalamus. Although it is well established that local thalamic damage diminishes cognitive capacity, such observations have been difficult to inform functional models. Recent progress in experimental techniques is beginning to enrich our understanding of the anatomical, physiological, and computational substrates underlying thalamic engagement in executive control. In this review, we discuss this progress and particularly focus on the mediodorsal thalamus, which regulates the activity within and across frontal cortical areas. We end with a synthesis that highlights frontal thalamocortical interactions in cognitive computations and discusses its functional implications in normal and pathological conditions.}
}
@article{MERRITT2024103670,
title = {Igniting kid power: The impact of environmental service-learning on elementary students' awareness of energy problems and solutions},
journal = {Energy Research & Social Science},
volume = {116},
pages = {103670},
year = {2024},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2024.103670},
url = {https://www.sciencedirect.com/science/article/pii/S2214629624002615},
author = {Eileen G. Merritt and Andrea E. Weinberg and Candace Lapan and Sara E. Rimm-Kaufman},
keywords = {Environmental service-learning, Energy literacy, Elementary students, Science education, Randomized controlled trial},
abstract = {Energy concepts are taught in many schools, but children rarely have an opportunity to grapple with energy problems and work on their own solutions. This study explores the impacts of Connect Science, a service-learning (SL) program developed to enhance elementary students' energy literacy in the United States. Program impacts were explored within the context of a randomized controlled trial. Teachers in the SL intervention group were provided with professional development, coaching and curricular materials. Each fourth grade class chose an energy problem to address, and designed projects to test out a solution. Teachers in a waitlist control group taught their typical energy unit. Upon completion of the unit, students were asked to write about a problem related to energy production or use and propose a potential solution. Inductive content analysis was used to code 703 student responses (377 from control group and 326 from SL group). The majority of students expressed concerns about wasting or using too much electricity or the use of nonrenewable energy sources. Solutions focused on energy conservation and the use of renewable or clean resources were mentioned most frequently overall. Students in the SL group were significantly more likely to mention environmental impacts of various energy sources and to suggest energy conservation solutions or educating others. Conversely, the control group student responses more often focused on electric circuits or electrical safety. Results from this study suggest the promise of environmental SL programs to advance energy literacy and promote critical thinking about how to address energy problems.}
}
@article{HIPOLITO2017432,
title = {Mind-life continuity: A qualitative study of conscious experience},
journal = {Progress in Biophysics and Molecular Biology},
volume = {131},
pages = {432-444},
year = {2017},
note = {Integral Biomathics 2017: The Necessary Conjunction of Western and Eastern Thought Traditions for Exploring the Nature of Mind and Life},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2017.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0079610717301165},
author = {Inês Hipólito and Jorge Martins},
keywords = {Conscious experience, Qualitative study, Meditation, , Mind-life continuity thesis},
abstract = {There are two fundamental models to understanding the phenomenon of natural life. One is the computational model, which is based on the symbolic thinking paradigm. The other is the biological organism model. The common difficulty attributed to these paradigms is that their reductive tools allow the phenomenological aspects of experience to remain hidden behind yes/no responses (behavioral tests), or brain ‘pictures’ (neuroimaging). Hence, one of the problems regards how to overcome methodological difficulties towards a non-reductive investigation of conscious experience. It is our aim in this paper to show how cooperation between Eastern and Western traditions may shed light for a non-reductive study of mind and life. This study focuses on the first-person experience associated with cognitive and mental events. We studied phenomenal data as a crucial fact for the domain of living beings, which, we expect, can provide the ground for a subsequent third-person study. The intervention with Jhana meditation, and its qualitative assessment, provided us with experiential profiles based upon subjects' evaluations of their own conscious experiences. The overall results should move towards an integrated or global perspective on mind where neither experience nor external mechanisms have the final word.}
}
@article{HARDING2024295,
title = {A new predictive coding model for a more comprehensive account of delusions},
journal = {The Lancet Psychiatry},
volume = {11},
number = {4},
pages = {295-302},
year = {2024},
issn = {2215-0366},
doi = {https://doi.org/10.1016/S2215-0366(23)00411-X},
url = {https://www.sciencedirect.com/science/article/pii/S221503662300411X},
author = {Jessica Niamh Harding and Noham Wolpe and Stefan Peter Brugger and Victor Navarro and Christoph Teufel and Paul Charles Fletcher},
abstract = {Summary
Attempts to understand psychosis—the experience of profoundly altered perceptions and beliefs—raise questions about how the brain models the world. Standard predictive coding approaches suggest that it does so by minimising mismatches between incoming sensory evidence and predictions. By adjusting predictions, we converge iteratively on a best guess of the nature of the reality. Recent arguments have shown that a modified version of this framework—hybrid predictive coding—provides a better model of how healthy agents make inferences about external reality. We suggest that this more comprehensive model gives us a richer understanding of psychosis compared with standard predictive coding accounts. In this Personal View, we briefly describe the hybrid predictive coding model and show how it offers a more comprehensive account of the phenomenology of delusions, thereby providing a potentially powerful new framework for computational psychiatric approaches to psychosis. We also make suggestions for future work that could be important in formalising this novel perspective.}
}
@article{YAN2025109327,
title = {An approach to calculate conceptual distance across multi-granularity based on three-way partial order structure},
journal = {International Journal of Approximate Reasoning},
volume = {177},
pages = {109327},
year = {2025},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2024.109327},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X24002147},
author = {Enliang Yan and Pengfei Zhang and Tianyong Hao and Tao Zhang and Jianping Yu and Yuncheng Jiang and Yuan Yang},
keywords = {Partial order structure, Concept-cognitive learning, Knowledge distance, Three-way decision, Granular computing, Concept graph},
abstract = {The computation of concept distances aids in understanding the interrelations among entities within knowledge graphs and uncovering implicit information. The existing studies predominantly focus on the conceptual distance of specific hierarchical levels without offering a unified framework for comprehensive exploration. To overcome the limitations of unidimensional approaches, this paper proposes a method for calculating concept distances at multiple granularities based on a three-way partial order structure. Specifically: (1) this study introduces a methodology for calculating inter-object similarity based on the three-way attribute partial order structure (APOS); (2) It proposes the application of the similarity matrix to delineate the structure of categories; (3) Based on the similarity matrix describing the three-way APOS of categories, we establish a novel method for calculating inter-category distance. The experiments on eight datasets demonstrate that this approach effectively differentiates various concepts and computes their distances. When applied to classification tasks, it exhibits outstanding performance.}
}
@article{KELLEY2021439,
title = {Applying Independent Core Observer Model Cognitive Architecture to a Collective Intelligence System},
journal = {Procedia Computer Science},
volume = {190},
pages = {439-449},
year = {2021},
note = {2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.06.052},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921012977},
author = {David Kelley},
keywords = {Collective Intelligence Systems, Independent Core Observer Model, Artificial General Intelligence, mediated Artificial Superintelligence, Hive Mind, AGI, ICOM, mASI.},
abstract = {This paper shows how the Independent Core Observer Model (ICOM) Cognitive Architecture for Artificial General Intelligence (AGI) can be applied to building a collective intelligence system called a mediated Artificial Superintelligence (mASI). The details include breaking down the ICOM implementation in the form of the mASI system and the general performance of initial studies with the mASI. Details of the primary difference between the Independent Core Observer Model Cognitive Architecture and the mASI architecture variant include inserting humanity in the contextual engine components of ICOM, creating a type of collective intelligence. Humans can ‘mediate’ new system-generated thinking keeping the thought process accessible and slow enough for humans to oversee and understand. This also allows the modification of emotional valences of the thought process of the mASI system to help the system generate complex contextual models (knowledge graphs) of new ideas and which speeds up the learning process. With the humans acting as control rods in a reactor and emotional drivers, the mASI system maintains safety where the system would cease to function if humans walked away.}
}
@article{ACISECHE2023109386,
title = {A perspective on the sharing of docking data},
journal = {Data in Brief},
volume = {49},
pages = {109386},
year = {2023},
issn = {2352-3409},
doi = {https://doi.org/10.1016/j.dib.2023.109386},
url = {https://www.sciencedirect.com/science/article/pii/S2352340923004985},
author = {Samia Aci-Sèche and Stéphane Bourg and Pascal Bonnet and Joseph Rebehmed and Alexandre G. {de Brevern} and Julien Diharce},
keywords = {3D coordinates, Docking, Files, SDF, Sharing, FAIR principles},
abstract = {Computational approaches are nowadays largely applied in drug discovery projects. Among these, molecular docking is the most used for hit identification against a drug target protein. However, many scientists in the field shed light on the lack of availability and reproducibility of the data obtained from such studies to the whole community. Consequently, sustaining and developing the efforts toward a large and fully transparent sharing of those data could be beneficial for all researchers in drug discovery. The purpose of this article is first to propose guidelines and recommendations on the appropriate way to conduct virtual screening experiments and second to depict the current state of sharing molecular docking data. In conclusion, we have explored and proposed several prospects to enhance data sharing from docking experiment that could be developed in the foreseeable future.}
}