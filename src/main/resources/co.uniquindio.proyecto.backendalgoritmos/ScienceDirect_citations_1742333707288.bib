@article{HUANG1993717,
title = {An investigation of gender differences in cognitive abilities among Chinese high school students},
journal = {Personality and Individual Differences},
volume = {15},
number = {6},
pages = {717-719},
year = {1993},
issn = {0191-8869},
doi = {https://doi.org/10.1016/0191-8869(93)90012-R},
url = {https://www.sciencedirect.com/science/article/pii/019188699390012R},
author = {Jiafen Huang},
abstract = {This study investigated gender differences in 11 cognitive tests from a sample of grade 11 students in Shanghai, China. Research found that the girls outperformed boys significantly on Word Knowledge and Word Span tasks, and also on a Computational Speed and Accuracy test. Boys outperformed girls only on the Paper Folding test. Factor based scores showed that girls were superior to boys on memory, and verbal composites, whereas boys were superior to girls on the spatial composites. No gender differences were found on the Mathematical Thinking test and other reasoning tests. The research findings seemed to suggest that where the social conditions were more uniform the gender differences on visual-spatial and mathematical reasoning skills would be smaller.}
}
@article{NAVLAKHA201864,
title = {Network Design and the Brain},
journal = {Trends in Cognitive Sciences},
volume = {22},
number = {1},
pages = {64-78},
year = {2018},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2017.09.012},
url = {https://www.sciencedirect.com/science/article/pii/S1364661317302000},
author = {Saket Navlakha and Ziv Bar-Joseph and Alison L. Barth},
abstract = {Neural circuits have evolved to accommodate similar information processing challenges as those faced by engineered systems. Here, we compare neural versus engineering strategies for constructing networks. During circuit development, synapses are overproduced and then pruned back over time, whereas in engineered networks, connections are initially sparse and are then added over time. We provide a computational perspective on these two different approaches, including discussion of how and why they are used, insights that one can provide the other, and areas for future joint investigation. By thinking algorithmically about the goals, constraints, and optimization principles used by neural circuits, we can develop brain-derived strategies for enhancing network design, while also stimulating experimental hypotheses about circuit development and function.}
}
@incollection{WANG2001297,
title = {Computational Intelligence in Agile Manufacturing Engineering},
editor = {A. Gunasekaran},
booktitle = {Agile Manufacturing: The 21st Century Competitive Strategy},
publisher = {Elsevier Science Ltd},
address = {Oxford},
pages = {297-315},
year = {2001},
isbn = {978-0-08-043567-1},
doi = {https://doi.org/10.1016/B978-008043567-1/50016-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780080435671500164},
author = {Kesheng Wang}
}
@article{GEMMELL201720,
title = {Establishing the structures within populations of models},
journal = {Progress in Biophysics and Molecular Biology},
volume = {129},
pages = {20-24},
year = {2017},
note = {Validation of Computer Modelling},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2017.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0079610716300128},
author = {Philip M. Gemmell},
abstract = {As computational biology matures as a field, increasing attention is being paid to the relation of computational models to their target. One aspect of this is addressing how computational models can appropriately reproduce the variation seen in experimental data, with one solution being to use populations of models united by a common set of equations (the framework), with each individual member of the population (each model) possessing its own unique set of equation parameters. These model populations are then calibrated and validated against experimental data, and as a whole reproduce the experimentally observed variation. The primary focus of validation thus becomes the population, with the individual models' validation seemingly deriving from their membership of this population. The role of individual models within the population is not clear, with uncertainty regarding the relationship between individual models and the population they make up. This work examines the role of models within the population, how they relate to the population they make up, and how both can be said to be validated in this context.}
}
@article{TURKSON2020110464,
title = {Sustainability assessment of energy production: A critical review of methods, measures and issues},
journal = {Journal of Environmental Management},
volume = {264},
pages = {110464},
year = {2020},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2020.110464},
url = {https://www.sciencedirect.com/science/article/pii/S0301479720303984},
author = {Charles Turkson and Adolf Acquaye and Wenbin Liu and Thanos Papadopoulos},
keywords = {Sustainability, Energy production, Systematic review, Systems thinking, Energy policy, Sustainability assessment},
abstract = {Sustainable operations of energy production systems have become an increasingly important policy agenda globally because of the massive pressure placed on energy resources needed to support economic development and population growth. Due to the increasing research interest in examining the operational impacts of energy production systems on the society and the environment, this paper critically reviews the academic literature on the clean, affordable and secure supply of energy focussing on methods of assessments, measures of sustainability and emerging issues in the literature. While there have been some surveys on the sustainability of energy production systems they have either tended to focus on one assessment approach or one type of energy generation technology. This study builds on previous studies by providing a broader and comprehensive examination of the literature across generation technologies and assessment methods. A systematic review of 128 scholarly articles covering a 20-year period, ending 2018, and gathered from ProQuest, Scopus, and manual search is conducted. Synthesis and critical evaluation of the reviewed papers highlight a number of research gaps that exist within the sustainable energy production systems research domain. In addition, using mapping and cluster analyses, the paper visually highlights the network of dominant research issues, which emerged from the review.}
}
@article{PARKER20161,
title = {Coastal planning should be based on proven sea level data},
journal = {Ocean & Coastal Management},
volume = {124},
pages = {1-9},
year = {2016},
issn = {0964-5691},
doi = {https://doi.org/10.1016/j.ocecoaman.2016.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0964569116300205},
author = {A. Parker and C.D. Ollier},
keywords = {Sea level, Measurements, Computations, Tide gauges, Coastal management},
abstract = {There are two related measures of sea level, the absolute sea level, which is the increase in the sea level in an absolute reference frame, and relative sea level, which is the increase in sea level recorded by tide gauges. The first measure is a rather abstract computation, far from being reliable, and is preferred by activists and politicians for no scientific reason. For local and global problems it is better to use local tide gauge data. Proper coastal management should be based on proved measurements of sea level. Tide gauges provide the most reliable measurements, and best data to assess the rate of change. We show as the naïve averaging of all the tide gauges included in the PSMSL surveys show “relative” rates of rise about +1.04 mm/year (570 tide gauges of any length). If we consider only 100 tide gauges with more than 80 years of recording the rise is only +0.25 mm/year. This naïve averaging has been stable and shows that the sea levels are slowly rising but not accelerating. We also show as the additional information provided by GPS and satellite altimetry is of very little help. Computations of “absolute” sea levels suffer from inaccuracies with errors larger than the estimated trends. The GPS is more reliable than satellite altimetry, but the accuracy of the estimation of the vertical velocity at GPS domes is still well above ±1 mm/year and the relative motion of tide gauges vs. GPS domes is mostly unassessed. The satellite altimetry returns a noisy signal so that a +3.2 mm/year trend is only achieved by arbitrary “corrections”. We conclude that if the sea levels are only oscillating about constant trends everywhere as suggested by the tide gauges, then the effects of climate change are negligible, and the local patterns may be used for local coastal planning without any need of purely speculative global trends based on emission scenarios. Ocean and coastal management should acknowledge all these facts. As the relative rates of rises are stable worldwide, coastal protection should be introduced only where the rate of rise of sea levels as determined from historical data show a tangible short term threat. As the first signs the sea levels will rise catastrophically within few years are nowhere to be seen, people should start really thinking about the warnings not to demolish everything for a case nobody knows will indeed happen.}
}
@article{MARTINS20183890,
title = {2MBio, a novel tool to encourage creative participatory conceptual design of bioenergy systems – The case of wood fuel energy systems in south Mozambique},
journal = {Journal of Cleaner Production},
volume = {172},
pages = {3890-3906},
year = {2018},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2017.05.062},
url = {https://www.sciencedirect.com/science/article/pii/S0959652617309873},
author = {Ricardo Martins and Judith A. Cherni and Nuno Videira},
keywords = {Design thinking, Systems thinking, Mozambique, Participatory design tools, Wood fuel energy systems, Bioenergy},
abstract = {This paper proposes a new conceptual design tool for bioenergy systems, the 2MBio, and its implementation on the case of wood fuel energy systems (WES) in South Mozambique. Dependence on wood fuel characterises most Sub-Saharan countries and WES are complex socio-ecological systems dynamically linked to crucial development issues, e.g., deforestation and poverty. In Mozambique WES supply over 70% of the national energy needs through an informal business network worth around one million euros each year. In contrast with the 2MBio, currently available tools often aim at supporting decision-making on WES with off-the-shelf expert solutions and optimisation of WES efficiency, supply chains and resource management. While relevant and useful, such approaches are frequently unsuitable to engage the knowledge and creativity of a wide range of crucial actors. The 2MBio addresses this gap providing a simple, visual platform on paper that supports from illiterate to professional users, to stimulate creative ideas and apply current knowledge while designing their own WES. The results of implementation in real settings in South Mozambique produced relevant design breakthroughs. Compared with the absence of any other support tool, and faced with same design challenges, the 2MBio participatory design workshops in south Mozambique resulted in comprehensive analysis of wood fuel energy systems, and innovative integrated WES solutions design. The proposed approach raised participants’ awareness about opportunities and constrains linked to their WES while also facilitating information sharing new learning dynamics and enhance creativity.}
}
@article{DIMAKOU2025,
title = {The predictive nature of spontaneous brain activity across scales and species},
journal = {Neuron},
year = {2025},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2025.02.009},
url = {https://www.sciencedirect.com/science/article/pii/S0896627325001278},
author = {Anastasia Dimakou and Giovanni Pezzulo and Andrea Zangrossi and Maurizio Corbetta},
keywords = {spontaneous brain activity, predictive brains, behavioral priors, task-rest similarity, metabolic priors},
abstract = {Summary
Emerging research suggests the brain operates as a “prediction machine,” continuously anticipating sensory, motor, and cognitive outcomes. Central to this capability is the brain's spontaneous activity—ongoing internal processes independent of external stimuli. Neuroimaging and computational studies support that this activity is integral to maintaining and refining mental models of our environment, body, and behaviors, akin to generative models in computation. During rest, spontaneous activity expands the variability of potential representations, enhancing the accuracy and adaptability of these models. When performing tasks, internal models direct brain regions to anticipate sensory and motor states, optimizing performance. This review synthesizes evidence from various species, from C. elegans to humans, highlighting three key aspects of spontaneous brain activity’s role in prediction: the similarity between spontaneous and task-related activity, the encoding of behavioral and interoceptive priors, and the high metabolic cost of this activity, underscoring prediction as a fundamental function of brains across species.}
}
@article{BIRO2015876,
title = {Measuring the Level of Algorithmic Skills at the End of Secondary Education in Hungary},
journal = {Procedia - Social and Behavioral Sciences},
volume = {176},
pages = {876-883},
year = {2015},
note = {International Educational Technology Conference, IETC 2014, 3-5 September 2014, Chicago, IL, USA},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.01.553},
url = {https://www.sciencedirect.com/science/article/pii/S187704281500590X},
author = {Piroska Biró and Mária Csernoch and János Máth and Kálmán Abari},
keywords = {level of digital thinking, algorithmic skills, school leaving exams in Informatics and Mathematics},
abstract = {Students starting their tertiary studies in Informatics are found to have a low level of algorithmic skills and understanding of programming, which leads to the high number of drop out students and failed semesters during their studies. The students’ low level of programming skills contrasts with their excellent results in the school leaving exams. To find out the reasons for this we have launched the TAaAS project (Testing Algorithmic and Application Skills), which focuses on the students’ algorithmic skills and programming ability in traditional and non-traditional programming environments. Our analyses proved that school leaving exams are not able to measure these abilities of the students, and beyond that, are not able to distinguish between the different levels of the students. Students are accepted into the universities and start their studies based on the misleading results of the school leaving exams.}
}
@article{KOK2016342,
title = {Crowd behavior analysis: A review where physics meets biology},
journal = {Neurocomputing},
volume = {177},
pages = {342-362},
year = {2016},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2015.11.021},
url = {https://www.sciencedirect.com/science/article/pii/S0925231215017403},
author = {Ven Jyn Kok and Mei Kuan Lim and Chee Seng Chan},
keywords = {Crowd behavior analysis, Biologically inspired, Physics-inspired, Computer vision, Survey},
abstract = {Although the traits emerged in a mass gathering are often non-deliberative, the act of mass impulse may lead to irrevocable crowd disasters. The two-fold increase of carnage in crowd since the past two decades has spurred significant advances in the field of computer vision, towards effective and proactive crowd surveillance. Computer vision studies related to crowd are observed to resonate with the understanding of the emergent behavior in physics (complex systems) and biology (animal swarm). These studies, which are inspired by biology and physics, share surprisingly common insights, and interesting contradictions. However, this aspect of discussion has not been fully explored. Therefore, this survey provides the readers with a review of the state-of-the-art methods in crowd behavior analysis from the physics and biologically inspired perspectives. We provide insights and comprehensive discussions for a broader understanding of the underlying prospect of blending physics and biology studies in computer vision.}
}
@article{SHIRALKAR2023100115,
title = {An intelligent method for supply chain finance selection using supplier segmentation: A payment risk portfolio approach},
journal = {Cleaner Logistics and Supply Chain},
volume = {8},
pages = {100115},
year = {2023},
issn = {2772-3909},
doi = {https://doi.org/10.1016/j.clscn.2023.100115},
url = {https://www.sciencedirect.com/science/article/pii/S2772390923000240},
author = {Kedar Shiralkar and Arunkumar Bongale and Satish Kumar and Anupkumar M. Bongale},
keywords = {Supply chain finance (SCF), Supplier segmentation, Supplier categorization, Risk portfolio model, Supply chain sustainability, Supplier relationship management, Modern portfolio theory, Trade credit, Factoring, Dynamic discounting},
abstract = {The COVID-19 pandemic-driven financial crisis grew significant interest among firms to adopt supply chain finance (SCF) to optimize working capital for the financial stability of the supply chain. However, it is impractical for firms with a diverse and extensive supplier base to strategize the SCF solutions for individual suppliers by assessing their financial risk. Hence, this study conceptualizes an intelligent method to demonstrate how supplier segmentation based on suppliers’ payment risk portfolios helps supply chain practitioners to assess suppliers’ financial risk and strategize manageable supply chain finance solutions for them. This method employs a stochastic optimization model to compute suppliers’ optimum payment risk portfolios and generate a supplier segmentation matrix to offer supply chain practitioners the cognitive ability to select appropriate SCF solutions for their suppliers. The proposed method can be implemented into an AI-driven explainable recommendation system to aid supply chain practitioners in applying smart strategic thinking in supply chain finance decision-making.}
}
@article{DEALMEIDA2022478,
title = {Assisting in the choice to fill a vacancy to compose the PROANTAR team: Applying VFT and the CRITIC-GRA-3N methodology},
journal = {Procedia Computer Science},
volume = {214},
pages = {478-486},
year = {2022},
note = {9th International Conference on Information Technology and Quantitative Management},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.11.202},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922019123},
author = {Isaque David Pereira {de Almeida} and Lucas Ramon dos Santos Hermogenes and Igor Pinheiro de Araújo Costa and Miguel Ângelo Lellis Moreira and Carlos Francisco Simões Gomes and Marcos {dos Santos} and David de Oliveira Costa and Ian José Agra Gomes},
keywords = {CRITIC-GRA-3N method, Brazilian Navy, COVID-19},
abstract = {Antarctica is the southernmost continent of our planet, and it has been verified as the coldest region on earth. The Brazilian Antarctic Program (PROANTAR) has as its main objective the promotion of high-quality scientific research in the Antarctic region, seeking to understand the events that occur there. PROANTAR, coordinated by the Navy Commander, has some sectors that are based in Brazil and others that are located in the Antarctic continent. The military that volunteers to occupy any vacancy that is allocated to that continent needs, besides passing through several pre-established criteria, to pass the selection process. The purpose of this article is to help the Naval Administration in the selection of volunteer officers to occupy a vacancy in the Antarctic continent. To obtain the alternatives, the officers that best fit the established vacancy, and the criteria to be evaluated, Value-Focused Thinking (VFT) was applied. Next, with all the necessary data, the CRITIC-GRA-3N method was used as a Multicriteria Decision Support (MDS) technique, the CRITIC-GRA-3N method, the CRITIC Importance Through Intercriteria Correlation (CRITIC) method to obtain the criteria weights and the Grey Relational Analysis (GRA) method, with three normalizations, to order the alternatives. At the end of the application of the methods, the article can generate five ordinations of the volunteer officers to occupy the vacancy offered in PROANTAR.}
}
@article{HABTEMARIAM1990653,
title = {Research in computational epidemiology},
journal = {Mathematical and Computer Modelling},
volume = {14},
pages = {653-658},
year = {1990},
issn = {0895-7177},
doi = {https://doi.org/10.1016/0895-7177(90)90263-M},
url = {https://www.sciencedirect.com/science/article/pii/089571779090263M},
author = {T. Habtemariam and D. Oryang and F. Gabreab and V. Robnett and G. Trammell},
abstract = {The emerging new area referred to as computational science or science done on a computer adds a third dimension to the traditional methods of theoretical and experimental approaches. Counterparts to computational science such as computational linguistice, computational engineering and others arc beginning to take roots. Naturally, new research paths and opportunities in computational epidemiology must also be explored. One of the major challenges in epidemiologic research is the issue of how to realistically and effectively handle complex bioepidemiologic dynamics involving interactions between humans or animals, etiological agents and the multiple array of environmental and socioeconomic determinants which affect these populations. To understand the behavior of such complex biological systems, it is useful to devise computer based simulation models. Computational epidemiologic approaches now provide alternative avenues to classical laboratory and/or field experimental methods. Systems which may be impractical because they are too large, or, not feasible because the cost is too prohibitive can now be simulated realistically. In the past obtaining solutions to biomathematical equations with any degree of complexity was impossible. However, the availability of powerful computers now makes the quantitative analysis of such systems feasible and indeed practical. With this in mind our research at Tuskegee University has focused on: a) Epidemiologic modelling and expert systems, and, b) Hypertext/hypermedia based epidemiologic knowledge management. The case studies for our research involve the bioepidemiologic dynamics of two complex host-parasite systems of trypanosoma and schistosoma. The ultimate goal is to develop resources and methodologies based on computational technology to advance epidemiologic research. The paper will address the methodological issues and findings as well as questions related to configuring an appropriate research workstation for computational epidemiology.}
}
@article{EDLA2015254,
title = {Is heart rate variability better than routine vital signs for prehospital identification of major hemorrhage?},
journal = {The American Journal of Emergency Medicine},
volume = {33},
number = {2},
pages = {254-261},
year = {2015},
issn = {0735-6757},
doi = {https://doi.org/10.1016/j.ajem.2014.11.046},
url = {https://www.sciencedirect.com/science/article/pii/S073567571400881X},
author = {Shwetha Edla and Andrew T. Reisner and Jianbo Liu and Victor A. Convertino and Robert Carter and Jaques Reifman},
abstract = {Objective
During initial assessment of trauma patients, metrics of heart rate variability (HRV) have been associated with high-risk clinical conditions. Yet, despite numerous studies, the potential of HRV to improve clinical outcomes remains unclear. Our objective was to evaluate whether HRV metrics provide additional diagnostic information, beyond routine vital signs, for making a specific clinical assessment: identification of hemorrhaging patients who receive packed red blood cell (PRBC) transfusion.
Methods
Adult prehospital trauma patients were analyzed retrospectively, excluding those who lacked a complete set of reliable vital signs and a clean electrocardiogram for computation of HRV metrics. We also excluded patients who did not survive to admission. The primary outcome was hemorrhagic injury plus different PRBC transfusion volumes. We performed multivariate regression analysis using HRV metrics and routine vital signs to test the hypothesis that HRV metrics could improve the diagnosis of hemorrhagic injury plus PRBC transfusion vs routine vital signs alone.
Results
As univariate predictors, HRV metrics in a data set of 402 subjects had comparable areas under receiver operating characteristic curves compared with routine vital signs. In multivariate regression models containing routine vital signs, HRV parameters were significant (P < .05) but yielded areas under receiver operating characteristic curves with minimal, nonsignificant improvements (+0.00 to +0.05).
Conclusions
A novel diagnostic test should improve diagnostic thinking and allow for better decision making in a significant fraction of cases. Our findings do not support that HRV metrics add value over routine vital signs in terms of prehospital identification of hemorrhaging patients who receive PRBC transfusion.}
}
@article{SILVA2017137,
title = {Evaluating the usefulness of the structural accessibility layer for planning practice – Planning practitioners’ perception},
journal = {Transportation Research Part A: Policy and Practice},
volume = {104},
pages = {137-149},
year = {2017},
issn = {0965-8564},
doi = {https://doi.org/10.1016/j.tra.2017.05.014},
url = {https://www.sciencedirect.com/science/article/pii/S0965856417304755},
author = {Cecília Silva and Tiago Patatas and Ana Amante},
keywords = {Accessibility instrument, Implementation gap, Planning practice, Usefulness in practice},
abstract = {There has been a growing attention on accessibility concepts from both planning practice and research recognising their relevance in understanding the evolution of urban areas. However, despite the large number of accessibility measures available in the literature, they are not widely used to support urban planning practices. Much has been said about the implementation gap of Planning Support Systems with a significant attention paid to usability and more recently to the usefulness of Accessibility Instruments. The paper aims to assess the usefulness of a specific accessibility instrument – the Structural Accessibility Layer (SAL) – and by doing so exploring the strengths of accessibility instruments holding similar characteristics. To this end, we follow a multidimensional assessment framework under development in the Planning Support System literature. This paper explores the main findings of a workshop bringing together local planning practitioners and the developers of the SAL in an experiment using the SAL. The assessment of usefulness of SAL identified the instrument’s strengths with regard to insight into participants’ assumptions, communication, commitment and development of shared language. Regardless, the low fit between planning concerns of participants (in this case study context) and of the SAL seemed to limit its potential use in practice and as such undermines the strengths identified in the usefulness assessment. The assessment developed here only partially confirmed objectives and purposes defined for the SAL. Results confirm the usefulness of the SAL as diagnosis tool, however, the ability of the SAL to contribute to a joint thinking of land use and transport constraints on mobility was not confirmed. Finally, this research raises questions on the role of PSS in changing strategic thinking in planning and how this might conflict with the current PSS research concern in improving usefulness of tools.}
}
@article{SUN2025e01027,
title = {First-principles calculations of electronic and mechanical properties of magnesium indium intermetallic compounds},
journal = {Computational Condensed Matter},
volume = {43},
pages = {e01027},
year = {2025},
issn = {2352-2143},
doi = {https://doi.org/10.1016/j.cocom.2025.e01027},
url = {https://www.sciencedirect.com/science/article/pii/S2352214325000267},
author = {Liang Sun and Yidan Huang and Kaifeng Zhao and Zuoming Chen and Xiongtao Shang and Wenzhen Xu and wenyan Zhai and Pengyue Han and Jin Jia and Jianhong Peng},
keywords = {Mg-In intermetallic compounds, First-principles calculations, Phonon spectra, Anisotropy, Mechanical properties, Electronic properties},
abstract = {In the search for innovative alternatives to aluminum-magnesium alloys, this study takes a unique approach by focusing on magnesium-indium binary alloys, with an emphasis on the intermetallic compounds Mg2In, MgIn3, Mg5In2, and Mg3In. With the help of cutting-edge first-principles computational techniques, the four compounds are comprehensively and thoroughly analyzed in terms of crystal structure, anisotropy, phonon spectra, electronic properties, and mechanical properties. The charge transfer phenomenon from magnesium to indium is found for the first time, and the s-orbital density of indium is at its peak in the Mg-In phase. In terms of mechanical properties, Mg2In, Mg5In2, and MgIn3 exhibit similar bulk moduli, while the shear modulus, Young's modulus, and hardness of MgIn3 are significantly lower than those of the other phases, emphasizing its unique deformability. Taking the results together, MgIn3 shows great potential for application in cutting-edge fields such as biomedical materials due to its compact size, corrosion resistance, low hardness, and high plasticity, which opens up a new way of thinking for the development of Mg-In alloy-based advanced materials.}
}
@article{OZENCIRA2023101273,
title = {Mapping research on musical creativity: A bibliometric review of the literature from 1990 to 2022},
journal = {Thinking Skills and Creativity},
volume = {48},
pages = {101273},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101273},
url = {https://www.sciencedirect.com/science/article/pii/S1871187123000433},
author = {Gözde Ozenc-Ira},
keywords = {Musical creativity, Creativity, Bibliometric review, Science mapping, VOSviewer},
abstract = {This study aims to map the research literature on musical creativity that was published from 1990 to 2022 by using metadata extracted from 1,177 Web of Science-indexed publications in terms of trends in publications and citations data, leading journals, authors, institutions/organizations, and countries, collaborative networks between authors, institutions, and countries, and trends in keyword frequencies and co-occurrences. The main findings of this study are that (1) research on musical creativity has undergone an incipient phase and has had a growing scientific interest since the mid-2000s, (2) musical creativity is a relatively more specific research field compared to general creativity research that has been represented by more specific sub-fields, e.g., music psychology and ethnomusicology, (3) a small number of scholars – especially from the USA, England, Russia, Spain, Australia, and some countries from South Europe – have made the more impactful contribution as regards musical creativity, (4) there is a small number of research collaborations among scholars, yet the collaborative networks among countries and institutions occur intercontinentally, (5) musical creativity research is growing with cross-disciplinary links with several branches of psychology, neurosciences, cognitive sciences, education, sociology, arts and humanities, and computer sciences, and (6) eight main topical foci have been founded in the literature from 1990 to date – i.e., computational creativity, processes of improvisation, improvisation teaching and learning, interactions/collaboration during improvisation, effects of improvisation practice, innovative music technology, esthetic aspect of everyday creativity, and music therapy. Further research on musical creativity could map the literature by focusing on contextual themes.}
}
@article{ZHANG201499,
title = {Profiles of psychiatric symptoms among amphetamine type stimulant and ketamine using inpatients in Wuhan, China},
journal = {Journal of Psychiatric Research},
volume = {53},
pages = {99-102},
year = {2014},
issn = {0022-3956},
doi = {https://doi.org/10.1016/j.jpsychires.2014.02.010},
url = {https://www.sciencedirect.com/science/article/pii/S0022395614000508},
author = {Yao Zhang and Zaifeng Xu and Sheng Zhang and Alethea Desrosiers and Richard S. Schottenfeld and Marek C. Chawarski},
keywords = {Amphetamine type stimulants (ATS), Ketamine, Psychiatric symptoms},
abstract = {Amphetamine type stimulants (ATS) and ketamine have emerged as major drug problems in China, and chronic extensive exposure to these substances frequently co-occurs with psychiatric symptoms. This study compares the psychiatric symptoms of patients reporting ATS use only, ATS and ketamine use, or ketamine use only who were admitted to an inpatient psychiatry ward in Wuhan, China between 2010 and 2011. Data on 375 study participants collected during their ward admission and extracted from their clinical records included their socio-demographics, scores on the Brief Psychiatric Rating Scale (BPRS), and urine toxicology screens.
Results
The ketamine-only group had significantly lower total BPRS scores and significantly lower scores on Thinking Disorder, Activity, and Hostility-Suspicion BPRS subscales than the ATS-only and ATS + ketamine groups (p < 0.001 for all comparisons). The ketamine-only group also had significantly higher scores on the subscales of Anxiety-Depression and Anergia. The ATS-only group had significantly higher scores on subscales of Thinking Disorder, Activity, and Hostility-Suspicion and significantly lower scores on Anxiety-Depression and Anergia subscales than the ketamine-only and ATS + ketamine groups (p < 0.001 for all comparisons). A K-means cluster method identified three distinct clusters of patients based on the similarities of their BPRS subscale profiles, and the identified clusters differed markedly on the proportions of participants reporting different primary drugs of abuse. The study findings suggest that ketamine and ATS users present with different profiles of psychiatric symptoms at admission to inpatient treatment.}
}
@article{GRAGERT199711,
title = {Differential geometric computations and computer algebra},
journal = {Mathematical and Computer Modelling},
volume = {25},
number = {8},
pages = {11-24},
year = {1997},
issn = {0895-7177},
doi = {https://doi.org/10.1016/S0895-7177(97)00055-1},
url = {https://www.sciencedirect.com/science/article/pii/S0895717797000551},
author = {P.K.H Gragert and P.H.M Kersten},
keywords = {Computer algebra, Differential geometry, Literate programming, Supersymmetry},
abstract = {The use of computer algebra in the field of differential geometry and its applications to geometric structures of partial differential equations is discussed. The differential geometric setting is shortly described; a number of programs are slightly touched, some examples given, and an application to the construction of supersymmetric extensions of the Korteweg-de Vries equation is demonstrated.}
}
@article{STORAASLI1993349,
title = {Computational mechanics analysis tools for parallel-vector supercomputers},
journal = {Computing Systems in Engineering},
volume = {4},
number = {4},
pages = {349-354},
year = {1993},
note = {Parallel Computational Methods for Large-Scale Structural Analysis and Design},
issn = {0956-0521},
doi = {https://doi.org/10.1016/0956-0521(93)90002-E},
url = {https://www.sciencedirect.com/science/article/pii/095605219390002E},
author = {O.O. Storaasli and D.T. Nguyen and M.A. Baddourah and J. Qin},
abstract = {Computational algorithms for structural analysis on parallel-vector supercomputers are reviewed. These parallel algorithms, developed by the authors, are for the assembly of structural equations, “out-of-core” strategies for linear equation solution, massively distributed-memory equation solution, unsymmetric equation solution, general eigen-solution, geometrically nonlinear finite element analysis, design sensitivity analysis for structural dynamics, optimization algorithm and domain decomposition. The source code for many of these algorithms is available from NASA Langley.}
}
@article{GENTILI2024150060,
title = {Living cells and biological mechanisms as prototypes for developing chemical artificial intelligence},
journal = {Biochemical and Biophysical Research Communications},
volume = {720},
pages = {150060},
year = {2024},
issn = {0006-291X},
doi = {https://doi.org/10.1016/j.bbrc.2024.150060},
url = {https://www.sciencedirect.com/science/article/pii/S0006291X24005965},
author = {Pier Luigi Gentili and Pasquale Stano},
keywords = {Chemical AI, Synthetic cell, Chemical neural networks, Neuromorphic engineering, Molecular fuzzy sets, Molecular computing},
abstract = {Artificial Intelligence (AI) is having a revolutionary impact on our societies. It is helping humans in facing the global challenges of this century. Traditionally, AI is developed in software or through neuromorphic engineering in hardware. More recently, a brand-new strategy has been proposed. It is the so-called Chemical AI (CAI), which exploits molecular, supramolecular, and systems chemistry in wetware to mimic human intelligence. In this work, two promising approaches for boosting CAI are described. One regards designing and implementing neural surrogates that can communicate through optical or chemical signals and give rise to networks for computational purposes and to develop micro/nanorobotics. The other approach concerns “bottom-up synthetic cells” that can be exploited for applications in various scenarios, including future nano-medicine. Both topics are presented at a basic level, mainly to inform the broader audience of non-specialists, and so favour the rise of interest in these frontier subjects.}
}
@article{LIU2024111728,
title = {DuaPIN: Auxiliary task enhanced dual path interaction network for civil court view generation},
journal = {Knowledge-Based Systems},
volume = {295},
pages = {111728},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111728},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124003630},
author = {Nayu Liu and Luyao Ma and Yiquan Wu and Kaiwen Wei and Cunhang Fan and Yating Zhang},
keywords = {Dual path interaction network, Auxiliary task, Civil court view generation, Natural language processing},
abstract = {Civil court view generation (CCVG) is a novel but important task for legal intelligence that aims to automatically generate a judge’s opinion based on the plaintiff’s claims and fact descriptions to interpret the judgment result. The task is more challenging than criminal court view generation as the latter generates views based only on criminal facts as input, whereas the CCVG must consider both the plaintiff’s claims and civil facts under the principle of “no claim, no trial.” However, current approaches still follow criminal domain practices to solve problems in civil cases. Moreover, the explicit modeling of the potential correspondence between claims and facts has often been neglected, as court views are required to respond to each corresponding claim based on factual evidence. To address the issues, we propose a dual path interaction network augmented by two self-supervised auxiliary tasks (named DuaPIN), which follows a bionic design by simulating the thinking logic of judges when writing opinions. Specifically, we construct a structurally symmetric Transformer-based dual path multi-encoder–decoder model such that the two inputs, claim and fact, contribute equally to the generation of civil court views. Moreover, an auxiliary task enhanced (ATE) training paradigm using multiple DuaPIN decoders is proposed to explicitly model the potential correspondence between claims and facts. Extensive experiments on public legal document dataset demonstrated that DuaPIN achieves competitive performance compared with previous methods and offers certain performance improvements to popular pre-trained language models via the ATE training method.}
}
@article{PIERONI2016412,
title = {Transforming a Traditional Product Offer into PSS: A Practical Application},
journal = {Procedia CIRP},
volume = {47},
pages = {412-417},
year = {2016},
note = {Product-Service Systems across Life Cycle},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2016.03.036},
url = {https://www.sciencedirect.com/science/article/pii/S2212827116300051},
author = {Marina Pieroni and Caio Marques and Carina Campese and Daniel Guzzo and Glauco Mendes and Janaína Costa and Maiara Rosa and Maicon Gouveia de Oliveira and Victor Macul and Henrique Rozenfeld},
keywords = {product-service system, servitization, business model, design thinking, practical application, action research},
abstract = {In the last decades, companies have shifted from traditional business models based on selling products to product-service systems (PSS). Despite this tendency, there is a paucity of complete methodologies and tools to guide companies on how the transition should occur. To address this issue, the goal of this research is to present a complete framework to support manufacturing companies in the servitization journey. This novel proposal involves the application of design thinking to define the value proposition integrated with a PSS oriented business model creation, that goes beyond generic methods normally applied; and the specification of business process architecture to support PSS implementation. This research followed a prescriptive approach by means of action research technique. Key findings of the framework application are presented.}
}
@article{HANNA2025100705,
title = {Future of Artificial Intelligence—Machine Learning Trends in Pathology and Medicine},
journal = {Modern Pathology},
volume = {38},
number = {4},
pages = {100705},
year = {2025},
issn = {0893-3952},
doi = {https://doi.org/10.1016/j.modpat.2025.100705},
url = {https://www.sciencedirect.com/science/article/pii/S0893395225000018},
author = {Matthew G. Hanna and Liron Pantanowitz and Rajesh Dash and James H. Harrison and Mustafa Deebajah and Joshua Pantanowitz and Hooman H. Rashidi},
keywords = {artificial intelligence, computational pathology, machine learning, operations},
abstract = {Artificial intelligence (AI) and machine learning (ML) are transforming the field of medicine. Health care organizations are now starting to establish management strategies for integrating such platforms (AI-ML toolsets) that leverage the computational power of advanced algorithms to analyze data and to provide better insights that ultimately translate to enhanced clinical decision-making and improved patient outcomes. Emerging AI-ML platforms and trends in pathology and medicine are reshaping the field by offering innovative solutions to enhance diagnostic accuracy, operational workflows, clinical decision support, and clinical outcomes. These tools are also increasingly valuable in pathology research in which they contribute to automated image analysis, biomarker discovery, drug development, clinical trials, and productive analytics. Other related trends include the adoption of ML operations for managing models in clinical settings, the application of multimodal and multiagent AI to utilize diverse data sources, expedited translational research, and virtualized education for training and simulation. As the final chapter of our AI educational series, this review article delves into the current adoption, future directions, and transformative potential of AI-ML platforms in pathology and medicine, discussing their applications, benefits, challenges, and future perspectives.}
}
@incollection{AMJAD202559,
title = {Chapter 6 - Kinetics and dynamics of biological systems},
editor = {Babak Sokouti},
booktitle = {Systems Biology and In-Depth Applications for Unlocking Diseases},
publisher = {Academic Press},
pages = {59-67},
year = {2025},
isbn = {978-0-443-22326-6},
doi = {https://doi.org/10.1016/B978-0-443-22326-6.00006-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780443223266000067},
author = {Elham Amjad and Babak Sokouti},
keywords = {Biological systems, Computational methods, Dynamics, Enzyme kinetics, Simulations},
abstract = {Understanding the kinetics and dynamics of biological systems is crucial for elucidating the mechanisms that govern their behavior and function. This chapter discusses the fundamentals of kinetics, which deals with the rates of chemical reactions, and dynamics, which studies how biological systems change over time. Mathematical modeling approaches for analyzing kinetics and dynamics, including deterministic, stochastic, and hybrid models, are presented. Experimental techniques like transport measurements, calorimetry, fluorescence correlation spectroscopy, and neutron scattering are highlighted for investigating kinetics and dynamics. Computational methods such as molecular dynamics simulations and deep learning are also explored for studying biomolecular processes. Applications of kinetics and dynamics are illustrated through examples in areas like fuel-driven dynamic combinatorial libraries, self-assembly, and gene regulatory networks. Key discoveries enabled by kinetics and dynamics research are summarized, including dynamic heterogeneity, kinetic proofreading, biological switches, and single-molecule insights. The chapter emphasizes on the importance of integrating experimental data with computational modeling to deepen the understanding of biological systems across multiple scales. Future directions are outlined, such as studying interactomes, DNA mechanics, metabolic regulation, and developing advanced microtechnologies to further unravel the complex kinetic and dynamic behavior underlying diverse biological phenomena.}
}
@article{BLISS19921,
title = {Reasoning supported by computational tools},
journal = {Computers & Education},
volume = {18},
number = {1},
pages = {1-9},
year = {1992},
issn = {0360-1315},
doi = {https://doi.org/10.1016/0360-1315(92)90030-9},
url = {https://www.sciencedirect.com/science/article/pii/0360131592900309},
author = {Joan Bliss and Jon Ogborn and Richard Boohan and Jonathan Briggs and Tim Brosnan and Derek Brough and Harvey Mellar and Rob Miller and Caroline Nash and Cathy Rodgers and Babis Sakonidis},
abstract = {This paper sets out the work of the Tools for Exploratory Learning Programme within the ESRC Initiative Information Technology in Education. The research examines young secondary children's reasoning with computational tools. We distinguish between exploratory and expressive modes of learning, that is, interaction with another's model and creation of one's own model, respectively. The research focuses on reasoning, rather than learning, along three dimensions: quantitative, qualitative, and semi-quantitative. It provides a 3 × 2 classification of tasks according to modes of learning and types of reasoning. Modelling tools were developed for the study and descriptions of these are given. The research examined children's reasoning with tools in all three dimensions looking more exhaustively at the semi-quantitative. Pupils worked either in an exploratory mode or an expressive mode on one of the following topics: Traffic, Health and Diet, and Shops and Profits. They spent 3–4 h individually with a researcher over 2 weeks, carrying out four different activities: reasoning without the computer; learning to manipulate first the computer then later the tool and finally carrying out a task with the modelling tool. Pupils were between 12 and 14 yr. Research questions both about children's reasoning when working with or creating models and about the nature of the tools used are discussed. Finally an analytic scheme is set out which describes the nature of the causal and non-causal reasoning observed together with some tentative results.}
}
@article{MANDAVE2023100276,
title = {Bio-inspired computing algorithms in dementia diagnosis – a application-oriented review},
journal = {Results in Control and Optimization},
volume = {12},
pages = {100276},
year = {2023},
issn = {2666-7207},
doi = {https://doi.org/10.1016/j.rico.2023.100276},
url = {https://www.sciencedirect.com/science/article/pii/S2666720723000784},
author = {Deepa D. Mandave and Lalit V. Patil},
keywords = {Dementia, Biomotivated algorithms, Image segmentation, Meta-heuristic, Alzheimer, Optimization, Feature selection},
abstract = {Dementia is a major neurocognitive disease which affects memory, thinking skills, attitudes, and social behavior, extremely causing disturbances in daily routine activities and social activities. Alzheimer is the most general form of dementia in the elderly. Recently, biomotivated techniques have become famous in the domain of healthcare and have obtained appreciable success. This review shows that these techniques are mostly utilized to resolve various problems such as image segmentation, feature selection, classification, and optimization in the detection of various disorders like cancer, anemia, Alzheimer, kidney and skin diseases. It is observed that the dementia diagnosis was performed using classical approaches which led to reduced performance (accuracy, precision). This performance parameter can be enhanced by using biomotivated techniques. This paper presents a comprehensive analysis of the different role of biomotivated metaheuristics in the domain of dementia diagnosis with a detailed analysis of published work. The results showed that a biomotivated technique plays an important role in dementia diagnosis.}
}
@article{WASKAN2003259,
title = {Intrinsic cognitive models},
journal = {Cognitive Science},
volume = {27},
number = {2},
pages = {259-283},
year = {2003},
issn = {0364-0213},
doi = {https://doi.org/10.1016/S0364-0213(02)00119-2},
url = {https://www.sciencedirect.com/science/article/pii/S0364021302001192},
author = {Jonathan A Waskan},
keywords = {Philosophy, Artificial intelligence, Psychology, Representation, Philosophy of mind, Philosophy of computation, Causal reasoning, Knowledge representation, Computer simulation},
abstract = {Theories concerning the structure, or format, of mental representation should (1) be formulated in mechanistic, rather than metaphorical terms; (2) do justice to several philosophical intuitions about mental representation; and (3) explain the human capacity to predict the consequences of worldly alterations (i.e., to think before we act). The hypothesis that thinking involves the application of syntax-sensitive inference rules to syntactically structured mental representations has been said to satisfy all three conditions. An alternative hypothesis is that thinking requires the construction and manipulation of the cognitive equivalent of scale models. A reading of this hypothesis is provided that satisfies condition (1) and which, even though it may not fully satisfy condition (2), turns out (in light of the frame problem) to be the only known way to satisfy condition (3).}
}
@article{ERKELENS19982999,
title = {A computational model of depth perception based on headcentric disparity},
journal = {Vision Research},
volume = {38},
number = {19},
pages = {2999-3018},
year = {1998},
issn = {0042-6989},
doi = {https://doi.org/10.1016/S0042-6989(98)00084-4},
url = {https://www.sciencedirect.com/science/article/pii/S0042698998000844},
author = {Casper J. Erkelens and Raymond {van Ee}},
keywords = {Binocular vision, Stereopsis, Disparity, Binocular saccades},
abstract = {It is now well established that depth is coded by local horizontal disparity and global vertical disparity. We present a computational model which explains how depth is extracted from these two types of disparities. The model uses the two (one for each eye) headcentric directions of binocular targets, derived from retinal signals and oculomotor signals. Headcentric disparity is defined as the difference between headcentric directions of corresponding features in the left and right eye’s images. Using Helmholtz’s coordinate systems we decompose headcentric disparity into azimuthal and elevational disparity. Elevational disparities of real objects are zero if the signals which contribute to headcentric disparity do not contain any errors. Azimuthal headcentric disparity is a 1D quantity from which an exact equation relating distance and disparity can be derived. The equation is valid for all headcentric directions and for all binocular fixation positions. Such an equation does not exist if disparity is expressed in retinal coordinates. Possible types of errors in oculomotor signals (six) produce global elevational disparity fields which are characterised by different gradients in the azimuthal and elevational directions. Computations show that the elevational disparity fields uniquely characterise both the type and size of the errors in oculomotor signals. Our model uses a measure of the global elevational disparity field together with local azimuthal disparity to accurately derive headcentric distance throughout the visual field. The model explains existing data on whole-field disparity transformations as well as hitherto unexplained aspects of stereoscopic depth perception.}
}
@article{BEECH2023105401,
title = {Consequences of phonological variation for algorithmic word segmentation},
journal = {Cognition},
volume = {235},
pages = {105401},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2023.105401},
url = {https://www.sciencedirect.com/science/article/pii/S0010027723000355},
author = {Caroline Beech and Daniel Swingley},
keywords = {Language acquisition, Computational modeling, Word segmentation, Phonological variation},
abstract = {Over the first year, infants begin to learn the words of their language. Previous work suggests that certain statistical regularities in speech could help infants segment the speech stream into words, thereby forming a proto-lexicon that could support learning of the eventual vocabulary. However, computational models of word segmentation have typically been tested using language input that is much less variable than actual speech is. We show that using actual, transcribed pronunciations rather than dictionary pronunciations of the same speech leads to worse segmentation performance across models. We also find that phonologically variable input poses serious problems for lexicon building, because even correctly segmented word forms exhibit a complex, many-to-many relationship with speakers' intended words. Many phonologically distinct word forms were actually the same intended word, and many identical transcriptions came from different intended words. The fact that previous models appear to have substantially overestimated the utility of simple statistical heuristics suggests a need to consider the formation of the lexicon in infancy differently.}
}
@article{YANG2023414,
title = {A review of sequential three-way decision and multi-granularity learning},
journal = {International Journal of Approximate Reasoning},
volume = {152},
pages = {414-433},
year = {2023},
issn = {0888-613X},
doi = {https://doi.org/10.1016/j.ijar.2022.11.007},
url = {https://www.sciencedirect.com/science/article/pii/S0888613X2200192X},
author = {Xin Yang and Yanhua Li and Tianrui Li},
keywords = {Three-way decision, Granular computing, Sequential three-way decision, Three-way multi-granularity learning},
abstract = {The concept of three-way decision, interpreted and described as thinking, problem solving, and information processing in “threes”, has been widely studied and applied in machine learning and data engineering in recent years. In open-world environment, the connection and interaction of dynamic and uncertainty by multi-granularity learning gives more vitality to three-way decision. In this paper, we investigate and summarize the initial and development models of three-way decision. Then we revisit the historical line of sequential three-way decision from rough set to granular computing. Besides, we focus on exploring a unified framework of three-way multi-granularity learning with four crucial problems on mining uncertain region continually. Finally, we give some proposals on three-way decision associated with open-continual learning.}
}
@article{DELEON2003507,
title = {On the computation of the Lichnerowicz–Jacobi cohomology},
journal = {Journal of Geometry and Physics},
volume = {44},
number = {4},
pages = {507-522},
year = {2003},
issn = {0393-0440},
doi = {https://doi.org/10.1016/S0393-0440(02)00056-6},
url = {https://www.sciencedirect.com/science/article/pii/S0393044002000566},
author = {Manuel {de León} and Belén López and Juan C. Marrero and Edith Padrón},
keywords = {Jacobi manifolds, Poisson manifolds, Lie algebroids, Lichnerowicz–Jacobi cohomology, Contact manifolds, Locally conformal symplectic manifolds},
abstract = {Lichnerowicz–Jacobi cohomology of Jacobi manifolds is reviewed. The use of the associated Lie algebroid allows to prove that the Lichnerowicz–Jacobi cohomology is invariant under conformal changes of the Jacobi structure. We also compute the Lichnerowicz–Jacobi cohomology for a large variety of examples.}
}
@article{VANSANTEN19902001,
title = {Computational advances in catalyst modelling.},
journal = {Chemical Engineering Science},
volume = {45},
number = {8},
pages = {2001-2011},
year = {1990},
issn = {0009-2509},
doi = {https://doi.org/10.1016/0009-2509(90)80073-N},
url = {https://www.sciencedirect.com/science/article/pii/000925099080073N},
author = {R.A. {van Santen}},
keywords = {Molecular Catalysis, Theoretical Chemistry, Catalyst Modelling, Zeolite Stability, Theoretical Kinitics.},
abstract = {Fruitful theoretical approaches to predict catalyst stability, to simulate transition states or assist catalyst characterization become available due to the computational possibilities generated by supercomputers. Advances in theoretical chemistry and catalysis provide the conceptual framework that enables application in catalyst modelling. Especially in zeolite catalysis computational techniques are increasingly applied. Because of their well-defined structures they are very suitable for the application of graphics approaches. Techniques have been developed to determine interaction-potentials on the basis of quantumchemical cluster-calculations and to verify them by comparison with experimental and spectroscopic data. Stimulated by quantum chemical studies in chemisorption as well as organometallic chemistry, computational studies of reaction intermediates in homogeneous as well as heterogeneous catalytic reactions have been undertaken. The development of potential energy surface parametrization schemes is of importance to enable the application of molecular dynamics studies to catalyst stability and reactivity}
}
@article{HILLERT2021103158,
title = {How did language evolve in the lineage of higher primates?},
journal = {Lingua},
volume = {264},
pages = {103158},
year = {2021},
issn = {0024-3841},
doi = {https://doi.org/10.1016/j.lingua.2021.103158},
url = {https://www.sciencedirect.com/science/article/pii/S0024384121001303},
author = {Dieter Hillert},
keywords = {Broca’s area, Comparative studies, Homo erectus, Language capacity, Neural circuits, Prehistoric artefacts},
abstract = {Speech components emerged in the hominin lineage before the rise of modern human behavior and were already in place in monkey species. Evidence from genetics to archaeological records points to an accumulative increase of those computational properties required for modern language. At about 2.4 mya, the polytypical species Homo erectus sensu lato (s.l.) appeared with significant cortical growth indicated by neural migration factors and fossil skulls. The evidence suggests that early Homo erectus s.l. was equipped with a computational capacity for premodern language. The same species developed Acheulean toolmaking and showed signs of a symbolic and aesthetic mind at about half a mya. We conclude that the modern language capacity evolved at around 1 mya in the merging species late Homo erectus s.l. and pre-archaic Homo sapiens.}
}
@article{SCHACTER1999403,
title = {Computer-based performance assessments: a solution to the narrow measurement and reporting of problem-solving☆☆The findings and opinions expressed in this report do not reflect the position or policies of ISX, Advanced Research Projects Agency, the Department of the Navy, or the Department of Defense; nor do they reflect the positions or policies of the National Institute on Student Achievement, Curriculum, and Assessment, the Office of Educational Research and Improvement, or the US Department of Education.},
journal = {Computers in Human Behavior},
volume = {15},
number = {3},
pages = {403-418},
year = {1999},
issn = {0747-5632},
doi = {https://doi.org/10.1016/S0747-5632(99)00029-1},
url = {https://www.sciencedirect.com/science/article/pii/S0747563299000291},
author = {J. Schacter and H.E. Herl and G.K.W.K. Chung and R.A. Dennis and H.F. O'Neil},
keywords = {Assessment, Problem solving, Computers, Internet, Technology, Education},
abstract = {Although performance assessments test for higher order thinking and problem solving, they rarely report students' thinking process data back to teachers, students, or the public. Web-based database-backed performance assessments provide a viable means for concurrently reporting both performance and thinking process data. In the research conducted here, we report our findings from a study that assessed student problem solving using networked computers. Both performance and process data could be reported back to teachers and students such that they could diagnose and understand how they performed and what problem-solving processes contributed to or detracted from their performance.}
}
@article{DALLAT2019266,
title = {Risky systems versus risky people: To what extent do risk assessment methods consider the systems approach to accident causation? A review of the literature},
journal = {Safety Science},
volume = {119},
pages = {266-279},
year = {2019},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2017.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0925753517305295},
author = {Clare Dallat and Paul M. Salmon and Natassia Goode},
keywords = {Risk, Risk assessment, Risk assessment methods, Systems thinking},
abstract = {Accidents are now widely acknowledged to be a systems phenomenon. As part of a proactive approach to safety management, organisations use risk assessment methods to identify the hazards and associated risks that may lead to accidents. Although there is an extensive body of literature on the need for a systems thinking approach in accident analysis, little has been said regarding the theoretical underpinnings of risk assessment methods. The aim of this paper was to systematically review the risk assessment methods presented in the literature and evaluate the extent to which they are underpinned by a systems thinking approach. A total of 342 methods spanning a range of safety-critical domains were evaluated using Rasmussen’s tenets of accident causation. A key finding is that the majority of existing risk assessment methods are not consistent with Rasmussen’s model of accident causation (arguably the most popular model in safety science circles). Instead, the majority of risk assessment methods focus on risks at the so called sharp-end and largely view accidents as emerging from a linear, or chain-of-events process. This overlooks emergent risks at other levels of the system, including supervisory, managerial, regulatory and government levels. The findings therefore suggest that the majority of existing risk assessment methods may be inadequate for identifying hazards and analysing risks within complex sociotechnical systems. The implications for risk assessment practice are discussed.}
}
@article{RASMUSSEN2007195,
title = {Reinventing solutions to systems of linear differential equations: A case of emergent models involving analytic expressions},
journal = {The Journal of Mathematical Behavior},
volume = {26},
number = {3},
pages = {195-210},
year = {2007},
note = {An Inquiry Oriented Approach to Differential Equations},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2007.09.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732312307000338},
author = {Chris Rasmussen and Howard Blumenfeld},
keywords = {Modeling, Undergraduate mathematics, Realistic mathematics education, Student thinking, Proportional reasoning},
abstract = {An enduring challenge in mathematics education is to create learning environments in which students generate, refine, and extend their intuitive and informal ways of reasoning to more sophisticated and formal ways of reasoning. Pressing concerns for research, therefore, are to detail students’ progressively sophisticated ways of reasoning and instructional design heuristics that can facilitate this process. In this article we analyze the case of student reasoning with analytic expressions as they reinvent solutions to systems of two differential equations. The significance of this work is twofold: it includes an elaboration of the Realistic Mathematics Education instructional design heuristic of emergent models to the undergraduate setting in which symbolic expressions play a prominent role, and it offers teachers insight into student thinking by highlighting qualitatively different ways that students reason proportionally in relation to this instructional design heuristic.}
}
@article{ZHOU1997497,
title = {Three-dimensional computations of solution hydrodynamics during the growth of potassium dihydrogen phosphate I. Spin up and steady rotation},
journal = {Journal of Crystal Growth},
volume = {180},
number = {3},
pages = {497-509},
year = {1997},
note = {Modelling in Crystal Growth},
issn = {0022-0248},
doi = {https://doi.org/10.1016/S0022-0248(97)00251-0},
url = {https://www.sciencedirect.com/science/article/pii/S0022024897002510},
author = {Yuming Zhou and Jeffrey J. Derby},
keywords = {Solution growth, Three-dimensional modeling, Fluid flow},
abstract = {A novel, massively parallel implementation of the Galerkin finite element method is used to study three-dimensional, time-dependent flows which occur during the rapid growth of potassium dihydrogen phosphate crystals from solution in a system employed by researchers at Lawrence Livermore National Laboratory. Computations for the hydrodynamics of system spin up and steady rotation indicate the importance of time-dependent flow phenomena and emphasize the significant role played by the support and crystal geometry in forming the complicated flows in this system. Predicted flow structures correlate well with experimental observations of inclusion formation.}
}
@article{MOLINARO20231150,
title = {A goal-centric outlook on learning},
journal = {Trends in Cognitive Sciences},
volume = {27},
number = {12},
pages = {1150-1164},
year = {2023},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2023.08.011},
url = {https://www.sciencedirect.com/science/article/pii/S1364661323002073},
author = {Gaia Molinaro and Anne G.E. Collins},
keywords = {goals, learning, decision-making, reinforcement learning, rewards, abstraction, motivation, computational modeling},
abstract = {Goals play a central role in human cognition. However, computational theories of learning and decision-making often take goals as given. Here, we review key empirical findings showing that goals shape the representations of inputs, responses, and outcomes, such that setting a goal crucially influences the central aspects of any learning process: states, actions, and rewards. We thus argue that studying goal selection is essential to advance our understanding of learning. By following existing literature in framing goal selection within a hierarchy of decision-making problems, we synthesize important findings on the principles underlying goal value attribution and exploration strategies. Ultimately, we propose that a goal-centric perspective will help develop more complete accounts of learning in both biological and artificial agents.}
}
@article{POWELL2016147,
title = {Deconstructing intellectual curiosity},
journal = {Personality and Individual Differences},
volume = {95},
pages = {147-151},
year = {2016},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2016.02.037},
url = {https://www.sciencedirect.com/science/article/pii/S0191886916300927},
author = {Christopher Powell and Ted Nettelbeck and Nicholas R. Burns},
keywords = {Curiosity, Intellectual curiosity, Epistemic Curiosity, Need for Cognition, Typical Intellectual Engagement, Intellect},
abstract = {Scales of Need for Cognition (NFC), Typical Intellectual Engagement (TIE), and Epistemic Curiosity (EC) measure intellectual curiosity (IC). These scales correlate strongly and have been factor-analyzed individually but not together. Here N=396 (143 males) undergraduates completed measures of NFC, TIE, and EC. Six factors, labeled Intellectual Avoidance, Deprivation, Problem Solving, Abstract Thinking, Reading, and Wide Interest, were identified. TIE is the broadest scale, measuring all factors except Deprivation; NFC measures Intellectual Avoidance and Problem Solving, plus Abstract Thinking and Deprivation to a lesser degree; and EC largely measures Deprivation. Moreover, Reading may not fit in the IC domain; higher-order factor analysis indicated that, whereas items measuring Reading loaded more strongly on their first-order factor, items measuring the other factors strongly loaded on a general factor of IC. These results are significant for understanding the contents of these scales, and for future scale development.}
}
@article{GREENSPAN1990490,
title = {A counterexample of the use of energy as a measure of computational accuracy},
journal = {Journal of Computational Physics},
volume = {91},
number = {2},
pages = {490-494},
year = {1990},
issn = {0021-9991},
doi = {https://doi.org/10.1016/0021-9991(90)90051-2},
url = {https://www.sciencedirect.com/science/article/pii/0021999190900512},
author = {Donald Greenspan}
}
@article{BOVE20031040,
title = {Computational fluid dynamics in the evaluation of hemodynamic performance of cavopulmonary connections after the norwood procedure for hypoplastic left heart syndrome},
journal = {The Journal of Thoracic and Cardiovascular Surgery},
volume = {126},
number = {4},
pages = {1040-1047},
year = {2003},
issn = {0022-5223},
doi = {https://doi.org/10.1016/S0022-5223(03)00698-6},
url = {https://www.sciencedirect.com/science/article/pii/S0022522303006986},
author = {Edward L. Bove and Marc R. {de Leval} and Francesco Migliavacca and Gualtiero Guadagni and Gabriele Dubini},
keywords = {17, 21},
abstract = {Objective
Computational fluid dynamics have been used to study the hemodynamic performance of surgical operations, resulting in improved design. Efficient designs with minimal energy losses are especially important for cavopulmonary connections. The purpose of this study was to compare hydraulic performance between the hemi-Fontan and bidirectional Glenn procedures, as well as the various types of completion Fontan operations.
Methods
Three-dimensional models were constructed of typical hemi-Fontan and bidirectional Glenn operations according to anatomic data derived from magnetic resonance scans, angiocardiograms, and echocardiograms. Boundary conditions were imposed, and fluid dynamics were calculated from a mathematic code. Power losses, flow distribution to each lung, and pressures were measured at three predetermined levels of pulmonary arteriolar resistance. Models of the lateral tunnel, total cavopulmonary connection, and extracardiac conduit completion Fontan operations were constructed, and power losses, total flow distribution, vena caval and pulmonary arterial pressures, and flow distribution of inferior vena caval return were calculated.
Results
The hemi-Fontan and bidirectional Glenn procedures performed nearly identically, with similar power losses and nearly equal flow distributions to each lung at all levels of pulmonary arteriolar resistance. However, the lateral tunnel Fontan procedure as performed after the hemi-Fontan operation had lower power losses (6.9 mW, pulmonary arteriolar resistance 3 units) than the total cavopulmonary connection (40.5 mW) or the extracardiac conduit (42.9 mW), although the inclusion of an enlargement patch toward the right in the total cavopulmonary connection was effective in reducing the difference (10.0 mW). Inferior vena caval flow to the right lung was 52% for the lateral tunnel, compared with 19%, 30%, 19%, and 15% for the total cavopulmonary connection, total cavopulmonary connection with right-sided enlargement patch, extracardiac conduit, and extracardiac conduit with a bevel to the left lung, respectively.
Conclusions
According to these methods, the hemi-Fontan and bidirectional Glenn procedures performed equally well, but important differences in energy losses and flow distribution were found after the completion Fontan procedures. The superior hydraulic performance of the lateral tunnel Fontan operation after the hemi-Fontan procedure relative to any other method may be due to closer to optimal caval offset achieved in the surgical reconstruction.}
}
@article{KAVLOCK2005265,
title = {Computational Toxicology: Framework, Partnerships, and Program Development: September 29–30, 2003, Research Triangle Park, North Carolina},
journal = {Reproductive Toxicology},
volume = {19},
number = {3},
pages = {265-280},
year = {2005},
note = {Systems Biology/Computational Toxicology},
issn = {0890-6238},
doi = {https://doi.org/10.1016/j.reprotox.2004.04.013},
url = {https://www.sciencedirect.com/science/article/pii/S0890623804000747},
author = {Robert Kavlock and Gerald T. Ankley and Tim Collette and Elaine Francis and Karen Hammerstrom and Jack Fowle and Hugh Tilson and Greg Toth and Patricia Schmieder and Gilman D. Veith and Eric Weber and Douglas C. Wolf and Doug Young}
}
@incollection{SALIMI201883,
title = {Chapter 2 - Fundamentals of Systemic Approach},
editor = {Fabienne Salimi and Frederic Salimi},
booktitle = {A Systems Approach to Managing the Complexities of Process Industries},
publisher = {Elsevier},
pages = {83-180},
year = {2018},
isbn = {978-0-12-804213-7},
doi = {https://doi.org/10.1016/B978-0-12-804213-7.00002-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128042137000025},
author = {Fabienne Salimi and Frederic Salimi},
keywords = {Systems engineering, systems thinking, critical thinking, Safety Critical Element (SCE), Project Management, Complexity, Emergence, SE Competency, Type of Systems, IIoT, Big Data},
abstract = {System thinking, system engineering, and complexity management are the back bone of any operational excellence and process safety management system. This chapter aims to give a solid but concise background for the fundamentals of system engineering, system thinking, and complexity management for process industry. Different type of processes, requirement engineering and management, safety critical systems, critical thinking, and SE competency framework are discussed. It also addresses issues that pertain to human judgment and how people employ rules of thumb and heuristics to problem-solving situations. Various modes of engineering are discussed along with the complexities and concerns within each: cognitive systems engineering, control engineering, software engineering, industrial engineering, performance engineering, and several others. A distinction is also made between technical performance measures and key performance parameters. A list of leading indicators, insights, and requirements are then delineated among the various aspects of system engineering. Finally, an overall analysis of systems thinking, which concerns the process of understanding how various systems are implemented, is provided.}
}
@article{GANAPATHY20158064,
title = {Optimum steepest descent higher level learning radial basis function network},
journal = {Expert Systems with Applications},
volume = {42},
number = {21},
pages = {8064-8077},
year = {2015},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2015.06.036},
url = {https://www.sciencedirect.com/science/article/pii/S0957417415004388},
author = {Kirupa Ganapathy and V. Vaidehi and Jesintha B. Chandrasekar},
keywords = {Neural network, Radial basis function, Dynamic learning, Optimum steepest descent, Higher level components, Healthcare},
abstract = {Dynamically changing real world applications, demands for rapid and accurate machine learning algorithm. In neural network based machine learning algorithms, radial basis function (RBF) network is a simple supervised learning feed forward network. With its simplicity, this network is highly suitable to model and control the nonlinear systems. Existing RBF networks in literature are applied to static applications and also faces challenges such as increased model size, neuron removal, improper center selection etc leading to erroneous output. To overcome the challenges and handle complex real world problems, this paper proposes a new optimum steepest descent based higher level learning radial basis function network (OSDHL-RBFN). The proposed OSDHL-RBFN implements major components inspired from the human brain for efficient learning, adaptive structure and accurate classification. Higher level learning and thinking components of the proposed network are sample deletion, neuron addition, neuron migration, sample navigation and neuroplasticity. These components helps the classifier to think before learning the samples and regulates the learning strategy. The knowledge gained from the trained samples are used by the network to identify the incomplete sample, optimal center and bond strength of hidden & output neurons. Adaptive network structure is employed to minimize classification error. The proposed work also uses optimum steepest descent method for weight parameter update to minimize the sum square error. OSDHL-RBFN is tested and evaluated in both static and dynamic environments on nine benchmark classification (binary and multiclass) problems for balanced, unbalanced, small, large, low dimensional and high dimensional datasets. The overall and class wise efficiency of OSDHL-RBFN is improved when compared to other RBFN’s in the literature. The performance results clearly show that the proposed OSDHL-RBFN reduces the architecture complexity and computation time compared to other RBFN’s. Overall, the proposed OSDHL-RBFN is efficient and suitable for dynamic real world applications in terms of detection time and accuracy. As a case study, OSDHL-RBFN is implemented in real time remote health monitoring application for classifying the various abnormality levels in vital parameters.}
}
@incollection{TILLAS2017101,
title = {Chapter 7 - On the Redundancies of “Social Agency”},
editor = {Jon Leefmann and Elisabeth Hildt},
booktitle = {The Human Sciences after the Decade of the Brain},
publisher = {Academic Press},
address = {San Diego},
pages = {101-120},
year = {2017},
isbn = {978-0-12-804205-2},
doi = {https://doi.org/10.1016/B978-0-12-804205-2.00007-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128042052000070},
author = {A. Tillas},
keywords = {Structure, agency, concepts, intuitions, decision-making, actions},
abstract = {This chapter presents a philosophical argument about the “structure vs agency” debate—one of the central debates in social sciences. I do not argue for the primacy of either of the two but suggest an empirically vindicated view about the nature of thinking, in light of which the traditional debate as well as the notion of “social agency,” is redundant. I argue that thinking is contingent on the weightings of the synaptic connections between neuronal groups grounding it. In turn, socialization is a process of adjusting or conditioning the appropriate synaptic connection weightings. Both conscious (reasoning) and unconscious (intuitions) determinants of sociologically nontrivial actions derive from perceptual encounters with our sociophysical environment. In turn, agents—as social scientists use the term—simply do not exist. Finally, I appeal to neuroscientific evidence and show that we still qualify as agents, if only with regards to sociologically trivial actions.}
}
@article{ITO2024326,
title = {A Discrete Predator-Prey Brain Storm Optimization Technique for Optimal Allocation of Micro-PMUs in Distribution System State Estimation},
journal = {IFAC-PapersOnLine},
volume = {58},
number = {13},
pages = {326-331},
year = {2024},
note = {12th IFAC Symposium on Control of Power and Energy Systems - CPES 2024},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2024.07.503},
url = {https://www.sciencedirect.com/science/article/pii/S2405896324006025},
author = {Akio Ito and Hiroyuki Mori and Hsiao-Dong Chiang},
keywords = {distribution network state estimation, distribution automation, evolutionary computation, micro-PMU, smart grids, intelligent control of power systems},
abstract = {This paper proposes a practical method for determining the optimal allocation of micro-Phasor Measurement Units (μPMUs) for distribution system state estimation (DSSE). The proposed method formulates the state estimation based on the nest structure of DistFlow, which is used for power flow calculation in distribution systems due to the redundancy of less than 1. The use of μPMUs can significantly improve the accuracy of estimates. However, Distribution System Operators (DSOs) need to consider optimization constraints before deciding on the optimal location of μPMUs. To tackle this issue, this paper proposes the use of DPPBSO (Discrete Predator-Prey Brain Storm Optimization) of Evolutionary Computation (EC) to optimize the location of μPMUs. PPBSO is an extension method that applies the Predator-Prey strategy to Brain Storm Optimization (BSO), and DPPSO is the discrete version of PPBSO used in solving combinatorial optimization problems. The Predator-Prey strategy is critical in improving the solution candidates by intensifying and diversifying solution searches in EC. Simulation results demonstrate the effectiveness of the proposed method in the IEEE 69-node distribution system.}
}
@article{BELLA2023100509,
title = {Circular dichroism simulations of chiral buckybowls by means curvature analyses},
journal = {FlatChem},
volume = {40},
pages = {100509},
year = {2023},
issn = {2452-2627},
doi = {https://doi.org/10.1016/j.flatc.2023.100509},
url = {https://www.sciencedirect.com/science/article/pii/S2452262723000417},
author = {Giovanni Bella and Giuseppe Bruno and Antonio Santoro},
keywords = {Buckybowl, Chirality, Curvature, TD-DFT, Circular dichroism},
abstract = {A detailed understanding and interpretation of chiral properties of molecular systems, especially in condensed phase, often requires computational models that allow their structural and electronic features to be connected to the observed experimental spectra. The present paper is focused on modelling the circular dichroism spectra of chiral buckybowls, combining topological aspects and the density functional theory. For the first time Ball Pivoting Algorithm was proposed to hook up the chemical topology to the DFT through the surface reconstruction. Particularly, the gaussian curvature of a constructed probe set of corannulene and sumanene derivatives was used as discriminant parameter to benchmark a list of 10 functionals (B3LYP, B97D, M06-2X, HSEH1PBE, wB97XD, CAM-B3LYP, LC-wPBE, TPSSTPSS, mPW1PW91 and APFD). The latter provide to be noticeably accurate to reproduce the curvature effect of the considered molecules. A TD-DFT/BOMD mixed approach provided a comprehensive overview of the spectral chiral pattern prediction trends when multiple DFT functionals are scanned. The preliminary topological analysis efforts were then recompensed with the very precise computed CD spectra, again APFD confirmed as the leader functional, this time for TD-DFT vertical transition calculations. Therefore, we strongly recommend the use of the of dispersion embedded APFD functional coupled with the 6–311++G(2d,2p) basis set for the computation of the functionalized chiral buckybowls ECD spectra. © 2017 Elsevier Inc. All rights reserved.}
}
@article{VIGNAPIANO201999,
title = {Disorganization and cognitive impairment in schizophrenia: New insights from electrophysiological findings},
journal = {International Journal of Psychophysiology},
volume = {145},
pages = {99-108},
year = {2019},
note = {The Neurophysiology of Schizophrenia: A Critical Update},
issn = {0167-8760},
doi = {https://doi.org/10.1016/j.ijpsycho.2019.03.008},
url = {https://www.sciencedirect.com/science/article/pii/S016787601830984X},
author = {Annarita Vignapiano and Thomas Koenig and Armida Mucci and Giulia M. Giordano and Antonella Amodio and Mario Altamura and Antonello Bellomo and Roberto Brugnoli and Giulio Corrivetti and Giorgio {Di Lorenzo} and Paolo Girardi and Palmiero Monteleone and Cinzia Niolu and Silvana Galderisi and Mario Maj},
keywords = {Disorganization dimension, Difficulty in abstract thinking, Neurocognitive domains, Alpha rhythm, Spectral power, Topographic analysis},
abstract = {In subjects with schizophrenia (SCZ), the disorganization dimension is a strong predictor of real-life functioning. “Conceptual disorganization” (P2), “Difficulty in abstract thinking” (N5) and “Poor attention” (G11) are core features of the disorganization factor, evaluated using the Positive and Negative Syndrome Scale. The heterogeneity of this dimension and its overlap with neurocognitive deficits are still debated. Within the multicenter study of the Italian Network for Research on Psychoses, we investigated electrophysiological and neurocognitive correlates of disorganization and its component items to assess the heterogeneity of this dimension and its possible overlap with neurocognitive deficits. Resting state EEG was recorded in 145 stabilized SCZ and 69 matched healthy controls (HC). Spectral amplitude was averaged in ten frequency bands. Neurocognitive domains were assessed by MATRICS Consensus Cognitive Battery (MCCB). RAndomization Graphical User software explored band spectral amplitude differences between groups and correlations with disorganization and MCCB scores in SCZ. Correlations between disorganization and MCCB scores were also investigated. Compared to HC, SCZ showed increased delta, theta, and beta 1 and decreased alpha 2 activity. A negative correlation between alpha 1 and disorganization was observed in SCZ. At the item level, only “N5” showed the same correlation. MCCB neurocognitive composite score was associated with disorganization, “P2” and “N5”. Our findings suggest only a partial overlap between disorganization and neurocognitive impairment. The association of alpha 1 with the “N5” item suggests that some aspects of disorganization could be underpinned by the impairment of basic neurobiological functions that are only partially evaluated using MCCB.}
}
@article{SIMON1993431,
title = {Experience in using SIMD and MIMD parallelism for computational fluid dynamics},
journal = {Applied Numerical Mathematics},
volume = {12},
number = {5},
pages = {431-442},
year = {1993},
issn = {0168-9274},
doi = {https://doi.org/10.1016/0168-9274(93)90103-X},
url = {https://www.sciencedirect.com/science/article/pii/016892749390103X},
author = {Horst D. Simon and Leonardo Dagum},
keywords = {Parallel architectures, MIMD, SIMD, computational fluid dynamics.},
abstract = {One of the key objectives of the Applied Research Branch in the Numerical Aerodynamic Simulation (NAS) Systems Division at NASA Ames Research Center is the accelerated introduction of highly parallel machines into a fully operational environment. In this report we summarize some of the experiences with the parallel testbed machines at the NAS Applied Research Branch. We discuss the performance results obtained from the implementation of two computational fluid dynamics (CFD) applications, an unstructured grid solver and a particle simulation, on the Connection Machine CM-2 and the Intel iPSC/860.}
}
@article{GOSAK2024103888,
title = {The ChatGPT effect and transforming nursing education with generative AI: Discussion paper},
journal = {Nurse Education in Practice},
volume = {75},
pages = {103888},
year = {2024},
issn = {1471-5953},
doi = {https://doi.org/10.1016/j.nepr.2024.103888},
url = {https://www.sciencedirect.com/science/article/pii/S1471595324000179},
author = {Lucija Gosak and Lisiane Pruinelli and Maxim Topaz and Gregor Štiglic},
keywords = {Artificial Intelligence, ChatGPT, Documentation, Education, Nursing, Nursing Diagnosis},
abstract = {Aim
The aim of this study is to present the possibilities of nurse education in the use of the Chat Generative Pre-training Transformer (ChatGPT) tool to support the documentation process.
Background
The success of the nursing process is based on the accuracy of nursing diagnoses, which also determine nursing interventions and nursing outcomes. Educating nurses in the use of artificial intelligence in the nursing process can significantly reduce the time nurses spend on documentation.
Design
Discussion paper.
Methods
We used a case study from Train4Health in the field of preventive care to demonstrate the potential of using Generative Pre-training Transformer (ChatGPT) to educate nurses in documenting the nursing process using generative artificial intelligence. Based on the case study, we entered a description of the patient's condition into Generative Pre-training Transformer (ChatGPT) and asked questions about nursing diagnoses, nursing interventions and nursing outcomes. We further synthesized these results.
Results
In the process of educating nurses about the nursing process and nursing diagnosis, Generative Pre-training Transformer (ChatGPT) can present potential patient problems to nurses and guide them through the process from taking a medical history, setting nursing diagnoses and planning goals and interventions. Generative Pre-training Transformer (ChatGPT) returned appropriate nursing diagnoses, but these were not in line with the North American Nursing Diagnosis Association – International (NANDA-I) classification as requested. Of all the nursing diagnoses provided, only one was consistent with the most recent version of the North American Nursing Diagnosis Association – International (NANDA-I). Generative Pre-training Transformer (ChatGPT) is still not specific enough for nursing diagnoses, resulting in incorrect answers in several cases.
Conclusions
Using Generative Pre-training Transformer (ChatGPT) to educate nurses and support the documentation process is time-efficient, but it still requires a certain level of human critical-thinking and fact-checking.}
}
@article{CHING201765,
title = {Children's understanding of the commutativity and complement principles: A latent profile analysis},
journal = {Learning and Instruction},
volume = {47},
pages = {65-79},
year = {2017},
issn = {0959-4752},
doi = {https://doi.org/10.1016/j.learninstruc.2016.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0959475216301906},
author = {Boby Ho-Hong Ching and Terezinha Nunes},
keywords = {Additive reasoning, Commutativity principle, Complement principle, Latent profile analysis},
abstract = {This study examined patterns of individual differences in the acquisition of the knowledge of the commutativity and complement principles in 115 five-to six-year-old children and explored the role of concrete materials in helping children understand the prinicples. On the basis of latent profile analysis, four groups of children were identified: The first group succeeded in commutativity tasks with concrete materials but in no other tasks; the second succeeded in commutativity tasks in both concrete and abstract conditions, but not in complement tasks; the third group succeeded in all commutativity tasks and in complement tasks with concrete materials, and the final group succeeded in all the tasks. The four groups of children suggest a developmental trend – (1) Knowledge of the commutativity and of the complement principles seems to develop from thinking in the context of specific quantities to thinking about more abstract symbols; (2) There may be an order of understanding of the principles – from the commutativity to the complement principle; (3) Children may acquire the knowledge of the commutativity principle in the more abstract tasks before they start to acquire the knowledge of the complement principle. This study contributes to the literature by showing that assessing additive reasoning in different ways and identifying profiles with classification analyses may be useful for educators to understand more about the developmental stage where each child is placed. It appears that a more fine-grained assessment of additive reasoning can be achieved by incorporating both concrete materials and relatively abstract symbols in the assessment.}
}
@article{LANGE2024101191,
title = {What are explanatory proofs in mathematics and how can they contribute to teaching and learning?},
journal = {The Journal of Mathematical Behavior},
volume = {76},
pages = {101191},
year = {2024},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2024.101191},
url = {https://www.sciencedirect.com/science/article/pii/S0732312324000683},
author = {Marc Lange},
keywords = {Explanation, Proof, Generalization, Pedagogy, Unification, Coincidence},
abstract = {This paper will examine several simple examples (drawn from the mathematics literature) where there are multiple proofs of the same theorem, but only some of these proofs are widely regarded by mathematicians as explanatory. These examples will motivate an account of explanatory proofs in mathematics. Along the way, the paper will discuss why deus ex machina proofs are not explanatory, what a mathematical coincidence is, and how a theorem's proper setting reflects the naturalness of various mathematical kinds. The paper will also investigate how context influences which features of a theorem are salient and consequently which proofs are explanatory. The paper will discuss several ways in which explanatory proofs can contribute to teaching and learning, including how shifts in context (and hence in a proof’s explanatory power) can be exploited in a classroom setting, leading students to dig more deeply into why some theorem holds. More generally, the paper will examine how “Why?” questions operate in mathematical thinking, teaching, and learning.}
}
@incollection{PESCE2024123,
title = {Chapter Seven - Creativity and consciousness in motion: The roundtrip of “mindful” and “mindless” processes in embodied creativity},
editor = {Tal Dotan Ben-Soussan and Joseph Glicksohn and Narayanan Srinivasan},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {287},
pages = {123-151},
year = {2024},
booktitle = {The Neurophysiology of Silence (C): Creativity, Aesthetic Experience and Time},
issn = {0079-6123},
doi = {https://doi.org/10.1016/bs.pbr.2024.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0079612324000785},
author = {Caterina Pesce and Nicoletta Tocci},
keywords = {Creative thinking, Motor creativity, Embodiment, Hypofrontality, Flow, Incubation, Mind wandering, Nature, Green exercise, Mindful movements},
abstract = {In this opinion paper, we make a journey across different accounts of creativity that emphasize either the mindful, conscious and cognitive expression of creativity, or its mindless, unconscious and sensorimotor expression. We try to go beyond dichotomy, putting creativity in motion and outlining its embodied and enactive features. Based on the assumption that no creative act is purely conscious or purely unconscious, our discussion on creativity relies on the distinction of three types of creativity that complementarily contribute to the creative process through shifts in the activation of their substrates in the brain: the deliberate, spontaneous and flow types of creativity. The latter is a hybrid and embodied type, in which movement and physical activity meet creativity. We then focus on the most fascinating contribution of unconscious processes and mind wandering to spontaneous and flow modes of creativity, exploring what happens when the individual apparently takes a break from a deliberate and effortful search for solutions and the creative process progresses through an incubation phase. This phase and the overall creative process can be facilitated by physical activity which, depending on its features and context, can disengage the cognitive control network and free the mind from filters that constrain cognitive processes or, conversely, can engage attentional control on sensorimotor and cognitive task components in a mindful way. Lastly, we focus on the unique features of the outer natural environment of physical activity and of the inner environment during mindful movements that can restore capacities and boost creativity.}
}
@article{ZHENG20034147,
title = {A novel approach of three-dimensional hybrid grid methodology: Part 1. Grid generation},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {192},
number = {37},
pages = {4147-4171},
year = {2003},
issn = {0045-7825},
doi = {https://doi.org/10.1016/S0045-7825(03)00385-2},
url = {https://www.sciencedirect.com/science/article/pii/S0045782503003852},
author = {Yao Zheng and Meng-Sing Liou},
keywords = {Computational fluid dynamics, Grid generation, Hybrid grid},
abstract = {We propose a novel approach of three-dimensional hybrid grid methodology, the DRAGON grid method in the three-dimensional space. The DRAGON grid is created by means of a Direct Replacement of Arbitrary Grid Overlapping by Nonstructured grid, and is structured-grid dominated with unstructured grids in small regions. The DRAGON grid scheme is an adaptation to the Chimera thinking. It is capable of preserving the advantageous features of both the structured and unstructured grids, and eliminates/minimizes their shortcomings. In the present paper, we describe essential and programming aspects, and challenges of the three-dimensional DRAGON grid method, with respect to grid generation. We demonstrate the capability of generating computational grids for multi-components complex configurations.}
}
@article{BARROUILLET2011151,
title = {Dual-process theories of reasoning: The test of development},
journal = {Developmental Review},
volume = {31},
number = {2},
pages = {151-179},
year = {2011},
note = {Special Issue: Dual-Process Theories of Cognitive Development},
issn = {0273-2297},
doi = {https://doi.org/10.1016/j.dr.2011.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0273229711000177},
author = {Pierre Barrouillet},
keywords = {Dual-process theories, Cognitive development, Conditional reasoning},
abstract = {Dual-process theories have become increasingly influential in the psychology of reasoning. Though the distinction they introduced between intuitive and reflective thinking should have strong developmental implications, the developmental approach has rarely been used to refine or test these theories. In this article, I review several contemporary dual-process accounts of conditional reasoning that theorize the distinction between the two systems of reasoning as a contrast between heuristic and analytic processes, probabilistic and mental model reasoning, or emphasize the role of metacognitive processes in reflective reasoning. These theories are evaluated in the light of the main developmental findings. It is argued that a proper account of developmental phenomena requires the integration of the main strengths of these three approaches. I propose such an integrative theory of conditional understanding and argue that the modern dual-process framework could benefit from earlier contributions that made the same distinction between intuition and reflective thinking, such as Piaget’s theory.}
}
@article{HALLOWELL2023100240,
title = {Democratising or disrupting diagnosis? Ethical issues raised by the use of AI tools for rare disease diagnosis},
journal = {SSM - Qualitative Research in Health},
volume = {3},
pages = {100240},
year = {2023},
issn = {2667-3215},
doi = {https://doi.org/10.1016/j.ssmqr.2023.100240},
url = {https://www.sciencedirect.com/science/article/pii/S2667321523000240},
author = {Nina Hallowell and Shirlene Badger and Francis McKay and Angeliki Kerasidou and Christoffer Nellåker},
keywords = {Computational phenotyping, Rare disease, Diagnosis, AI, Qualitative interviews},
abstract = {Computational phenotyping (CP) technology uses facial recognition algorithms to classify and potentially diagnose rare genetic disorders on the basis of digitised facial images. This AI technology has a number of research as well as clinical applications, such as supporting diagnostic decision-making. Using the example of CP, we examine stakeholders’ views of the benefits and costs of using AI as a diagnostic tool within the clinic. Through a series of in-depth interviews (n ​= ​20) with: clinicians, clinical researchers, data scientists, industry and support group representatives, we report stakeholder views regarding the adoption of this technology in a clinical setting. While most interviewees were supportive of employing CP as a diagnostic tool in some capacity we observed ambivalence around the potential for artificial intelligence to overcome diagnostic uncertainty in a clinical context. Thus, while there was widespread agreement amongst interviewees concerning the public benefits of AI assisted diagnosis, namely, its potential to increase diagnostic yield and enable faster more objective and accurate diagnoses by up skilling non specialists and thereby enabling access to diagnosis that is potentially lacking, interviewees also raised concerns about ensuring algorithmic reliability, expunging algorithmic bias and that the use of AI could result in deskilling the specialist clinical workforce. We conclude that, prior to widespread clinical implementation, on-going reflection is needed regarding the trade-offs required to determine acceptable levels of bias and conclude that diagnostic AI tools should only be employed as an assistive technology within the dysmorphology clinic.}
}
@article{DUKHANOV2016449,
title = {Big Data and Artificial Intelligence for Digital Humanities: An International Master Program via Trans-Eurasian Universities Network},
journal = {Procedia Computer Science},
volume = {101},
pages = {449-451},
year = {2016},
note = {5th International Young Scientist Conference on Computational Science, YSC 2016, 26-28 October 2016, Krakow, Poland},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.11.052},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916327211},
author = {Alexey Dukhanov and Alexander Boukhanovsky and Tatyana Sidorova and Natalya Spitsyna},
keywords = {trans-Eurasian universities’ network, international Master's program, digital humanities, skills of contemporary professional},
abstract = {This paper presents an intention of two Russian universities located at opposite sides of Russia to build with partners – leading world educational centers (in the Top-100 Universities of Times Higher Education) – a trans-Eurasian international network with Master's program “Big Data and Artificial Intelligence for Digital Humanities.” This program significantly extends the area of fostering students’ talent. In addition, it allows students to develop valuable global skills of a contemporary professional: domain expertise, soft skills including creative and system thinking, self-development, working in an international and intercultural team on a research project, etc. After graduation, the alumni will have a wide choice of opportunities to continue their academic career or to get a well-paid job in developing and developed countries around the World.}
}
@article{GUO2024324,
title = {Optimization of robot manipulator configuration calibration by using Zhang neural network for repetitive motion},
journal = {Applied Mathematical Modelling},
volume = {134},
pages = {324-348},
year = {2024},
issn = {0307-904X},
doi = {https://doi.org/10.1016/j.apm.2024.06.008},
url = {https://www.sciencedirect.com/science/article/pii/S0307904X24002853},
author = {Pengfei Guo and Yunong Zhang and Shuai Li and Ning Tan},
keywords = {Temporally dependent quadratic programming, Filtered reciprocal-kind Zhang neural network, Lyapunov stability, Robot manipulator configuration calibration},
abstract = {High precision and low complexity control algorithm plays an important role in the developing of the end-effector instrumentation of different robot manipulators. In order to reduce the kinetic energy and the high-speed drift phenomenon of the repetitive motion tracking task, the robot manipulator needs to calibrate its configuration. In this paper, we formulate the configuration calibration of the robot manipulator for the repetitive motion task as a future quadratic programming optimization problem constrained with equality constraints, which is also regarded as a fundamental problem in artificial intelligence and modern control engineering. Zhang neural network, which is a canonical method, can be adopted to deal with the continuous form of the future optimization problem, named as temporally dependent quadratic programming problem with equality constraints. In order to overcome the issue of temporally dependent inverse computing, a novel Zhang neural network model and its uncertain disturbance tolerant model, which are termed as filtered reciprocal-kind Zhang neural network model and uncertain disturbance tolerant filtered reciprocal-kind Zhang neural network model, respectively, are proposed by integrating the energy-type cost function and Zhang neural network design formula for solving the temporally dependent quadratic programming problem with equality constraints in this paper. Based on the Euler discrete formula and the models, the discrete filtered reciprocal-kind Zhang neural network and the discrete uncertain disturbance tolerant filtered reciprocal-kind Zhang neural network algorithms are proposed for solving the future quadratic programming problem with equality constraints and the robot manipulator configuration calibration problem of repetitive motion. The convergence properties of the reciprocal-kind Zhang neural network model and its corresponding uncertain disturbance tolerant model are obtained by Lyapunov stability theory of nonlinear system and its corresponding perturbed system, while the convergence property of the filtered reciprocal-kind Zhang neural network model is analyzed by the limit thinking. For the repetitive motion task, three experiments for solving the configuration calibration problem of PUMA560, Kinova Jaco2, and Franka Emika Panda robot manipulators are performed to illustrate the effectiveness, robustness and superiority of our proposed discrete filtered reciprocal-kind Zhang neural network algorithms.}
}
@article{DURSO2015336,
title = {The Threat-Strategy Interview},
journal = {Applied Ergonomics},
volume = {47},
pages = {336-344},
year = {2015},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2014.08.001},
url = {https://www.sciencedirect.com/science/article/pii/S0003687014001409},
author = {Francis T. Durso and Sadaf Kazi and Ashley N. Ferguson},
keywords = {Strategies, Knowledge elicitation, Threat and error management},
abstract = {Operators in dynamic work environments use strategies to manage threats in order to achieve task goals. We introduce a structured interview method, the Threat-Strategy Interview (TSI), and an accompanying qualitative analysis to induce operator-level threats, strategies, and the cues that give rise to them. The TSI can be used to elicit knowledge from operators who are on the front line of managing threats to provide an understanding of strategic thinking, which in turn can be applied toward a variety of problems.}
}
@article{HROBARIK20066,
title = {Computational study of bonding trends in the metalloactinyl series EThM and MThM′ (E=N−, O, F+; M, M′=Ir−, Pt, Au+)},
journal = {Chemical Physics Letters},
volume = {431},
number = {1},
pages = {6-12},
year = {2006},
issn = {0009-2614},
doi = {https://doi.org/10.1016/j.cplett.2006.08.144},
url = {https://www.sciencedirect.com/science/article/pii/S0009261406013741},
author = {Peter Hrobárik and Michal Straka and Pekka Pyykkö},
abstract = {The title systems, including EThE′, are treated at DFT level using a B3LYP functional and small-core quasirelativistic pseudopotentials. Most of the studied systems are bent, like their isoelectronic ThO2 analogue, except for some anionic systems containing Ir. The bond lengths vary considerably and can lie above or below the sum of triple-bond covalent radii. Among the studied systems, the iridium-containing species show the strongest back-donation to Th. The bonding can be simply understood and could theoretically go up to a ‘24-electron principle’ limit at the actinide.}
}
@article{KOSCHINSKY2013172,
title = {The case for spatial analysis in evaluation to reduce health inequities},
journal = {Evaluation and Program Planning},
volume = {36},
number = {1},
pages = {172-176},
year = {2013},
note = {Special Section: Rethinking Evaluation of Health Equity Initiatives},
issn = {0149-7189},
doi = {https://doi.org/10.1016/j.evalprogplan.2012.03.004},
url = {https://www.sciencedirect.com/science/article/pii/S0149718912000237},
author = {Julia Koschinsky},
keywords = {Spatial analysis, Spatial perspective, Program evaluation, Evaluation, Health inequities, Realist evaluation, Randomized control trials (RCTs)},
abstract = {The article begins by giving an overview of spatial thinking concepts that are relevant to evaluation. The article relates the spatial perspective to both a realist evaluation and a randomized control trial perspective in evaluation to demonstrate the benefits of a spatialized program and evaluation perspective. The article mainly suggests that the adoption of a spatial perspective can add new insights to the theory and practice of evaluation in ways that helps evaluation move closer to reducing health inequities.}
}
@article{XIA2023100730,
title = {Understanding common human driving semantics for autonomous vehicles},
journal = {Patterns},
volume = {4},
number = {7},
pages = {100730},
year = {2023},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2023.100730},
url = {https://www.sciencedirect.com/science/article/pii/S2666389923000703},
author = {Yingji Xia and Maosi Geng and Yong Chen and Sudan Sun and Chenlei Liao and Zheng Zhu and Zhihui Li and Washington Yotto Ochieng and Panagiotis Angeloudis and Mireille Elhajj and Lei Zhang and Zhenyu Zeng and Bing Zhang and Ziyou Gao and Xiqun (Michael) Chen},
keywords = {human-machine interaction, neuroscience, hierarchical understanding abstraction, electroencephalography, neural-informed model, driving behavior perception, driving semantics, autonomous vehicle},
abstract = {Summary
Autonomous vehicles will share roads with human-driven vehicles until the transition to fully autonomous transport systems is complete. The critical challenge of improving mutual understanding between both vehicle types cannot be addressed only by feeding extensive driving data into data-driven models but by enabling autonomous vehicles to understand and apply common driving behaviors analogous to human drivers. Therefore, we designed and conducted two electroencephalography experiments for comparing the cerebral activities of human linguistics and driving understanding. The results showed that driving activates hierarchical neural functions in the auditory cortex, which is analogous to abstraction in linguistic understanding. Subsequently, we proposed a neural-informed, semantics-driven framework to understand common human driving behavior in a brain-inspired manner. This study highlights the pathway of fusing neuroscience into complex human behavior understanding tasks and provides a computational neural model to understand human driving behaviors, which will enable autonomous vehicles to perceive and think like human drivers.}
}
@incollection{FROEHLICH2023685,
title = {Mixed methods and social network analysis},
editor = {Robert J Tierney and Fazal Rizvi and Kadriye Ercikan},
booktitle = {International Encyclopedia of Education (Fourth Edition)},
publisher = {Elsevier},
edition = {Fourth Edition},
address = {Oxford},
pages = {685-692},
year = {2023},
isbn = {978-0-12-818629-9},
doi = {https://doi.org/10.1016/B978-0-12-818630-5.11059-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128186305110590},
author = {Dominik E. Froehlich},
keywords = {Data collection, Education research, Ethics, Mixed methods social network analysis, Mixed methods, Relational methods, Research design, Social network analysis, Structure},
abstract = {In this chapter, we discuss the application of mixed methods thinking to social network analysis, a methodological approach that focuses on social relationships and structures. For that purpose, we first define mixed methods and social network analysis and their intersection, which we call Mixed Methods Social Network Analysis (MMSNA). We then summarize the historical developments in social network analysis, which also explain the reason for the increasing application of MMSNA in educational research. The majority of the chapter then focuses on how MMSNA is applied in educational research and what the main topics of the current academic debates are.}
}
@article{REN20231643,
title = {An Edge Computing Algorithm Based on Multi-Level Star Sensor Cloud},
journal = {CMES - Computer Modeling in Engineering and Sciences},
volume = {136},
number = {2},
pages = {1643-1659},
year = {2023},
issn = {1526-1492},
doi = {https://doi.org/10.32604/cmes.2023.025248},
url = {https://www.sciencedirect.com/science/article/pii/S1526149223002813},
author = {Siyu Ren and Shi Qiu and Keyang Cheng},
keywords = {Star-sensing, sensor cloud, fuzzy set, edge computing, mapping},
abstract = {Star sensors are an important means of autonomous navigation and access to space information for satellites. They have been widely deployed in the aerospace field. To satisfy the requirements for high resolution, timeliness, and confidentiality of star images, we propose an edge computing algorithm based on the star sensor cloud. Multiple sensors cooperate with each other to form a sensor cloud, which in turn extends the performance of a single sensor. The research on the data obtained by the star sensor has very important research and application values. First, a star point extraction model is proposed based on the fuzzy set model by analyzing the star image composition, which can reduce the amount of data computation. Then, a mapping model between content and space is constructed to achieve low-rank image representation and efficient computation. Finally, the data collected by the wireless sensor is delivered to the edge server, and a different method is used to achieve privacy protection. Only a small amount of core data is stored in edge servers and local servers, and other data is transmitted to the cloud. Experiments show that the proposed algorithm can effectively reduce the cost of communication and storage, and has strong privacy.}
}
@article{THOMPSON2011107,
title = {Intuition, reason, and metacognition},
journal = {Cognitive Psychology},
volume = {63},
number = {3},
pages = {107-140},
year = {2011},
issn = {0010-0285},
doi = {https://doi.org/10.1016/j.cogpsych.2011.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0010028511000454},
author = {Valerie A. Thompson and Jamie A. {Prowse Turner} and Gordon Pennycook},
keywords = {Metacognition, Reasoning, Dual Process Theories, Intuition, Analytic thinking, Retrospective confidence},
abstract = {Dual Process Theories (DPT) of reasoning posit that judgments are mediated by both fast, automatic processes and more deliberate, analytic ones. A critical, but unanswered question concerns the issue of monitoring and control: When do reasoners rely on the first, intuitive output and when do they engage more effortful thinking? We hypothesised that initial, intuitive answers are accompanied by a metacognitive experience, called the Feeling of Rightness (FOR), which can signal when additional analysis is needed. In separate experiments, reasoners completed one of four tasks: conditional reasoning (N=60), a three-term variant of conditional reasoning (N=48), problems used to measure base rate neglect (N=128), or a syllogistic reasoning task (N=64). For each task, participants were instructed to provide an initial, intuitive response to the problem along with an assessment of the rightness of that answer (FOR). They were then allowed as much time as needed to reconsider their initial answer and provide a final answer. In each experiment, we observed a robust relationship between the FOR and two measures of analytic thinking: low FOR was associated with longer rethinking times and an increased probability of answer change. In turn, FOR judgments were consistently predicted by the fluency with which the initial answer was produced, providing a link to the wider literature on metamemory. These data support a model in which a metacognitive judgment about a first, initial model determines the extent of analytic engagement.}
}
@article{ASIF202532,
title = {Machine learning-driven catalyst design, synthesis and performance prediction for CO2 hydrogenation},
journal = {Journal of Industrial and Engineering Chemistry},
volume = {144},
pages = {32-47},
year = {2025},
issn = {1226-086X},
doi = {https://doi.org/10.1016/j.jiec.2024.09.035},
url = {https://www.sciencedirect.com/science/article/pii/S1226086X24006269},
author = {Muhammad Asif and Chengxi Yao and Zitu Zuo and Muhammad Bilal and Hassan Zeb and Seungjae Lee and Ziyang Wang and Taesung Kim},
keywords = {Heterogeneous catalysis, DFT calculation, Machine learning, 3D printing, CO hydrogenation},
abstract = {Atmospheric concentrations of CO2 must be lowered to mitigate climate change and rising global temperatures. CO2 utilization is the most promising approach for the sustainable reduction of CO2 emissions. Interdisciplinary research is gaining increasing attention due to its broader application potential and the promising results of combining various fields. Computational approaches in catalytic research could be cost-effective and environmentally friendly. Machine Learning (ML) and 3D printing technologies may soon be able to produce nanoscale raw materials to synthesize the catalyst for commercial-scale applications. In this review article, recent advances in catalyst synthesis using 3D printing technologies and ML-based catalytic reactions, particularly those in CO2 hydrogenation, are critically analyzed, with a focus on the function of ML model prediction. ML approaches with high prediction accuracies are discussed comprehensively. Based on the literature Gray-box models can provide useful insights by revealing the essential catalytic traits, factors, and circumstances that affect the results. They can also provide a practical solution by fusing the benefits of black-box algorithms, such as ensemble models and NNs, with feature importance analysis. Finally, suggestions and recommendations for the potential applications of ML in chemical science, especially in heterogeneous catalysis, are provided along with future research directions.}
}
@article{WANG2024109589,
title = {Cellular gradient algorithm for solving complex mechanical optimization design problems},
journal = {International Journal of Mechanical Sciences},
volume = {282},
pages = {109589},
year = {2024},
issn = {0020-7403},
doi = {https://doi.org/10.1016/j.ijmecsci.2024.109589},
url = {https://www.sciencedirect.com/science/article/pii/S0020740324006301},
author = {Rugui Wang and Xinpeng Li and Haibo Huang and Zhipeng Fan and Fuqiang Huang and Ningjuan Zhao},
keywords = {Mechanical optimization, Optimization algorithm, Discrete integrable problem, Cellular automaton, Gradient descent},
abstract = {In mechanical optimization design problems, there are often some non-continuous or non-differentiable objective functions. For these non-continuous and non-differentiable optimization objectives, it is often difficult for existing optimal design algorithms to find the desired optimal solutions. In this paper, we incorporate the idea of gradient descent into cellular automata and propose a Cellular Gradient (CG) method. First, we have given the basic rules and algorithmic framework of CG and designed three kinds of growth and extinction rules respectively. Then, the three evolutionary rules for cellular within a single cycle are analyzed separately for form and ordering. The best expressions for the cellular jealous neighbor rule and the solitary regeneration rule are given, and the most appropriate order in which the rules are run is selected. Finally, the solution results of the cellular gradient algorithm and other classical optimization design algorithms are compared with a multi-objective multi-parameter mechanical optimization design problem as an example. The computational results show that the cellular gradient algorithm has an advantage over other algorithms in solving global and dynamic mechanical optimal design problems. The novelty of CG is to provide a new way of thinking for solving optimization problems with global discontinuities.}
}
@article{JING2020644,
title = {A Learner Model Integrating Cognitive and Metacognitive And Its Application on Scratch Programming Projects},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {5},
pages = {644-649},
year = {2020},
note = {3rd IFAC Workshop on Cyber-Physical & Human Systems CPHS 2020},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.04.154},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321002913},
author = {Sifeng Jing and Ying Tang and Xiwei Liu and Xiaoyan Gong},
keywords = {learner model, cognitive state, metacognitive ability, individualized teaching},
abstract = {learner’s cognitive and metacognitive are key personal profile for individualized teaching. To evaluate learner’s comprehensive characteristics, existing learner model were reviewed. Two challenges of constructing an accurate and comprehensive learner model integrating cognitive and metacognitive were summarized. A plan of constructing a comprehensive learner model was made based on analysis of existing massive online learning environment, sensor information technology and educational data-mining. As a case study, a method of how to map learning data onto learners’ cognitive and metacognitive was proposed based on an analysis of a number of pupils’ Scratch projects. Three mapping table were established. Pupil’s cognitive skill could be evaluated from technology shown from Scratch project, namely, data structure, algorithm, computational practices and overall evaluation. Content shown from Scratch project were used to infer pupil’s cognitive style. Meta-cognitive ability can be measured from computational practices and behavior in programming process.}
}
@article{DAVID2019646,
title = {Development of Escape Room Game using VR Technology},
journal = {Procedia Computer Science},
volume = {157},
pages = {646-652},
year = {2019},
note = {The 4th International Conference on Computer Science and Computational Intelligence (ICCSCI 2019) : Enabling Collaboration to Escalate Impact of Research Results for Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2019.08.223},
url = {https://www.sciencedirect.com/science/article/pii/S1877050919311421},
author = {David David and  Edwin and Edward Arman and  Hikari and Natalia Chandra and Nadia Nadia},
keywords = {Virtual Reality, Presence, Prototype, Unity, Samsung Gear VR},
abstract = {Escape room is one of the media games that can improve the logic of thinking. Puzzles in the escape room traditionally have disadvantages because the type of puzzle that is made requires a lot of material. The purpose of this research is to produce a game with Escape Room as the basic theme with Virtual Reality technology. Virtual Reality technology is used to develop presence in users, attendance is about the intimacy of users with the gaming world. By using Virtual Reality, the puzzle elements that are created can be replaced regularly without the need to change the building’s skeleton. The development method used is a prototype model using Unity game machines. The research method was carried out using a questionnaire for user analysis. The application generated from this research is the Escape Room VR game that can be played on an Android smartphone that is compatible with Samsung Gear VR. The application can be used as an additional means for traditional Escape Room games.}
}
@article{KIRIMTAY2025111785,
title = {Tau and MAP6 establish labile and stable domains on microtubules},
journal = {iScience},
volume = {28},
number = {3},
pages = {111785},
year = {2025},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2025.111785},
url = {https://www.sciencedirect.com/science/article/pii/S2589004225000446},
author = {Koray Kirimtay and Wenqiang Huang and Xiaohuan Sun and Liang Qiang and Dong V. Wang and Calvin T. Sprouse and Erin M. Craig and Peter W. Baas},
keywords = {Cell Biology, Cellular neuroscience},
abstract = {Summary
We previously documented that individual microtubules in the axons of cultured juvenile rodent neurons consist of a labile domain and a stable domain and that experimental depletion of tau results in selective shortening and partial stabilization of the labile domain. After first confirming these findings in adult axons, we sought to understand the mechanism that accounts for the formation and maintenance of these microtubule domains. We found that fluorescent tau and MAP6 ectopically expressed in RFL-6 fibroblasts predominantly segregate on different microtubules or different domains on the same microtubule, with the tau-rich ones becoming more labile than in control cells and the MAP6-rich ones being more stable than in control cells. These and other experimental findings, which we studied further using computational modeling with tunable parameters, indicate that these two MAPs do not merely bind to pre-existing stable and labile domains but actually create stable and labile domains on microtubules.}
}
@article{STANCIU2015312,
title = {Embodied Creativity: A Critical Analysis of an Underdeveloped Subject},
journal = {Procedia - Social and Behavioral Sciences},
volume = {187},
pages = {312-317},
year = {2015},
note = {INTERNATIONAL CONFERENCE PSIWORLD 2014 - 5th edition},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2015.03.058},
url = {https://www.sciencedirect.com/science/article/pii/S1877042815018510},
author = {Marius M. Stanciu},
keywords = {Embodied, Creativity, Cognition, Research, Review},
abstract = {While the idea that cognition is embodied appeared in the literature more than four decades ago, studies concerned with how and to what degree might the body and the environment influence creative thinking represent a relatively recent scientific endeavor. In this paper we wish to provide a critical examination of the core ideas of this new field, suggesting new experimental paradigms for testing the more radical and often ignored assertions of the embodied cognition program. We conclude that given the extremely small number of papers that are produced on this subject, as well as its obscurity within the scientific community, future research will have to expand its theoretical considerations greatly if the field is to survive and flourish.}
}
@article{MARINI201828,
title = {Life cycle perspective in RC building integrated renovation},
journal = {Procedia Structural Integrity},
volume = {11},
pages = {28-35},
year = {2018},
note = {XIV INTERNATIONAL CONFERENCE ON BUILDING PATHOLOGY AND CONSTRUCTIONS REPAIR, FLORENCE, ITALY, JUNE 20-22, 2018},
issn = {2452-3216},
doi = {https://doi.org/10.1016/j.prostr.2018.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S2452321618301069},
author = {A. Marini and C. Passoni and A. Belleri},
keywords = {Life Cycle thinking, Deep renovation, Integrated retrofit, Resilience, Sustainability},
abstract = {Enormous resources are invested in Europe for the transition into a sustainable, low carbon, and resilient society. In the construction sector, these concepts are slowly being applied to the renovation of the existing building stock by enforcing their deep and holistic renovation targeting sustainability, safety and resilience. Effectiveness of such an approach to the renovation with respect to traditional retrofit actions emerges when broadening the time frame of the analyses, shifting from the construction time to a life cycle perspective. In this case, the potential of the holistic approach becomes clear in reducing costs, impacts on the inhabitants and impacts on the environment over the building life cycle. Within such a new perspective, new technology options are needed to innovatively combine structural retrofit, architectural restyling and energy efficiency measures. Furthermore, a new design approach conjugating the principles of sustainability, safety and resilience over the building life cycle is required. In such a transition, synergistic and cooperative work of researchers, design professionals, and all the stakeholders in the construction sector is required. In this paper, the basic features of an expanded Life Cycle Thinking (eLCT) approach will be presented, which not only entails the use of recyclable/reusable materials, but also encourages interventions carried out from the outside the buildings to reduce building downtime and avoid inhabitant relocation. In addition, such an expanded LCT fosters the adoption of reparable, easy maintainable, adaptable and fully demountable solutions, such as those featuring dry, demountable and pre-fabricated components. Finally, it addresses the need to account for the End of Life scenario from the initial design stages to guarantee selective dismantling and reuse or recycle to reduce construction waste. Finally, a discussion on the main barriers and challenges in the transition towards this new approach to the renovation of existing building stock is briefly presented.}
}
@article{NAGLE2019100684,
title = {Using APOS theory as a framework for considering slope understanding},
journal = {The Journal of Mathematical Behavior},
volume = {54},
pages = {100684},
year = {2019},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2018.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0732312318301469},
author = {Courtney Nagle and Rafael Martínez-Planell and Deborah Moore-Russo},
keywords = {APOS, Slope, APOS levels between stages, Precalculus, Rate of change, Totality},
abstract = {In this paper a framework for slope is proposed using APOS (Action-Process-Object-Schema) Theory and conceptualizations of slope previously identified in research. The proposed APOS-slope framework allows for discussion of students’ cognitive development in relation to different conceptualizations of slope. As such, it may be adopted as a means to advance future research or as a way to plan instruction. In particular, the framework uses specific examples to consider interrelations between the ways of thinking about slope that have been reported to provide additional insight on how individuals understand this concept. The proposed framework contributes to the field by bringing together a number of past studies related to slope and providing a common ground under which these works might be interpreted.}
}
@article{KORN2023102578,
title = {Navigating large chemical spaces in early-phase drug discovery},
journal = {Current Opinion in Structural Biology},
volume = {80},
pages = {102578},
year = {2023},
issn = {0959-440X},
doi = {https://doi.org/10.1016/j.sbi.2023.102578},
url = {https://www.sciencedirect.com/science/article/pii/S0959440X23000520},
author = {Malte Korn and Christiane Ehrt and Fiorella Ruggiu and Marcus Gastreich and Matthias Rarey},
abstract = {The size of actionable chemical spaces is surging, owing to a variety of novel techniques, both computational and experimental. As a consequence, novel molecular matter is now at our fingertips that cannot and should not be neglected in early-phase drug discovery. Huge, combinatorial, make-on-demand chemical spaces with high probability of synthetic success rise exponentially in content, generative machine learning models go hand in hand with synthesis prediction, and DNA-encoded libraries offer new ways of hit structure discovery. These technologies enable to search for new chemical matter in a much broader and deeper manner with less effort and fewer financial resources. These transformational developments require new cheminformatics approaches to make huge chemical spaces searchable and analyzable with low resources, and with as little energy consumption as possible. Substantial progress has been made in the past years with respect to computation as well as organic synthesis. First examples of bioactive compounds resulting from the successful use of these novel technologies demonstrate their power to contribute to tomorrow's drug discovery programs. This article gives a compact overview of the state-of-the-art.}
}
@article{NDUNGO2020,
title = {mSphere of Influence: Learning from Nature—Antibody Profiles Important for Protection of Young Infants},
journal = {mSphere},
volume = {5},
number = {5},
year = {2020},
issn = {2379-5042},
doi = {https://doi.org/10.1128/msphere.01021-20},
url = {https://www.sciencedirect.com/science/article/pii/S2379504220001356},
author = {Esther Ndungo},
keywords = {antibody profiles, enteric pathogens, maternal-infant immunity, systems serology},
abstract = {Esther Ndungo works in the field of maternal-infant immunity against enteric pathogens. In this mSphere of Influence article, she reflects on how the paper “Fc glycan-mediated regulation of placental antibody transfer” by Jennewein et al. (M. F. Jennewein, I. Goldfarb, S. Dolatshahi, C. Cosgrove, et al., Cell 178:202–215.e14, 2019, https://doi.org/10.1016/j.cell.2019.05.044) impressed upon her the value of thinking “outside the box” and looking to nature to guide her research.
ABSTRACT
Esther Ndungo works in the field of maternal-infant immunity against enteric pathogens. In this mSphere of Influence article, she reflects on how the paper “Fc glycan-mediated regulation of placental antibody transfer” by Jennewein et al. (M. F. Jennewein, I. Goldfarb, S. Dolatshahi, C. Cosgrove, et al., Cell 178:202–215.e14, 2019, https://doi.org/10.1016/j.cell.2019.05.044) impressed upon her the value of thinking “outside the box” and looking to nature to guide her research.}
}
@article{KRYSSANOV2001329,
title = {Understanding design fundamentals: how synthesis and analysis drive creativity, resulting in emergence},
journal = {Artificial Intelligence in Engineering},
volume = {15},
number = {4},
pages = {329-342},
year = {2001},
note = {Methodology of Emergent Sythesis},
issn = {0954-1810},
doi = {https://doi.org/10.1016/S0954-1810(01)00023-1},
url = {https://www.sciencedirect.com/science/article/pii/S0954181001000231},
author = {V.V Kryssanov and H Tamaki and S Kitamura},
keywords = {Engineering design, Creativity, Semiotics, Emergence},
abstract = {This paper presents results of an ongoing interdisciplinary study to develop a computational theory of creativity for engineering design. Human design activities are surveyed, and popular computer-aided design methodologies are examined. It is argued that semiotics has the potential to merge and unite various design approaches into one fundamental theory that is naturally interpretable and so comprehensible in terms of computer use. Reviewing related work in philosophy, psychology, and cognitive science provides a general and encompassing vision of the creativity phenomenon. Basic notions of algebraic semiotics are given and explained in terms of design. This is to define a model of the design creative process, which is seen as a process of semiosis, where concepts and their attributes represented as signs organized into systems are evolved, blended, and analyzed, resulting in the development of new concepts. The model allows us to formally describe and investigate essential properties of the design process, namely its dynamics and non-determinism inherent in creative thinking. A stable pattern of creative thought — analogical and metaphorical reasoning — is specified to demonstrate the expressive power of the modeling approach; illustrative examples are given. The developed theory is applied to clarify the nature of emergence in design: it is shown that while emergent properties of a product may influence its creative value, emergence can simply be seen as a by-product of the creative process. Concluding remarks summarize the research, point to some unresolved issues, and outline directions for future work.}
}
@article{QIAN2024120487,
title = {E3WD: A three-way decision model based on ensemble learning},
journal = {Information Sciences},
volume = {667},
pages = {120487},
year = {2024},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2024.120487},
url = {https://www.sciencedirect.com/science/article/pii/S0020025524004006},
author = {Jin Qian and Di Wang and Ying Yu and XiBei Yang and Shang Gao},
keywords = {Three-way decision, Ensemble strategy, Cluster ensemble, Membership degree, Critical threshold},
abstract = {Three-way decision model is an effective way to deal with complex decision problems. However, since the three-way decision models now proposed are all based on a single decision criterion, the decision results typically reflect only one preference of decision-makers. Thus, these models may also not effectively deal with complex decision-making problems. To solve the above problems, this paper proposes a new three-way decision model based on ensemble learning. Specifically, we first obtain different three-way decision results by employing different decision criteria. Then, we can acquire the core and candidate sets of the positive and negative regions through set operations. Next, we use the K-means algorithm to divide the candidate sets into three disjoint subsets based on similarities. After that, we adopt a hierarchical filtering method to select suitable objects from the candidate sets and add them to the core sets. Finally, we employ four three-way decision models with different decision criteria as examples to conduct experiments on eight datasets. Experimental results show that our proposed model can obtain higher classification accuracy and lower deferment rate than other traditional three-way decision models under most experimental conditions.}
}
@article{ALDAYA2024116708,
title = {Tachyons in “momentum-space” representation},
journal = {Nuclear Physics B},
volume = {1008},
pages = {116708},
year = {2024},
issn = {0550-3213},
doi = {https://doi.org/10.1016/j.nuclphysb.2024.116708},
url = {https://www.sciencedirect.com/science/article/pii/S0550321324002748},
author = {V. Aldaya and J. Guerrero and F.F. López-Ruiz},
abstract = {Obtaining the momentum space associated with tachyonic “particles” from the Poincaré group manifold proves to be rather intricate, departing very much from the ordinary dual to Minkowski space directly parametrized by space-time translations of the Poincaré group. In fact, although described by the constants of motion (Noether invariants) associated with space-time translations, they depend non-trivially on the parameters of the rotation subgroup. However, once the momentum space is parametrized by the Noether invariants, it behaves as that of ordinary particles. On the other hand, the evolution parameter is no longer the one associated with time translation, whose Noether invariant, Po, is now a basic one. Evolution takes place in a spatial direction. These facts not only make difficult the computation of the corresponding representation, but also force us to a sound revision of several traditional ingredients related to Cauchy hypersurface, scalar product and, of course, causality. After that, the theory becomes consistent and could shed new light on some special physical situations like inflation or traveling inside a black hole.}
}
@article{SAQIB2024105516,
title = {Novel Recurrent neural networks for efficient heat transfer analysis in radiative moving porous triangular fin with heat generation},
journal = {Case Studies in Thermal Engineering},
volume = {64},
pages = {105516},
year = {2024},
issn = {2214-157X},
doi = {https://doi.org/10.1016/j.csite.2024.105516},
url = {https://www.sciencedirect.com/science/article/pii/S2214157X24015478},
author = {Sana Ullah Saqib and Umar Farooq and Nahid Fatima and Yin-Tzer Shih and Ahmed Mir and Lioua Kolsi},
keywords = {Permeable fin in a triangle form, Convection radiation fin effectiveness, Recurrent neural networks (RNNs), Lobatto III-A technique, AI-Based intelligent computing},
abstract = {This paper investigates the use of Artificial Intelligence (AI), notably Recurrent Neural Networks (RNNs), to analyze heat transfer in moving radiative porous triangular systems with heat generation (HTMPTHG). AI-based RNN models are employed to simulate and forecast the complex heat transfer behavior in these environments, offering a more precise and efficient analysis as compared to traditional numerical methods. The findings of the study highlights the intricate interactions among thermal radiation, porous media, and internal heat generation which plays an integral role in a number of industrial and engineering applications. Recurrent neural network (RNN) is validated to examine the temperature distribution efficiency in a new configuration of triangular, porous, moving fins. Various dimensionless parameters are analyzed for their impact on the effectiveness of portable, transparent, triangular fins. These parameters include permeability, radiation-conduction, Peclet number, thermo-geometric factors, convection-conduction, and surface temperature. The Lobatto III-A numerical technique for HTMPTHG is simulated computationally to provide the synthetic datasets. Then, the RNN supervised computational technique is applied to the generated datasets and the RNN outputs show negligible errors and closely align with numerical observations for all model variant. The effectiveness of Recurrent Neural Networks (RNNs) is rigorously proved through extensive experiments, demonstrating iterative convergence curves for mean squared error, control metrics of optimization and error distribution via histograms.The mean absolute percent error (MAPE), mean absolute error (MAE), and Nash-Sutcliffe efficiency (NSE) are all nearly zero, while the coefficient of determination (R2) is close to 1.Furthermore, there is strong evidence of the prediction accuracy and dependability of the RNN in the regression results for the HTMPTHG model.}
}
@incollection{OLIVEIRA200793,
title = {3 - Fundamentals of Quantum Computation and Quantum Information},
editor = {Ivan S. Oliveira and Tito J. Bonagamba and Roberto S. Sarthour and Jair C.C. Freitas and Eduardo R. deAzevedo},
booktitle = {NMR Quantum Information Processing},
publisher = {Elsevier Science B.V.},
address = {Amsterdam},
pages = {93-136},
year = {2007},
isbn = {978-0-444-52782-0},
doi = {https://doi.org/10.1016/B978-044452782-0/50005-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780444527820500051},
author = {Ivan S. Oliveira and Tito J. Bonagamba and Roberto S. Sarthour and Jair C.C. Freitas and Eduardo R. deAzevedo}
}
@article{FARHAT199361,
title = {Two-dimensional viscous flow computations on the Connecti on Machine: Unstructured meshes, upwind schemes and massively parallel computations},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {102},
number = {1},
pages = {61-88},
year = {1993},
issn = {0045-7825},
doi = {https://doi.org/10.1016/0045-7825(93)90141-J},
url = {https://www.sciencedirect.com/science/article/pii/004578259390141J},
author = {Charbel Farhat and Loula Fezoui and Stéphane Lanteri},
abstract = {Here we report on our effort in simulating two-dimensional viscous flows on the Connection Machine, using a second-order accurate monotomic upwind scheme for conservation laws (MUSCL) on fully unstructured grids. The spatial approximation combines an upwind finite volume method for the discretization of the convective fluxes with a classical Galerkin finite element method for the discretization of the diffusive fluxes. The resulting semi-discrete equations are time integrated with a second-order low-storage explicit Runge-Kutta method. A communication efficient strategy for mapping thousands of processors onto an arbitrary mesh is presented and proposed as an alternative to the fast north-east-west-south (NEWS) communication mechanism, which is restricted to structured grids. Measured performance results for the simulation of low Reynolds number chaotic flows indicate that an 8K CM-2 (8192 processors) with single precision floating point arithmetic is at least as fast as one CRAY-2 processor.}
}
@article{LIU2018164,
title = {Neural and genetic determinants of creativity},
journal = {NeuroImage},
volume = {174},
pages = {164-176},
year = {2018},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2018.02.067},
url = {https://www.sciencedirect.com/science/article/pii/S1053811918301745},
author = {Zhaowen Liu and Jie Zhang and Xiaohua Xie and Edmund T. Rolls and Jiangzhou Sun and Kai Zhang and Zeyu Jiao and Qunlin Chen and Junying Zhang and Jiang Qiu and Jianfeng Feng},
abstract = {Creative thinking plays a vital role in almost all aspects of human life. However, little is known about the neural and genetic mechanisms underlying creative thinking. Based on a cross-validation based predictive framework, we searched from the whole-brain connectome (34,716 functional connectivities) and whole genome data (309,996 SNPs) in two datasets (all collected by Southwest University, Chongqing) consisting of altogether 236 subjects, for a better understanding of the brain and genetic underpinning of creativity. Using the Torrance Tests of Creative Thinking score, we found that high figural creativity is mainly related to high functional connectivity between the executive control, attention, and memory retrieval networks (strong top-down effects); and to low functional connectivity between the default mode network, the ventral attention network, and the subcortical and primary sensory networks (weak bottom-up processing) in the first dataset (consisting of 138 subjects). High creativity also correlates significantly with mutations of genes coding for both excitatory and inhibitory neurotransmitters. Combining the brain connectome and the genomic data we can predict individuals' creativity scores with an accuracy of 78.4%, which is significantly better than prediction using single modality data (gene or functional connectivity), indicating the importance of combining multi-modality data. Our neuroimaging prediction model built upon the first dataset was cross-validated by a completely new dataset of 98 subjects (r = 0.267, p = 0.0078) with an accuracy of 64.6%. In addition, the creativity–related functional connectivity network we identified in the first dataset was still significantly correlated with the creativity score in the new dataset (p<10−3). In summary, our research demonstrates that strong top-down control versus weak bottom-up processes underlie creativity, which is modulated by competition between the glutamate and GABA neurotransmitter systems. Our work provides the first insights into both the neural and the genetic bases of creativity.}
}
@article{NISSEL2024105856,
title = {Why wearing a yellow hat is impossible: Chinese and U.S. children's possibility judgments},
journal = {Cognition},
volume = {251},
pages = {105856},
year = {2024},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2024.105856},
url = {https://www.sciencedirect.com/science/article/pii/S0010027724001422},
author = {Jenny Nissel and Jiaying Xu and Lihanjing Wu and Zachary Bricken and Jennifer M. Clegg and Hui Li and Jacqueline D. Woolley},
keywords = {Cognitive development, Social development, Possibility, Intuitive theories, Cross-cultural, LIWC},
abstract = {When thinking about possibility, one can consider both epistemic and deontic principles (i.e., physical possibility and permissibility). Cultural influences may lead individuals to weigh epistemic and deontic obligations differently; developing possibility conceptions are therefore positioned to be affected by cultural surroundings. Across two studies, 251 U.S. and Chinese 4-, 6-, and 8-year-olds sampled from major metropolitan areas in Texas and the Hubei, Sichuan, Gansu, and Guangdong Provinces judged the possibility of impossible, improbable, and ordinary events. Across cultures and ages, children judged ordinary events as possible and impossible events as impossible; cultural differences emerged in developing conceptions of improbable events. Whereas U.S. children became more likely to judge these events possible with age, Chinese children's judgments remained consistent with age: Chinese 4- to 8-year-olds judged these events to be possible ∼25% of the time. In Study 2, to test whether this difference was attributable to differential prioritization of epistemic versus deontic constraints, children also judged whether each event was an epistemic violation (i.e., required magic to happen) and a deontic violation (i.e., would result in someone getting in trouble). With age, epistemic judgments were increasingly predictive of possibility judgments for improbable events for U.S. children, and decreasingly so for Chinese children. Contrary to our predictions, deontic judgments were not predictive. We propose that cultural valuation of norms might shape children's developing intuitions about possibility. We discuss our findings in light of three accounts of possibility conceptions, suggesting ways to integrate cultural context into each.}
}
@incollection{LEE2016135,
title = {Chapter 7 - Identifying and Tracking Emotional and Cognitive Mathematical Processes of Middle School Students in an Online Discussion Group},
editor = {Sharon Y. Tettegah and Michael P. McCreery},
booktitle = {Emotions, Technology, and Learning},
publisher = {Academic Press},
address = {San Diego},
pages = {135-153},
year = {2016},
series = {Emotions and Technology},
isbn = {978-0-12-800649-8},
doi = {https://doi.org/10.1016/B978-0-12-800649-8.00002-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012800649800002X},
author = {Amos Lee and Sharon Tettegah},
keywords = {Online discourse, Math discussions, Math learning, Systemic functional linguistics, Identification analysis},
abstract = {Math discussions are important when learning math. Explaining one’s thinking, listening to other’s thoughts, and reflecting are but a few of the benefits derived from discussions held in class. However, with the growth of online courses, how do math discussions change when in an online setting? While much research exists about math discussions in classrooms, there is not much research on math discussions held online. Due to the important role of discussions in learning math, along with the growing trend of online classes, this study begins to take a look at how students make sense of and keep track of each other’s comments in an online discussion. In these online discussions, turn taking is not as intuitive as face-to-face interactions. Making sense of the discussion sequence and theme can also be challenging. In this study, I found that students used terms that represented mathematical operations to better explain their thought processes and also kept track of how their peers used these terms as well. These findings suggest that, for these students, when in an online discussion, the terms used were of importance when trying to make their thinking clear to their classmates. Also, in these groups, the mathematical terms were commonly used and re-used by more than one individual in trying to gain a consensus in their group thinking. These findings are important when thinking about how to best foster math discussion and learning in an online environment and for designing online classes that institutions use to supplement or support students.}
}
@article{SOUZA2025112323,
title = {Techniques for eliciting IoT requirements: Sensorina Map and Mind IoT},
journal = {Journal of Systems and Software},
volume = {222},
pages = {112323},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112323},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224003674},
author = {Sabrina Souza and Eriky Rodrigues and Maria Meireles and Tanara Lauschner and Leandro Carvalho and José Carlos Maldonado and Tayana Conte},
keywords = {Requirements engineering, Design Thinking, Requirements’ elicitation techniques, Internet of Things},
abstract = {Context:
The Internet of Things (IoT) involves heterogeneous devices that interact and process data via the Internet. In the development of IoT systems, requirement elicitation is crucial. However, challenges such as heterogeneity, interoperability, scalability, and requirements volatility necessitate new approaches or adapting traditional techniques.
Objective:
In this context, this work proposes the Sensorina Map and IoT Mind as techniques adapted from the Empathy Map and Mind Map, respectively, to support requirement elicitation in IoT systems.
Method:
Two empirical studies were conducted in an academic environment to assess the feasibility of the techniques, then, a case study in industry environment.
Results:
The first study analyzed the ease of use and evaluated if it assisted software engineers in remembering the system requirements. The participants’ perceptions were collected through a Focus Group, refining the techniques. Subsequently, an observational study evaluated the techniques’ usefulness and ease of use. The results of the study demonstrated that the participants considered the methods feasible. The case study results revealed that the Sensorina Map is more suitable for advanced stages. At the same time, the Mind IoT suits better the initial phases, emphasizing the need for practical examples and adaptations to suit diverse user profiles.
Conclusion:
This work is expected to advance research in IoT systems and benefit professionals and researchers in this area.}
}
@article{SINGH2024101269,
title = {An empirical approach to understand the role of emotions in code comprehension},
journal = {Journal of Computer Languages},
volume = {79},
pages = {101269},
year = {2024},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2024.101269},
url = {https://www.sciencedirect.com/science/article/pii/S2590118424000121},
author = {Divjot Singh and Ashutosh Mishra and Ashutosh Aggarwal},
keywords = {Code comprehension, Systematic literature review, Emotions, Cognitive skills},
abstract = {Programming and cognitive skills are two pivotal abilities of programmers to maintain software products. First, this study included a systematic literature review on code comprehension, emotions, cognitive psychology, and belief-desire-intention domains to analyse various code comprehension monitoring techniques, performance metrics, and computational methodologies. Second, a case study is conducted to examine the influence of various emotional stages on programmers’ programming and cognitive skills while comprehending the software code. The categorization of the participants is done empirically based on their expertism level, and the same results are verified using various machine learning models and performance metrics.}
}
@article{MATHIS20245814,
title = {Decoding the brain: From neural representations to mechanistic models},
journal = {Cell},
volume = {187},
number = {21},
pages = {5814-5832},
year = {2024},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2024.08.051},
url = {https://www.sciencedirect.com/science/article/pii/S0092867424009802},
author = {Mackenzie Weygandt Mathis and Adriana {Perez Rotondo} and Edward F. Chang and Andreas S. Tolias and Alexander Mathis},
keywords = {decoding, encoding, deep learning, data-driven, normative models, BCIs, language},
abstract = {Summary
A central principle in neuroscience is that neurons within the brain act in concert to produce perception, cognition, and adaptive behavior. Neurons are organized into specialized brain areas, dedicated to different functions to varying extents, and their function relies on distributed circuits to continuously encode relevant environmental and body-state features, enabling other areas to decode (interpret) these representations for computing meaningful decisions and executing precise movements. Thus, the distributed brain can be thought of as a series of computations that act to encode and decode information. In this perspective, we detail important concepts of neural encoding and decoding and highlight the mathematical tools used to measure them, including deep learning methods. We provide case studies where decoding concepts enable foundational and translational science in motor, visual, and language processing.}
}
@article{STEPHEN2021103085,
title = {Automated essay scoring (AES) of constructed responses in nursing examinations: An evaluation},
journal = {Nurse Education in Practice},
volume = {54},
pages = {103085},
year = {2021},
issn = {1471-5953},
doi = {https://doi.org/10.1016/j.nepr.2021.103085},
url = {https://www.sciencedirect.com/science/article/pii/S1471595321001219},
author = {Tracey C. Stephen and Mark C. Gierl and Sharla King},
keywords = {Automated essay scoring, Constructed-response examinations, Nursing education assessment, Reliability measures},
abstract = {Nursing students’ higher-level thinking skills are ideally assessed through constructed-response items. At the baccalaureate level in North America, however, this exam format has largely fallen into disuse owing to the labor-intensive process of scoring written exam papers. The authors sought to determine if automated essay scoring (AES) would be an efficient and reliable alternative to human scoring. Four constructed-response exam items were administered to an initial cohort of 359 undergraduate nursing students in 2016 and to a second cohort of 40 students in 2018. The items were graded by two human raters (HR1 & HR2) and an AES software platform. AES approximated or surpassed agreement and reliability measures achieved by the HR1 and HR2 with each other, and AES surpassed both human raters in efficiency. A list of answer keywords was created to increase the efficiency and reliability of AES. Low agreement between human raters may be explained by rater drift and fatigue, and shortcomings in the development of Item 1 may have reduced its overall agreement and reliability measures. It can be concluded that AES is a reliable and cost-effective means of scoring constructed-response nursing examinations, but further studies employing greater sample sizes are needed to establish this definitively.}
}
@article{LEUNG2020345,
title = {Limited cognitive ability and selective information processing},
journal = {Games and Economic Behavior},
volume = {120},
pages = {345-369},
year = {2020},
issn = {0899-8256},
doi = {https://doi.org/10.1016/j.geb.2020.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0899825620300063},
author = {Benson Tsz Kin Leung},
keywords = {Limited ability, Information overload, Information avoidance, Confirmation bias, Wishful thinking, Polarization},
abstract = {This paper studies the information processing behavior of a decision maker (DM) who can only process a subset of all information he receives: before taking an action, the DM receives sequentially a number of signals and decides whether to process or ignore each of them as it is received. The model generates an information processing behavior consistent with that documented in the psychological literature: first, the DM chooses to process signals that are strong; second, his processing strategy exhibits confirmation bias if he has a strong prior belief; third, he tends to process signals that suggest favorable outcomes (wishful thinking). As an application I analyze how the Internet and the induced change in information availability affects the processing behavior of the DM. I show that providing more/better information to the DM could strengthen his confirming bias.}
}
@article{ORJI2022100626,
title = {Assessing the pre-conditions for the pedagogical use of digital tools in the Nigerian higher education sector},
journal = {The International Journal of Management Education},
volume = {20},
number = {2},
pages = {100626},
year = {2022},
issn = {1472-8117},
doi = {https://doi.org/10.1016/j.ijme.2022.100626},
url = {https://www.sciencedirect.com/science/article/pii/S1472811722000283},
author = {Ifeyinwa Juliet Orji and Frank Ojadi and Ukoha Kalu Okwara},
keywords = {Digitalization, Higher education, TOE theory, Social media, Learning outcomes, Nigeria},
abstract = {Currently, there is a burgeoning interest in digitalization as evidenced in extant literature. Nevertheless, the effect, based on teachers’ own perspectives, of the pedagogical use of digital technologies on learning outcomes in the higher education sector has been under-investigated. Thus, this paper aims to investigate the pre-conditions for the effective adoption of social media tools in the Nigerian higher education sector and to assess the impact of the adoption on specific learning outcomes. A multi-criteria decision-making (MCDM) methodology was proposed for study analysis, aided by views of experts with sufficient teaching experience in Nigerian business school programs. The results indicate that adequate budgetary allocations, technical competence, a sufficient level of privacy, and an effective government regulatory framework are the most important of the investigated pre-conditions. Additionally, the pedagogical use of social media in business school programs is more strongly associated with learning outcomes such as professionalism and strategic thinking, emotional intelligence, and social maturity. Hence, the article offers guidance to decision-makers in the higher education sector on how to actualize the successful adoption of social media for pedagogical use and build effective business strategies at various levels of the digitalization process.}
}
@article{WANG2024111842,
title = {Three-way decision based island harmony search algorithm for robust flow-shop scheduling with uncertain processing times depicted by big data},
journal = {Applied Soft Computing},
volume = {162},
pages = {111842},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111842},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624006161},
author = {Bing Wang and Pengfei Zhang and Xiaozhi Wang and Quanke Pan},
keywords = {Robust flow-shop scheduling, Island harmony search, Big data, Three-way decision, Surrogate worst-case scenario},
abstract = {This paper discusses an uncertain two-machine permutation flow-shop scheduling problem (2PFSP) with total weighted tardiness and common due date. Uncertain processing times are described by a large set of discrete scenarios, which is a type of big data. The objective is to minimize the schedule performance under the worst-case scenario. Identifying the worst-case scenario for each evaluated schedule is quite time-consuming in the situation that the scenario set size is large so that the objective evaluation might be computationally expensive. To handle this difficulty, three-way decision is used to preprocess the large-size scenario set to get a reduced scenario set so that the concept of surrogate worst-case scenario is adopted. A hybrid harmony search algorithm of combining three-island framework and the scenario-based local search is developed to solve the discussed problem. Based on the single-scenario knowledge of 2PFSP, a problem-specific scenario-dependent neighborhood structure is constructed under the surrogate worst-case scenario. An extensive experiment was carried out. The computational results show that the application of surrogate worst-case scenario based on three-way decision is effective in reducing the time consuming while keeping schedule performance evaluation. Being compared to the worst-case scenario objective evaluation, for an example in the case of the middle bad-scenario ratio, the surrogate worst-case scenario objective evaluation made the solution algorithm save 12.95 % in average CPU time for all instances while the relative performance difference is only 1.809 % in average. Being compared to possible alternative algorithms derived from the state-of-the-art algorithms, the developed algorithm is advantageous for the addressed problems.}
}
@article{KINNEAR2024101190,
title = {Lecturers' use of questions in undergraduate mathematics lectures},
journal = {The Journal of Mathematical Behavior},
volume = {76},
pages = {101190},
year = {2024},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2024.101190},
url = {https://www.sciencedirect.com/science/article/pii/S0732312324000671},
author = {George Kinnear and Gemma Hood and Eloise Lardet and Colette Sheard and Colin Foster},
keywords = {Funneling, Lecturing styles, Questioning, Student participation, University mathematics},
abstract = {Mathematics lecturers frequently ask questions in their lectures, and these questions presumably play an important role in students’ thinking about and learning of the lecture content. We replicated and developed a coding scheme used in previous research in the US to categorise lecturers’ questions in a sample of 136 lectures given by 24 lecturers at a research-intensive UK university. We found that the coding scheme could be applied reliably, and that factual questions were predominant (as in previous research). We explore differences in the lecturers’ use of questions – both between our UK sample and the previous US work, and between individual lecturers in our sample. We note the presence of strings of related successive questions from the lecturer, which we term ‘question chains’. We explore the nature of these, examine their prevalence, and seek to account for them in terms of the lecturers’ possible intentions.}
}
@article{XU2023108916,
title = {Joint optimization task offloading and trajectory control for unmanned-aerial-vehicle-assisted mobile edge computing},
journal = {Computers and Electrical Engineering},
volume = {111},
pages = {108916},
year = {2023},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2023.108916},
url = {https://www.sciencedirect.com/science/article/pii/S0045790623003403},
author = {Fei Xu and Sen Wang and Weiya Su and Lin Zhang},
keywords = {Edge computing, Computation offloading, Deep reinforcement learning, Unmanned Aerial Vehicle, Trajectory control},
abstract = {The appearance of Mobile Edge Computing (MEC) and Unmanned Aerial Vehicle (UAV) is significant for the future progress of the Internet of Things (IoT). Since the system model with a continuous action space and high-dimensional state space, the joint optimization of UAV trajectory and the computational offloading problem is non-convex, and traditional algorithms for instance ant colony algorithm, genetic algorithm, Actor Critic (AC) algorithm, and Deep Deterministic Policy Gradient (DDPG) algorithm are difficult to cope with. Reasonably formulating the computational task offloading strategy and the trajectory control of the UAV is crucial for the high-efficiency completion of the task. In this paper, a computational offloading and trajectory control system model for UAV-assisted MEC is proposed. We seek to maximize the user ratio of coverage by jointly optimizing computing offload scheduling and UAV trajectories. We propose an improved DDPG algorithm to optimize the objective function and achieve the optimal solution. Meanwhile, our algorithm can achieve an improvement in the user rate of coverage while avoiding obstacles as compared with baseline algorithms, AC, and DDPG.}
}
@article{ZHOU2023126996,
title = {Coal consumption prediction in thermal power units: A feature construction and selection method},
journal = {Energy},
volume = {273},
pages = {126996},
year = {2023},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2023.126996},
url = {https://www.sciencedirect.com/science/article/pii/S0360544223003900},
author = {Jian Zhou and Wei Zhang},
keywords = {Thermal power units, Coal consumption prediction, Regression analysis, K-means algorithm, Genetic algorithm},
abstract = {Digitization and related facilities have enabled the thermal power generation enterprises to record real-time data of thermal power units. There are many data-driven applications based on real-time monitoring and operational data in power units, while limited studies lay on the operational improvements, especially on coal consumption prediction under all working conditions. We build an intelligent prediction model of coal consumption based on key features selection, working condition clustering, and regression analysis. We combine feature construction and feature selection methods to cope with the problem caused by directly specifying feature subset for model building of traditional prediction method, which may fall into the thinking pattern and miss potentially better feature subset. Besides, to cope with the different coal consumption under different working conditions, we apply cluster analysis to construct a sub-coal consumption prediction model for each cluster category. Numerical results show that compared with other methods, it has the advantages of lower regression error and moderate model complexity, which can provide efficient decision support for operational improvement in thermal power generation.}
}
@article{CHANG20114075,
title = {Dynamic multi-criteria evaluation of co-evolution strategies for solving stock trading problems},
journal = {Applied Mathematics and Computation},
volume = {218},
number = {8},
pages = {4075-4089},
year = {2011},
issn = {0096-3003},
doi = {https://doi.org/10.1016/j.amc.2011.09.032},
url = {https://www.sciencedirect.com/science/article/pii/S0096300311012033},
author = {Ying-Hua Chang and Tz-Ting Wu},
keywords = {Co-evolutionary model, Evolution strategies, Artificial neural network, Dynamic stock trading decision making, Optimization},
abstract = {Risk and return are interdependent in a stock portfolio. To achieve the anticipated return, comparative risk should be considered simultaneously. However, complex investment environments and dynamic change in decision making criteria complicate forecasts of risk and return for various investment objects. Additionally, investors often fail to maximize their profits because of improper capital allocation. Although stock investment involves multi-criteria decision making (MCDM), traditional MCDM theory has two shortfalls: first, it is inappropriate for decisions that evolve with a changing environment; second, weight assignments for various criteria are often oversimplified and inconsistent with actual human thinking processes. In 1965, Rechenberg proposed evolution strategies for solving optimization problems involving real number parameters and addressed several flaws in traditional algorithms, such as their use of point search only and their high probability of falling into optimal solution area. In 1992, Hillis introduced the co-evolutionary concept that the evolution of living creatures is interactive with their environments (multi-criteria) and constantly improves the survivability of their genes, which then expedites evolutionary computation. Therefore, this research aimed to solve multi-criteria decision making problems of stock trading investment by integrating evolutionary strategies into the co-evolutionary criteria evaluation model. Since co-evolution strategies are self-calibrating, criteria evaluation can be based on changes in time and environment. Such changes not only correspond with human decision making patterns (i.e., evaluation of dynamic changes in criteria), but also address the weaknesses of multi-criteria decision making (i.e., simplified assignment of weights for various criteria). Co-evolutionary evolution strategies can identify the optimal capital portfolio and can help investors maximize their returns by optimizing the preoperational allocation of limited capital. This experimental study compared general evolution strategies with artificial neural forecast model, and found that co-evolutionary evolution strategies outperform general evolution strategies and substantially outperform artificial neural forecast models. The co-evolutionary criteria evaluation model avoids the problem of oversimplified adaptive functions adopted by general algorithms and the problem of favoring weights but failing to adaptively adjust to environmental change, which is a major limitation of traditional multi-criteria decision making. Doing so allows adaptation of various criteria in response to changes in various capital allocation chromosomes. Capital allocation chromosomes in the proposed model also adapt to various criteria and evolve in ways that resemble thinking patterns.}
}
@incollection{TONDEUR2024184,
title = {Chapter 6 - Batch control spike},
editor = {Yves Tondeur},
booktitle = {Sustainable Quality Improvements for Isotope Dilution in Molecular Ultratrace Analyses},
publisher = {Elsevier},
pages = {184-239},
year = {2024},
isbn = {978-0-443-29034-3},
doi = {https://doi.org/10.1016/B978-0-443-29034-3.00031-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443290343000314},
author = {Yves Tondeur},
keywords = {Compliance with PBMS/methods innovation rule & ISO 17025, Critical step, Erroneous beliefs & mental models, Function of the labeled standards, Out-of-control analytical system, Performance improvement, Sample fortification integrity, Systematic errors, Technology-in-use definition, Thinking method, Traditional QC samples effectiveness, Working relative response factors},
abstract = {When we are unaware of the causes for the observed deviations, we are more likely to react. When we react or are unaware of problems, we fail. Increasing the effectiveness of quality control samples starts with addressing what we do not know about them. So, this chapter first clarifies what we want, then describes what we have, and lastly what can be done to fill the gaps. The development, validation, and application of the batch control spike is a great illustration highlighting the importance the quality of the performance feedback and learning capacity the control samples are supposed to provide. When done correctly—while contextually questioning the relevance and appropriateness of current operating criteria, imposed limits and standards—the introduction of the batch control spike allows a process of critical errors identification, compensation, and progressive elimination to take place so that at the end, no significant systematic errors remain. This fact is demonstrated using z-scores from multiple international round-robin studies. In the context of achieving accuracy, the batch control spike examines the relationships between the standards used, when they are spiked, how they are spiked, and their purpose, that is, it renders the technology-in-use (isotope dilution) transparent and keeps it honest. It is also an excellent teaching tool. It is a quality learning sample helping the transformation of our methods into “thinking tools.”}
}
@article{RIEBEL2024105084,
title = {Transient modeling of stratified thermal storage tanks: Comparison of 1D models and the Advanced Flowrate Distribution method},
journal = {Case Studies in Thermal Engineering},
volume = {61},
pages = {105084},
year = {2024},
issn = {2214-157X},
doi = {https://doi.org/10.1016/j.csite.2024.105084},
url = {https://www.sciencedirect.com/science/article/pii/S2214157X24011158},
author = {Adrian Riebel and Ian Wolde and Rodrigo Escobar and Rodrigo Barraza and José M. Cardemil},
keywords = {Sensible heat storage, TES, Thermal modeling, Transient simulation, Experimental validation},
abstract = {Thermal energy storage (TES) is one of the key technologies for enabling a higher deployment of renewable energy. In this context, the present study analyzes the modeling strategies of one of the most common TES systems: stratified thermal storage tanks. These systems are essential to many solar thermal installations and heat pumps, among other clean energy technologies. Three different one-dimensional tank models are compared by their computing speed and resilience to long time steps. Two of the models analyzed are numerical, one being explicit and the other one implicit, and the other is analytical. The models are validated against data from experiments carried out considering small-scale stratified tanks, showing that their performance can be improved by using the Advanced Flowrate Distribution (AFD) method. The results show that the analytical model maintains its accuracy with longer time steps and is robust against divergence. Conversely, the numerical models show equivalent performance for short time steps, while the computation time is reduced. Although the AFD method shows promising results by achieving an improvement of 43% in terms of Dynamic Time Warping, its parameter optimization must be generalized for different tank designs, flow rates, and temperatures.}
}
@article{LEONIDOV2022112279,
title = {Strategic stiffening/cooling in the Ising game},
journal = {Chaos, Solitons & Fractals},
volume = {160},
pages = {112279},
year = {2022},
issn = {0960-0779},
doi = {https://doi.org/10.1016/j.chaos.2022.112279},
url = {https://www.sciencedirect.com/science/article/pii/S0960077922004891},
author = {Andrey Leonidov and Ekaterina Vasilyeva},
keywords = {Binary choice game, Ising game, Graph, Forward-looking, Myopic, Noise},
abstract = {The dynamic noisy binary choice (Ising) game of forward-looking agents on a complete graph is analysed. It is shown that strategic considerations lead to effective interaction strengthening (noise reduction) as compared to the myopic game. We show that strategic agents are able to come to consensus in the wider range of noise values than myopic ones. Effective population dynamics with time-dependent probabilities reflecting this strategic stiffening/cooling effect is described.}
}
@article{PACINI200969,
title = {Synergy: A Framework for Leadership Development and Transformation},
journal = {Perioperative Nursing Clinics},
volume = {4},
number = {1},
pages = {69-74},
year = {2009},
note = {Leadership},
issn = {1556-7931},
doi = {https://doi.org/10.1016/j.cpen.2008.10.010},
url = {https://www.sciencedirect.com/science/article/pii/S1556793108001022},
author = {Christine M. Pacini},
keywords = {Synergy, Leadership development, Orientation, Professional development, Staff development, Clinical education},
abstract = {Given the current demands of the health care environment, the need for nurses minimally competent in clinical judgment, caring practice, advocacy and moral agency, collaboration, responsiveness to diversity, systems thinking, inquiry, and facilitation of learning is critical in light of ever-increasing contextual complexity and variability of patient needs. The Synergy Model provides an exemplary and relevant framework for clinical practice with the ultimate aim of improving patient outcomes. Tenets of accountability and professionalism are central to the model and, in its entirety, it provides a practical and useful approach for thinking about and redesigning educational products and processes in clinical settings.}
}