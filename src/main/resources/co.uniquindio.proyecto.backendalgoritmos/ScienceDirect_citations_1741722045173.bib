@article{RONGHUA2024e27753,
title = {Improved ant colony optimization for safe path planning of AUV},
journal = {Heliyon},
volume = {10},
number = {7},
pages = {e27753},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e27753},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024037848},
author = {Meng Ronghua and Cheng Xinhao and Wu Zhengjia and  {Du xuan}},
keywords = {Improved ant colony optimization, Safety factors, Dam inspections},
abstract = {In order to address the autonomous underwater vehicle navigation challenge for dam inspections, with the goal of enabling safe inspections and reliable obstacle avoidance, an improved smooth Ant Colony Optimization algorithm is proposed for path planning. This improved algorithm would optimize the smoothness of the path besides the robustness, avoidance of local optima, and fast computation speed. To achieve the goal of reducing turning time and improving the directional effect of path selection, a corner-turning heuristic function is introduced. Experimental simulation results show that the improved algorithm performs best than other algorithms in terms of path smoothness and iteration stability in path planning.}
}
@article{SALINGER1994139,
title = {Massively parallel finite element computations of three-dimensional, time-dependent, incompressible flows in materials processing systems},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {119},
number = {1},
pages = {139-156},
year = {1994},
issn = {0045-7825},
doi = {https://doi.org/10.1016/0045-7825(94)00081-6},
url = {https://www.sciencedirect.com/science/article/pii/0045782594000816},
author = {Andrew G. Salinger and Qiang Xiao and Yuming Zhou and Jeffrey J. Derby},
abstract = {A parallel implementation of the Galerkin finite element method for three-dimensional, incompressible flows is presented. The inherent element-by-element parallelism of the method is exploited to make efficient use of the architecture of the CM-5 computer. Our implementation features a mixed formulation to expand the primitive variables using triquadratic brick elements with linear, discontinuous pressure basis functions, and the GMRES method with diagonal preconditioning is employed to solve the linear system at each Newton iteration. Transitions among flow states in the classical Taylor-Couette system, which are representative of the complexity of flows found in materials processing systems, are computed as benchmark solutions, and preliminary results are presented for flow in a large-scale, solution crystal growth system. Sustained calculation rates of up to 6 GigaFLOPS are achieved on 512 processors of the CM-5.}
}
@incollection{SRIPRASADH202545,
title = {Chapter 3 - Review of existing neuromorphic systems},
editor = {Harish Garg and Jyotir {Moy Chatterjee} and R. Sujatha and Shatrughan Modi},
booktitle = {Primer to Neuromorphic Computing},
publisher = {Academic Press},
pages = {45-66},
year = {2025},
isbn = {978-0-443-21480-6},
doi = {https://doi.org/10.1016/B978-0-443-21480-6.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780443214806000018},
author = {K. Sriprasadh},
keywords = {Deep learning, Neural network, Machine learning, Expert systems},
abstract = {Computer systems try to run in a similar manner like human brain. Interfacing the human brain activity with the computer device and making it to learn by thinking as human is coined the name neuromorphic system. Basically, neuromorphic form of computation performance ideology was initiated from the mathematical analysis started from the year of 1936, by mathematician and computer scientist Alan Turing, who created an algorithm to perform mathematical equation or solve mathematical problems through a machine. In 1949, he published the paper in name of intelligent machinery. The machine was named after him as Turing machine, which solved mathematical equations. This format was compared with humans; comparatively, humans were able to perform better than the system. The proposed model was coined the name cognitive modeling machinery. This model was the first step made by humans to create system model like the human brain. In 1949, Canadian psychologist Donald Hebb identified a supportive model of neuroscience correlating synaptic plasticity and learning, connecting human brain activity with an algorithm. In the year of 1950, Turing tested his Turing machine, which rendered his results. Based on the result, US navy created the Perceptron and human brain activity was mapped up to the level. But total activity of human brain cannot map due to lack of technology support. From 1980, neuromorphic research was taken through by Caltech professor Carver Mead. He created a analog silicon retina model and cochlea in 1981. Mead identified and proposed that computers can perform every action that human nervous system is capable of doing. In 2013, Henry Markram launched a system HBP like the human brain form, capable of understanding the human brain activity up to 10years and tried apply this format in science and technology. Recently, the neuromorphic system relies on AI and machine learning models and tries to support humans in detecting and decision-making in some critical situations. Neuromorphic systems basically make the decision through fuzzy neural and deep learning inputs. In this chapter, a review is made on features of trending neuromorphic system, and the future of the neuromorphic system is analyzed. The readers will gain the knowledge about the features of the neuromorphic systems and get an idea how it could be developed for different applications similar to decision-making and as expert system model in the form of humanoid.}
}
@incollection{ASCHEID200711,
title = {Chapter 2 - Opportunities for Application-Specific Processors: The Case of Wireless Communications},
editor = {Paolo Lenne and Rainer Leupers},
booktitle = {Customizable Embedded Processors},
publisher = {Morgan Kaufmann},
address = {Burlington},
pages = {11-37},
year = {2007},
series = {Systems on Silicon},
issn = {18759661},
doi = {https://doi.org/10.1016/B978-012369526-0/50003-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780123695260500036},
author = {Gerd Ascheid and Heinrich Meyr},
abstract = {Publisher Summary
A paradigm change in designing complex systems-on-chip (SoCs) occurs roughly every 12 years because of the exponentially increasing number of transistors on a chip. This paradigm change is characterized by a move to a higher level of abstraction. Instead of thinking in register-transfer level (RTL) blocks and wires, computing elements and interconnect are needed to be thought. The next design discontinuity will lead to different solutions, depending on the application. The following core propositions for wireless communications are made: future SoC for wireless communications will be heterogeneous, reconfigurable Multi-Processor System-on-Chip (MPSoC). They will contain computational elements that cover the entire spectrum, from fixed functionality blocks to domain-specific DSPs and general-purpose processors. A key role will be played by ASIPs. ASIPs exploit the full architectural space (memory, interconnect, instruction set, parallelism), so they are optimally matched to a specific task. The heterogeneous computational elements will communicate via a network-on-chip (NoC), as the conventional bus structures do not scale. These MPSoC platforms will be designed by a cross-disciplinary team. This chapter substantiates this proposition. It begins by analyzing the properties of future wireless communication systems and observes that the systems are computationally demanding. Furthermore, they need innovative architectural concepts to be energy efficient. The chapter discusses the canonical structure of a digital receiver for wireless communication and addresses the design of ASIPs.}
}
@article{VISWAN2023102808,
title = {Understanding molecular signaling cascades in neural disease using multi-resolution models},
journal = {Current Opinion in Neurobiology},
volume = {83},
pages = {102808},
year = {2023},
issn = {0959-4388},
doi = {https://doi.org/10.1016/j.conb.2023.102808},
url = {https://www.sciencedirect.com/science/article/pii/S0959438823001332},
author = {Nisha Ann Viswan and Upinder Singh Bhalla},
abstract = {If the genome defines the program for the operations of a cell, signaling networks execute it. These cascades of chemical, cell-biological, structural, and trafficking events span milliseconds (e.g., synaptic release) to potentially a lifetime (e.g., stabilization of dendritic spines). In principle almost every aspect of neuronal function, particularly at the synapse, depends on signaling. Thus dysfunction of these cascades, whether through mutations, local dysregulation, or infection, leads to disease. The sheer complexity of these pathways is matched by the range of diseases and the diversity of their phenotypes. In this review, we discuss how to build computational models, how these models are essential to tackle this complexity, and the benefits of using families of models at different levels of detail to understand signaling in health and disease.}
}
@article{ADAMO2024109162,
title = {Crop planting layout optimization in sustainable agriculture: A constraint programming approach},
journal = {Computers and Electronics in Agriculture},
volume = {224},
pages = {109162},
year = {2024},
issn = {0168-1699},
doi = {https://doi.org/10.1016/j.compag.2024.109162},
url = {https://www.sciencedirect.com/science/article/pii/S0168169924005532},
author = {Tommaso Adamo and Lucio Colizzi and Giovanni Dimauro and Emanuela Guerriero and Deborah Pareo},
keywords = {Constraint programming, Optimization crop planting layout, AI planning, Smart Agriculture, Intercropping systems},
abstract = {In sustainable agriculture, intercropping systems represent a valuable approach. These systems involve placing mutually beneficial plant types in close proximity to each other, with the goal of exploiting biodiversity to reduce pesticide and water usage, as well as improve soil nutrient utilization. Despite its potential, the optimization of intercropping systems has received limited attention in previous studies. One of the first steps in the design of an intercropping system is the solution of the crop planting layout problem, which involves meeting crop demand while maximizing positive interactions between adjacent plants. We perform a complexity analysis of this problem and solve it through constraint programming, an artificial intelligence technique, which relies on automated reasoning, constraint propagation and search heuristics. To this aim, we present two constraint programming models based on integer variables and interval variables, respectively. Through a computational study on real-life instances, we examine the impact of different modelling approaches on the difficulty of solving the crop planting layout problem with standard constraint programming solvers. This research work has also provided the groundwork for a sowing robotic arm (under development), aiming to automate intercropping systems and assist farm workers.}
}
@article{JONES2013122,
title = {Understanding the integral: Students’ symbolic forms},
journal = {The Journal of Mathematical Behavior},
volume = {32},
number = {2},
pages = {122-141},
year = {2013},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2012.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0732312312000612},
author = {Steven R. Jones},
keywords = {Calculus, Integral, Student understanding, Undergraduate mathematics education, Symbolic form, Accumulation},
abstract = {Researchers are currently investigating how calculus students understand the basic concepts of first-year calculus, including the integral. However, much is still unknown regarding the cognitive resources (i.e., stable cognitive units that can be accessed by an individual) that students hold and draw on when thinking about the integral. This paper presents cognitive resources of the integral that a sample of experienced calculus students drew on while working on pure mathematics and applied physics problems. This research provides evidence that students hold a variety of productive cognitive resources that can be employed in problem solving, though some of the resources prove more productive than others, depending on the context. In particular, conceptualizations of the integral as an addition over many pieces seem especially useful in multivariate and physics contexts.}
}
@article{YANG2022101239,
title = {Identifying keyword sleeping beauties: A perspective on the knowledge diffusion process},
journal = {Journal of Informetrics},
volume = {16},
number = {1},
pages = {101239},
year = {2022},
issn = {1751-1577},
doi = {https://doi.org/10.1016/j.joi.2021.101239},
url = {https://www.sciencedirect.com/science/article/pii/S1751157721001103},
author = {Jinqing Yang and Yi Bu and Wei Lu and Yong Huang and Jiming Hu and Shengzhi Huang and Li Zhang},
keywords = {Sleeping beauty, Delayed recognition, Knowledge diffusion trajectory, Survival analysis},
abstract = {Knowledge diffusion is a significant driving force behind discipline development and technological innovation. Keyword is a unique knowledge diffusion trajectory, in which the sleeping beauty phenomenon sometimes appears. In this paper, we first put forward the concept of Keyword Sleeping Beauties (KSBs) on the basis of the scientific literature phenomenon of sleeping beauties. Then, we construct a parameter-free identification method to distinguish KSBs based on beauty coefficient criteria. Furthermore, we analyze the intrinsic and extrinsic influencing factors to explore the awakening mechanism of KSBs. The experiment results show that sleeping beauty phenomena also exist in the keyword diffusion trajectory and 284 KSBs are identified. The depth of sleep has a positive correlation with awakening intensity, while the length of sleep has a negative correlation with awakening intensity. In the two years of pre-awakening, KSBs tend to appear in the journals with a higher impact factor. In addition, the adoption frequency and the number of KSBs both increase obviously in the one year of pre-awakening. The findings of this paper enrich the patterns of knowledge diffusion and extend academic thinking on the sleeping beauty in science.}
}
@article{WU2024112197,
title = {A multi-strategy three-way decision approach for tri-state risk loss under q-rung orthopair fuzzy environment},
journal = {Applied Soft Computing},
volume = {167},
pages = {112197},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.112197},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624009712},
author = {Ping Wu and Yihua Zhong and Chuan Chen and Yanlin Wang and Chao Min},
keywords = {Three-way decision, q-rung orthopair fuzzy sets, Tri-state risk loss, Multi-strategy perspective, Threshold theorem},
abstract = {Addressing the decision-making challenge arising from the uncertainty of human cognition, three-way decision (3WD) and q-rung orthopair fuzzy sets (q-ROFSs) are integrated in this paper to propose a multi-strategy three-way decision approach (MS3WDA) for tri-state risk loss (TSRL) under q-rung orthopair fuzzy environment. Based on the ternary thinking of human cognition, the risk loss with hesitation state is considered and constructed under q-rung orthopair fuzzy environment. The TSRL with hesitation state is further constructed by combining the q-rung orthopair fuzzy (q-ROF) information. The conditional probability adopted by the original object classes is improved and extended by the three components of q-ROFSs. Next, the TSRL with q-ROF information and three components of q-ROFSs are integrated with decision-theoretic rough sets (DTRSs) to establish a novel 3WD model. Some relevant properties are also analyzed and discussed for the developed 3WD model. Then, its multi-strategy decision method is proposed based on the multi-strategy perspective. The related strategies with five different levels are designed by considering three different risk appetite perspectives and four different aspects of q-ROF information. The relevant threshold theorems are also given and proved to further provide the theoretical support for our MS3WDA. According to the five different strategies, we further derive the corresponding decision rules of MS3WDA. The key steps and specific algorithm are summarized for MS3WDA. Finally, a case study is provided to demonstrate the practicability and feasibility of MS3WDA. Meanwhile, the rationality, robustness and superiority of MS3WDA are further validated by the sensitivity analysis and comparative analysis.}
}
@article{XHAXHIU2024270,
title = {Seaweed boards as value-added natural waste product for insulation and building materials},
journal = {Energy Storage and Saving},
volume = {3},
number = {4},
pages = {270-277},
year = {2024},
issn = {2772-6835},
doi = {https://doi.org/10.1016/j.enss.2024.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S2772683524000359},
author = {Kledi Xhaxhiu and Avni Berisha and Nensi Isak and Besnik Baraj and Adelaida Andoni},
keywords = {Seaweed, Natural waste, Waste recycling, Building material, Insulation, Thermal and mechanical properties calculations},
abstract = {Large amounts of seaweed are deposited on shores worldwide daily. The presence of this natural pollutant on the coast is not only considered an environmental burden but also often hinders the development of tourism in the affected areas. Depending on the beach surface area, local governments worldwide spend considerable portions of their budgets to remove seaweed from beaches. Moreover, the removed seaweed occupies increasing space in landfills where it is disposed. Seaweed is noncombustible and decomposes slowly over long periods. In this study, we consider the use of seaweed (a natural waste) as a value-added product for insulation and building materials. Seaweed (Posidonia Oceanica) boards with dimensions of 250 mm × 60 mm × 10 mm were obtained by pressing a mixture of processed seaweed and an organic binder. The as-prepared boards were analyzed for their physical–mechanical properties according to the British standards. The boards with a mean humidity level of 9.15% and density of 404.5 g·cm−3 demonstrated a maximum bending resistance of 2.72 × 103 N·m−2 and mean expansion upon water adsorption of ∼10% with regards to length and width and ∼30% with regards to height. The tested samples showed significant humidity resistance according to the boiling test and an average thermal conductivity of 0.047 W·m−1·K−1, which is comparable to that of polystyrene. Computational analysis of the “seaweed material” model revealed significant thermal and mechanical properties. The mechanical strength of the computed material, including its high Young’s and shear moduli, renders it a promising candidate in construction.}
}
@article{ADHIKARI20121374,
title = {Multi-commodity network flow models for dynamic energy management – Smart Grid applications},
journal = {Energy Procedia},
volume = {14},
pages = {1374-1379},
year = {2012},
note = {2011 2nd International Conference on Advances in Energy Engineering (ICAEE)},
issn = {1876-6102},
doi = {https://doi.org/10.1016/j.egypro.2011.12.1104},
url = {https://www.sciencedirect.com/science/article/pii/S1876610211045243},
author = {R.S. Adhikari and N. Aste and M. Manfren},
keywords = {Dynamic energy management, Smart Grid, Multi-commodity network flow models},
abstract = {The strong interconnection between human activities, energy use and pollution reduction strategies in contemporary society has determined the necessity of collecting scientific knowledge from different fields to provide useful methods and models to foster the transition towards more sustainable energy systems. This is a challenging task in particular for contemporary communities where an increasing demand for services is combined with rapidly changing lifestyles and habits. The Smart Grid concept is the result of a confluence of issues and a convergence of objectives, which include national energy security, climate change, pollution reduction, grid reliability, etc. While thinking about a paradigm shift in energy systems, drivers, characteristics, market segments, applications and other interconnected aspects must be taken into account simultaneously. In this context, the use of multi-commodity network flow models for dynamic energy management aims at finding a compromise between model usefulness, accuracy, flexibility, solvability and scalability in Smart Grid applications.}
}
@article{UDDIN2021106,
title = {Application of Theory in Chronic Pain Rehabilitation Research and Clinical Practice},
journal = {The Open Sports Sciences Journal},
volume = {14},
pages = {106-113},
year = {2021},
issn = {1875-399X},
doi = {https://doi.org/10.2174/1875399X02114010106},
url = {https://www.sciencedirect.com/science/article/pii/S1875399X2100013X},
author = {Zakir Uddin and Joy C. MacDermid and Fatma A. Hegazy and Tara L. Packham},
keywords = {Chronic pain , Hypersensitivity , Theory , Rehabilitation , Disability , T-cell},
abstract = {Introduction
Chronic pain has multiple aetiological factors and complexity. Pain theory helps us to guide and organize our thinking to deal with this complexity. The objective of this paper is to critically review the most influential theory in pain science history (the gate control theory of pain) and focus on its implications in chronic pain rehabilitation to minimize disability.
Methods
In this narrative review, all the published studies that focused upon pain theory were retrieved from Ovoid Medline (from 1946 till present), EMBAS, AMED and PsycINFO data bases.
Results
Chronic pain is considered a disease or dysfunction of the nervous system. In chronic pain conditions, hypersensitivity is thought to develop from changes to the physiological top-down control (inhibitory) mechanism of pain modulation according to the pain theory. Pain hypersensitivity manifestation is considered as abnormal central inhibitory control at the gate controlling mechanism. On the other hand, pain hypersensitivity is a prognostic factor in pain rehabilitation. It is clinically important to detect and manage hypersensitivity responses and their mechanisms.
Conclusion
Since somatosensory perception and integration are recognized as a contributor to the pain perception under the theory, then we can use the model to direct interventions aimed at pain relief. The pain theory should be leveraged to develop and refine measurement tools with clinical utility for detecting and monitoring hypersensitivity linked to chronic pain mechanisms.}
}
@incollection{RENNE2022147,
title = {Chapter 8 - Measuring and assessing resilience},
editor = {John L. Renne and Brian Wolshon and Anurag Pande and Pamela Murray-Tuite and Karl Kim},
booktitle = {Creating Resilient Transportation Systems},
publisher = {Elsevier},
pages = {147-192},
year = {2022},
isbn = {978-0-12-816820-2},
doi = {https://doi.org/10.1016/B978-0-12-816820-2.00005-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128168202000050},
author = {John L. Renne and Brian Wolshon and Anurag Pande and Pamela Murray-Tuite and Karl Kim},
keywords = {Resilience, Measurement, Assessment, Goals, Metrics, Data collection},
abstract = {The ability to effectively apply resilience-oriented thinking into practice starts with understanding, measuring, and evaluating the benefits and costs of resilience. With this knowledge it also becomes possible to comparatively assess potential planning, design, and maintenance options to plan and allocate financial and personnel resources most effectively to address needs. Other key components of practical and meaningful measurements and assessments of resilience are establishing metrics that quantify its performance and knowing what and how much data to collect. Then, understanding what these data mean so that goals, objectives, and expectations of resilience can be set, both within transportation organizations and for the consumers of the services they provide. Unfortunately, there is no universal agreement on what resilience even is, let alone how to systematically measure and assess it. However, recent reviews of practice and research show that ideas and methods to evaluate and assess resilience are evolving at a rapid pace, both within and outside of transportation. This chapter presents a summary of these ideas and compares and contrasts the effort they require to implement and the benefits they are expected to bring.}
}
@article{LUO2023101957,
title = {A design model of FBS based on interval-valued Pythagorean fuzzy sets},
journal = {Advanced Engineering Informatics},
volume = {56},
pages = {101957},
year = {2023},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2023.101957},
url = {https://www.sciencedirect.com/science/article/pii/S147403462300085X},
author = {Yuhan Luo and Minna Ni and Feng Zhang},
keywords = {FBS model, Pythagorean fuzzy sets, AHP, HOQ},
abstract = {The FBS (Function-Behaviour-Structure) model is a research model that stimulates creative thinking of designers in the design process. In order to reduce the influence of user requirement ambiguity on design results in the product design process and improve the accuracy of user requirements in the function-behavior-structure (FBS) design model, this paper proposes an interval-valued Pythagorean fuzzy set-based FBS model integrating AHP and HOQ methods. Firstly, the design model will use IVPF-AHP method to study user requirements and use interval-valued Pythagorean linguistic terms to replace the traditional scoring method of AHP to get the weight of each user requirement. Secondly, the conversion between user requirements and functions will be realized by IVPF-HOQ method, which converts customer requirements into functional characteristics and calculates the weights of each functional characteristic. Finally, the design focus will be filtered according to the order of importance of the functional characteristics, which will be used as functions to guide the development of the FBS model. In this paper, the feasibility and effectiveness of the proposed method will be verified by an application example of a hand-held fluorescence spectrometer. The results show that the proposed FBS model can effectively reduce the subjectivity and ambiguity in the decision-making process, improve the accuracy and information richness of user requirements, and effectively highlight the focus of the design study. The innovation of the proposed method is to provide a more objective and accurate innovative design method for user requirements through the integration of AHP, HOQ and FBS to effectively explore and analyze user requirements. The use of IVPFS to deal with fuzzy information in the design process in a more flexible manner can reduce the ambiguity of requirements when user data is small, and effectively improve the limitations of the FBS design model which is more subjective.}
}
@article{FALZON2006629,
title = {Using Bayesian network analysis to support centre of gravity analysis in military planning},
journal = {European Journal of Operational Research},
volume = {170},
number = {2},
pages = {629-643},
year = {2006},
issn = {0377-2217},
doi = {https://doi.org/10.1016/j.ejor.2004.06.028},
url = {https://www.sciencedirect.com/science/article/pii/S0377221704005156},
author = {Lucia Falzon},
keywords = {Military, Decision analysis, Probabilistic models, Bayesian networks},
abstract = {Centre of gravity (COG) analysis is an integral and cognitively demanding aspect of military operational planning. It involves identifying the enemy and friendly COG and subsequently determining the critical vulnerabilities that have to be degraded or negated to influence the COG of each side. This paper describes a modelling framework based on the causal relationships among the critical capabilities and requirements for an operation. The framework is subsequently used as a basis for the construction, population and analysis of Bayesian networks to support a rigorous and systematic approach to COG analysis. The importance of this work is that it uses existing planning process concepts to facilitate the construction of comprehensive models in which uncertainties and subjective judgements are clearly represented, thus enabling future re-use and traceability. The visual representation of the COG causal structure helps to clarify thinking and provides a way to record and impart this thinking. Moreover, it gives planners the capability to perform impact analysis, that is, to determine which actions are most likely to achieve a desirable end-state. The paper discusses the methodology, development and implementation of the COG Network Effects Tool (COGNET) suite for model population and model checking as well as impact analysis.}
}
@article{BACHMANN2020102937,
title = {Account of consciousness by Christof Koch: Review and questions},
journal = {Consciousness and Cognition},
volume = {82},
pages = {102937},
year = {2020},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2020.102937},
url = {https://www.sciencedirect.com/science/article/pii/S1053810020300143},
author = {Talis Bachmann},
keywords = {Consciousness, Integrated information, Cognitive computation, Microgenesis, Phenomenal experience},
abstract = {This review is set to present the gist of the theoretical account of consciousness recently presented by Christof Koch and pose a couple of questions instigated by this account. The expected answers to these questions would hopefully help to advance our understanding of the basic nature of the conscious mind.}
}
@article{WANG2025111994,
title = {A lightweight progressive joint transfer ensemble network inspired by the Markov process for imbalanced mechanical fault diagnosis},
journal = {Mechanical Systems and Signal Processing},
volume = {224},
pages = {111994},
year = {2025},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2024.111994},
url = {https://www.sciencedirect.com/science/article/pii/S0888327024008926},
author = {Changdong Wang and Jingli Yang and Huamin Jie and Zhen Tao and Zhenyu Zhao},
keywords = {Class imbalance, Ensemble learning, Fault diagnosis, Markov process, Progressive joint-transfer strategy},
abstract = {Owing to safety limitations and data collection costs, scenarios with imbalanced data usually arise, posing a great challenge for precise fault diagnosis. Targeting imbalanced fault diagnosis and the high computational cost of mainstream ensemble learning methods currently used, this article proposes a lightweight and accurate scheme based on a progressive joint-transfer ensemble network (PJTEN) and a Markov-lightweight strategy (MLS). Specifically, a PJTEN is developed, incorporating a multiple excitation-channel attention basic estimator and progressive joint-transfer strategy (PJTS) to maintain diversity of basic estimators better and focus more on key information from minority classes. Besides, the MLS guided by Markov transition probabilities is for the first time constructed for ensemble learning to reduce the network redundancy by alternating optimization. Using a standard dataset and a brand-new dataset of a real ship propulsion system, the proposed method achieves leading results in Accuracy, F1 score and MCC, compared with eight cutting-edge methods, thereby validating its substantial value. In terms of lightweight operation, such as temporal complexity (TC), spatial complexity (SC), and time efficiency, it is also ahead of the latest ensemble-based methods.}
}
@article{COSMIDES198951,
title = {Evolutionary psychology and the generation of culture, part II: Case study: A computational theory of social exchange},
journal = {Ethology and Sociobiology},
volume = {10},
number = {1},
pages = {51-97},
year = {1989},
issn = {0162-3095},
doi = {https://doi.org/10.1016/0162-3095(89)90013-7},
url = {https://www.sciencedirect.com/science/article/pii/0162309589900137},
author = {Leda Cosmides and John Tooby},
keywords = {Reciprocal Altruism, Cooperation, Tit for tat, Cognition, Reasoning, Evolution, Learning, Culture},
abstract = {Models of the various adaptive specializations that have evolved in the human psyche could become the building blocks of a scientific theory of culture. The first step in creating such models is the derivation of a so-called “computational theory” of the adaptive problem each psychological specialization has evolved to solve. In Part II, as a case study, a sketch of a computational theory of social exchange (cooperation for mutual benefit) is developed. The dynamics of natural selection in Pleistocene ecological conditions define adaptive information processing problems that humans must be able to solve in order to participate in social exchange: individual recognition, memory for one's history of interaction, value communication, value modeling, and a shared grammar of social contracts that specifies representational structure and inferential procedures. The nature of these adaptive information processing problems places constraints on the class of cognitive programs capable of solving them; this allows one to make empirical predictions about how the cognitive processes involved in attention, communication, memory, learning, and reasoning are mobilized in situations of social exchange. Once the cognitive programs specialized for regulating social exchange are mapped, the variation and invariances in social exchange within and between cultures can be meaningfully discussed.}
}
@article{CRANFORD2022100638,
title = {Navigating the “Kessel Run” of digital materials acceleration},
journal = {Patterns},
volume = {3},
number = {11},
pages = {100638},
year = {2022},
issn = {2666-3899},
doi = {https://doi.org/10.1016/j.patter.2022.100638},
url = {https://www.sciencedirect.com/science/article/pii/S2666389922002707},
author = {Steve Cranford},
abstract = {Computational methods such as machine learning, artificial intelligence, and big data in physical sciences, particularly materials science, have been exponentially growing in terms of progress, method development, and number of studies and related publications. This aggregate momentum of the community is palpable, and many exciting discoveries are likely on the horizon. But, like all endeavors, some thought should be given to the current trajectory of the field, ensuring the full potential of the new digital space.}
}
@article{AGNOLI2020116385,
title = {Predicting response originality through brain activity: An analysis of changes in EEG alpha power during the generation of alternative ideas},
journal = {NeuroImage},
volume = {207},
pages = {116385},
year = {2020},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2019.116385},
url = {https://www.sciencedirect.com/science/article/pii/S1053811919309760},
author = {Sergio Agnoli and Marco Zanon and Serena Mastria and Alessio Avenanti and Giovanni Emanuele Corazza},
keywords = {EEG, Alpha power, Originality, Idea generation, Divergent-thinking, Temporal dynamics, Creativity},
abstract = {Growing neurophysiological evidence points to a role of alpha oscillations in divergent thinking (DT). In particular, studies have shown a consistent EEG alpha synchronization during performance on the Alternative Uses Task (AUT), a well-established DT task. However, there is a need for investigating the brain dynamics underlying the production of a sequence of multiple, alternative ideas at the AUT and their relationship with idea originality. In twenty young adults, we investigated changes in alpha power during performance on a structured version of the AUT, requiring to ideate four alternative uses for conventional objects in distinct and sequentially balanced time periods. Data analysis followed a three-step approach, including behaviour aspects, physiology aspects, and their mutual relationship. At the behavioural level, we observed a typical serial order effect during DT production, with an increase of originality associated with an increase in ideational time and a decrease in response percentage over the four responses. This pattern was paralleled by a shift from alpha desynchronization to alpha synchronization across production of the four alternative ideas. Remarkably, alpha power changes were able to explain response originality, with a differential role of alpha power over different sensor sites. In particular, alpha synchronization over frontal, central, and temporal sites was able to predict the generation of original ideas in the first phases of the DT process, whereas alpha synchronization over centro-parietal sites persistently predicted response originality during the entire DT production. Moreover, a bilateral hemispheric effect in frontal sites and a left-lateralized effect in central, temporal, and parietal sensor sites emerged as predictors of the increase in response originality. These findings highlight the temporal dynamics of DT production across the generation of alternative ideas and support a partially distinct functional role of specific cortical areas during DT.}
}
@incollection{GARDNER2024103,
title = {Chapter 5 - Smart design for socially engaging environments},
editor = {Nicole Gardner},
booktitle = {Scaling the Smart City},
publisher = {Elsevier},
pages = {103-128},
year = {2024},
series = {Smart Cities},
isbn = {978-0-443-18452-9},
doi = {https://doi.org/10.1016/B978-0-443-18452-9.00006-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780443184529000069},
author = {Nicole Gardner},
keywords = {Cyber-physical system, Design, Interaction, Physical computing, Play, Smart city, Smart cities, Social engagement, Social capital, Social interaction, Urban play, Urban technology},
abstract = {Smart city initiatives typically aim to optimize the efficiency of essential urban infrastructure and urban service delivery and performance. This chapter considers how smart technologies can also be deployed in ways to catalyze social interactions among citizens in urban public realm spaces to create socially engaging environments. Drawing on a range of concepts such as social cohesion, social capital, and object-centered sociality, this chapter considers how existing and speculative urban technology projects that combine spatial design thinking and physical computing can scaffold and amplify opportunities for social engagement. It considers how urban technology projects that mobilize tactics of proximity, curiosity, and play can create new and different ways for people to relate to each other in urban space.}
}
@article{KORDAKI2017122,
title = {Digital card games in education: A ten year systematic review},
journal = {Computers & Education},
volume = {109},
pages = {122-161},
year = {2017},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2017.02.011},
url = {https://www.sciencedirect.com/science/article/pii/S036013151730043X},
author = {Maria Kordaki and Anthi Gousiou},
keywords = {Applications in subject areas, Interactive learning environments, Pedagogical issues, Review, Digital card games},
abstract = {This paper presents a 10-year review study that focuses on the investigation of the use of Digital Card Games (DCGs) as learning tools in education. Specific search terms keyed into 10 large scientific electronic databases identified 50 papers referring to the use of non-commercial DCGs in education during the last decade (2003–2013). The findings revealed that the DCGs reported in the reviewed papers: (a) were used for the learning of diverse subject disciplines across all educational levels and leaning towards the school curriculum, in two ways: game-construction and game-play, (b) were mainly proposed by their designers as meaningful, familiar and appealing learning contexts, in order to motivate and engage players/students and also to promote social, rich and constructivist educational experiences while at the same time integrating modern technologies and innovative gamed-based approaches, (c) were implemented using a plethora of digital tools, (d) mainly adopted social and constructivist views of learning during their design and use, although the views were explicitly reported in only a few of these, (e) were evaluated – in more than half of the studies – with positive results in terms of: student learning, attitudes towards DCGs and enrichment of social interaction and collaboration, (f) appeared to support students to acquire essential thinking skills through DCG-play. However, despite the rich DCG-game experiences reported in the reviewed papers, some essential but under-researched topics were also specified.}
}
@article{DEVGUN2023141,
title = {Pre-cath Laboratory Planning for Left Atrial Appendage Occlusion – Optional or Essential?},
journal = {Cardiac Electrophysiology Clinics},
volume = {15},
number = {2},
pages = {141-150},
year = {2023},
note = {Left Atrial Appendage Occlusion},
issn = {1877-9182},
doi = {https://doi.org/10.1016/j.ccep.2023.01.009},
url = {https://www.sciencedirect.com/science/article/pii/S1877918223000205},
author = {Jasneet Devgun and Tom {De Potter} and Davide Fabbricatore and Dee Dee Wang},
keywords = {Left atrial appendage occlusion, Left atrial appendage, Atrial fibrillation, Cardiac CT, 3D printing, Imaging, Structural heart disease}
}
@incollection{KURGANSKAYA2024760,
title = {Multi-scale modeling of crystal-fluid interactions: State-of-the-art, challenges and prospects},
editor = {Klaus Wandelt and Gianlorenzo Bussetti},
booktitle = {Encyclopedia of Solid-Liquid Interfaces (First Edition)},
publisher = {Elsevier},
edition = {First Edition},
address = {Oxford},
pages = {760-792},
year = {2024},
isbn = {978-0-323-85670-6},
doi = {https://doi.org/10.1016/B978-0-323-85669-0.00034-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780323856690000349},
author = {I. Kurganskaya and R.D. Rohlfs and A. Luttge},
keywords = {Crystal-water interface, Electric double layer, Grand canonical Monte Carlo, Kinetic Monte Carlo, Kinetics, Mineral–water interface, Parameterization, Reaction pathways, Reaction probability, Reaction rates, Statistical mechanics of interfaces, Stepwave, Stochastic model, Upscaling, Voronoi},
abstract = {We describe theoretical and conceptual approaches to treat crystal-fluid interactions across the scales in the communities studying mineral-fluid interactions for a variety of purposes, from understanding fundamental principles to geological reservoir characterization and environmental mitigation. We delineate basics of theory, recent breakthroughs, and challenges in modeling approaches from the atomistic scale to the mesoscale. Quantum Mechanics, Molecular Dynamics, Kinetic Monte Carlo and Voronoi computational geometry are covered. We discuss possible theoretical and conceptual developments to overcome those challenges toward more reliable predictive models. A special attention is given to the development of interfaces between the techniques addressing different scales.}
}
@article{GRANJO202021,
title = {Enhancing the autonomy of students in chemical engineering education with LABVIRTUAL platform},
journal = {Education for Chemical Engineers},
volume = {31},
pages = {21-28},
year = {2020},
issn = {1749-7728},
doi = {https://doi.org/10.1016/j.ece.2020.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S174977282030018X},
author = {José F.O. Granjo and Maria G. Rasteiro},
keywords = {Web platform, Virtual labs, Chemical processes, Autonomous learning},
abstract = {Engineering educators have been developing different approaches to supplement scientific background and further develop the ability for autonomous and critical thinking in students. In 2009, the University of Coimbra has made available on-line a virtual platform with a wide scope, directed towards Chemical Engineering education. The platform is divided into four different educational topics: Unit Operations and Separations, Chemical Reaction, Process Systems Engineering and Biological Processes. These sections include simulators, applications, and case studies to help understanding chemical/biochemical processes and improve their autonomy. This paper presents an assessment of the use of that platform by two different groups of students in the school years of 2015/2016 and 2018/2019: a group from the 3rd-year of Chemical Engineering, and another one from a Project Design course (2nd cycle, MSc of Chemical Engineering). A case study addressing the synthesis of phthalic anhydride by o-xylene oxidation on a fixed-bed catalytic reactor is also given to show the use of existing simulators in LABVIRTUAL.}
}
@article{SZYMANSKI2021,
title = {Words Are Essential, but Underexamined, Research Tools for Microbes and Microbiomes},
journal = {mSystems},
volume = {6},
number = {4},
year = {2021},
issn = {2379-5077},
doi = {https://doi.org/10.1128/msystems.00769-21},
url = {https://www.sciencedirect.com/science/article/pii/S2379507721002683},
author = {Erika Szymanski},
keywords = {discourse, engineering, metaphor, microbiome, science and technology studies, synthetic biology, synthetic yeast},
abstract = {Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive.
ABSTRACT
Language constitutes an essential set of scientific construction tools, not only for communicating knowledge, but for conceptualizing the world. Metaphors in particular, as conventions that guide and reproduce analogical reasoning, merit attention that they largely do not receive. My research addresses this deficit by examining how metaphors for handling microbes shape possibilities for working with yeast and bacteria in synthetic biology, microbiome research, and other fields that reconfigure what microbes can be. Though poised to reexamine assumptions, these fields routinely rest on metaphors and other language tools that quietly embed ways of thinking that may work against wider aims—for example, imagining bacteria as imperfect machines that should therefore be rendered increasingly passive and controllable. Researchers, therefore, need to examine how language tools structure their observations and expectations so that the tools they choose are appropriate for the work they want to do.}
}
@article{RANDALL1991219,
title = {Review of linear least squares computations: by R.W. Farebrother},
journal = {Linear Algebra and its Applications},
volume = {153},
pages = {219-223},
year = {1991},
issn = {0024-3795},
doi = {https://doi.org/10.1016/0024-3795(91)90221-H},
url = {https://www.sciencedirect.com/science/article/pii/002437959190221H},
author = {John H. Randall}
}
@article{GARDECKI2018138,
title = {Innovative Internet of Things-reinforced Human Recognition for Human-Machine Interaction Purposes},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {6},
pages = {138-143},
year = {2018},
note = {15th IFAC Conference on Programmable Devices and Embedded Systems PDeS 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.07.143},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318308875},
author = {Arkadiusz Gardecki and Michal Podpora and Aleksandra Kawala-Janik},
keywords = {Human-Machine Interaction, Internet of Things, Human Recognition, Humanoid Robots, Human Identification},
abstract = {Accurate and reliable human recognition and parametrisation have always been an important challenge in efficient Man-Machine Interaction. A humanoid robot is able to offer a much richer and more natural behaviour and human-like communication, but only if the robot possesses sufficient knowledge about the interlocutor, such as inter alia: gender, age, mood, behaviour data, interaction history. In this paper authors introduced an innovative conception in Human-Machine Interaction, where instead of thinking about an interaction as an event (which uses and produces information) an innovative point of view was proposed, where the interaction is just an event in a continuous flow of information. The difference, once perceived, results in an astounding change of conception, as well as a whole new set of ideas. The human detection, information acquisition, human recognition – can be performed earlier, before a human reaches the humanoid robot, also the history of interactions and possible interests of the interlocutor can be predicted before they would even start the conversation. This paper contains a detailed analysis of the proposed environment-based approach to interaction, as well as the Internet of Things-reinforced information acquisition.}
}
@article{FOSGERAU2021109911,
title = {Some remarks on CCP-based estimators of dynamic models},
journal = {Economics Letters},
volume = {204},
pages = {109911},
year = {2021},
issn = {0165-1765},
doi = {https://doi.org/10.1016/j.econlet.2021.109911},
url = {https://www.sciencedirect.com/science/article/pii/S0165176521001889},
author = {Mogens Fosgerau and Emerson Melo and Matthew Shum and Jesper R.-V. Sørensen},
keywords = {Dynamic discrete choice, Random utility, Linear programming, Convex analysis, Convex optimization},
abstract = {This note provides several remarks relating to the conditional choice probability (CCP) based estimation approaches for dynamic discrete-choice models. Specifically, the Arcidiacono and Miller (2011) estimation procedure relies on the ”inverse-CCP” mapping ψp from CCPs to choice-specific value functions. Exploiting the convex-analytic structure of discrete choice models, we discuss two approaches for computing this mapping, using either linear or convex programming, for models where the utility shocks can follow arbitrary parametric distributions. Furthermore, the ψ function is generally distinct from the ”selection adjustment” term (i.e. the expectation of the utility shock for the chosen alternative), so that computational approaches for computing the latter may not be appropriate for computing ψ.}
}
@article{VASILE201177,
title = {Entry points, interests and attitudes. An integrative approach of learning},
journal = {Procedia - Social and Behavioral Sciences},
volume = {11},
pages = {77-81},
year = {2011},
note = {Teachers for the Knowledge Society},
issn = {1877-0428},
doi = {https://doi.org/10.1016/j.sbspro.2011.01.037},
url = {https://www.sciencedirect.com/science/article/pii/S1877042811000395},
author = {Cristian Vasile},
keywords = {multiple intelligence, entry points, personality, interests},
abstract = {The relationship between personality and intelligence is of a major importance in the learning process. Interests and attitudes are related to the entry points on emotional ground. In some educational systems the focus on cognitive abilities and cognitive functions increased, amplified by the neuroscience and the computational approach. The cognitive approach should be enriched with major aspects from the global human psychological system like interests/motivation, emotional profile, attitudes and so on. The focus on cognition only, or the computational view should be completed with personality approaches and behavior regulation, all of these influencing without doubt the intelligence.}
}
@article{OLADEJO2024111880,
title = {The Hiking Optimization Algorithm: A novel human-based metaheuristic approach},
journal = {Knowledge-Based Systems},
volume = {296},
pages = {111880},
year = {2024},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2024.111880},
url = {https://www.sciencedirect.com/science/article/pii/S0950705124005148},
author = {Sunday O. Oladejo and Stephen O. Ekwe and Seyedali Mirjalili},
keywords = {Optimization, Metaheuristics, Hiking, Tobler’s Hiking function, Algorithm, Benchmark, Problem solving},
abstract = {In this paper, a novel metaheuristic called ‘The Hiking Optimization Algorithm’ (HOA) is proposed. HOA is inspired by hiking, a popular recreational activity, in recognition of the similarity between the search landscapes of optimization problems and the mountainous terrains traversed by hikers. HOA’s mathematical model is premised on Tobler’s Hiking Function (THF), which determines the walking velocity of hikers (i.e. agents) by considering the elevation of the terrain and the distance covered. THF is employed in determining hikers’ positions in the course of solving an optimization problem. HOA’s performance is demonstrated by benchmarking with 29 well-known test functions (including unimodal, multimodal, fixed-dimension multimodal, and composite functions), three engineering design problems (EDPs), (including I-beam, tension/compression spring, and gear train problems) and two N-P Hard problems (i.e. Traveling Salesman’s and Knapsack Problems). Moreover, HOA’s results are verified by comparison to 14 other metaheuristics, including Teaching Learning Based Optimization (TLBO), Genetic Algorithm (GA), Differential Evolution (DE), Particle Swarm Optimization, Grey Wolf Optimizer (GWO) as well as newly introduced algorithms such as Komodo Mlipir Algorithm (KMA), Quadratic Interpolation Optimization (QIO), and Coronavirus Optimization Algorithm (COVIDOA). In this study, we employ statistical tests such as the Wilcoxon rank sum, Friedman test, and Dunn’s post hoc test for the performance evaluation. HOA’s results are competitive and, in many instances, outperform the aforementioned well-known metaheuristics. The source codes of HOA and related metaheuristics can be accessed publicly via this link: https://github.com/DayoSun/The-Hiking-Optimization-Algorithm.}
}
@article{MARTINI2022105446,
title = {R_IC: A novel and versatile implementation of the index of connectivity in R},
journal = {Environmental Modelling & Software},
volume = {155},
pages = {105446},
year = {2022},
issn = {1364-8152},
doi = {https://doi.org/10.1016/j.envsoft.2022.105446},
url = {https://www.sciencedirect.com/science/article/pii/S1364815222001529},
author = {Lorenzo Martini and Tommaso Baggio and Loris Torresani and Stefano Crema and Marco Cavalli},
keywords = {Sediment connectivity, Geomorphometry, R_IC, Open-source},
abstract = {Sediment connectivity is the capability of a system to regulate the exchange of sediment in catchments. The Index of Connectivity (IC) has become a widely used tool, offering a practical way to assess sediment connectivity from hillslopes to downstream channels. We present a novel implementation of IC in R environment to expand the audience of users and encourage alternative applications of the index. The R_IC is an open-source and freely available tool composed by three codes. Standard R_IC runs the IC and it represents the core of the other variants. Custom R_IC offers a more flexible script, allowing the computation of alternative weighting factors and the possibility of running a further profile IC analysis. Batch R_IC performs batch processing of the index. For each code variant, a geomorphological application is presented to illustrate how the R_IC could be used in watershed management and practical issues related to sediment dynamics.}
}
@article{BEYNON2008476,
title = {Experimenting with computing},
journal = {Journal of Applied Logic},
volume = {6},
number = {4},
pages = {476-489},
year = {2008},
note = {The Philosophy of Computer Science},
issn = {1570-8683},
doi = {https://doi.org/10.1016/j.jal.2008.09.008},
url = {https://www.sciencedirect.com/science/article/pii/S157086830800044X},
author = {Meurig Beynon and Steve Russ},
keywords = {Empirical Modelling, Observation, Experiment, Computing, Theory, Radical empiricism, Dependency, Agency},
abstract = {We distinguish two kinds of experimental activity: post-theory and exploratory. Post-theory experiment enjoys computer support that is well-aligned to the classical theory of computation. Exploratory experiment, in contrast, arguably demands a broader conception of computing. Empirical Modelling (EM) is proposed as a more appropriate conceptual framework in which to provide computational support for exploratory experiment. In the process, it promises to provide integrated computational support for both exploratory and post-theory experiment. We first sketch the motivation for EM and illustrate its potential for supporting experimentation, then briefly highlight the semantic challenge it poses and the philosophical implications.}
}
@article{ENRIQUEZHIDALGO2025123924,
title = {Evaluation of decision-support tools for coastal flood and erosion control: A multicriteria perspective},
journal = {Journal of Environmental Management},
volume = {373},
pages = {123924},
year = {2025},
issn = {0301-4797},
doi = {https://doi.org/10.1016/j.jenvman.2024.123924},
url = {https://www.sciencedirect.com/science/article/pii/S0301479724039112},
author = {Andrés M. Enríquez-Hidalgo and Andrés Vargas-Luna and Andrés Torres},
keywords = {Decision support tool, Coastal erosion and flood management, Development pathways, Coastal archetypes, Multi-criteria decision analysis},
abstract = {Coastal areas face significant challenges due to natural and anthropogenic changes, such as sea level rise, extreme events and coastal erosion. The coastal management requires the consideration of socioeconomic and environmental factors to address these variables. The selection of an appropriate Decision Support Tool (DST) based on decision matrix method plays a crucial role in implementing coastal management strategies to tackle climate change-related issues. This has posed considerable challenges for decision-makers, aligning with the Sustainable Development Goals (SDG). This review provides an overview of the practical experience in the application of DSTs for coastal erosion and flood risk, emphasizing the use of Multi-Criteria Decision Analysis (MCDA). DST choice depends on the coastal archetype, including its geographical features and sociocultural context. The purpose is to clarify how the integration of DSTs maximizes flexibility and supports the implementation of future Decision Support System (DSS) tailored to the needs of coastal cities with development pathways (DP). This review assesses different MCDA methods, highlighting their applicability, utility, and integration in coastal management, while evaluating each method's strengths, weaknesses, and specific applications, with a focus on sustainability and resilience. The review highlights the necessity of expert knowledge in accurately defining criteria and weighting factors to ensure that the chosen MCDA method reflects the complexities of the coastal environment. Depending on the scenario, methods like PROMETHEE and ELECTRE are recommended for their flexibility and robustness in handling complex decision-making processes, especially in data-rich and well-structured environments. In contrast, TOPSIS and AHP are suitable for scenarios with limited information or requiring minimal interaction with decision-makers. For more challenging contexts, where computational resources and expertise are constrained, methods like MAUT, VIKOR, and TODAIM emerge as viable alternatives due to their adaptability and reduced reliance on extensive datasets.}
}
@article{KONDINSKI20241071,
title = {Hacking decarbonization with a community-operated CreatorSpace},
journal = {Chem},
volume = {10},
number = {4},
pages = {1071-1083},
year = {2024},
issn = {2451-9294},
doi = {https://doi.org/10.1016/j.chempr.2023.12.018},
url = {https://www.sciencedirect.com/science/article/pii/S2451929423006198},
author = {Aleksandar Kondinski and Sebastian Mosbach and Jethro Akroyd and Andrew Breeson and Yong Ren Tan and Simon Rihm and Jiaru Bai and Markus Kraft},
keywords = {decarbonization, chemistry, knowledge graphs, agents, CreatorSpace},
abstract = {Summary
The pressing challenge of decarbonization encompasses a vast combinatorial space of interlinked technologies, thus necessitating an increased reliance on artificial intelligence (AI)-assisted molecular modeling and data analytics. Our backcasting analysis proposes a future rich in efficient decarbonization technologies, such as sustainable fuels for aviation and shipping, as well as carbon capture and utilization. We then retrace the path to this proposed future with the guidance of two constraints: the maximization of scientists’ creative capacities and the evolution of a world-centric AI. Our exploration leads us to the concept of a “CreatorSpace,” a distributed digital system resembling existing hackerspaces and makerspaces known for accelerating the prototyping of new technologies worldwide. The CreatorSpace serves as a virtual, semantic platform where chemists, engineers, and materials scientists can freely collaborate, integrating chemical knowledge with cross-scale, cross-technology tools, and operations. This streamlined molecular-to-process-design pathway facilitates a diverse array of solutions for decarbonization and other sustainability technologies.}
}
@article{PANULAONTTO2019292,
title = {The AXIOM approach for probabilistic and causal modeling with expert elicited inputs},
journal = {Technological Forecasting and Social Change},
volume = {138},
pages = {292-308},
year = {2019},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2018.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0040162518305870},
author = {Juha Panula-Ontto},
keywords = {Systems modeling, Modeling techniques, Decision support, Cross-impact analysis, Belief networks, Expert elicitation},
abstract = {Expert informants can be used as the principal information source in the modeling of socio-techno-economic systems or problems to support planning, foresight and decision-making. Such modeling is theory-driven, grounded in expert judgment and understanding, and can be contrasted with data-driven modeling approaches. Several families of approaches exist to enable expert elicited systems modeling with varying input information requirements and analytical ambitions. This paper proposes a novel modeling language and computational process, which combines aspects from various other approaches in an attempt to create a flexible and practical systems modeling approach based on expert elicitation. It is intended to have high fitness in modeling of systems that lack statistical data and exhibit low quantifiability of important system characteristics. AXIOM is positioned against Bayesian networks, cross-impact analysis, structural analysis, and morphological analysis. The modeling language and computational process are illustrated with a small example model. A software implementation is also presented.}
}
@article{KRUSKOPF2024104574,
title = {Future teachers’ self-efficacy in teaching practical and algorithmic ICT competencies – Does background matter?},
journal = {Teaching and Teacher Education},
volume = {144},
pages = {104574},
year = {2024},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2024.104574},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X24001069},
author = {Milla Kruskopf and Rekar Abdulhamed and Mette Ranta and Heidi Lammassaari and Kirsti Lonka},
keywords = {Teaching self-efficacy, Self-efficacy, ICT competence, Digital competence, Programming, 21st century competencies, Teacher students},
abstract = {Future teachers need to be confidently equipped to teach 21st century ICT skills. We investigated teaching self-efficacy (TSE) in ICT competencies among teacher students. We confirmed distinct ICT competencies among two cohorts from teacher training programs (n = 347; n = 428): practical (i.e., device and data management), and algorithmic (i.e., programming, and data security). Regression analyses indicated TSE-biases regarding younger age, male gender, and a background in natural sciences, with significant interactions between age, gender, and having learned such ICT-skills already in school. The findings point to a need for tailored strategies in teacher education to mitigate TSE disparities.}
}
@article{BELLANTE2025104341,
title = {Evaluating the potential of quantum machine learning in cybersecurity: A case-study on PCA-based intrusion detection systems},
journal = {Computers & Security},
pages = {104341},
year = {2025},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2025.104341},
url = {https://www.sciencedirect.com/science/article/pii/S0167404825000306},
author = {Armando Bellante and Tommaso Fioravanti and Michele Carminati and Stefano Zanero and Alessandro Luongo},
keywords = {Quantum computing, Quantum machine learning, QML, Evaluation, Framework, Impact, PCA, Principal component analysis, Network intrusion detection, Network security},
abstract = {Quantum computing promises to revolutionize our understanding of the limits of computation, and its implications in cryptography have long been evident. Today, cryptographers are actively devising post-quantum solutions to counter the threats posed by quantum-enabled adversaries. Meanwhile, quantum scientists are innovating quantum protocols to empower defenders. However, the broader impact of quantum computing and quantum machine learning (QML) on other cybersecurity domains still needs to be explored. In this work, we investigate the potential impact of QML on cybersecurity applications of traditional ML. First, we explore the potential advantages of quantum computing in machine learning problems specifically related to cybersecurity. Then, we describe a methodology to quantify the future impact of fault-tolerant QML algorithms on real-world problems. As a case study, we apply our approach to standard methods and datasets in network intrusion detection, one of the most studied applications of machine learning in cybersecurity. Our results provide insight into the conditions for obtaining a quantum advantage and the need for future quantum hardware and software advancements.}
}
@article{YU2023114721,
title = {Numerical investigation of splitter blades on the performance of a forward-curved impeller used in a pump as turbine},
journal = {Ocean Engineering},
volume = {281},
pages = {114721},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.114721},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823011058},
author = {He Yu and Tao Wang and Yuancheng Dong and Qiuqin Gou and Lei Lei and Yunqi Liu},
keywords = {Special impeller, Pump as turbine, Splitter blade, Entropy generation, Computational fluid dynamics},
abstract = {Abstract
As a type of economical energy recovery device, pump as turbine (PAT) is generally used in micro-hydropower plants and energy recovery. To study the influence of the splitter blade on a special impeller used in PAT, impellers without and with splitter blades are designed in this paper. The influences of splitter blade on the energy loss, external characteristics and internal flow field distribution of a PAT were simulated via a verified computational fluid dynamics (CFD) method. The consequences present that the shaft power, efficiency, and head corresponding to the BEP of the PAT with splitter blades are 16.4%, 1.3%, and 8.8% better than those of the PAT without splitter blades. The total entropy production of the PAT without splitter blade is higher than that of the PAT with splitter blades at the same flow rate. Adding splitter blade increased the number of effective blades, made the fluid flow more evenly along the impeller flow passage, and reduced the flow separation inside the impeller. This paper displays that adding splitter blades not only obviously increases hydraulic performance under large flow conditions but also significantly widens the high-efficiency range of PATs.}
}
@article{MONNAHAN2024107024,
title = {Toward good practices for Bayesian data-rich fisheries stock assessments using a modern statistical workflow},
journal = {Fisheries Research},
volume = {275},
pages = {107024},
year = {2024},
issn = {0165-7836},
doi = {https://doi.org/10.1016/j.fishres.2024.107024},
url = {https://www.sciencedirect.com/science/article/pii/S0165783624000882},
author = {Cole C. Monnahan},
keywords = {No-U-turn sampler (NUTS), Bayesian integration, Prior predictive checks, Posterior predictive checks, Cross validation},
abstract = {Bayesian inference has long been recognized as useful for fisheries stock assessments but it is less common than maximum likelihood approaches due to long run times and a lack of good practices. Recent computational advances leave developing good practices and user-friendly interfaces as the most important hurdles to wider use of this powerful statistical paradigm. Here, I argue that the modern Bayesian workflow proposed by Gelman et al. (2020) should form the basis for proposed good practices in fisheries sciences. Their workflow is a conceptual roadmap for iterative model building which includes the philosophical role of priors and how to apply statistical tools to construct them, how to validate and compare models, and how to overcome computational problems. Adapted for stock assessment, this leads to the following good practices for analysts. Diagnostics from multiple no-U-turn sampler (NUTS) chains (the recommended MCMC algorithm) should pass and be reported, specifically that the potential scale reduction Rˆ is <1.01 and the effective sample size is >400 for all parameters, and there are no NUTS divergences. When direct a priori information is unavailable on parameters, use prior predictive checking to build, assess, and adjust priors to enforce desired constraints on complexity, or to conform to a priori expectations or physical/biological limitations on derived quantities. Use posterior predictive checks to validate models by confirming simulated data and summaries (e.g., variance of compositional data) are similar to the observed counterparts. Process error variances can be estimated jointly with random effects and other parameters when desired, and should be for important model components. An approximate cross-validation technique called PSIS-LOO is the most practical tool for model selection, but can also provide important insights into model deficiencies. I also recommended that model developers build and parameterize models to have minimal parameter correlations and marginal variances close to one, have options for diverse (multivariate) priors, do predictive modeling, and ensure that the tools comprising a workflow are accessible and straightforward for routine use. I review, adapt, and illustrate a Bayesian workflow on AD Model Builder and Stock Synthesis models, but these good practices apply to models from any software platform, including Template Model Builder and Stan. Finally, I argue that the Bayesian and frequentist paradigms complement each other, with both helping analysts better understand different aspects of their models and data. Wider adoption of Bayesian methods using the good practices proposed here would therefore lead to improved scientific advice used to manage fisheries.}
}
@incollection{VALLERO2014953,
title = {Chapter 33 - Grand Challenges},
editor = {Daniel Vallero},
booktitle = {Fundamentals of Air Pollution (Fifth Edition)},
publisher = {Academic Press},
edition = {Fifth Edition},
address = {Boston},
pages = {953-961},
year = {2014},
isbn = {978-0-12-401733-7},
doi = {https://doi.org/10.1016/B978-0-12-401733-7.00033-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780124017337000335},
author = {Daniel Vallero},
keywords = {Bayesian, Biogeochemical cycles, Categorical imperative, Circle of poisons, CO, Command and control, Computational tools, Fossil fuels, Fundamentals of air pollution, Geographic information system (GIS), Geostatistics, Global greenhouse gas emissions (GHG), Grand Challenges, Immanuel Kant, Indoor air pollution, Informatics, Kriging, National Academy of Engineering, Precautionary principle, Pre-Kindergarten, Real-world exposures, Reductionist, Risk, Sustainability, Systems thinking, Transdisciplinary, Translational science},
abstract = {This chapter looks to the future of air quality and how the lessons learned in recent decades can be applied to new problems. The challenges include finding ways to prevent emerging economies from repeating the air pollution mistakes and harm that developed nations have experienced in arriving at solutions to air pollution problems. Other challenges include: global problems, such as long-range transport of pollutants, climate change; real-world-exposures (including indoor air pollution); improvements in technologies to remove difficult-to-treat pollutants; and addressing the growing number of mobile sources. This will require more systems thinking and sustainable, transdisciplinary solutions. The legacy of the current cadre of air pollution experts must be one of translational science and the enhancement of early air pollution education for the next generation.}
}
@article{THOMPSON2024939,
title = {Leveraging marine biotechnology for an All-Atlantic sustainable blue economy},
journal = {Trends in Biotechnology},
volume = {42},
number = {8},
pages = {939-941},
year = {2024},
issn = {0167-7799},
doi = {https://doi.org/10.1016/j.tibtech.2023.12.011},
url = {https://www.sciencedirect.com/science/article/pii/S0167779923003670},
author = {Cristiane Thompson and Alice C. Ortmann and Thulani Makhalanyane and Fabiano Thompson},
keywords = {All Atlantic, food security, biotechnology, low-carbon aquaculture, integrated multitrophic aquaculture, biofloc technology},
abstract = {Despite the lack of research, development, and innovation funds, especially in South Atlantic countries, the Atlantic is suited to supporting a sustainable marine bioeconomy. Novel low-carbon mariculture systems can provide food security, new drugs, and climate mitigation. We suggest how to develop this sustainable marine bioeconomy across the entire Atlantic.}
}
@article{BUCKNER200749,
title = {Self-projection and the brain},
journal = {Trends in Cognitive Sciences},
volume = {11},
number = {2},
pages = {49-57},
year = {2007},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2006.11.004},
url = {https://www.sciencedirect.com/science/article/pii/S1364661306003275},
author = {Randy L. Buckner and Daniel C. Carroll},
abstract = {When thinking about the future or the upcoming actions of another person, we mentally project ourselves into that alternative situation. Accumulating data suggest that envisioning the future (prospection), remembering the past, conceiving the viewpoint of others (theory of mind) and possibly some forms of navigation reflect the workings of the same core brain network. These abilities emerge at a similar age and share a common functional anatomy that includes frontal and medial temporal systems that are traditionally associated with planning, episodic memory and default (passive) cognitive states. We speculate that these abilities, most often studied as distinct, rely on a common set of processes by which past experiences are used adaptively to imagine perspectives and events beyond those that emerge from the immediate environment.}
}
@article{AUGIER2001307,
title = {Sublime Simon: The consistent vision of economic psychology's Nobel laureate},
journal = {Journal of Economic Psychology},
volume = {22},
number = {3},
pages = {307-334},
year = {2001},
issn = {0167-4870},
doi = {https://doi.org/10.1016/S0167-4870(01)00036-8},
url = {https://www.sciencedirect.com/science/article/pii/S0167487001000368},
author = {Mie Augier},
keywords = {Herbert Simon, Bounded rationality, Carnegie Mellon University, Economics and psychology},
abstract = {This essay contains a study of some of Herbert Simon's ideas, with particular emphasis on the role of bounded rationality in Simon's thinking and his contributions to economics and psychology. I describe Simon's visions for challenging rational choice theory, through limited rationality, and for bringing psychology into economics, putting this in perspective by describing the evolution of some of this thoughts, focusing on the continuity in his work.}
}
@article{LUCKRING2024100998,
title = {Prediction of concentrated vortex aerodynamics: Current CFD capability survey},
journal = {Progress in Aerospace Sciences},
volume = {147},
pages = {100998},
year = {2024},
issn = {0376-0421},
doi = {https://doi.org/10.1016/j.paerosci.2024.100998},
url = {https://www.sciencedirect.com/science/article/pii/S0376042124000241},
author = {James M. Luckring and Arthur Rizzi},
abstract = {Concentrated vortex flows contribute to the aerodynamic performance of aircraft at elevated load conditions. For military interests, the vortex flows are exploited at maneuver conditions of combat aircraft and missiles. For transport interests, the vortex flows are exploited at takeoff and landing conditions as well as at select transonic conditions. Aircraft applications of these vortex flows are reviewed with a historical perspective followed by a discussion of the underlying physics of a concentrated vortex flow. A hierarchy of computational fluid dynamics simulation technology is then presented followed by findings from a capability survey for predicting concentrated vortex flows with computational fluid dynamics. Results are focused on military and civil fixed-wing aircraft; only limited results are included for missiles, and rotary-wing applications are not assessed. Opportunities for predictive capability advancement are then reported with comments related to digital transformation interests. A hierarchical approach that merges a physics-based perspective of the concentrated vortex flows with a systems engineering viewpoint of the air vehicle is also used to frame much of the discussion.}
}
@article{XIAO1995169,
title = {Three-dimensional melt flows in Czochralski oxide growth: high-resolution, massively parallel, finite element computations},
journal = {Journal of Crystal Growth},
volume = {152},
number = {3},
pages = {169-181},
year = {1995},
issn = {0022-0248},
doi = {https://doi.org/10.1016/0022-0248(95)00090-9},
url = {https://www.sciencedirect.com/science/article/pii/0022024895000909},
author = {Qiang Xiao and Jeffrey J. Derby},
abstract = {Three-dimensional, time-dependent features of melt flows which occur during the Czochralski growth of oxide crystals are analyzed using a theoretical bulk-flow model. The transition from a steady, axisymmetric flow to a time-dependent, three-dimensional state characterized by an annular wave structure is found to strongly affect the temperature distribution and heat transfer through the melt. The results are obtained using a novel, massively parallel implementation of the Galerkin finite element method which affords high spatial resolution of the computed flows.}
}
@incollection{QAZI2025245,
title = {Chapter 11 - Deep learning in clinical genomics-based cancer diagnosis},
editor = {Khalid Raza},
booktitle = {Deep Learning in Genetics and Genomics},
publisher = {Academic Press},
pages = {245-259},
year = {2025},
isbn = {978-0-443-27574-6},
doi = {https://doi.org/10.1016/B978-0-443-27574-6.00014-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044327574600014X},
author = {Sahar Qazi and Raiyan Ali and Manoj Kumar Jana and Bimal Prasad Jit and Neeraj Gurung and Ashok Sharma},
keywords = {Artificial intelligence, Bioinformatics, Cancer, Deep learning, Diagnosis, Next generation sequencing, Variant calling},
abstract = {Deep learning, an artificial intelligence facet, has impacted distinct fields, including natural language processing and computer vision. Its advancements have transformed how computational and data scientists approach data, turning unstructured information into valuable insights. This is particularly impactful in clinical genomics, where high-throughput sequencing generates vast amounts of data. Techniques such as whole genome sequencing and transcriptomic profiling produce enormous datasets that are challenging to analyze manually. Deep learning tools like “Deep Variant” enhance accuracy in variant calling, improving diagnostic precision. By adapting to factors such as genetic mutations and disease progression, deep learning aids in early cancer diagnosis and better clinical outcomes. This chapter explores these transformative applications in clinical genomic research.}
}
@article{1985174,
title = {Learning to use a word processor: by doing, by thinking, and by knowing: John M. Carroll and Robert L. Mack: Rep. RC 9481, IBM T.J. Watson Research Center, Yorktown Heights, New York 10598, U.S.A., (July 1982)},
journal = {Decision Support Systems},
volume = {1},
number = {2},
pages = {174},
year = {1985},
issn = {0167-9236},
doi = {https://doi.org/10.1016/0167-9236(85)90071-5},
url = {https://www.sciencedirect.com/science/article/pii/0167923685900715}
}
@incollection{VANLOAN1992247,
title = {Chapter 6 A survey of matrix computations},
series = {Handbooks in Operations Research and Management Science},
publisher = {Elsevier},
volume = {3},
pages = {247-321},
year = {1992},
booktitle = {Computing},
issn = {0927-0507},
doi = {https://doi.org/10.1016/S0927-0507(05)80203-8},
url = {https://www.sciencedirect.com/science/article/pii/S0927050705802038},
author = {Charles {Van Loan}},
abstract = {Publisher Summary
This chapter presents three-level introduction to the field of matrix computations. The chapter discusses analytic and computational tools that underpin numerical linear algebra. Low dimension examples are the rule with appropriate generalizations to follow. The central themes include (a) the language of matrix factorizations, (b) the art of introducing zeros into a matrix, (c) the exploitation of structure, and (d) the distinction between problem sensitivity and algorithmic stability. Matrix factorizations that play a central role in numerical linear algebra are also presented in the chapter. The chapter also discusses factorization. For each factorization, algorithms are surveyed, associated mathematical properties, and applications are discussed. One factorization (Chotesky) is used to illustrate various aspects of high performance matrix computations. Successful computing requires the design of codes that pay careful attention to the flow of data during execution.}
}
@article{SOLIMAN2025100131,
title = {A comparative analysis of encoder only and decoder only models for challenging LLM-generated STEM MCQs using a self-evaluation approach},
journal = {Natural Language Processing Journal},
volume = {10},
pages = {100131},
year = {2025},
issn = {2949-7191},
doi = {https://doi.org/10.1016/j.nlp.2025.100131},
url = {https://www.sciencedirect.com/science/article/pii/S294971912500007X},
author = {Ghada Soliman and Hozaifa Zaki and Mohamed Kilany},
keywords = {NLP, LLM, SLM, Self-evaluation, MCQ},
abstract = {Large Language Models (LLMs) have demonstrated impressive capabilities in various tasks, including Multiple-Choice Question Answering (MCQA) evaluated on benchmark datasets with few-shot prompting. Given the absence of benchmark Science, Technology, Engineering, and Mathematics (STEM) datasets on Multiple-Choice Questions (MCQs) created by LLMs, we employed various LLMs (e.g., Vicuna-13B, Bard, and GPT-3.5) to generate MCQs on STEM topics curated from Wikipedia. We evaluated open-source LLM models such as Llama 2-7B and Mistral-7B Instruct, along with an encoder model such as DeBERTa v3 Large, on inference by adding context in addition to fine-tuning with and without context. The results showed that DeBERTa v3 Large and Mistral-7B Instruct outperform Llama 2-7B, highlighting the potential of LLMs with fewer parameters in answering hard MCQs when given the appropriate context through fine-tuning. We also benchmarked the results of these models against closed-source models such as Gemini and GPT-4 on inference with context, showcasing the potential of narrowing the gap between open-source and closed-source models when context is provided. Our work demonstrates the capabilities of LLMs in creating more challenging tasks that can be used as self-evaluation for other models. It also contributes to understanding LLMs’ capabilities in STEM MCQs tasks and emphasizes the importance of context for LLMs with fewer parameters in enhancing their performance.}
}
@article{LEALVILLASECA2025105833,
title = {Interpreting Deepkriging for spatial interpolation in geostatistics},
journal = {Computers & Geosciences},
volume = {196},
pages = {105833},
year = {2025},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2024.105833},
url = {https://www.sciencedirect.com/science/article/pii/S0098300424003169},
author = {Fabian Leal-Villaseca and Edward Cripps and Mark Jessell and Mark Lindsay},
keywords = {Spatial statistics, Deep learning interpretability, Shapley values, Deepkriging, Batched Shapley},
abstract = {In the current era marked by an unprecedented abundance of data, the usage of conventional methods such as kriging persists in some applications of geostatistics, despite their limitations in adequately capturing the intricate relationships found in contemporary, multivariate datasets. Although deep neural networks (DNNs) have demonstrated remarkable efficacy in capturing complex nonlinear feature relationships across various domains, their success in geostatistical applications has been limited. This can be partly attributed to two significant challenges. Firstly, the opaque nature of these black box models raises concerns about the dependability of their outputs for critical decision-making, as the inner workings of the model remain less interpretable. Secondly, DNNs do not explicitly capture spatial dependencies within data. To address these shortcomings, we employ a methodology to interpret the recently proposed spatial DNNs known as Deepkriging, and we apply it to dry bulk rock density estimation, an often-overlooked aspect in mineral resource estimation. Through our adaptation of Shapley values—Batched Shapley—we overcome significant computational challenges to quantify feature importance for Deepkriging. This approach takes into account feature interactions, which is crucial for DNNs, as they rely on high-order interactions, especially in a complex application like mineral resource estimation. Additionally, we demonstrate in the 3D case that Deepkriging outperforms ordinary kriging and regression kriging in terms of mean squared errors, in both the purely spatial case and in the presence of auxiliary variables. Our study produces the first methodology to interpret Deepkriging, which is suitable for any model with a large number of features; it reaffirms the efficacy of Deepkriging through several comparisons in a 3D application, and most importantly; it underscores the adaptability and broader potential of DNNs to cater to various challenges in geostatistics.}
}
@incollection{BALANAY201949,
title = {3 - Tools for circular economy: Review and some potential applications for the Philippine textile industry},
editor = {Subramanian Senthilkannan Muthu},
booktitle = {Circular Economy in Textiles and Apparel},
publisher = {Woodhead Publishing},
pages = {49-75},
year = {2019},
series = {The Textile Institute Book Series},
isbn = {978-0-08-102630-4},
doi = {https://doi.org/10.1016/B978-0-08-102630-4.00003-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780081026304000030},
author = {Raquel Balanay and Anthony Halog},
keywords = {Circular economy, Industrial sustainability, Life cycle thinking, Sustainable development, Systems modelling, Textile industry},
abstract = {Instituting circular economy (CE) for sustainability is the aim of taking stock of various analytical/assessment tools. A review of these tools reveals a continuing endeavor to incorporate in the procedures the systems and life cycle thinking and the triple bottom-line framework of sustainable development (economic, social, and environmental). Over time, the CE tools have been modified with the incorporation of some unique attributes in the cases being studied. Life cycle assessment (LCA) remains the popular and the only standardized procedure to analyze CE issues in industries, specifically in the environmental aspect. However, consistency, measurement, and aggregation issues are the major setbacks of having an integrated LCA for economic, social, and environmental impacts. The alternative tools used across the world to study the economic, social, and environmental aspects of CE have increased in both number and sophistication. Optimization and systems models have been increasingly used on a case-based format. Although the downside is the less standardized approach with less chances of comparability in terms of results, these models have been designed appropriately to tackle challenges associated with intricate, multifaceted, and encompassing sustainability and CE issues to improve policy development. In the textile industry, LCA as a popular tool is only used for environmental sustainability assessment but not much in social and economic aspects. The Philippine textile industry still has to catch up in the application of those tools for sustainability assessment. A framework has been suggested for the country's roadmap/guide to attain circularity in textile industry operations.}
}
@article{MCLEAN20248,
title = {Autoantibodies against acetylcholine receptors are increased in archived serum samples from patients with schizophrenia},
journal = {Schizophrenia Research},
volume = {267},
pages = {8-13},
year = {2024},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2024.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0920996424001129},
author = {Ryan Thomas McLean and Elizabeth Buist and David {St. Clair} and Jun Wei},
keywords = {Neurotransmitter receptor, CHRM4, GRM3, CHRNA4, CHRNA5 neuroimmunology},
abstract = {Previous studies have demonstrated that the levels of IgG against neurotransmitter receptors are increased in patients with schizophrenia. Genome-wide association (GWA) studies of schizophrenia confirmed that 108 loci harbouring over 300 genes were associated with schizophrenia. Although the functional implications of genetic variants are unclear, theoretical functional alterations of these genes could be replicated by the presence of autoantibodies. This study examined the levels of plasma IgG antibodies against four neurotransmitter receptors, CHRM4, GRM3, CHRNA4 and CHRNA5, using an in-house ELISA in 247 patients with schizophrenia and 344 non-psychiatric controls. Four peptides were designed based on in silico analysis with computational prediction of HLA-DRB1 restricted and B-cell epitopes. The relationship between plasma IgG levels and psychiatric symptoms, as defined by the Operational Criteria Checklist for Psychotic Illness and Affective Illness (OPCRIT), were examined. The results showed that the levels of plasma IgG against peptides derived from CHRM4 and CHRNA4 were significantly increased in patients with schizophrenia compared with control subjects, but there was no significant association of plasma IgG levels with any symptom domain or any specific symptoms. These preliminary results suggest that CHRM4 and CHRNA4 may be novel targets for autoantibody responses in schizophrenia, although the pathogenic relationship between increased serum autoantibody levels and schizophrenia symptoms remains unclear.}
}
@article{GU2023120105,
title = {Detection of Attention Deficit Hyperactivity Disorder in children using CEEMDAN-based cross frequency symbolic convergent cross mapping},
journal = {Expert Systems with Applications},
volume = {226},
pages = {120105},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2023.120105},
url = {https://www.sciencedirect.com/science/article/pii/S0957417423006073},
author = {Danlei Gu and Aijing Lin and Guancen Lin},
keywords = {Cross-frequency coupling, Convergent cross mapping, Complete ensemble empirical mode decomposition with adaptive noise (CEEMDAN), Attention Deficit Hyperactivity Disorder (ADHD), Electroencephalogram (EEG), Dispersion},
abstract = {The cross-frequency coupling relationship of EEG signals is of great significance to identify abnormal EEG signals and diagnose diseases. This paper proposes a new algorithm, CEEMDAN (complete ensemble empirical mode decomposition with adaptive noise)-based cross-frequency symbolic convergent cross mapping (CEEMDAN CF-SCCM). In the numerical simulation test, we have confirmed that CEEMDAN CF-SCCM is an effective method to quantify the cross-frequency information transmission of complex system signals from the three dimensions of robustness to noise, coupling sensitivity, and data length sensitivity. It can successfully distinguish the driving factors and response factors in the phase–amplitude interaction and has good robustness to noise. With this approach, we examined differences in cross-frequency phase–amplitude coupling between ADHD patients and normal individuals over classical brain frequency bands (Alpha, Beta, Delta, Gamma, Theta). According to the position of the electrodes, the brain was divided into four regions: front, back, left, and right, and the phase–amplitude coupling between different frequencies in each region was compared. Compared with the normal group, there was more information transmission in the anterior region of Delta waves and Theta waves. The front and left sides of the brain are responsible for thinking, mental and auditory functions. This information helps us gain insight into the brain dynamics of ADHD patients and contributes to the diagnosis of the disease.}
}
@incollection{NICHELLI2016379,
title = {Chapter 23 - Consciousness and Aphasia},
editor = {Steven Laureys and Olivia Gosseries and Giulio Tononi},
booktitle = {The Neurology of Conciousness (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {San Diego},
pages = {379-391},
year = {2016},
isbn = {978-0-12-800948-2},
doi = {https://doi.org/10.1016/B978-0-12-800948-2.00023-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128009482000236},
author = {Paolo Nichelli},
keywords = {language impairment, anarthria, dynamic aphasia, fMRI, neurophysiological measures},
abstract = {Different language impairments allow us to investigate how much the use of language can influence the content of conscious awareness and therefore of thinking and reasoning. Pure anarthria (different from mutism) and verbal short-term memory deficits are associated with an impairment of the effect of covert speech on the content of working memory. Dynamic aphasia impairs the processes involved in the transition between thinking and speaking. However, even the most severe agrammatic patients can retain reasoning about others’ beliefs that according to some theories can only take place in explicit sentences of a natural language. Error monitoring is also impaired in many aphasic patients and in some of them is associated with complete lack of error awareness (anosognosia for aphasia). In patients with impaired consciousness whenever language examination is impossible or unreliable, fMRI and neurophysiological measures such as event-related potentials can provide a window for examining residual language capabilities.}
}
@incollection{SHAYNAROSENBAUM201787,
title = {2.06 - Episodic and Semantic Memory},
editor = {John H. Byrne},
booktitle = {Learning and Memory: A Comprehensive Reference (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Oxford},
pages = {87-118},
year = {2017},
isbn = {978-0-12-805291-4},
doi = {https://doi.org/10.1016/B978-0-12-809324-5.21037-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128093245210377},
author = {R. {Shayna Rosenbaum} and Alice S.N. Kim and Stevenson Baker},
keywords = {Aging, Amnesia, Autobiographical memory, Autonoetic consciousness, Child development, Default mode network, Familiarity, fMRI, Future imagining, Hippocampus, Medial temporal lobe, Mental time travel, Patient K.C., Personal semantic memory, Recollection, Spatial memory, Temporal neocortex},
abstract = {Much of the richness in human life derives from episodic memory, mental representations of detailed experiences from our personal pasts. To make sense of those experiences, knowledge about the world and oneself must also exist in a form that is free of context – known as semantic memory. This chapter revisits and builds on Tulving's distinction between episodic and semantic memory, with a focus on their differences, similarities, and interactions, informed by cognitive, neuropsychological, and neuroimaging studies. Extensions of this distinction into spatial memory, and beyond memory into future thinking, are considered in the context of process views of memory organization.}
}
@incollection{CATTANEO2015220,
title = {Mental Imagery: Visual Cognition},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {220-227},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.57024-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008097086857024X},
author = {Zaira Cattaneo and Juha Silvanto},
keywords = {Brain stimulation, Creative thinking, Depictivist, Imagery, Imagery debate, Mathematics, Memory, Neuroimaging, Occipital cortex, Perception, Propositional, Reasoning, Vision, Visual cognition, Working memory},
abstract = {Mental imagery can be defined as a quasi-perceptual experience occurring in the absence of perceptual input. The present article provides a review of the key processes involved in mental imagery, the relationship of imagery to working memory, and of the debate on the underlying format of mental images. We also review the functional significance of imagery in a range of cognitive processes, such as memory, creative thinking, reasoning, and problem solving. Finally, the brain basis of mental imagery and its overlap with the cortical regions involved in visual perception are discussed.}
}
@article{MEARA2000345,
title = {Vocabulary and neural networks in the computational assessment of texts written by second-language learners},
journal = {System},
volume = {28},
number = {3},
pages = {345-354},
year = {2000},
issn = {0346-251X},
doi = {https://doi.org/10.1016/S0346-251X(00)00016-6},
url = {https://www.sciencedirect.com/science/article/pii/S0346251X00000166},
author = {Paul Meara and Catherine Rodgers and Gabriel Jacobs},
keywords = {Neural network, Computational assessment, Vocabulary, French},
abstract = {This paper explores the potential of a neural network in language assessment. Many examination systems rely on subjective judgments made by examiners as a way of grading the writing of non-native speakers. Some research (e.g. Engber, 1995. The relationship of lexical proficiency to the quality of ESL compositions. Journal of Second Language Writing 4(2), 139–155) has shown that these subjective judgements are influenced to a very large extent by the lexical choices made by candidates. We took Engber's basic model, but automated the evaluation of lexical content. A group of non-native speakers of French were asked to produce a short text in response to a picture stimulus. The texts were graded by French native speaker teachers. We identified a number of words which occurred in about half the texts, and coded each text for the occurrence and non-occurrence of each word. We then trained a neural network to grade the texts on the basis of these codings. The results suggest that it might be possible to teach a neural network to mimic the judgements made by human markers.}
}
@article{HUANG2025134690,
title = {Operation optimization of Combined Heat and Power microgrid in buildings consider renewable energy, electric vehicles and hydrogen fuel},
journal = {Energy},
volume = {319},
pages = {134690},
year = {2025},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2025.134690},
url = {https://www.sciencedirect.com/science/article/pii/S0360544225003329},
author = {Yongyi Huang and Shoaib Ahmed and Soichiro Ueda and Xunyu Liang and Harun Or Rashid Howlader and Mohammed Elsayed Lotfy and Tomonobu Senjyu},
keywords = {Microgrid, Renewable energy, Electric vehicles, Monte Carlo simulation, K-means, Real-time pricing, Particle swarm optimization, Chance-constrained programming, Combined heat and power},
abstract = {This paper introduces a forward-thinking framework that integrates renewable energy, Electric Vehicles (EVs), and hydrogen within Combined Heat and Power (CHP) microgrids (MGs) for effective building energy management. By utilizing Particle Swarm Optimization (PSO) to find the optimal solution and incorporating Chance-Constrained Programming (CCP) to handle uncertainties in renewable energy generation and EV loads, this framework addresses the complexities of modern energy systems. The study employs Monte Carlo (MC) to simulate the EV load profile, applies K-means clustering to categorize load and renewable generation patterns, and uses a Sigmoid function-based model for Real-Time Pricing (RTP). The combination of PSO and CCP is used to optimize the system’s operating strategy. This evaluates the system’s economic benefits and impact on carbon emissions by analyzing different scenarios, such as weekdays versus weekends and various weather conditions (sunny, cloudy, rainy). The results show that due to the high price of hydrogen, it is currently costly to replace hydrogen completely. However, this integrated approach not only improves energy efficiency and reduces carbon footprint but also ensures system reliability under uncertain conditions, contributing to broader environmental sustainability.}
}
@incollection{VALLERO20211,
title = {Chapter 1 - Systems science},
editor = {Daniel A. Vallero},
booktitle = {Environmental Systems Science},
publisher = {Elsevier},
pages = {1-24},
year = {2021},
isbn = {978-0-12-821953-9},
doi = {https://doi.org/10.1016/B978-0-12-821953-9.00014-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128219539000143},
author = {Daniel A. Vallero},
keywords = {Systems science, Scientific method, Data-intensive discovery, Computational methods, Emergence, Fuzziness, Ecotone, Ecocline, Environmental risk, Biosolids},
abstract = {This chapter manifests how some of the connotations of systems apply to environmental science. The discussion begins with the history and evolution of scientific methods and paradigms, especially the agreement on the scientific method, spatial and temporal complexity, and the first principles of thermodynamics and motion. These are compared to modern environmental applications, including computational methods, governance, and emergence. Scientific and technical communication approaches needed for environmental systems science are described.}
}
@article{LI2024120,
title = {Conformal structure-preserving SVM methods for the nonlinear Schrödinger equation with weakly linear damping term},
journal = {Applied Numerical Mathematics},
volume = {205},
pages = {120-136},
year = {2024},
issn = {0168-9274},
doi = {https://doi.org/10.1016/j.apnum.2024.06.024},
url = {https://www.sciencedirect.com/science/article/pii/S0168927424001727},
author = {Xin Li and Luming Zhang},
keywords = {Damped nonlinear Schrödinger equation, Conformal properties, Supplementary variable method, High-order accuracy, Optimization model},
abstract = {In this paper, by applying the supplementary variable method (SVM), some high-order, conformal structure-preserving, linearized algorithms are developed for the damped nonlinear Schrödinger equation. We derive the well-determined SVM systems with the conformal properties and they are then equivalent to nonlinear equality constrained optimization problems for computation. The deduced optimization models are discretized by using the Gauss type Runge-Kutta method and the prediction-correction technique in time as well as the Fourier pseudo-spectral method in space. Numerical results and some comparisons between this method and other reported methods are given to favor the suggested method in the overall performance. It is worthwhile to emphasize that the numerical strategy in this work could be extended to other conservative or dissipative system for designing high-order structure-preserving algorithms.}
}
@incollection{KUMARI2025219,
title = {Chapter Nine - Harnessing artificial intelligence in identifying and isolation of marine peptides},
editor = {Akanksha Srivastava and Vaibhav Mishra},
series = {Methods in Microbiology},
publisher = {Academic Press},
volume = {56},
pages = {219-242},
year = {2025},
booktitle = {Artificial Intelligence in Microbiology: Scope and Challenges Volume 2},
issn = {0580-9517},
doi = {https://doi.org/10.1016/bs.mim.2024.10.002},
url = {https://www.sciencedirect.com/science/article/pii/S0580951724000461},
author = {Priyanshi Kumari and Bhavya Gaur and Vaibhav Mishra},
keywords = {Marine peptides, Artificial intelligence, Therapeutic values, Machine learning, Deep learning model, AMP's discovery},
abstract = {Marine ecosystem is a vast and relatively unexplored environment, where innumerable resources reside, including marine microbes, animals, algae, and other organisms' those have potential to produce different bioactive microbial peptides. Moreover, marine peptides are structurally unique and known for their exceptional bioactivity, with minimal to no harmful side effects. These bioactive peptides, isolated from marine sources, exhibit various properties, including antimicrobial, antiviral, anti-obesity, antioxidant, anti-inflammatory, and more hence, are deemed as future drugs. Furthermore, discovery of potential active peptides is exorbitant and laborious with traditional methods. Whereas, advanced computational techniques like Artificial Intelligence (AI) and their prime models make easier in the prediction and detection of important marine peptides. In this chapter we are highlighting modern AI based Machine Learning (ML) and Deep Learning (DL) models including k-Nearest Neighbour (kNN), Random Forest (RF), Artificial Neural Networks (ANNs) as ML, Fuzzy Logic or Adaptive Neuro-Fuzzy Inference System (ANFIS), Support Vector Machine (SVMs), and many more other DL models. Moreover, employing these advanced AI models to ease the isolation and identification of the bioactive microbial peptides from marine environments.}
}
@article{SELESNICK2012115,
title = {Quantum-like logics and schizophrenia},
journal = {Journal of Applied Logic},
volume = {10},
number = {1},
pages = {115-126},
year = {2012},
note = {Special issue on Automated Specification and Verification of Web Systems},
issn = {1570-8683},
doi = {https://doi.org/10.1016/j.jal.2011.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1570868311000656},
author = {S.A. Selesnick and G.S. Owen},
keywords = {Logic, Quantum logic, Linear logic, Schizophrenia},
abstract = {Many researchers in different disciplines have independently concluded that brains are, possibly among other things, vector processing devices. In this paper we offer support for this hypothesis coming from a new perspective. Namely, we test it against some known anomalies in the processing by schizophrenic patients of certain logical tasks: they perform better at them than normal controls, despite the observation that they do not generally employ “normal” or “commonsense” logic. On the assumption that they are compelled to use the intrinsic logic of the brain instead of commonsense logic, and that this logic is linear or quantum-like, we are able to resolve these and other anomalies. Our conclusions support the idea that human brains (at least) perform intrinsic logical operations according to the dictates of a linear (or Grassmannian, or quantum-like) logic rather than “classical” or Aristotelian logic (which seems not to be intrinsic to brains, these having evolved under the pressure of different constraints). If this is the case, then commonsense logic must be acquired through experience and the construction of contexts, an ability schizophrenic patients seem to lack, and who are consequently compelled to rely on the intrinsic logic, which is quantum-like and more efficient at certain tasks. Moreover, the proclivity toward errors of von Domarus type (namely the inference that shared attributes imply identity), which seems to be endemic to human thinking and has been discussed in connection with schizophrenia, is also explained on this basis.}
}
@incollection{TAYLOR1995227,
title = {Chapter 13 - Computational needs for process tomography},
editor = {R.A. Williams and M.S. Beck},
booktitle = {Process Tomography},
publisher = {Butterworth-Heinemann},
address = {Oxford},
pages = {227-249},
year = {1995},
isbn = {978-0-08-093801-1},
doi = {https://doi.org/10.1016/B978-0-08-093801-1.50017-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780080938011500174},
author = {R.W. Taylor}
}
@article{AMIROUCHE1991293,
title = {Gain in computational efficiency by vectorization in the dynamic simulation of multi-body systems},
journal = {Computers & Structures},
volume = {41},
number = {2},
pages = {293-302},
year = {1991},
issn = {0045-7949},
doi = {https://doi.org/10.1016/0045-7949(91)90432-L},
url = {https://www.sciencedirect.com/science/article/pii/004579499190432L},
author = {F.M.L. Amirouche and N.H. Shareef},
abstract = {This paper presents a new technique developed for increasing the computational efficiency of the dynamic simulation of multi-body systems, providing the computer code with the speed of execution, which is an order of magnitude ahead of the procedure outlined in S. K. Ider and F. M. L. Amirouche [J. appl. Mech.56, (2) (1989)]. This technique is useful with the finite element based algorithm for the solution of dynamical equations of motion for the constrained and unconstrained systems with flexible/rigid interconnected bodies. The implementation of the technique has totally eliminated the costly multiplications of large Boolean matrices, where intensive cpu utilization was required. The overall expensive computer time has been drastically reduced, particularly for the three-dimensional systems involving large degrees of freedom, as a result of their intricate geometry. The algorithmic procedure has been presented in a matrix form and is based on the recursive formulation using Kane's equation, strain energy, mode synthesis, finite element approach, a stable and efficient method for reducing the number of equations subsequent to the constraints resulting from closed loops and/or prescribed motions. Further enhancement in the speed of execution has been achieved by subjecting the developed code to vectorization on the vector-processing machine. A study of simple robot with flexible links has been presented comparing the execution times on the scalar machine (IBM-3081) and the vector-processor (IBM-3090) with and without vector options. Performance figures has been plotted demonstrating the large gains achieved by the technique developed.}
}
@article{SANCHIS2023102162,
title = {Towards a general equilibrium theory of allocation of time for the digital revolution era},
journal = {Technology in Society},
volume = {72},
pages = {102162},
year = {2023},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2022.102162},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X22003037},
author = {Raúl G. Sanchis},
keywords = {Household economics, Time allocation, Consumer behaviour, Firm behaviour, General equilibrium},
abstract = {The Digital Revolution we are witnessing has started a new era in modern societies and economies. Time inputs, whether these are from human beings and non-human, electronical or mechanical devices are increasingly more important, especially –but not uniquely– in most advanced economies and societies. Existing economic theory strives to accommodate time inputs into mainstream economic theory. This paper contributes to the existing literature on time allocation theoretical models by suggesting a general equilibrium framework likely to respond to some existing challenges in modern economies. In the general equilibrium modelling process, some improvements are made to time allocation models from the consumer side which concern the inclusion of non-human time inputs and multitasking, and a novel development on a producer theory of allocation of time is designed to determine the underpinnings of a computationally tractable general equilibrium theory of allocation of time. Both the solution and usefulness of this work will require the help of cutting-edge computational techniques in future work.}
}
@article{KORMAN201530,
title = {The social life of cognition},
journal = {Cognition},
volume = {135},
pages = {30-35},
year = {2015},
note = {The Changing Face of Cognition},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2014.11.005},
url = {https://www.sciencedirect.com/science/article/pii/S001002771400225X},
author = {Joanna Korman and John Voiklis and Bertram F. Malle},
keywords = {History, Social psychology, Theory of mind, Communication, Robotics, Social cognition, Computation},
abstract = {We begin by illustrating that long before the cognitive revolution, social psychology focused on topics pertaining to what is now known as social cognition: people’s subjective interpretations of social situations and the concepts and cognitive processes underlying these interpretations. We then examine two questions: whether social cognition entails characteristic concepts and cognitive processes, and how social processes might themselves shape and constrain cognition. We suggest that social cognition relies heavily on generic cognition but also on unique concepts (e.g., agent, intentionality) and unique processes (e.g., projection, imitation, joint attention). We further suggest that social processes play a prominent role in the development and unfolding of several generic cognitive processes, including learning, attention, and memory. Finally, we comment on the prospects of a recently developing approach to the study of social cognition (social neuroscience) and two potential future directions (computational social cognition and social–cognitive robotics).}
}
@incollection{YELLA2022770,
title = {2.32 - Magic bullets: Drug repositioning and drug combinations},
editor = {Terry Kenakin},
booktitle = {Comprehensive Pharmacology},
publisher = {Elsevier},
address = {Oxford},
pages = {770-788},
year = {2022},
isbn = {978-0-12-820876-2},
doi = {https://doi.org/10.1016/B978-0-12-820472-6.00116-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012820472600116X},
author = {Jaswanth K. Yella and Anil G. Jegga},
keywords = {Artificial intelligence, Computer-aided drug synthesis, COVID-19, De novo drug discovery, Drug combinations, Drug repurposing, Drug synergy, Machine learning, Network analysis},
abstract = {Discovery and development of novel pharmaceuticals continue to be a very costly, time-consuming and uncertain process impacting negatively not only the research and development of pharmaceutical industry but also health care. Although the number of novel drugs approved each year has grown over by as much as 60% compared to the past decade, there are still many diseases that do not have any approved drug. Recent technological advances in the biomedical, genomics, and computational science domains accompanied by multisource and multidimensional data opened new opportunities and challenges. The drug discovery paradigm is increasingly shifting from hypothesis-driven to data-driven approaches. While the search for the magic bullets of medicine continues, the magic—crunching the data deluge into knowledge and hypotheses nuggets—is mostly driven by machines and machine intelligence. This review will primarily focus on three facets of computational drug discovery approaches, namely, drug repositioning, de novo drug discovery, and drug combinations, and reflect on computational approaches which are reproducible and seem most promising for the machine learning-driven drug discovery. Finally, using COVID-19 as an example, we discuss how the computational approaches are aiding and accelerating the process of discovery of magic bullet(s) for this dreadful pandemic.}
}
@article{SELVERSTON1988109,
title = {A consideration of invertebrate central pattern generators as computational data bases},
journal = {Neural Networks},
volume = {1},
number = {2},
pages = {109-117},
year = {1988},
issn = {0893-6080},
doi = {https://doi.org/10.1016/0893-6080(88)90013-5},
url = {https://www.sciencedirect.com/science/article/pii/0893608088900135},
author = {Allen I Selverston},
abstract = {The essential features of real neural networks are discussed with respect to their usefulness for connectionist modeling. These features are broken down into cellular and synaptic properties and related to a form of neural circuit known as central pattern generators. The gastric and pyloric rhythm of the lobster stomatogastric system are presented as possible computational data bases for modeling studies.}
}
@article{ROTHMCDUFFIE2018173,
title = {Middle school mathematics teachers’ orientations and noticing of features of mathematics curriculum materials},
journal = {International Journal of Educational Research},
volume = {92},
pages = {173-187},
year = {2018},
issn = {0883-0355},
doi = {https://doi.org/10.1016/j.ijer.2018.09.019},
url = {https://www.sciencedirect.com/science/article/pii/S0883035518305512},
author = {Amy {Roth McDuffie} and Jeffrey Choppin and Corey Drake and Jon Davis},
keywords = {Curriculum, Curriculum analysis, Teacher orientation, Middle school mathematics, Teacher noticing},
abstract = {We report findings on teachers’ noticing of features in the teacher resources of mathematics curriculum programs. Based on prior analysis, we selected teachers using one of two curriculum types: delivery mechanism or thinking device. The participating teachers and the curriculum programs aimed to align with the Common Core Standards for Mathematics, and thus, they ostensibly held a common aim for instruction. We analyzed 147 lesson planning interviews with 20 middle school mathematics teachers. We found that teachers attended to similar features of teacher resources; however, patterns for interpreting and planning decisions varied based on teachers’ orientations and curriculum type.}
}
@article{GOLDMAN2025104323,
title = {The Value of Real-time Automated Explanations in Stochastic Planning},
journal = {Artificial Intelligence},
pages = {104323},
year = {2025},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2025.104323},
url = {https://www.sciencedirect.com/science/article/pii/S0004370225000426},
author = {Claudia V. Goldman and Ronit Bustin and Wenyuan Qi and Zhengyu Xing and Rachel McPhearson-White and Sally Rogers},
keywords = {Explainable AI, Decision-Making, Human-Computer Interaction},
abstract = {Recently, we are witnessing an increase in computation power and memory, leading to strong AI algorithms becoming applicable in areas affecting our daily lives. We focus on AI planning solutions for complex, real-life decision-making problems under uncertainty, such as autonomous driving. Human trust in such AI-based systems is essential for their acceptance and market penetration. Moreover, users need to establish appropriate levels of trust to benefit the most from these systems. Previous studies have motivated this work, showing that users can benefit from receiving (handcrafted) information about the reasoning of a stochastic AI planner, for example, controlling automated driving maneuvers. Our solution to automating these hand-crafted notifications with explainable AI algorithms, XAI, includes studying: (1) what explanations can be generated from an AI planning system, applied to a real-world problem, in real-time? What is that content that can be processed from a planner's reasoning that can help users understand and trust the system controlling a behavior they are experiencing? (2) when can this information be displayed? and (3) how shall we display this information to an end user? The value of these computed XAI notifications has been assessed through an online user study with 800 participants, experiencing simulated automated driving scenarios. Our results show that real time XAI notifications decrease significantly subjective misunderstanding of participants compared to those that received only a dynamic HMI display. Also, our XAI solution significantly increases the level of understanding of participants with prior ADAS experience and of participants that lack such experience but have non-negative prior trust to ADAS features. The level of trust significantly increases when XAI was provided to a more restricted set of the participants, including those over 60 years old, with prior ADAS experience and non-negative prior trust attitude to automated features.}
}
@incollection{DALE201343,
title = {Chapter Two - The Self-Organization of Human Interaction},
editor = {Brian H. Ross},
series = {Psychology of Learning and Motivation},
publisher = {Academic Press},
volume = {59},
pages = {43-95},
year = {2013},
issn = {0079-7421},
doi = {https://doi.org/10.1016/B978-0-12-407187-2.00002-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780124071872000022},
author = {Rick Dale and Riccardo Fusaroli and Nicholas D. Duran and Daniel C. Richardson},
keywords = {Alignment, Conversation, Coordination, Dynamics, Interaction, Language, Self-organization, Synergy},
abstract = {We describe a “centipede’s dilemma” that faces the sciences of human interaction. Research on human interaction has been involved in extensive theoretical debate, although the vast majority of research tends to focus on a small set of human behaviors, cognitive processes, and interactive contexts. The problem is that naturalistic human interaction must integrate all of these factors simultaneously, and grander theoretical mitigation cannot come only from focused experimental or computational agendas. We look to dynamical systems theory as a framework for thinking about how these multiple behaviors, processes, and contexts can be integrated into a broader account of human interaction. By introducing and utilizing basic concepts of self-organization and synergy, we review empirical work that shows how human interaction is flexible and adaptive and structures itself incrementally during unfolding interactive tasks, such as conversation, or more focused goal-based contexts. We end on acknowledging that dynamical systems accounts are very short on concrete models, and we briefly describe ways that theoretical frameworks could be integrated, rather than endlessly disputed, to achieve some success on the centipede’s dilemma of human interaction.}
}
@article{KUZNETSOV20206378,
title = {Harmonic balance analysis of pull-in range and oscillatory behavior of third-order type 2 analog PLLs},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {6378-6383},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.1773},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320323818},
author = {N.V. Kuznetsov and M.Y. Lobachev and M.V. Yuldashev and R.V. Yuldashev and G. Kolumbán},
keywords = {Phase-locked loop, third-order PLL, type 2 PLL, nonlinear analysis, harmonic balance method, describing function, global stability, birth of oscillations, hold-in range, pull-in range, lock-in range, Egan conjecture},
abstract = {The most important design parameters of each phase-locked loop (PLL) are the local and global stability properties, and the pull-in range. To extend the pull-in range, engineers often use type 2 PLLs. However, the engineering design relies on approximations which prevent a full exploitation of the benefits of type 2 PLLs. Using an exact mathematical model and relying on a rigorous mathematical thinking this problem is revisited here and the stability and pull-in properties of the third-order type 2 analog PLLs are determined. Both the local and global stability conditions are derived. As a new idea, the harmonic balance method is used to derive the global stability conditions. That approach offers an extra advantage, the birth of unwanted oscillations can be also predicted. As a verification it is shown that the sufficient conditions of global stability derived by the harmonic balance method proposed here and the well-known direct Lyapunov approach coincide with each other, moreover, the harmonic balance predicts the birth of oscillations in the gap between the local and global stability conditions. Finally, an example when the conditions for local and global stability coincide, is considered.}
}
@article{SALEM2025141924,
title = {Novel eco-friendly nicotinonitrile derivative as a corrosion inhibitor for carbon steel: Synthesis, inhibitive efficiency, and DFT analysis},
journal = {Journal of Molecular Structure},
volume = {1335},
pages = {141924},
year = {2025},
issn = {0022-2860},
doi = {https://doi.org/10.1016/j.molstruc.2025.141924},
url = {https://www.sciencedirect.com/science/article/pii/S0022286025006106},
author = {Aya M. Salem and Ahmed Nassef and Ahmed M. Wahba and Samar M. Mohammed},
keywords = {Corrosion, Inhibition, C-steel, HCl, Nicotinonitrile derivatives, Langmuir isotherm},
abstract = {Two new variations of Nicotinonitrile were synthesized, namely: "4-(4-Chlorophenyl)-3-cyano-6-(thien-2-yl)-1H-pyridin-2-one (3A) and 4-(4-Chlorophenyl)-2-oxo-1-(prop‑2-yn-1-yl)-6-(thien-2-yl)-1,2-dihydropyridine-3-carbonitrile (4A). The chemical structures were examined and confirmed using IR and 1H NMR. This method's notable features include being solvent-free, catalyst-free, economical, and having great yields without the need for a catalyst. These compounds were then evaluated as corrosion inhibitors for carbon steel (CS) in 1 M HCl media. Both weight loss (WL) and electrochemical methods such as potentiodynamic polarization (PDP) and electrochemical impedance spectroscopy (EIS) were employed for the investigation. The synthesis and assessment of a novel series of organic compounds with related chemical structures as hydrochloric acid corrosion inhibitors of C-steel is a novel aspect of this study. The results showed that the Nicotinonitrile derivatives were effective corrosion inhibitors, with inhibition efficiencies ( %η) of 86.4 % and 90.7 % for 3A and 4A, respectively, at a concentration of 15×10−5 M. Theoretical computations are applied using the density functional theory. The experiments' findings show that these compounds are effective corrosion inhibitors, and that the concentration of the substances increases the inhibition efficiency. When compared to 3A, the 4A molecule has the maximum efficiency. Monte Carlo simulations and quantum chemical calculations were also performed to analyse and discuss the behaviour of these derivatives. Surface analysis using Scanning Electron Microscopy (SEM) and Energy Dispersive X-ray (EDX) was conducted to verify the results obtained from atomic force microscope measurements. Excellent agreement is found between the outcomes of theoretical computations and experimental measurements.}
}
@article{BAUER2024100002,
title = {What if? Numerical weather prediction at the crossroads},
journal = {Journal of the European Meteorological Society},
volume = {1},
pages = {100002},
year = {2024},
issn = {2950-6301},
doi = {https://doi.org/10.1016/j.jemets.2024.100002},
url = {https://www.sciencedirect.com/science/article/pii/S2950630124000024},
author = {Peter Bauer},
keywords = {Numerical weather prediction, Machine learning, High-performance computing},
abstract = {This paper provides an outlook on the future of operational weather prediction given the recent evolution in science, computing and machine learning. In many parts, this evolution strongly deviates from the strategy operational centres have formulated only several years ago. New opportunities in digital technology have greatly accelerated progress, and the full integration of computational science in numerical weather prediction centres is common knowledge now. Within the last few years, a vast machine learning research community has emerged for creating new and tailor-made products, accelerating processing and – most of all – creating emulators for the entire production of global forecasts that outperform traditional systems at the spatial resolution of the training data. In this context, the role of both numerical models and observations is changing from being equation to data driven. Model simulations and reanalyses are becoming the new currency for training machine learning, and operational centres are in a powerful position as they generate these datasets based on decades worth of experience. This environment creates incredible opportunities to progress much faster than in the past but also uncertainties about what the strategic implications on defining cost-effective and sustainable research and operations are, and how to achieve sufficient high-performance computing and data handling capacities. It will take individual national public services a while to understand what to focus on and how to coordinate their substantial investments in staff and infrastructure at institutional, national and international level. This paper addresses this new situation operational weather prediction finds itself in through formulating the most likely “what if?” scenarios for the near future. It also provides an outline for how weather centres could adapt.}
}
@incollection{ERNST2021265,
title = {Chapter 22 - Pharmaceutical toxicology},
editor = {Martin Wehling},
booktitle = {Principles of Translational Science in Medicine (Third Edition)},
publisher = {Academic Press},
edition = {Third Edition},
address = {Boston},
pages = {265-279},
year = {2021},
isbn = {978-0-12-820493-1},
doi = {https://doi.org/10.1016/B978-0-12-820493-1.00008-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128204931000088},
author = {Steffen W. Ernst and Richard Knight and Jenny Royle and Laura Stephenson},
keywords = {Regulatory toxicology, discovery toxicology, dose-resonse relationship, pharmaceutial safety, drug develoment, risk assessment},
abstract = {This chapter aims to highlight the core principles of pharmaceutical toxicology. It is an interrelated discipline that needs to be applied at all stages of the drug development process to appropriately characterise the safety profile of a drug compound and acknowledge the uncertainties associated with models available. With a strategic mindset, the preclinical safety activities aim to build a comprehensive profile of the drug so that potential hazards can be identified and the risks for healthy trial subjects or patients quantified, and, if necessary, suitable means for eliminating or reducing unacceptable risks can be put in place. We focus on the 2 distinct phases of pharmaceutical toxicology:  Discovery toxicology and regulatory toxicology, to explain how the thinking is built upon at each stage and how the mindset shifts from enabling the selection of an optimally derisked clinical candidate through to thorough risk characterisation and management of those risks for clinical development. Considering attrition due to safety reasons, whether clinical or preclinical, is one of the main reasons for drug project failure, safety assessments should be viewed with equal importance as drug efficacy assessments.}
}
@article{HAN2025104938,
title = {Synergizing Artificial Intelligence and Probiotics: A Comprehensive Review of Emerging Applications in Health Promotion and Industrial Innovation},
journal = {Trends in Food Science & Technology},
pages = {104938},
year = {2025},
issn = {0924-2244},
doi = {https://doi.org/10.1016/j.tifs.2025.104938},
url = {https://www.sciencedirect.com/science/article/pii/S0924224425000743},
author = {Xin Han and Qingqiu Liu and Yun Li and Meng Zhang and Kaiyang Liu and Lai-Yu Kwok and Heping Zhang and Wenyi Zhang},
keywords = {Artificial Intelligence, Gastrointestinal health, Personalized medicine, Probiotic Metabolite},
abstract = {Background
Probiotics play a vital role in human health, garnering significant scientific and public interest. The integration of artificial intelligence (AI) into probiotic research and applications promises to revolutionize strain discovery, health outcomes, and food industry innovations.
Scope and approach
This review explores the intersection of AI and probiotics, focusing on AI-powered machine learning models that revolutionize strain screening, biomarker prediction, and metabolite analysis. Artificial intelligence enables early diagnosis and personalized nutrition by predicting biomarkers for conditions like inflammatory bowel disease and irritable bowel syndrome. It also identifies key probiotic metabolites, such as antimicrobial peptides, exopolysaccharides, and phenolic compounds, advancing fermentation technology and probiotic efficacy. Challenges, including data quality computational demands, and experimental validation, are also discussed.
Key findings and conclusions
Artificial intelligence outperforms conventional methods, offering rapid, high-precision screening, scalable data analysis, and automated strain optimization. Case studies demonstrate AI models achieving over 97% accuracy in bacterial identification and accelerated metabolite discovery. However, challenges like data quality, computational costs, and model interpretability remain. Overcoming these will strengthen the role of AI in precision nutrition, functional food development, and personalized medicine. This review concludes with future perspectives, emphasizing the potential of AI to revolutionize gut microbiome research and probiotic-based therapeutics.}
}
@article{CARVALHAES2021102165,
title = {An overview & synthesis of disaster resilience indices from a complexity perspective},
journal = {International Journal of Disaster Risk Reduction},
volume = {57},
pages = {102165},
year = {2021},
issn = {2212-4209},
doi = {https://doi.org/10.1016/j.ijdrr.2021.102165},
url = {https://www.sciencedirect.com/science/article/pii/S221242092100131X},
author = {Thomaz M. Carvalhaes and Mikhail V. Chester and Agami T. Reddy and Braden R. Allenby},
keywords = {Complex adaptive systems, Resilience, Indicators, Disaster index, Urban systems, Socio-ecological systems},
abstract = {Identifying Disaster resilience indices (DRI) for cities and communities remains a common approach for assessing their structural ability and inherent capacity to cope with, recover from, and adapt to disasters. Particularly popular are composite DRI methodologies that are quantitative, top-down, and geographically mappable. DRI have become more comprehensive as the complexity of urban systems is increasingly acknowledged. However, DRI remain criticized as static, reductive, and inadequate when viewed under a complexity paradigm, which views urban systems as Complex Adaptive Systems (CAS), where observed properties (like resilience) emerge from many interactions among heterogenous agents in a network. Literature reviews have covered the state and trends for DRI development. Our objective is to synthesize literature at the nexus of these reviews, CAS, and Socio-ecological Systems (SES) to determine the extent to which commonly adopted indicators relate to widely accepted tenets of CAS. Findings show that DRI indicators usually relate more closely to temporal snapshots of vulnerability, and alternative framings of current indicators along with interdisciplinary approaches could better capture CAS aspects of urban resilience. Research and development should strive to develop DRI based on underlying principles of CAS and SES, and consider adapting top-down quantitative approaches with thick data, network models, and mixed-method triangulations. Explicitly associating complexity theory with DRI can (i) help researchers in socio-technical and socio-ecological domains develop improved resilience indicators and assessment methods that are clearly differentiated from vulnerability metrics, and (ii) guide policy and decision-makers, amid future uncertainty, to better identify, implement and track capacity-enhancing measures.}
}
@incollection{GOI2024353,
title = {13 - Perspective on photonic neuromorphic computing},
editor = {Min Gu and Elena Goi and Yangyundou Wang and Zhengfen Wan and Yibo Dong and Yuchao Zhang and Haoyi Yu},
booktitle = {Neuromorphic Photonic Devices and Applications},
publisher = {Elsevier},
pages = {353-375},
year = {2024},
series = {Photonic Materials and Applications Series},
isbn = {978-0-323-98829-2},
doi = {https://doi.org/10.1016/B978-0-323-98829-2.00009-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780323988292000098},
author = {Elena Goi and Min Gu},
keywords = {Neuromorphic photonics, photonic memories, all-optical AI microscopy, hybrid platforms},
abstract = {Bioinspired neuromorphic algorithms can process information more rapidly and more accurately than conventional algorithms, in the attempt to achieve brain-like capacity and efficiency in tasks that are challenging for traditional computers but easy for humans. With the development of applications more performing than ever, the computational requirements for running neuromorphic models are increasing exponentially, motivating efforts to develop new, specialized hardware for fast and efficient execution. Neuromorphic photonics, the implementation of neuromorphic information processing with optoelectronic hardware, is a new computational paradigm based on photons aiming to achieve brain-like information processing in the optical domain, and an interdisciplinary field that is expanding in a multitude of directions. In this chapter, we first revise what we believe are currently the main theoretical and technical challenges in the field and then give a broad perspective on the new directions and opportunities that, in our opinion, represent the current frontiers of neuromorphic photonics.}
}
@article{GAO2025111002,
title = {Learning and knowledge-guided evolutionary algorithm for the large-scale buffer allocation problem in production lines},
journal = {Computers & Industrial Engineering},
volume = {203},
pages = {111002},
year = {2025},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2025.111002},
url = {https://www.sciencedirect.com/science/article/pii/S0360835225001482},
author = {Sixiao Gao and Fan Zhang and Shuo Shi},
keywords = {Buffer allocation, Large-scale, Evolutionary algorithm, Learning-guided, Knowledge-guided},
abstract = {The large-scale buffer allocation problem (LBAP) in production lines represents a significant optimization challenge, centered on the efficient allocation of limited temporary storage areas. Prior research has predominantly addressed the LBAP through dynamic programming, search algorithms, and metaheuristics. However, these methodologies are often problem-specific and inefficient when applied to large-scale scenarios. Consequently, there is a pressing need to investigate innovative algorithms beyond existing approaches. This paper presents a novel learning and knowledge-guided evolutionary algorithm designed for the LBAP in production lines. The proposed algorithm develops an adaptive genetic algorithm and a variable neighborhood search algorithm, incorporating a simulated annealing-based strategy. An online Q-learning algorithm is employed to dynamically select the more effective of the two preceding algorithms for solution updates, while the simulated annealing-based strategy regulates the acceptance of these updated solutions. Furthermore, The proposed algorithm dynamically adjusts crossover, mutation, and shaking rates to adapt to the neighborhood structure. It also leverages conflict knowledge obtained from prior update experiences to inform the search process, thereby enhancing solution quality and computational efficiency. Numerical results indicate that the proposed algorithm surpasses state-of-the-art methods in addressing the LBAP. Additionally, empirical ablation studies demonstrate that the knowledge-guided approach efficiently explores promising solution regions by eliminating low-value solutions, while the learning-guided approach effectively generates improved solutions by selecting optimal strategies. This proposed algorithm significantly advances dynamic production resource allocation in large-scale systems.}
}
@article{ZHANG20211358,
title = {Deep learning-based evaluation of factor of safety with confidence interval for tunnel deformation in spatially variable soil},
journal = {Journal of Rock Mechanics and Geotechnical Engineering},
volume = {13},
number = {6},
pages = {1358-1367},
year = {2021},
issn = {1674-7755},
doi = {https://doi.org/10.1016/j.jrmge.2021.09.001},
url = {https://www.sciencedirect.com/science/article/pii/S1674775521001268},
author = {Jinzhang Zhang and Kok Kwang Phoon and Dongming Zhang and Hongwei Huang and Chong Tang},
keywords = {Deep learning, Convolutional neural network (CNN), Tunnel safety, Confidence interval, Random field},
abstract = {The random finite difference method (RFDM) is a popular approach to quantitatively evaluate the influence of inherent spatial variability of soil on the deformation of embedded tunnels. However, the high computational cost is an ongoing challenge for its application in complex scenarios. To address this limitation, a deep learning-based method for efficient prediction of tunnel deformation in spatially variable soil is proposed. The proposed method uses one-dimensional convolutional neural network (CNN) to identify the pattern between random field input and factor of safety of tunnel deformation output. The mean squared error and correlation coefficient of the CNN model applied to the newly untrained dataset was less than 0.02 and larger than 0.96, respectively. It means that the trained CNN model can replace RFDM analysis for Monte Carlo simulations with a small but sufficient number of random field samples (about 40 samples for each case in this study). It is well known that the machine learning or deep learning model has a common limitation that the confidence of predicted result is unknown and only a deterministic outcome is given. This calls for an approach to gauge the model's confidence interval. It is achieved by applying dropout to all layers of the original model to retrain the model and using the dropout technique when performing inference. The excellent agreement between the CNN model prediction and the RFDM calculated results demonstrated that the proposed deep learning-based method has potential for tunnel performance analysis in spatially variable soils.}
}
@article{MORETTI1980145,
title = {Computational aerodynamics using mini computers},
journal = {Computers & Fluids},
volume = {8},
number = {1},
pages = {145-153},
year = {1980},
note = {Special Issue: Computers in Aerodynamics},
issn = {0045-7930},
doi = {https://doi.org/10.1016/0045-7930(80)90037-7},
url = {https://www.sciencedirect.com/science/article/pii/0045793080900377},
author = {Gino Moretti},
abstract = {The importance of minicomputers as a research tool in gasdynamics is explained, and a few examples are given to show their efficiency.}
}
@article{HU2024112598,
title = {Unraveling the dynamics of stacking fault nucleation in ceramics: A case study of aluminum nitride},
journal = {Computational Materials Science},
volume = {231},
pages = {112598},
year = {2024},
issn = {0927-0256},
doi = {https://doi.org/10.1016/j.commatsci.2023.112598},
url = {https://www.sciencedirect.com/science/article/pii/S092702562300592X},
author = {Yixuan Hu and Yumeng Zhang and Simanta Lahkar and Xiaodong Wang and Qi An and Kolan {Madhav Reddy}},
keywords = {Ceramics, Aluminum nitride, Deformation, Stacking faults, Generalized stacking fault energy},
abstract = {Stacking fault (SF), originating from the emission of partial dislocations, wields significant influence over the structural and physicochemical traits of ceramic materials. Yet, the intricate atomic dynamics driving SF nucleation remain obscured. Here, we introduce an improved methodology for computing the generalized stacking fault energy (GSFE) in ceramics, integrating uneven Degrees of Freedom (DOFs) for distinct lattice sites. This refinement has yielded substantial energy advantages over the traditional rigid shift method inherited from metallic systems. Our findings underscore that the relaxation of nonmetallic N atoms within the SF region is pivotal for achieving a more realistic SF simulation. This, in turn, unveils the involvement of N atom migration within the SF region between different aluminum tetrahedral sites during SF nucleation. By alleviating the energy barrier, this relaxation contrasts with previous simulations where nonmetallic elements remained more rigid. This work demonstrates the atomic dynamics of SF nucleation in ceramics and breaks the conventional wisdom of uniformly applying constraints for GSFE computations.}
}
@article{HENNE2019157,
title = {A counterfactual explanation for the action effect in causal judgment},
journal = {Cognition},
volume = {190},
pages = {157-164},
year = {2019},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2019.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S0010027719301301},
author = {Paul Henne and Laura Niemi and Ángel Pinillos and Felipe {De Brigard} and Joshua Knobe},
keywords = {Action effect, Omissions, Omission effect, Causal reasoning, Counterfactual thinking, Causation by omission},
abstract = {People’s causal judgments are susceptible to the action effect, whereby they judge actions to be more causal than inactions. We offer a new explanation for this effect, the counterfactual explanation: people judge actions to be more causal than inactions because they are more inclined to consider the counterfactual alternatives to actions than to consider counterfactual alternatives to inactions. Experiment 1a conceptually replicates the original action effect for causal judgments. Experiment 1b confirms a novel prediction of the new explanation, the reverse action effect, in which people judge inactions to be more causal than actions in overdetermination cases. Experiment 2 directly compares the two effects in joint-causation and overdetermination scenarios and conceptually replicates them with new scenarios. Taken together, these studies provide support for the new counterfactual explanation for the action effect in causal judgment.}
}
@article{KHISTY200577,
title = {Possibilities of steering the transportation planning process in the face of bounded rationality and unbounded uncertainty},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {13},
number = {2},
pages = {77-92},
year = {2005},
note = {Handling Uncertainty in the Analysis of Traffic and Transportation Systems (Bari, Italy, June 10–13 2002)},
issn = {0968-090X},
doi = {https://doi.org/10.1016/j.trc.2005.04.003},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X05000161},
author = {C. Jotin Khisty and Turan Arslan},
keywords = {Paradigm shift, Planning, Rationality, Systemicity, Transportation, Uncertainty},
abstract = {This paper describes and discusses the possibilities of steering the transportation planning process in the face of bounded rationality and unbounded uncertainty: (a) through the introduction of the concept of ‘systemicity’; (b) by expanding the spectrum of the existing planning paradigm currently in use; (c) by reducing complexity through the application of tests of adequacy, dependency, suitability, and adaptability; (d) through the introduction of soft systems thinking; and (e) by using ‘abductive’ in addition to deductive and inductive inferencing. It is concluded that the application of these strategies, adjustments, and tests to the existing planning procedure will hopefully enrich and strengthen our planning effort and make it more robust.}
}
@article{JAHEL2023122624,
title = {The future of social-ecological systems at the crossroads of quantitative and qualitative methods},
journal = {Technological Forecasting and Social Change},
volume = {193},
pages = {122624},
year = {2023},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2023.122624},
url = {https://www.sciencedirect.com/science/article/pii/S0040162523003098},
author = {Camille Jahel and Robin Bourgeois and Jérémy Bourgoin and William's Daré and Marie {De Lattre-Gasquet} and Etienne Delay and Patrice Dumas and Christophe {Le Page} and Marc Piraux and Rémi Prudhomme},
keywords = {Quantitative, Qualitative, Anticipation, Foresight, Power relationship, Discontinuities},
abstract = {Urgent calls to transform societies toward more sustainability make the practice of anticipation more and more necessary. The progressive development of computational technologies has opened room for a growing use of quantitative methods to explore the future of social-ecological systems, in addition to qualitative methods. This warrants investigating issues of power relationships and discontinuities and unknowns that arise when mingling quantitative and qualitative anticipatory methods. We first reflected on the semantics attached to these methods. We then conducted a comparative analysis on the way the articulation of quantitative and qualitative methods was conducted, based on an in-depth analysis of a set of eleven anticipatory projects completed by several external case studies. We propose insights to classify projects according to the timing (successive, iterative or convergent) and the purpose of the articulation (imagination, refinement, assessment and awareness raising). We use these insights to explore methodological implications and power relationships and then discuss the ways to inform or frame anticipatory projects that seek to combine these methods.}
}
@article{ZHOU2024124298,
title = {Hyperspectral imaging combined with blood oxygen saturation for in vivo analysis of small intestinal necrosis tissue},
journal = {Spectrochimica Acta Part A: Molecular and Biomolecular Spectroscopy},
volume = {315},
pages = {124298},
year = {2024},
issn = {1386-1425},
doi = {https://doi.org/10.1016/j.saa.2024.124298},
url = {https://www.sciencedirect.com/science/article/pii/S1386142524004645},
author = {Yao Zhou and LeChao Zhang and DanFei Huang and Yong Zhang and LiBin Zhu and Xiaoqing Chen and Guihua Cui and Qifan Chen and XiaoJing Chen and Shujat Ali},
keywords = {Hyperspectral imaging, Tissue oxygenation, Small intestine tissue, Isosbestic points},
abstract = {Acute mesenteric ischemia (AMI) is a clinically significant vascular and gastrointestinal condition, which is closely related to the blood supply of the small intestine. Unfortunately, it is still challenging to properly discriminate small intestinal tissues with different degrees of ischemia. In this study, hyperspectral imaging (HSI) was used to construct pseudo-color images of oxygen saturation about small intestinal tissues and to discriminate different degrees of ischemia. First, several small intestine tissue models of New Zealand white rabbits were prepared and collected their hyperspectral data. Then, a set of isosbestic points were used to linearly transform the measurement data twice to match the reference spectra of oxyhemoglobin and deoxyhemoglobin, respectively. The oxygen saturation was measured at the characteristic peak band of oxyhemoglobin (560 nm). Ultimately, using the oxygenated hemoglobin reflectance spectrum as the benchmark, we obtained the relative amount of median oxygen saturation in normal tissues was 70.0 %, the IQR was 10.1 %, the relative amount of median oxygen saturation in ischemic tissues was 49.6 %, and the IQR was 14.6 %. The results demonstrate that HSI combined with the oxygen saturation computation method can efficiently differentiate between normal and ischemic regions of the small intestinal tissues. This technique provides a powerful support for internist to discriminate small bowel tissues with different degrees of ischemia, and also provides a new way of thinking for the diagnosis of AMI.}
}
@article{OUDMAN2018214,
title = {Effects of different cue types on the accuracy of primary school teachers' judgments of students' mathematical understanding},
journal = {Teaching and Teacher Education},
volume = {76},
pages = {214-226},
year = {2018},
issn = {0742-051X},
doi = {https://doi.org/10.1016/j.tate.2018.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0742051X17302305},
author = {Sophie Oudman and Janneke {van de Pol} and Arthur Bakker and Mirjam Moerbeek and Tamara {van Gog}},
keywords = {Teacher judgment, Judgment accuracy, Cue utilization, Primary education, Mathematics education, Decimals},
abstract = {To gain insight into how teachers' judgment accuracy can be improved, we investigated effects of cue-type availability. While thinking aloud, 21 teachers judged their fourth grade students' (n = 176) decimal magnitude understanding. Sensitivity (correctly judging what students did understand) did not improve from availability of both answer cues (students' answers to prior practice problems) and student cues (knowledge of students triggered by knowing their names), and was lower when only answer cues were available, compared to only student cues. Specificity (correctly judging what students did not understand) was higher when only answer cues were available, compared to only student cues or both student and answer cues.}
}
@incollection{MILLER2017141,
title = {8 - Doctoral and professional programs},
editor = {Susan M. Miller and Walter H. Moos and Barbara H. Munk and Stephen A. Munk},
booktitle = {Managing the Drug Discovery Process},
publisher = {Woodhead Publishing},
address = {Boston},
pages = {141-169},
year = {2017},
isbn = {978-0-08-100625-2},
doi = {https://doi.org/10.1016/B978-0-08-100625-2.00008-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780081006252000088},
author = {Susan M. Miller and Walter H. Moos and Barbara H. Munk and Stephen A. Munk},
keywords = {Critical thinking, Basic/applied/clinical, Problem identification, Research design, Teams, PhD/PharmD, Postdoc/postdoctoral, Writing/publishing.},
abstract = {In this chapter on graduate and professional education, we explore Doctoral and professional programs, posing a number of key questions you should ask yourself. Where to apply to graduate school or a postdoc, and why? With whom should you work? A PhD or a PharmD, and to postdoc or not? What must you do to be successful? Moreover, we touch on traits important to becoming an independent researcher and ask whether success in graduate school or a postdoctoral fellowship requires different skills than undergraduate degrees. Critical thinking habits and skills underpin this discussion. We outline possible career choices, touching on the knowledge and expertise used by drug hunters, and also ask what might be of most value to potential employers. Each of us is different, and what's best for you is something you will have to decipher, but hopefully only after you consult with family, friends, and advisors or mentors. Regardless, “the big leap” is coming, so get ready.}
}
@article{CARPENTER1992457,
title = {Chapter 4 Cognitively guided instruction: Building on the knowledge of students and teachers},
journal = {International Journal of Educational Research},
volume = {17},
number = {5},
pages = {457-470},
year = {1992},
issn = {0883-0355},
doi = {https://doi.org/10.1016/S0883-0355(05)80005-9},
url = {https://www.sciencedirect.com/science/article/pii/S0883035505800059},
author = {Thomas P. Carpenter and Elizabeth Fennema},
abstract = {This chapter summarizes the results of a series of correlational, experimental, and case studies on Cognitively Guided Instruction (CGI), a program designed to help teachers understand children's thinking and use this knowledge to make instructional decisions. Results of the studies show that teachers' knowledge and beliefs about students' thinking are related to students' achievement. There were significant differences between CGI classes and control classes on the emphasis on problem solving and low level skills, the freedom given to students to construct their own strategies for solving problems, the teachers' knowledge of their students thinking, and the students' achievement in both problem solving and skills.}
}
@article{EVANS201123659,
title = {Advancing Science through Mining Libraries, Ontologies, and Communities*},
journal = {Journal of Biological Chemistry},
volume = {286},
number = {27},
pages = {23659-23666},
year = {2011},
issn = {0021-9258},
doi = {https://doi.org/10.1074/jbc.R110.176370},
url = {https://www.sciencedirect.com/science/article/pii/S0021925819487164},
author = {James A. Evans and Andrey Rzhetsky},
keywords = {Biophysics, Computation, Computer Modeling, Drug Design, Epigenetics, Computational Biology, Information Cascade, Sociology of Science, Text Mining},
abstract = {Life scientists today cannot hope to read everything relevant to their research. Emerging text-mining tools can help by identifying topics and distilling statements from books and articles with increased accuracy. Researchers often organize these statements into ontologies, consistent systems of reality claims. Like scientific thinking and interchange, however, text-mined information (even when accurately captured) is complex, redundant, sometimes incoherent, and often contradictory: it is rooted in a mixture of only partially consistent ontologies. We review work that models scientific reason and suggest how computational reasoning across ontologies and the broader distribution of textual statements can assess the certainty of statements and the process by which statements become certain. With the emergence of digitized data regarding networks of scientific authorship, institutions, and resources, we explore the possibility of accounting for social dependences and cultural biases in reasoning models. Computational reasoning is starting to fill out ontologies and flag internal inconsistencies in several areas of bioscience. In the not too distant future, scientists may be able to use statements and rich models of the processes that produced them to identify underexplored areas, resurrect forgotten findings and ideas, deconvolute the spaghetti of underlying ontologies, and synthesize novel knowledge and hypotheses.}
}
@article{GADZHIEV2025101314,
title = {Creating A dynamic cognovisor – Brain activity recognition using principal Component analysis and Machine learning models},
journal = {Cognitive Systems Research},
volume = {89},
pages = {101314},
year = {2025},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2024.101314},
url = {https://www.sciencedirect.com/science/article/pii/S1389041724001086},
author = {Ismail M. Gadzhiev and Alexander S. Makarov and Vadim L. Ushakov and Vyacheslav A. Orlov and Georgy A. Ivanitsky and Sergei A. Dolenko},
keywords = {Brain Activity, Cognitive States, Machine Learning, Principal Component Analysis},
abstract = {This study explores the feasibility of developing a dynamic cognovisor capable of recognizing cognitive states and transitions using fMRI data. Data were collected from 31 participants performing spatial and verbal tasks during fMRI scanning and were preprocessed using a nine-step algorithm for artifact removal and denoising. Three types of classification problems were examined, with machine learning methods and dimensionality reduction techniques applied to classify activity states. The best-performing models were identified for each classification problem, providing insights into their applicability. Notably, binary classification of resting versus active states achieved good quality with relatively simple methods. A key finding underscores the importance of accounting for temporal history of the signal prior to the prediction moment to improve model performance.}
}
@article{AI2022631,
title = {Reconsidering autistic ‘camouflaging’ as transactional impression management},
journal = {Trends in Cognitive Sciences},
volume = {26},
number = {8},
pages = {631-645},
year = {2022},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2022.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S1364661322001061},
author = {Wei Ai and William A. Cunningham and Meng-Chuan Lai},
keywords = {autism, camouflaging, impression management, predictive coding, social alignment, wellbeing},
abstract = {Social performances pervade human interactions. Some autistic people describe their social performances as ‘camouflaging’ and engage in these performances to mitigate social challenges and survive in the neurotypical world. Here, we reconsider autistic camouflaging under the unifying framework of impression management (IM) by examining overlapping and unique motivations, neurocognitive mechanisms, and consequences. Predictive coding and Bayesian principles are synthesized into a computational model of IM that applies to autistic and neurotypical people. Throughout, we emphasize the inherently transactional, context-dependent nature of IM, the distinct computational challenges faced by autistic people, and the psychological toll that compelled IM can take. Viewing camouflaging through this lens highlights the pressing needs to change societal attitudes, destigmatize autism, refine social skills-building programs for autistic individuals, and integrate these programs with environment-focused support.}
}
@incollection{SCHNEEGANS2008241,
title = {13 - Dynamic Field Theory as a Framework for Understanding Embodied Cognition},
editor = {Paco Calvo and Antoni Gomila},
booktitle = {Handbook of Cognitive Science},
publisher = {Elsevier},
address = {San Diego},
pages = {241-271},
year = {2008},
series = {Perspectives on Cognitive Science},
issn = {15564495},
doi = {https://doi.org/10.1016/B978-0-08-046616-3.00013-X},
url = {https://www.sciencedirect.com/science/article/pii/B978008046616300013X},
author = {Sebastian Schneegans and Gregor Schöner},
abstract = {Publisher Summary
Embodied cognition is an approach to cognition that has roots in motor behavior. This approach emphasizes that cognition typically involves acting with a physical body on an environment in which that body is immersed. The approach of embodied cognition postulates that understanding cognitive processes entails understanding their close link to the motor surfaces that may generate action and to the sensory surfaces that provide sensory signals about the environment. To a certain extent, the embodiment stance implies a mistrust of the abstraction inherent in much information processing thinking, in which the interface between cognitive processes and their sensorimotor support is drawn at a level that is quite removed from both the sensory and the motor systems. New theoretical tools are needed to address cognition within the embodiment perspective. This chapter reviews one set of theoretical concepts which is believed to be particularly suited to address the constraints of embodiment and situatedness. It refers to this set of concepts as Dynamical Systems Thinking.}
}
@article{TALANOV2018473,
title = {Simulation of serotonin mechanisms in NEUCOGAR cognitive architecture},
journal = {Procedia Computer Science},
volume = {123},
pages = {473-478},
year = {2018},
note = {8th Annual International Conference on Biologically Inspired Cognitive Architectures, BICA 2017 (Eighth Annual Meeting of the BICA Society), held August 1-6, 2017 in Moscow, Russia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.01.072},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918300735},
author = {Max Talanov and Fail Gafarov and Jordi Vallverdú and Sergey Ostapenko and Marat Gazizov and Alexander Toschev and Alexey Leukhin and Salvatore Distefano},
keywords = {Serotonin, dopamine, disgust, artificial intelligence, simulation, affective computing, emotion modelling, neuromodulation},
abstract = {This work aims at demonstrating that the neuromodulatory mechanisms that control the emotional states of mammals (specifically rat’s brains) can be represented and re-implemented in a computational model processed by a machine. In particular we specifically focus on two neuro-transmitters, serotonin and dopamine, starting from their fundamental role in basic cognitive processes. In our specific implementation, we represent the simulation of the ‘disgust-like’ state based on the three dimensional neuromodulatory model of affects or emotions, according to the ‘cube of emotions’. These functional mechanisms can be transferred into an artificial cognitive system: inhibition, for example, can elicit a blocking behaviour that, depending on its intensity and duration, can push the system to a general emotional state. We have simulated 1000 milliseconds of the serotonin and dopamine systems using NEST Neural Simulation Tool with the rat brain as the model to artificially reproduce this mechanism on a computational system.}
}
@incollection{STEIN202139,
title = {Chapter 2 - Brain–minds: What’s the best metaphor?},
editor = {Dan J. Stein},
booktitle = {Problems of Living},
publisher = {Academic Press},
pages = {39-59},
year = {2021},
isbn = {978-0-323-90239-7},
doi = {https://doi.org/10.1016/B978-0-323-90239-7.00005-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323902397000055},
author = {Dan J. Stein},
keywords = {Psychiatry, Philosophy, Realism, Psychiatric classification, Pluralism, Erklären, Verstehen, Epistemic humility, Practical wisdom},
abstract = {This chapter addresses the question of how best to think about the brain–mind from both philosophical and psychiatric perspectives. The section on philosophy of mind notes the positions of physicalism, dualism, and functionalism, and proposes that emergent materialism has particular advantages. The section on psychiatry notes the positions of behaviourism and existentialism. Two key metaphors of the brain–mind are then critiqued: the hydraulic model of psychoanalysis, and the computational model of cognitive science. A third metaphor, that of ‘wetware’, which emphasizes that the brain–mind cannot simply be divided into hardware and software, but rather that it must be approached as a complex psychobiological phenomenon, is proposed. Several advantages of this metaphor are discussed, including that it is consistent with emergent materialism and a view of the brain–mind as embodied and embedded in social activity, as well as with current cognitive-affective and psychiatric science.}
}
@article{VANOPHEUSDEN2019127,
title = {Tasks for aligning human and machine planning},
journal = {Current Opinion in Behavioral Sciences},
volume = {29},
pages = {127-133},
year = {2019},
note = {Artificial Intelligence},
issn = {2352-1546},
doi = {https://doi.org/10.1016/j.cobeha.2019.07.002},
url = {https://www.sciencedirect.com/science/article/pii/S2352154619300622},
author = {Bas {van Opheusden} and Wei Ji Ma},
abstract = {Research on artificial intelligence and research on human intelligence rely on similar conceptual foundations and have long inspired each other [1,2•]. However, achieving concrete synergy has been difficult, with one obstacle being a lack of alignment of the tasks used in both fields. Artificial intelligence research has traditionally focused on tasks that are challenging to solve, often using human performance as a benchmark to surpass [3, 4, 5, 6, 7]. By contrast, cognitive science and psychology have moved towards tasks that are simple enough to allow for detailed computational modeling of people’s choices. These divergent objectives have led to a divide in the complexity of tasks studied, both in perception and cognition. The purpose of this paper is to explore the middle ground: are there tasks that are reasonably attractive to both fields and could provide fertile ground for synergy?}
}
@incollection{BLISS19921,
title = {REASONING SUPPORTED BY COMPUTATIONAL TOOLS},
editor = {MICHAEL R. KIBBY and J. ROGER HARTLEY},
booktitle = {Computer Assisted Learning: Selected Contributions from the CAL '91 Symposium},
publisher = {Pergamon},
address = {Amsterdam},
pages = {1-9},
year = {1992},
isbn = {978-0-08-041395-2},
doi = {https://doi.org/10.1016/B978-0-08-041395-2.50007-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780080413952500078},
author = {JOAN BLISS and JON OGBORN and RICHARD BOOHAN and JONATHAN BRIGGS and TIM BROSNAN and DEREK BROUGH and HARVEY MELLAR and ROB MILLER and CAROLINE NASH and CATHY RODGERS and BABIS SAKONIDIS},
abstract = {Abstract
This paper sets out the work of the Tools for Exploratory Learning Programme within the ESRC Initiative Information Technology in Education. The research examines young secondary children's reasoning with computational tools. We distinguish between exploratory and expressive modes of learning, that is, interaction with another's model and creation of one's own model, respectively. The research focuses on reasoning, rather than learning, along three dimensions: quantitative, qualitative, and semi-quantitative. It provides a 3 × 2 classification of tasks according to modes of learning and types of reasoning. Modelling tools were developed for the study and descriptions of these are given. The research examined children's reasoning with tools in all three dimensions looking more exhaustively at the semi-quantitative. Pupils worked either in an exploratory mode or an expressive mode on one of the following topics: Traffic, Health and Diet, and Shops and Profits. They spent 3-4 h individually with a researcher over 2 weeks, carrying out four different activities: reasoning without the computer; learning to manipulate first the computer then later the tool and finally carrying out a task with the modelling tool. Pupils were between 12 and 14 yr. Research questions both about children's reasoning when working with or creating models and about the nature of the tools used are discussed. Finally an analytic scheme is set out which describes the nature of the causal and non-causal reasoning observed together with some tentative results.}
}
@article{GOVIL2022103125,
title = {Validation of agile methodology as ideal software development process using Fuzzy-TOPSIS method},
journal = {Advances in Engineering Software},
volume = {168},
pages = {103125},
year = {2022},
issn = {0965-9978},
doi = {https://doi.org/10.1016/j.advengsoft.2022.103125},
url = {https://www.sciencedirect.com/science/article/pii/S0965997822000357},
author = {Nikhil Govil and Ashish Sharma},
keywords = {Software Development Process, Decision support system, Fuzzy logic, Agile Software Development, Fuzzy TOPSIS, Multi-Criteria Decision Making},
abstract = {Agile methodologies have been an emerging choice of software professionals for the past decade and a half. However, apart from this, some other SDLC models are also available for selection in front of software developers to develop any software. Usually, project managers select any of these models to develop software through their past experiences. There is no logical basis for this selection to be completely correct, as a result of which there is always a risk of software failure or over budget if an inappropriate model has opted. Keeping this problem of software industries in mind, an ideal SDLC model has been identified mathematically in this article. In this article, we applied the Fuzzy TOPSIS method that validates Agile software development as an ideal choice. We have taken a total of six software development processes that are being applied globally. Feedback from five experienced decision-makers has been taken in the form of linguistic terms and further converted into fuzzy values to perform the computation of the closeness coefficient rank of each experimented alternative software development process.}
}
@article{SHIN2023104897,
title = {Pedagogical discourse markers in online algebra learning: Unraveling instructor's communication using natural language processing},
journal = {Computers & Education},
volume = {205},
pages = {104897},
year = {2023},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2023.104897},
url = {https://www.sciencedirect.com/science/article/pii/S0360131523001744},
author = {Jinnie Shin and Renu Balyan and Michelle P. Banawan and Tracy Arner and Walter L. Leite and Danielle S. McNamara},
keywords = {Pedagogical communication, Online learning, Video lectures, Natural language processing},
abstract = {Despite the proliferation of video-based instruction and its benefits—such as promoting student autonomy and self-paced learning—the complexities of online teaching remain a challenge. To be effective, educators require extensive training in digital teaching methodologies. As such, there's a pressing need to examine and comprehend the intricacies of instructors' communication patterns within this context. This research addresses the pressing need to understand pedagogical discourse in online video lectures in Algebra classes by employing computational linguistic tools and natural language processing (NLP). Using transcripts from 125 Algebra 1 video lectures—comprising 4962 instances of pedagogical discourse—from five instructors at Math Nation, a virtual math learning environment, we analyzed the conveyance of linguistic, attitudinal, and emotional nuances. With the aid of 26 Coh-Metrix and SÉANCE features, we classified educators' language choices, achieving an accuracy of 86.7%. Furthermore, variations in language choices, as signified by discourse markers, were examined through a K-means clustering approach. The resulting 17 clusters were grouped into interpersonal, structural, and cognitive pedagogic functions. Through this exploration, we demonstrate the promising potential of NLP in efficiently deciphering pedagogical communication patterns in video lectures. These insights open a new avenue for research, aimed at assessing the efficacy of digital instruction by scrutinizing pedagogical discourse characteristics in computer-based learning environments.}
}