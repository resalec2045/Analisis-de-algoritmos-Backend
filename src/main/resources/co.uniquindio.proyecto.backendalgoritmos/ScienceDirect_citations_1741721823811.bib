@article{LEESON2008630,
title = {Cognitive ability, personality, and academic performance in adolescence},
journal = {Personality and Individual Differences},
volume = {45},
number = {7},
pages = {630-635},
year = {2008},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2008.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0191886908002444},
author = {Peter Leeson and Joseph Ciarrochi and Patrick C.L. Heaven},
keywords = {Cognitive ability, Personality, Academic performance, Adolescents, Hope, Self-esteem, Attributional style, Psychometric },
abstract = {Does positive thinking predict variance in school grades over and above that predicted by cognitive ability? Six hundred and thirty nine high school students participated in a three-year longitudinal study that predicted grades using cognitive ability and three positive thinking variables – self-esteem, hope, and attributional style. Hope, positive attributional style and cognitive ability predicted higher grades, whilst self-esteem was a less consistent predictor of academic performance. Structural equation modelling revealed significant paths from cognitive ability, gender, and a second order positive thinking factor to grades. The results suggest that intelligence, gender, and positive thinking each play a unique role in predicting academic performance in youth. Some suggestions for further research are made.}
}
@article{WANG2016747,
title = {Towards felicitous decision making: An overview on challenges and trends of Big Data},
journal = {Information Sciences},
volume = {367-368},
pages = {747-765},
year = {2016},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2016.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0020025516304868},
author = {Hai Wang and Zeshui Xu and Hamido Fujita and Shousheng Liu},
keywords = {Big Data, Data deluge, Decision making, Data analysis, Data-intensive applications, Computational social science},
abstract = {The era of Big Data has arrived along with large volume, complex and growing data generated by many distinct sources. Nowadays, nearly every aspect of the modern society is impacted by Big Data, involving medical, health care, business, management and government. It has been receiving growing attention of researches from many disciplines including natural sciences, life sciences, engineering and even art & humanities. It also leads to new research paradigms and ways of thinking on the path of development. Lots of developed and under-developing tools improve our ability to make more felicitous decisions than what we have made ever before. This paper presents an overview on Big Data including four issues, namely: (i) concepts, characteristics and processing paradigms of Big Data; (ii) the state-of-the-art techniques for decision making in Big Data; (iii) felicitous decision making applications of Big Data in social science; and (iv) the current challenges of Big Data as well as possible future directions.}
}
@article{BATT2002185,
title = {Lateral thinking: 2-D interpretation of thermochronology in convergent orogenic settings},
journal = {Tectonophysics},
volume = {349},
number = {1},
pages = {185-201},
year = {2002},
note = {Low Temperature Thermochronology: From Tectonics to Landscape Evolution},
issn = {0040-1951},
doi = {https://doi.org/10.1016/S0040-1951(02)00053-7},
url = {https://www.sciencedirect.com/science/article/pii/S0040195102000537},
author = {Geoffrey E. Batt and Mark T. Brandon},
keywords = {Lateral motion, Thermochronology, Orogenic regions},
abstract = {Lateral motion of material relative to the regional thermal and kinematic frameworks is important in the interpretation of thermochronology in convergent orogens. Although cooling ages in denuded settings are commonly linked to exhumation, such data are not related to instantaneous behavior but rather to an integration of the exhumation rates experienced between the thermochronological ‘closure’ at depth and subsequent exposure at the surface. The short spatial wavelength variation of thermal structure and denudation rate typical of orogenic regions thus renders thermochronometers sensitive to lateral motion during exhumation. The significance of this lateral motion varies in proportion with closure temperature, which controls the depth at which isotopic closure occurs, and hence, the range of time and length scales over which such data integrate sample histories. Different chronometers thus vary in the fundamental aspects of the orogenic character to which they are sensitive. Isotopic systems with high closure temperature are more sensitive to exhumation paths and the variation in denudation and thermal structure across a region, while those of lower closure temperature constrain shorter-term behaviour and more local conditions. Discounting lateral motion through an orogenic region and interpreting cooling ages purely in terms of vertical exhumation can produce ambiguous results because variation in the cooling rate can result from either change in kinematics over time or the translation of samples through spatially varying conditions. Resolving this ambiguity requires explicit consideration of the physical and thermal framework experienced by samples during their exhumation. This can be best achieved through numerical simulations coupling kinematic deformation to thermal evolution. Such an approach allows the thermochronological implications of different kinematic scenarios to be tested, and thus provides an important means of assessing the contribution of lateral motion to orogenic evolution.}
}
@article{JACKSON201386,
title = {Airflow reversal and alternating corkscrew vortices in foredune wake zones during perpendicular and oblique offshore winds},
journal = {Geomorphology},
volume = {187},
pages = {86-93},
year = {2013},
issn = {0169-555X},
doi = {https://doi.org/10.1016/j.geomorph.2012.12.037},
url = {https://www.sciencedirect.com/science/article/pii/S0169555X13000081},
author = {Derek W.T. Jackson and Meiring Beyers and Irene Delgado-Fernandez and Andreas C.W. Baas and Andrew J. Cooper and Kevin Lynch},
keywords = {Computational fluid dynamics, Aeolian, Foredunes, Transport, Airflow modelling, Lee side eddies},
abstract = {On all sandy coastlines fringed by dunes, understanding localised air flow allows us to examine the potential sand transfer between the beach and dunes by wind-blown (Aeolian) action. Traditional thinking into this phenomenon had previously included only onshore winds as effective drivers of this transfer. Recent research by the authors, however, has shown that offshore air-flow too can contribute significantly, through lee-side back eddies, to the overall windblown sediment budget to coastal dunes. Under rising sea levels and increased erosion scenarios, this is an important process in any post-storm recovery of sandy beaches. Until now though, full visualisation in 3D of this newly recognised mechanism in offshore flows has not been achieved. Here, we show for the first time, this return flow eddy system using 3D computational fluid dynamics modelling, and reveal the presence of complex corkscrew vortices and other phenomena. The work highlights the importance of relatively small surface undulations in the dune crest which act to induce the spatial patterns of airflow (and transport) found on the adjacent beach.}
}
@article{ZENASNI2009353,
title = {Perception of emotion, alexithymia and creative potential},
journal = {Personality and Individual Differences},
volume = {46},
number = {3},
pages = {353-358},
year = {2009},
issn = {0191-8869},
doi = {https://doi.org/10.1016/j.paid.2008.10.030},
url = {https://www.sciencedirect.com/science/article/pii/S0191886908004108},
author = {F. Zenasni and T.I. Lubart},
keywords = {Creativity, Ability EI, Alexythimia, Emotional creativity, Divergent thinking},
abstract = {Theoretical proposals suggest that emotional intelligence (EI) may favor creativity. In the present paper, two studies are reported with French adults to examine the degree to which the ability to identify emotion is related to creative performance. This component of ability EI was hypothesized to be positively associated with a divergent thinking task involving emotional information. Contrary to our expectations, the first study (n=95) indicated that ability to identify emotions in faces and images was negatively related to idea generation ability. The second study (n=100) including a measure of alexithymia confirmed this relation. Moreover, evaluating emotional creativity, we observed a significant negative link between the ability to identify emotions and the tendency to experience emotions differently from those of others. We discuss these results suggesting an opposition between consensual/convergent thinking concerning emotions (ability EI) and divergent thinking.}
}
@article{CARNEY2004135,
title = {Denis Noble discusses his career in computational biology},
journal = {Drug Discovery Today: BIOSILICO},
volume = {2},
number = {4},
pages = {135-137},
year = {2004},
issn = {1741-8364},
doi = {https://doi.org/10.1016/S1741-8364(04)02414-X},
url = {https://www.sciencedirect.com/science/article/pii/S174183640402414X},
author = {Stephen Carney},
keywords = {Interview, computer modeling, grid computing, cardiac electrophysiology, whole organ models, arrhythmia},
abstract = {Denis Noble was born in 1936 and obtained a BSc and PhD from University College London. He is one of the pioneers of computational biology related to cardiac cell electrophysiology and its incorporation into the first detailed biophysical models of the whole organ. He has made many major contributions to this work spanning from his groundbreaking work in 1960, showing that in heart, contrary to the situation in nerve, the first effect of membrane depolarisation is to greatly reduce potassium conductance, which in turn is greatly dependent on plasma potassium levels. His work over the last forty years has culminated in a highly successful virtual model of the heart, which has allowed theoretical interpretation of cardiac arrhythmias and the development of antiarrhythmic drugs. Professor Noble was made a fellow of the Royal Society in 1979, one of the highlights of his many awards. He is in great demand as a presenter of plenary lectures at many august meetings. In addition to his abilities as a computational biologist, Professor Noble is an accomplished linguist and has given lectures in French and Italian, and has significant abilities in Japanese, Korean and even Maori.}
}
@article{LI2024141569,
title = {Programming experiment course for innovative and sustainable education: A case study of Java for Millikan Oil-Drop experiment},
journal = {Journal of Cleaner Production},
volume = {447},
pages = {141569},
year = {2024},
issn = {0959-6526},
doi = {https://doi.org/10.1016/j.jclepro.2024.141569},
url = {https://www.sciencedirect.com/science/article/pii/S0959652624010175},
author = {Yizheng Li and Guandong Su and Haocheng Pan and Chengwei Tan and Gongsheng Li},
keywords = {Laboratory instruction, Java object-oriented programming, Innovative and sustainable education, Student-centered learning},
abstract = {A teaching approach of programming experiment courses with a new education process is proposed in this study to improve the sustainability of engineering education and extend the accessibility of lab experiments across experiment courses, with the goal of encouraging innovative and scientific thinking among students. The Millikan Oil-Drop experiment combined with Java object-oriented programming is demonstrated as a case study to validate the feasibility and advantages of this teaching approach. The new education process of the experiment course is designed based on Jean Piaget's cognitive development theory, which has high practical potential for popularization among tertiary institutions without additional cost. Additionally, this work discussed the relevance of the general criterion of the Accreditation Board for Engineering and Technology on student outcomes to educational accreditation, further indicating that introducing programming into experiment education can improve students' all-round ability and strengthen the triangular relationship among the three main subjects in tertiary education, namely, students, faculty, and higher educational institutions. This study may serve as an educational guide for teachers and tertiary institutions to pursue innovative and sustainable education.}
}
@article{GADALLA2023200201,
title = {Concepts and experiments on psychoanalysis driven computing},
journal = {Intelligent Systems with Applications},
volume = {18},
pages = {200201},
year = {2023},
issn = {2667-3053},
doi = {https://doi.org/10.1016/j.iswa.2023.200201},
url = {https://www.sciencedirect.com/science/article/pii/S2667305323000261},
author = {Minas Gadalla and Sotiris Nikoletseas and José Roberto {de A. Amazonas} and José D.P. Rolim},
keywords = {Lacanian discourses, Psychoanalysis computing, GPT-3},
abstract = {This research investigates the effective incorporation of the human factor and user perception in text-based interactive media. In such contexts, the reliability of user texts is often compromised by behavioural and emotional dimensions. To this end, several attempts have been made in the state of the art, to introduce psychological approaches in such systems, including computational psycholinguistics, personality traits and cognitive psychology methods. In contrast, our method is fundamentally different since we employ a psychoanalysis-based approach; in particular, we use the notion of Lacanian discourse types, to capture and deeply understand real (possibly elusive) characteristics, qualities and contents of texts, and evaluate their reliability. As far as we know, this is the first time computational methods are systematically combined with psychoanalysis. We believe such psychoanalytic framework is fundamentally more effective than standard methods, since it addresses deeper, quite primitive elements of human personality, behaviour and expression which usually escape methods functioning at “higher”, conscious layers. In fact, this research is a first attempt to form a new paradigm of psychoanalysis-driven interactive technologies, with broader impact and diverse applications. To exemplify this generic approach, we apply it to the case-study of fake news detection; we first demonstrate certain limitations of the well-known Myers–Briggs Type Indicator (MBTI) personality type method, and then propose and evaluate our new method of analysing user texts and detecting fake news based on the Lacanian discourses psychoanalytic approach.}
}
@article{SUN2025129677,
title = {StereoSqueezeNet: With fewer parameters but higher accuracy than SqueezeNet},
journal = {Neurocomputing},
volume = {627},
pages = {129677},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129677},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225003492},
author = {Qiaoyan Sun and Jianfei Chen},
keywords = {SSNet, Stereo, SqueezeNet, Network compression, Network parameters},
abstract = {Convolutional neural networks (CNNs) have evolved from the initial LeNet to date, and network models have become increasingly deep and comprehensive. It has been proven that deeper networks have better fitting effects, but the corresponding parameter size and computational complexity increase rapidly. With the continuous development of mobile Internet technology, portable devices have been rapidly popularized, and users have put forward more and more demands. Thus, how to design efficient and high-performance lightweight convolutional neural networks (CNNs) is the key to solve this challenging problem. Recently, this type of convolutional neural networks (CNNs)--lightweight convolutional neural networks (CNNs), which adopt the design concept of compression networks and maintain high accuracy with fewer parameters, has attracted increasing attention. SqueezeNet is a lightweight CNN adapting to edge device deployment. Its number of parameters is only 1/50 of AlexNet, but it achieves the same accuracy as AlexNet. In order to make the network more lightweight, inspired by SqueezeNet, MobileNet, SENet, SKNet, AlexNet, etc., in this paper we propose StereoSqueezeNet, using much fewer parameters but achieving even better accuracy than SqueezeNet.}
}
@article{ARUN2009S1115,
title = {P03-116 Damage to object oriented programming in the brain explains many of the psychopathological features of schizophrenia},
journal = {European Psychiatry},
volume = {24},
pages = {S1115},
year = {2009},
note = {17th EPA Congress - Lisbon, Portugal, January 2009, Abstract book},
issn = {0924-9338},
doi = {https://doi.org/10.1016/S0924-9338(09)71348-3},
url = {https://www.sciencedirect.com/science/article/pii/S0924933809713483},
author = {C.P. Arun},
abstract = {Introduction
Modern computers often use programs that incorporate a programming technique called Object Oriented Programming (OOP), allowing users to manipulate complex ‘computational objects’ such as menus, screen windows, etc with very little effort, say the click of a mouse. OOP deals with structures called objects and allows time and computational effort saving devices such as inheritance, polymorphism and encapsulation. We examine whether the brain itself may use OOP and if representation of objects suffers a breakdown in schizophrenia.
Review of literature
Previous models fail to provide a unifying explanation with a computational basis that could explain the psychopathology in schizophrenia. METHODS Using the object oriented programming language JavaTM we designed a system of self-objects named ‘hand’, ‘action monitor’ etc interacting with non-self objects ‘scissors’, ‘hammer’, ‘wall’, etc. In computational experiments, we allow the ‘action monitor’ to fail; the features of disparate objects are allowed to merge, some features of an object are allowed to be shared with other objects, etc.
Results
By transposing only a few lines of code, it is possible to duplicate various features of the psychopathology of schizophrenia.
Discussion
Our model can demonstrate overinclusion (overabstraction), concrete thinking (underabstraction), loss of ego boundaries (conjoining of disparate objects), delusions (misattribution of object function), lack of insight (poor monitoring of object activity) and passivity (loss of monitoring and misattribution of object activity).
Conclusion
The brain must use the OOP model in its computations. Failure of object representation and manipulation must lie at the core of the psychopathology of schizophrenia.}
}
@article{MARENDA20232152,
title = {Sliding pendulum isolators without secretes},
journal = {Procedia Structural Integrity},
volume = {44},
pages = {2152-2157},
year = {2023},
note = {XIX ANIDIS Conference, Seismic Engineering in Italy},
issn = {2452-3216},
doi = {https://doi.org/10.1016/j.prostr.2023.01.275},
url = {https://www.sciencedirect.com/science/article/pii/S2452321623002846},
author = {Ivan Marenda and Agostino Marioni and Marco Banfi and Roberto Dalpedri},
keywords = {friction coefficient, pendulum device, contact, isolation system},
abstract = {Over the last decades, anti-seismic devices have gained increasing interest in the civil engineering field. The introduction of the base isolation system has led to a new concept in the construction panorama in terms of human life safety, a new way of thinking on new constructions, improvement and retrofitting on existent structures. Therefore, rubber and friction isolators have been deeply investigated to hence performances and predict dynamic behaviour during an earthquake. While the response of the former is characterised by the composition of the elastomeric compound, the latter features special materials able to dissipate energy by moving on smooth surfaces. This paper focuses on friction pendulum devices and addresses its attention on the behaviour of sliding materials. It is well-known that stick-slip phenomenon occurs when friction excitation is present and, in the anti-seismic field is important to reduce it and have a well-representative mathematical law able to describe it. Therefore, Hirun International after performing several treatments of the sliding materials has set up a special processing to guarantee a stable response of the HI-M material used on pendulum devices. The paper, after a brief presentation of the special sliding material, shows a comparison between the material with and without the treatment in terms of the force-displacement law. The paper also analyses in detail the cinematic behaviour of the sliding pendulum with one or two main sliding surfaces, with and without central articulation and determines the stress distribution in the sliding surfaces for the different cases.}
}
@article{WEBB2008360,
title = {The role of teacher instructional practices in student collaboration},
journal = {Contemporary Educational Psychology},
volume = {33},
number = {3},
pages = {360-381},
year = {2008},
note = {Collaborative Discourse, Argumentation, and Learning},
issn = {0361-476X},
doi = {https://doi.org/10.1016/j.cedpsych.2008.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0361476X0800026X},
author = {Noreen M. Webb and Megan L. Franke and Marsha Ing and Angela Chan and Tondra De and Deanna Freund and Dan Battey},
keywords = {Instructional practices, Student collaboration},
abstract = {Prior research on collaborative learning identifies student behaviors that significantly predict student achievement, such as giving explanations of one’s thinking. Less often studied is the role of teachers’ instructional practices in collaboration among students. This article investigates the extent to which teachers engage in practices that support students’ explanations of their thinking, and how these teacher practices might be related to the nature of explanations that students give when asked by the teacher to collaborate with each other. The teachers observed here, all of whom received specific instruction in eliciting the details of student thinking, varied significantly in the extent to which they asked students to elaborate on their suggestions. This variation corresponded to variation across classrooms in the nature and extent of student explanations during collaborative conversations and to differences in student achievement.}
}
@article{SANTOS2023102749,
title = {Policy entrepreneurs in the global education complex: The case of Finnish education experts working in international organisations},
journal = {International Journal of Educational Development},
volume = {98},
pages = {102749},
year = {2023},
issn = {0738-0593},
doi = {https://doi.org/10.1016/j.ijedudev.2023.102749},
url = {https://www.sciencedirect.com/science/article/pii/S0738059323000263},
author = {Íris Santos and Elias Pekkola},
keywords = {Development cooperation for education, Influence, Finnish education experts, Complexity, Multiple streams approach},
abstract = {This article analyses the perceived role of Finnish education experts working in development cooperation for education. We interviewed 31 education experts working in international organisations representing Finland. A theoretically pluralist approach is utilised combining complexity thinking with a multiple streams approach. The analysis demonstrates that the context of educational development cooperation is ambiguous and complex. Influencing policymaking is a strategic, non-linear task which takes time, resources, and personal skills. Policy entrepreneurs need to understand the dynamics of development cooperation, identify actors that trust them, and recognise when policy windows are likely to open.}
}
@incollection{MAURYA2010175,
title = {Chapter 8 - Computational Challenges in Systems Biology},
editor = {Edison T. Liu and Douglas A. Lauffenburger},
booktitle = {Systems Biomedicine},
publisher = {Academic Press},
address = {San Diego},
pages = {175-223},
year = {2010},
isbn = {978-0-12-372550-9},
doi = {https://doi.org/10.1016/B978-0-12-372550-9.00008-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780123725509000080},
author = {Mano Ram Maurya and Shankar Subramaniam},
abstract = {Publisher Summary
This chapter examines the challenges and some of the recent advances in computational systems biology. Research in computational systems biology has moved beyond interaction networks based simply on clustering and correlation. There are two paradigms in computational systems biology: the iterative cycle of biochemical model—mathematical model—computational model, and integration of novel data and legacy knowledge to develop context-specific biochemical, mathematical, and computational models. Challenges in building biochemical models include the complexity of proteomic states and interactions, integration of diverse data to infer biochemical interactions, and the temporal state of biochemical models. Challenges in building mathematical models include incorporating statistical/probabilistic information into analytical models, using qualitative constraints in mathematical models, and incomplete knowledge and coarse-graining. Challenges in computational modeling include the absence of knowledge about model parameters such as rate constants, local versus global concentrations of species and multiple scales of distance and time, and variation among different cell types and subpopulation variability, or variability among biological repeats. Advanced research in coarse graining will pave the way for progress in the development of multiscale multidomain modeling that can connect fundamental research in network biology to clinical research.}
}
@article{AKTAYEVA2022285,
title = {Aesthetic education: the process of teaching mathematics with the open-source software},
journal = {Transportation Research Procedia},
volume = {63},
pages = {285-293},
year = {2022},
note = {X International Scientific Siberian Transport Forum — TransSiberia 2022},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2022.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S2352146522002708},
author = {Alena Aktayeva and Elena Zubareva and Aibek Dautov and Kymbat Saginbayeva and Rozamgul Niyazova and Sergey Khan and Aigerim Shonasheva},
keywords = {Aesthetic education, mathematical education, software, computer programs},
abstract = {In the article one of leading aims of educating mathematics is examined is aesthetic education of student facilities of mathematics. Presentation of aesthetic beauty at her decisions possibility of students is investigated, specifying them on the decision of one problem in several ways that assists the detailed consideration of idea of aesthetic education, through Open-source Software. The technical capabilities and elegant ease of use of systems Open-source Software provides a seamless, integrated and constantly expanding system that covers the breadth and depth of mathematical computing, and is available seamlessly through any web browser along with all modern systems used in the educational process. The article will describe understanding of beauty the decision of a problem; methods of decisions that are accompanied by the use make possible a uniquely flexible and convenient approach to charting and information visualization in a mathematical calculate. Such sort of activity assists aesthetic education, allowing to develop a culture and logical thinking, forming at students a different choice, grace of decision of problems.}
}
@article{IWASE20211,
title = {Towards a Noncompliant Pedagogy of the Image: Reading Negentropic Bifurcatory Potentials in Video Images},
journal = {Video Journal of Education and Pedagogy},
volume = {6},
number = {1},
pages = {1-27},
year = {2021},
issn = {2364-4583},
doi = {https://doi.org/10.1163/23644583-bja10020},
url = {https://www.sciencedirect.com/science/article/pii/S236445832300023X},
author = {Masayuki Iwase and Joff P. N. Bradley},
keywords = {urban film-making, critique, metamodelization, global mnemotechnical system, proletarianized knowledge, mnemonic control, artificial and living engines, machinic enslavement, negentropic bifurcation, Deleuze, Heidegger, Virilio, time-image, lectosign, spiritual automation, zooming-in/out, autistic milieus, diffractive becoming, radical pedagogy},
abstract = {The authors explore the noncompliant pedagogy of the image based on their video Autopoietic Veering: Schizo Socius of Tokyo and Vancouver (2021). It is not the kind of trendy modelized video abstract or kinetic presentation eagerly promoted by international publishers; it is a cross-cultural collaborative work intended to generate affirmative temporal ruptures of entropic habitual modes of seeing, memorizing, and thinking of human and nonhuman life in the cities of Tokyo (Japan) and Vancouver (Canada). The authors elucidate Stiegler’s (2015b) concept of a “global mnemotechnical system” that stores and produces human memories in vast digital archives and databases (tertiary retentions) through “mnemonic control” (Parisi & Goodman, 2011). The authors repurpose video images to interrupt and recontrol human perception and memories as “living engines” (Lazzarato, 2006). They foreground the philosophical work of Deleuze, Heidegger, and Virilio to rethink and revive the creative act of “critique” (Foucault, 1997) through “metamodelization” (Guattari, 1995; Manning, 2020); therefore, they plug these apparently incommensurable modes of thinking into their readings of the video’s images. They read the images as “time-images” and focus on their five dimensions that possibly activate “spiritual automation” (Deleuze, 1989), which they assess as “negentropic bifurcatory” potentials (Bradley & Kennedy, 2019).}
}
@article{HAWES201560,
title = {Effects of mental rotation training on children’s spatial and mathematics performance: A randomized controlled study},
journal = {Trends in Neuroscience and Education},
volume = {4},
number = {3},
pages = {60-68},
year = {2015},
issn = {2211-9493},
doi = {https://doi.org/10.1016/j.tine.2015.05.001},
url = {https://www.sciencedirect.com/science/article/pii/S2211949315000083},
author = {Zachary Hawes and Joan Moss and Beverly Caswell and Daniel Poliszczuk},
keywords = {Spatial thinking, Mental rotation, Spatial training, Computerized cognitive training, Mathematics education, STEM},
abstract = {The purpose of the current study was to (i) investigate the malleability of children’s spatial thinking, and (ii) the extent to which training-related gains in spatial thinking generalize to mathematics performance. Sixty-one 6- to 8-year-olds were randomly assigned to either computerized mental rotation training or literacy training. Training took place on iPad devices over a 6-week period as part of regular classroom activity. Results revealed that in comparison to the control group, children who received spatial training demonstrated significant gains on two measures of mental rotation and marginally significant improvements on an untrained mental transformation task; a finding that suggests that training may have had a general effect on children’s spatial ability. However, contrary to theoretical claims and prior empirical findings, there was no evidence that spatial training transferred to mathematics performance.}
}
@article{LOPEZORTEGA20133459,
title = {Computer-assisted creativity: Emulation of cognitive processes on a multi-agent system},
journal = {Expert Systems with Applications},
volume = {40},
number = {9},
pages = {3459-3470},
year = {2013},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2012.12.054},
url = {https://www.sciencedirect.com/science/article/pii/S095741741201295X},
author = {Omar López-Ortega},
keywords = {Computer-assisted creativity, Cognitive processes, Agent-oriented programming, Recursive systems},
abstract = {For creativity to be computed, it is paramount to understand the cognitive processes involved, which have been elucidated by either surveying creative people or discovering regions of the human brain that activate during creative endeavors. From this scattering, the author proposes a holistic framework to describe them and their interaction. Hence, creativity can be regarded as a meta process which coordinates autonomous cognitive processes such as planning or divergent thinking. To represent the interplay of cognitive processes around creativity, models are developed in the Agent Unified Modeling Language (AUML). Then, the execution of each process is delegated to autonomous agents and a global coordination protocol is devised. The implementation of the MAS is done on the JADE platform. Two modules of the resultant system are exemplified: opus planning and divergent exploration. The coordination protocol is also presented. The domain in which the software system is tested is the creation of musical pieces.}
}
@article{MUST20167,
title = {Predicting the Flynn Effect through word abstractness : Results from the National Intelligence Tests support Flynn's explanation},
journal = {Intelligence},
volume = {57},
pages = {7-14},
year = {2016},
issn = {0160-2896},
doi = {https://doi.org/10.1016/j.intell.2016.03.003},
url = {https://www.sciencedirect.com/science/article/pii/S0160289616300253},
author = {Olev Must and Aasa Must and Jaan Mikk},
keywords = {Flynn Effect, National Intelligence Tests, Abstract thinking, Guessing, Tork, Estonia},
abstract = {The current study investigates the Flynn Effect (FE) and its relation to abstract thinking ability. We compare two cohorts of Estonian students (1933/36, n=888; 2006, n=912) using the Concepts (Logical Selection) subtest of the Estonian adaptation of the National Intelligence Tests (NIT). The item presentation order of the subtest correlates with the abstractness of the words used in the items (r=.609) of the subtest. The different test results (right, wrong and missing answers) were analysed in order to make an estimate of the FE magnitude. The FE for abstract thinking ability of those samples was 1.06 Hedges' g (adjusted for guessing). The magnitude of the FE is dependent upon the degree of difficulty of the items (an item's difficulty is estimated by determining its abstractness and its familiarity to students). The more difficult part of the subtest (the second half) showed a FE=1.80 whereas the easier part (the first half) of the subtest showed a FE=.72. Word abstractness was a strong predictor of all the testing results in both cohorts (Beta=.700). The familiarity of words used in the test items has no correlation with the test results if word abstractness is controlled in both cohorts. Our findings support Flynn's explanation that the FE is primarily an indicator of the rise in abstract thinking ability.}
}
@article{LIU2024100012,
title = {Investigating coacervates as drug carriers using molecular dynamics},
journal = {Precision Medicine and Engineering},
volume = {1},
number = {2},
pages = {100012},
year = {2024},
issn = {2950-4821},
doi = {https://doi.org/10.1016/j.preme.2024.100012},
url = {https://www.sciencedirect.com/science/article/pii/S2950482124000123},
author = {Yang Liu and Rongrong Zou and Yiwei Wang and Minghao Wang and Fan Fan and Yeqiang Zhou and Huixu Xie and Mingming Ding},
keywords = {Drug delivery, Coacervate, Molecular dynamics, Machine learning, Protein encapsulation},
abstract = {Coacervates show promise in drug delivery systems due to their biocompatibility, versatility, and outstanding ability to penetrate cells. With the advent of all-atom (AA) and coarse-grained (CG) models, these computational tools function as ‘computational microscopes’, providing valuable insights to complement experimental research. This review covers the latest innovations in coacervate-based drug delivery systems. It summarizes the molecular properties and phase behavior of coacervates composed of polyelectrolytes, intrinsically disordered proteins (IDPs), and other biomolecules, along with their interactions with carried drugs and cell membranes. Additionally, this review highlights current challenges and limitations in this fast-moving field and proposes potential avenues for future research.}
}
@incollection{HERLIHY201469,
title = {Chapter 4 - Colorless Wait-Free Computation},
editor = {Maurice Herlihy and Dmitry Kozlov and Sergio Rajsbaum},
booktitle = {Distributed Computing Through Combinatorial Topology},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {69-95},
year = {2014},
isbn = {978-0-12-404578-1},
doi = {https://doi.org/10.1016/B978-0-12-404578-1.00004-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780124045781000048},
author = {Maurice Herlihy and Dmitry Kozlov and Sergio Rajsbaum},
keywords = {Configurations, Executions, Layered executions, Layered protocols, Processes, Protocols, Schedules, Tasks},
abstract = {We outline the basic connection between distributed computing and combinatorial topology in terms of two formal models: a conventional operational model, in which systems consist of communicating state machines whose behaviors unfold over time, and the combinatorial model, in which all possible behaviors are captured statically using topological notions. We start with one particular system model (shared memory) and focus on a restricted (but important) class of problems (so-called “colorless” tasks).}
}
@article{SHEFFIELD20221149,
title = {Belief Updating and Paranoia in Individuals With Schizophrenia},
journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
volume = {7},
number = {11},
pages = {1149-1157},
year = {2022},
issn = {2451-9022},
doi = {https://doi.org/10.1016/j.bpsc.2022.03.013},
url = {https://www.sciencedirect.com/science/article/pii/S2451902222000799},
author = {Julia M. Sheffield and Praveen Suthaharan and Pantelis Leptourgos and Philip R. Corlett},
keywords = {Belief updating, Computational psychiatry, Delusions, Paranoia, Volatility, Worry},
abstract = {Background
Persecutory delusions are among the most common delusions in schizophrenia and represent the extreme end of the paranoia continuum. Paranoia is accompanied by significant worry and distress. Identifying cognitive mechanisms underlying paranoia is critical for advancing treatment. We hypothesized that aberrant belief updating, which is related to paranoia in human and animal models, would also contribute to persecutory beliefs in individuals with schizophrenia.
Methods
Belief updating was assessed in 42 participants with schizophrenia and 44 healthy control participants using a 3-option probabilistic reversal learning task. Hierarchical Gaussian Filter was used to estimate computational parameters of belief updating. Paranoia was measured using the Positive and Negative Syndrome Scale and the revised Green et al. Paranoid Thoughts Scale. Unusual thought content was measured with the Psychosis Symptom Rating Scale and the Peters et al. Delusions Inventory. Worry was measured using the Dunn Worry Questionnaire.
Results
Paranoia was significantly associated with elevated win-switch rate and prior beliefs about volatility both in schizophrenia and across the whole sample. These relationships were specific to paranoia and did not extend to unusual thought content or measures of anxiety. We observed a significant indirect effect of paranoia on the relationship between prior beliefs about volatility and worry.
Conclusions
This work provides evidence that relationships between belief updating parameters and paranoia extend to schizophrenia, may be specific to persecutory beliefs, and contribute to theoretical models implicating worry in the maintenance of persecutory delusions.}
}
@article{RUNNELS201563,
title = {Capturing plasticity effects in overdriven shocks on the finite scale},
journal = {Mathematics and Computers in Simulation},
volume = {111},
pages = {63-79},
year = {2015},
issn = {0378-4754},
doi = {https://doi.org/10.1016/j.matcom.2014.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S0378475414003334},
author = {Scott R. Runnels},
keywords = {Shocks, Plasticity, Hardening, Hydrodynamics, Radial return},
abstract = {An ordinary differential equation (ODE) form of the radial return algorithm, which is essentially a Prandtl-Reuss material model, is combined with a strain-rate hardening model to produce an ODE that describes deviatoric stress through a prescribed density rise. An analytical solution is found to the resulting ODE for a specific choice of one of the hardening model’s parameters. That solution is used to prove that if the prescribed density rise is allowed to be infinitely thin, i.e., like a shock in the mathematical sense, the resulting deviatoric stress is still bounded. In other words, the singularity is integrable; integration of the radial return ODE regularizes the infinite strain rate and resulting yield stress in the presence of an ideal shock singularity. The analytical tools developed for this line of thinking are applied to study the variation of deviatoric stress through a nearly shock-like density rise using different density rise profiles, revealing the impact of the shape choice. The tools are also used to compute what rise times are needed to converge upon the correct value of deviatoric stress through a shock; the results indicate that most contemporary hydrocodes cannot be expected to achieve those rise times. A demonstration of connecting the analytical tools to a hydrocode, using surrogate numerical shock shapes, is provided thereby opening the door for using such surrogates to perform sub-grid computations of converged shock behavior for strain-rate hardening materials.}
}
@article{HU2024101646,
title = {A flexible BERT model enabling width- and depth-dynamic inference},
journal = {Computer Speech & Language},
volume = {87},
pages = {101646},
year = {2024},
issn = {0885-2308},
doi = {https://doi.org/10.1016/j.csl.2024.101646},
url = {https://www.sciencedirect.com/science/article/pii/S0885230824000299},
author = {Ting Hu and Christoph Meinel and Haojin Yang},
keywords = {Grafting, Dynamic inference, Large Language Models, Deep learning},
abstract = {Fine-tuning and inference on Large Language Models like BERT have become increasingly expensive regarding memory cost and computation resources. The recently proposed computation-flexible BERT models facilitate their deployment in varied computational environments. Training such flexible BERT models involves jointly optimizing multiple BERT subnets, which will unavoidably interfere with one another. Besides, the performance of large subnets is limited by the performance gap between the smallest subnet and the supernet, despite efforts to enhance the smaller subnets. In this regard, we propose layer-wise Neural grafting to boost BERT subnets, especially the larger ones. The proposed method improves the average performance of the subnets on six GLUE tasks and boosts the supernets on all GLUE tasks and the SQuAD data set. Based on the boosted subnets, we further build an inference framework enabling practical width- and depth-dynamic inference regarding different inputs by combining width-dynamic gating modules and early exit off-ramps in the depth dimension. Experimental results show that the proposed framework achieves a better dynamic inference range than other methods in terms of trading off performance and computational complexity on four GLUE tasks and SQuAD. In particular, our best-tradeoff inference result outperforms other fixed-size models with similar amount of computations. Compared to BERT-Base, the proposed inference framework yields a 1.3-point improvement in the average GLUE score and a 2.2-point increase in the F1 score on SQuAD, while reducing computations by around 45%.}
}
@incollection{KENDRICK2008685,
title = {Chapter 17 The Supporting Role of Molecular Modelling and Computational Chemistry in Polymer Analysis},
editor = {John M. Chalmers and Robert J. Meier},
series = {Comprehensive Analytical Chemistry},
publisher = {Elsevier},
volume = {53},
pages = {685-734},
year = {2008},
booktitle = {Molecular Characterization and Analysis of Polymers},
issn = {0166-526X},
doi = {https://doi.org/10.1016/S0166-526X(08)00417-0},
url = {https://www.sciencedirect.com/science/article/pii/S0166526X08004170},
author = {John Kendrick},
abstract = {Publisher Summary
Molecular modeling covers a wide range of techniques and the calculation of an even wider range of properties. Although for polymers, the possibility of treating a polymer chain quantum mechanically is formidable, it is clear that the modeling approach allows calculations on monomers, dimmers, and oligomers to guide the interpretation of many spectroscopic observations with great success. For those systems, where longer times scales and larger size scales are important, molecular mechanics and molecular dynamics methods are available, but the issue of the force field and the approximations that it introduces remain significant. The key to the change in attitude to modeling and its role have to lie in the availability of mature algorithms with well-known and well-understood properties. The density functional theory method in quantum mechanics has introduced a new era in applications of quantum mechanical methods.}
}
@article{CHENG2024104948,
title = {Exploring differences in self-regulated learning strategy use between high- and low-performing students in introductory programming: An analysis of eye-tracking and retrospective think-aloud data from program comprehension},
journal = {Computers & Education},
volume = {208},
pages = {104948},
year = {2024},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2023.104948},
url = {https://www.sciencedirect.com/science/article/pii/S0360131523002257},
author = {Gary Cheng and Di Zou and Haoran Xie and Fu Lee Wang},
keywords = {Introductory programming, Self-regulated learning strategies, Eye tracking, Retrospective think aloud, Higher education},
abstract = {Previous studies have reported mixed results regarding the relationship between students’ use of self-regulated learning (SRL) strategies and their performance in introductory programming courses. These studies were constrained by their reliance on self-report questionnaires as a means of collecting and analysing data. To address this limitation, this study aimed to employ eye-tracking and retrospective think-aloud techniques to identify differences in SRL strategy use for program comprehension tasks between high-performing students (N = 31) and low-performing students (N = 31) in an undergraduate programming course. All participants attended individual eye-tracking sessions to comprehend two Python program codes with different constructs. Their eye-tracking data and video-recalled retrospective think-aloud data were captured and recorded for analysis. The findings reveal that higher-order cognitive skills, such as elaboration and critical thinking, were mostly adopted by high-performing students, while basic cognitive and resource management strategy, such as rehearsal and help-seeking, were mostly employed by low-performing students when comprehending the program codes. This study not only demonstrates the design of combining eye-tracking and retrospective think-aloud data to explore students’ use of SRL strategies but also provides evidence to support the notion that program comprehension is a complex process that cannot be effectively addressed by employing merely rudimentary strategies, such as repetitively reading the same code segment. In the future, researchers could explore the possibility of using a webcam to monitor and assess students’ online programming processes and provide feedback based on their eye movements. They could also examine the effects of SRL strategies training on students’ motivation, engagement, and performance in various types of programming activities.}
}
@article{HOYTE2006S348,
title = {Computational model of levator ani muscle stretch during vaginal delivery},
journal = {Journal of Biomechanics},
volume = {39},
pages = {S348},
year = {2006},
note = {Abstracts of the 5th World Congress of Biomechanics},
issn = {0021-9290},
doi = {https://doi.org/10.1016/S0021-9290(06)84382-4},
url = {https://www.sciencedirect.com/science/article/pii/S0021929006843824},
author = {L. Hoyte and P. Krysl and G. Chukkapalli and A. Majumdar and D.J. Choi and A. Trivedi and S.K. Warfield and M.S. Damaser}
}
@article{JURISICA2024102006,
title = {Explainable biology for improved therapies in precision medicine: AI is not enough},
journal = {Best Practice & Research Clinical Rheumatology},
volume = {38},
number = {4},
pages = {102006},
year = {2024},
note = {Genetics of Rheumatic Diseases},
issn = {1521-6942},
doi = {https://doi.org/10.1016/j.berh.2024.102006},
url = {https://www.sciencedirect.com/science/article/pii/S1521694224000779},
author = {I Jurisica},
keywords = {Precision medicine, Rheumatoid arthritis, Artificial intelligence, Integrative computational biology},
abstract = {Technological advances and high-throughput bio-chemical assays are rapidly changing ways how we formulate and test biological hypotheses, and how we treat patients. Most complex diseases arise on a background of genetics, lifestyle and environment factors, and manifest themselves as a spectrum of symptoms. To fathom intricate biological processes and their changes from healthy to disease states, we need to systematically integrate and analyze multi-omics datasets, ontologies, and diverse annotations. Without proper management of such complex biological and clinical data, artificial intelligence (AI) algorithms alone cannot be effectively trained, validated, and successfully applied to provide trustworthy and patient-centric diagnosis, prognosis and treatment. Precision medicine requires to use multi-omics approaches effectively, and offers many opportunities for using AI, “big data” analytics, and integrative computational biology workflows. Advances in optical and biochemical assay technologies including sequencing, mass spectrometry and imaging modalities have transformed research by empowering us to simultaneously view all genes expressed, identify proteome-wide changes, and assess interacting partners of each individual protein within a dynamically changing biological system, at an individual cell level. While such views are already having an impact on our understanding of healthy and disease conditions, it remains challenging to extract useful information comprehensively and systematically from individual studies, ensure that signal is separated from noise, develop models, and provide hypotheses for further research. Data remain incomplete and are often poorly connected using fragmented biological networks. In addition, statistical and machine learning models are developed at a cohort level and often not validated at the individual patient level. Combining integrative computational biology and AI has the potential to improve understanding and treatment of diseases by identifying biomarkers and building explainable models characterizing individual patients. From systematic data analysis to more specific diagnostic, prognostic and predictive biomarkers, drug mechanism of action, and patient selection, such analyses influence multiple steps from prevention to disease characterization, and from prognosis to drug discovery. Data mining, machine learning, graph theory and advanced visualization may help identify diagnostic, prognostic and predictive biomarkers, and create causal models of disease. Intertwining computational prediction and modeling with biological experiments leads to faster, more biologically and clinically relevant discoveries. However, computational analysis results and models are going to be only as accurate and useful as correct and comprehensive are the networks, ontologies and datasets used to build them. High quality, curated data portals provide the necessary foundation for translational research. They help to identify better biomarkers, new drugs, precision treatments, and should lead to improved patient outcomes and their quality of life. Intertwining computational prediction and modeling with biological experiments, efficiently and effectively leads to more useful findings faster.}
}
@article{CRILLY2021309,
title = {The Evolution of “Co-evolution” (Part I): Problem Solving, Problem Finding, and Their Interaction in Design and Other Creative Practices},
journal = {She Ji: The Journal of Design, Economics, and Innovation},
volume = {7},
number = {3},
pages = {309-332},
year = {2021},
issn = {2405-8726},
doi = {https://doi.org/10.1016/j.sheji.2021.07.003},
url = {https://www.sciencedirect.com/science/article/pii/S2405872621000915},
author = {Nathan Crilly},
keywords = {Design process, Design thinking, Creativity, Design history, Interdisciplinarity},
abstract = {One of the most influential descriptions of design activity emphasizes how problems and solutions “co-evolve.” This concept has somehow escaped critical review and cross-disciplinary comparison, resulting in a fragmented approach to the subject. Reviewing the published literature on design co-evolution reveals that the term is used to refer to a range of distinct concepts, and the study of co-evolution has generated a number of elaborations and alternatives. Reviewing the broader literature in design and other disciplines further reveals that discussions of design co-evolution are disconnected from the history of relevant concepts in design research, and disconnected from a range of relevant concepts in other disciplines that describe creative work. Here I examine what the different concepts of design co-evolution are, how they have been modified and what they are related to. This leads to questioning the distinction between problems and solutions, defining them in relative terms, and drawing a connection between design co-evolution and design fixation.}
}
@article{GALTIER201683,
title = {Radiative transfer and spectroscopic databases: A line-sampling Monte Carlo approach},
journal = {Journal of Quantitative Spectroscopy and Radiative Transfer},
volume = {172},
pages = {83-97},
year = {2016},
note = {Eurotherm Conference No. 105: Computational Thermal Radiation in Participating Media V},
issn = {0022-4073},
doi = {https://doi.org/10.1016/j.jqsrt.2015.10.016},
url = {https://www.sciencedirect.com/science/article/pii/S0022407315003192},
author = {Mathieu Galtier and Stéphane Blanco and Jérémi Dauchet and Mouna {El Hafi} and Vincent Eymet and Richard Fournier and Maxime Roger and Christophe Spiesser and Guillaume Terrée},
keywords = {Radiative transfer, Monte Carlo method, Null-collision, Line sampling, Statistical approach, Spectroscopic databases},
abstract = {Dealing with molecular-state transitions for radiative transfer purposes involves two successive steps that both reach the complexity level at which physicists start thinking about statistical approaches: (1) constructing line-shaped absorption spectra as the result of very numerous state-transitions, (2) integrating over optical-path domains. For the first time, we show here how these steps can be addressed simultaneously using the null-collision concept. This opens the door to the design of Monte Carlo codes directly estimating radiative transfer observables from spectroscopic databases. The intermediate step of producing accurate high-resolution absorption spectra is no longer required. A Monte Carlo algorithm is proposed and applied to six one-dimensional test cases. It allows the computation of spectrally integrated intensities (over 25cm−1 bands or the full IR range) in a few seconds, regardless of the retained database and line model. But free parameters need to be selected and they impact the convergence. A first possible selection is provided in full detail. We observe that this selection is highly satisfactory for quite distinct atmospheric and combustion configurations, but a more systematic exploration is still in progress.}
}
@article{WANG20231225,
title = {Parameterization Design of 3D Fractal Images in Packaging Design Based on Genetic Algorithm},
journal = {Procedia Computer Science},
volume = {228},
pages = {1225-1232},
year = {2023},
note = {3rd International Conference on Machine Learning and Big Data Analytics for IoT Security and Privacy},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.11.104},
url = {https://www.sciencedirect.com/science/article/pii/S187705092301935X},
author = {Jinxia Wang},
keywords = {Genetic Algorithm, Packaging Design, 3D Fractal Image, NSGA - II},
abstract = {Packaging design, as an important element in product appearance, can directly affect customers' sensory perception of the product. Many universities even offer packaging design majors, which mainly use natural science and aesthetic knowledge to promote product sales. However, many old brands remain complacent and their packaging design still adopts traditional thinking, which to some extent affects their sales. Therefore, this article decided to use genetic algorithms as a tool to parameterize the 3D fractal images in packaging design, aiming to create more creative and eye-catching packaging designs. At the end of this article, an experiment was conducted on two branches of a certain brand. Branch 1 tried out the new design provided in this article, while Branch 2 continued to use the original design. After Branch 1 fully adopted the design, sales skyrocketed, from the original daily sales of 50-60 units to 70-85 units. Branch 2 remained unchanged, with a sharp contrast.}
}
@incollection{TIWARI2025389,
title = {Chapter 13 - Quantitative data analysis methods for air quality prediction},
editor = {Ranjeet S. Sokhi},
booktitle = {Air Quality},
publisher = {Elsevier},
pages = {389-409},
year = {2025},
isbn = {978-0-12-822591-2},
doi = {https://doi.org/10.1016/B978-0-12-822591-2.00013-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128225912000135},
author = {Pushp Raj Tiwari and Saurabh Kumar},
keywords = {Machine learning, data science, statistics, statistical applications, artificial intelligence},
abstract = {A quantitative research study gathers numerical data, which needs to be examined to make conclusions. Critical thinking regarding data analysis is facilitated primarily by quantitative data analysis. The purpose of data analysis is to identify the underlying trends, patterns, and connections in the context of a study. In quantitative data analysis, the focus is not just on applying statistical tests to existing data, but also on leveraging those tests as a means of deriving accurate insights from the data. This chapter aims to give a thorough theoretical review of current methodologies, related computational techniques, and dataset applications. A wide range of topics is covered, focusing on various statistical tests, interpolation, extrapolation, regression, modeling uncertainty analysis, machine learning methods, GIS techniques, data visualization tools etc. Each of these topics is extensively explored and discussed in terms of relevant literature and methods for air quality data and its applications.}
}
@article{CHISCI1995487,
title = {Fast Computation of Stabilizing Predictive Control Laws},
journal = {IFAC Proceedings Volumes},
volume = {28},
number = {5},
pages = {487-493},
year = {1995},
note = {3rd IFAC/IFIP Workshop on Algorithms and Architectures for Real-Time Control 1995 (AARTC'95), Ostend, Belgium, 31 May-2 June 1995},
issn = {1474-6670},
doi = {https://doi.org/10.1016/S1474-6670(17)47270-3},
url = {https://www.sciencedirect.com/science/article/pii/S1474667017472703},
author = {L. Chisci and A. Garulli and G. Zappa},
keywords = {Predictive control, linear quadratic regulators, control algorithms, fast parallel algorithms, fast Kalman algorithms, computational methods, adaptive control},
abstract = {A fast algorithm for Linear Quadratic(LQ) control with linear equality constraints is derived and exploited for stabilizing predictive control synthesis. The algorithm requires only O(Nn) computations for an nth order plant and N-steps prediction horizon, and possesses a remarkable numerical accuracy.}
}
@article{DENEF20071096,
title = {Computational complexity of the landscape: Part I},
journal = {Annals of Physics},
volume = {322},
number = {5},
pages = {1096-1142},
year = {2007},
issn = {0003-4916},
doi = {https://doi.org/10.1016/j.aop.2006.07.013},
url = {https://www.sciencedirect.com/science/article/pii/S0003491606001382},
author = {Frederik Denef and Michael R. Douglas},
abstract = {We study the computational complexity of the physical problem of finding vacua of string theory which agree with data, such as the cosmological constant, and show that such problems are typically NP hard. In particular, we prove that in the Bousso–Polchinski model, the problem is NP complete. We discuss the issues this raises and the possibility that, even if we were to find compelling evidence that some vacuum of string theory describes our universe, we might never be able to find that vacuum explicitly. In a companion paper, we apply this point of view to the question of how early cosmology might select a vacuum.}
}
@article{LEE2021100737,
title = {Turbulent boundary layer trailing-edge noise: Theory, computation, experiment, and application},
journal = {Progress in Aerospace Sciences},
volume = {126},
pages = {100737},
year = {2021},
issn = {0376-0421},
doi = {https://doi.org/10.1016/j.paerosci.2021.100737},
url = {https://www.sciencedirect.com/science/article/pii/S0376042121000427},
author = {Seongkyu Lee and Lorna Ayton and Franck Bertagnolio and Stephane Moreau and Tze Pei Chong and Phillip Joseph},
keywords = {Trailing-edge noise, Aeroacoustics, Turbulent boundary layer},
abstract = {When the pressure fluctuations caused by turbulence vorticity in the boundary layer are scattered by a sharp trailing edge, acoustic energy is generated and propagated to the far field. This trailing edge noise is emitted from aircraft wings, turbomachinery blades, wind turbine blades, helicopter blades, etc. Being dominant at high frequencies, this trailing-edge noise is a key element that annoys human hearing. This article covers virtually the entire landscape of modern research into trailing-edge noise including theoretical developments, numerical simulations, wind tunnel experiments, and applications of trailing-edge noise. The theoretical approach includes Green’s function formulations, Wiener–Hopf methods that solve the mixed boundary-value problem, Howe’s and Amiet’s models that relate the wall pressure spectrum to acoustic radiation. Recent analytical developments for poroelasticity and serrations are also included. We discuss a hierarchy of numerical approaches that range from semi-empirical schemes that estimate the wall pressure spectrum using mean-flow and turbulence statistics to high-fidelity unsteady flow simulations such as Large Eddy Simulation (LES) or Direct Numerical Simulation (DNS) that resolve the sound generation and scattering process based on the first-principles flow physics. Wind tunnel experimental research that provided benchmark data for numerical simulations and unravel flow physics is reviewed. In each theoretical, numerical, and experimental approach, noise control methods for mitigating trailing-edge noise are discussed. Finally, highlights of practical applications of trailing-edge noise prediction and reduction to wind turbine noise, fan noise, and rotorcraft noise are given. The current challenges in each approach are summarized with a look toward the future developments. The review could be useful as a primer for new researchers or as a reference point to the state of the art for experienced professionals.}
}
@article{AKCAOGLU201472,
title = {Cognitive outcomes from the Game-Design and Learning (GDL) after-school program},
journal = {Computers & Education},
volume = {75},
pages = {72-81},
year = {2014},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2014.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0360131514000372},
author = {Mete Akcaoglu and Matthew J. Koehler},
keywords = {Game-design, Problem-solving, Quasi-experimental, Constructionism},
abstract = {The Game-Design and Learning (GDL) initiative engages middle school students in the process of game-design in a variety of in-school, after-school, and summer camp settings. The goal of the GDL initiative is to leverage students' interests in games and design to foster their problem-solving and critical reasoning skills. The present study examines the effectiveness of an after-school version of the GDL program using a quasi-experimental design. Students enrolled in the GDL program were guided in the process of designing games aimed at solving problems. Compared to students in a control group who did not attend the program (n = 24), the children who attended the GDL program (n = 20) showed a significant increase in their problem-solving skills. The results provide empirical support for the hypothesis that participation in the GDL program leads to measurable cognitive changes in children's problem-solving skills. This study bears important implications for educators and theory.}
}
@article{CHU20181,
title = {Supporting scientific modeling through curriculum-based making in elementary school science classes},
journal = {International Journal of Child-Computer Interaction},
volume = {16},
pages = {1-8},
year = {2018},
issn = {2212-8689},
doi = {https://doi.org/10.1016/j.ijcci.2017.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S2212868917300545},
author = {Sharon Lynn Chu and Elizabeth Deuermeyer and Francis Quek},
keywords = {Making, Maker movement, Children, Science, Science models, Scientific modeling, Model thinking, Electronics, Programming},
abstract = {Our work investigates how Making may be used in the context of scientific modeling in formal elementary school science classes. This paper presents an investigation of fourth- and fifth-grade students engaging in Making activities to create simulation, concept-process, and illustrative models in the science classroom. Based on video analyses of the Making-based class sessions, a generalized process model was developed for each type of science model. In addition, cross-cutting themes were found in Making-based science modeling: first, there are two loops that intersect and interact with each other (modeling for Making and modeling for Science content), and they interrelate in various ways depending on science model type; and second, showcasing Making products (sharing with peers, teachers, or helpers) is a primary factor that determines students’ overall engagement with science in the activity. We suggest that Making-based science kit and lesson design needs to support students to showcase their Making output, on top of science-related reflections, and to consider the balance between Making and science activity. We conclude that Making has the potential to support the development of scientific model thinking in the elementary science classroom, but much further research is needed in this area.}
}
@article{ZHOU2025101900,
title = {Parking Vehicle-Assisted Task Offloading in Edge Computing: A dynamic multi-objective evolutionary algorithm with multi-strategy fusion response},
journal = {Swarm and Evolutionary Computation},
volume = {94},
pages = {101900},
year = {2025},
issn = {2210-6502},
doi = {https://doi.org/10.1016/j.swevo.2025.101900},
url = {https://www.sciencedirect.com/science/article/pii/S2210650225000586},
author = {Yingbo Zhou and Zheng-Yi Chai and Ya-Lun Li and Jun-Jie Li},
keywords = {Vehicle edge computing, Dynamic multi-objective optimization, Evolutionary algorithms, Computational offloading, Vehicle collaboration},
abstract = {Vehicle-edge computing, as a promising paradigm, is employed to support applications that require low latency and high computational capability. In this study, we consider the idle resources of the surrounding parked vehicles (PVs) and roadside units (RSUs) as service providers to enhance the performance of User Equipment (UE). We propose a joint offloading architecture that uses parked vehicles. Additionally, owing to the dynamic and uncertain nature of the environment, we model computation offloading as a dynamic multi-objective optimization problem to simultaneously optimize the latency and energy consumption of UE applications. In this study, we propose a dynamic multi-objective evolutionary algorithm with a multi-strategy fusion response (DMOEA/D-MSFR). Specifically, we introduce a population center positioning strategy and a learnable prediction mechanism using Long Short-Term Memory (LSTM) in DMOEA-MSFR, which divides the prediction optimization process into two stages and exhibits a rapid response to environmental changes. In the static optimization phase, an adaptive weight vector adjustment strategy is employed, which significantly aids in the distribution and diversity of the solutions. Comprehensive experiments demonstrate that our proposed framework balances the trade-off between latency and energy consumption, and the convergence, feasibility, and diversity of the non-dominated solutions obtained.}
}
@article{ZILLES20101072,
title = {The computational complexity of avoiding spurious states in state space abstraction},
journal = {Artificial Intelligence},
volume = {174},
number = {14},
pages = {1072-1092},
year = {2010},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2010.06.002},
url = {https://www.sciencedirect.com/science/article/pii/S0004370210000950},
author = {Sandra Zilles and Robert C. Holte},
keywords = {Abstraction, Heuristic search, Planning},
abstract = {Abstraction is a powerful technique for speeding up planning and search. A problem that can arise in using abstraction is the generation of abstract states, called spurious states, from which the goal state is reachable in the abstract space but for which there is no corresponding state in the original space from which the goal state can be reached. Spurious states can be harmful, in practice, because they can create artificial shortcuts in the abstract space that slow down planning and search, and they can greatly increase the memory needed to store heuristic information derived from the abstract space (e.g., pattern databases). This paper analyzes the computational complexity of creating abstractions that do not contain spurious states. We define a property—the downward path preserving property (DPP)—that formally captures the notion that an abstraction does not result in spurious states. We then analyze the computational complexity of (i) testing the downward path preserving property for a given state space and abstraction and of (ii) determining whether this property is achievable at all for a given state space. The strong hardness results shown carry over to typical description languages for planning problems, including sas+ and propositional strips. On the positive side, we identify and illustrate formal conditions under which finding downward path preserving abstractions is provably tractable.}
}
@article{SELBY20001491,
title = {Computational Aspects of Complex Securities},
journal = {Journal of Economic Dynamics and Control},
volume = {24},
number = {11},
pages = {1491-1497},
year = {2000},
issn = {0165-1889},
doi = {https://doi.org/10.1016/S0165-1889(99)00084-6},
url = {https://www.sciencedirect.com/science/article/pii/S0165188999000846},
author = {Michaël J.P Selby}
}
@article{CREELY2025101727,
title = {Creative partnerships with generative AI. Possibilities for education and beyond},
journal = {Thinking Skills and Creativity},
volume = {56},
pages = {101727},
year = {2025},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2024.101727},
url = {https://www.sciencedirect.com/science/article/pii/S1871187124002682},
author = {Edwin Creely and Jo Blannin},
keywords = {Generative AI, Creative production, Posthumanism, Education, Autoethnography, Alterity relations},
abstract = {The impact of generative artificial intelligence (AI) on creative production in industry and education is just beginning to be experienced and understood. This impact is likely to accelerate and become even more significant as the computational potential of generative AI grows through training on more diverse and more extensive language models and data sets. Emerging research in this new field suggests that previous models of understanding the interactions between machine and human may no longer be sufficient in a world of generative AI. The significant question is how emerging generative AI technologies will relate to and be a part of human creativity and creative outputs. In this article, we adopt a posthuman stance and conceive of creative output involving generative AI and humans in terms of a yet-to-be-fully-realised and emergent relationship that will likely become more integrated and complex. To investigate and experiment with this relational notion, each of us (as part of an autoethnographic approach) developed a creative output using ChatGPT: a poem and a multimodal narrative. We then employed the idea of alterity relations from the American philosopher of technology, Don Ihde, to conceive of the possibilities and limitations in working relationally and productively with generative AI. As two academics working in teacher education, we applied our learning from this exploration to possibilities in educational contexts. In this article, we offer several important implications and provocations for practitioners, researchers, educators and policymakers, not only in terms of practical concerns but also for rethinking the nature of the creative output.}
}
@article{ATALLAH2022B2,
title = {Society for Maternal-Fetal Medicine Special Statement: Cognitive bias and medical error in obstetrics—challenges and opportunities},
journal = {American Journal of Obstetrics and Gynecology},
volume = {227},
number = {2},
pages = {B2-B10},
year = {2022},
issn = {0002-9378},
doi = {https://doi.org/10.1016/j.ajog.2022.04.033},
url = {https://www.sciencedirect.com/science/article/pii/S0002937822003143},
author = {Fouad Atallah and Rebecca F. Hamm and Christina M. Davidson and C. Andrew Combs},
keywords = {decision-making, diagnostic error, disparities, implicit bias, inequity, medical error, racism},
abstract = {The processes of diagnosis and management involve clinical decision-making. However, decision-making is often affected by cognitive biases that can lead to medical errors. This statement presents a framework of clinical thinking and decision-making and shows how these processes can be bias-prone. We review examples of cognitive bias in obstetrics and introduce debiasing tools and strategies. When an adverse event or near miss is reviewed, the concept of a cognitive autopsy—a root cause analysis of medical decision-making and the potential influence of cognitive biases—is promoted as part of the review process. Finally, areas for future research on cognitive bias in obstetrics are suggested.}
}
@article{KUMAR20101805,
title = {A generalized computational approach to stability of static equilibria of nonlinearly elastic rods in the presence of constraints},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {199},
number = {25},
pages = {1805-1815},
year = {2010},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2010.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0045782510000654},
author = {Ajeet Kumar and Timothy J. Healey},
keywords = {Stability, Elasticity, Rods, Constraints},
abstract = {We present a generalized approach to stability of static equilibria of nonlinearly elastic rods, subjected to general loading, boundary conditions and constraints (of both point-wise and integral type), based upon the linearized dynamics stability criterion. Discretization of the governing equations leads to a non-standard (singular) generalized eigenvalue problem. A new efficient sparse-matrix-friendly algorithm is presented to determine its few left-most eigenvalues, which, in turn, yield stability/instability information. For conservative problems, the eigenvalue problem arising from the linearized dynamics stability criterion is also shown to be equivalent to that arising in the determination of constrained local minima of the potential energy. We illustrate the method with several examples. A novel variational formulation for extensible and unshearable rods is also proposed within the context of one of the example problems.}
}
@article{BENSASSI20233123,
title = {Fuzzy knowledge based assessment system for K-12 Scientific Reasoning Competencies},
journal = {Procedia Computer Science},
volume = {225},
pages = {3123-3132},
year = {2023},
note = {27th International Conference on Knowledge Based and Intelligent Information and Engineering Sytems (KES 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.10.306},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923014643},
author = {Manel BenSassi and Henda Ben Ghezala},
keywords = {Learning analytics, educational recommendations, fuzzy competencies assessment, knowledge representation, fuzzy ontology, Scientific Reasoning competencies},
abstract = {Developing Scientific Reasoning (SR) competencies at an early age, are challenging to meet expectation of the 4th sustainable development goal. Hence, educators and educational decision-makers try to embed these competencies into such subjects as the arts, language, technology, economics, mathematics and science, using an inter-disciplinary approach. In this context, this paper proposes a fuzzy knowledge-based solution to build practical pupils, educators, and decision-makers recommender system to support the development of SR competencies in a data driver manner. Our system consists of:(1) inferring and computational module that calculates in a fuzzy manner the global appreciation to each SR-competencies. (2) recommendation module that aims to help learners, educators and decision makers to assess the degree of development of SR competencies and to get alternative suggestion of remediation. The proposed solution has been tested on the last two levels of science education in four Tunisian elementary schools in different regions. A preliminary analysis showed that the learning process should be more focused on Tunisian pupil's profile, and that investigation and collaborative based learning should be applied further in Tunisian classroom.}
}
@article{NOROOZI2019295,
title = {Multidisciplinary innovations and technologies for facilitation of self-regulated learning},
journal = {Computers in Human Behavior},
volume = {100},
pages = {295-297},
year = {2019},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2019.07.020},
url = {https://www.sciencedirect.com/science/article/pii/S0747563219302638},
author = {Omid Noroozi and Sanna Järvelä and Paul A. Kirschner},
abstract = {Technology-enhanced learning environments provide ample opportunities for learners to self-regulate their learning processes and activities for achieving the intended learning outcomes in various disciplines from soft to hard sciences and from humanities to the natural and social sciences. This special issue discusses the emerging technological advancements and cutting-edge research on self-regulated learning dealing with different cognitive, motivational, emotional, and social processes of learning both at the individual and group levels. Specifically, it discusses how to optimally use advanced technologies to facilitate learners’ self-regulated learning for achieving their own individual learning needs and goals. In this special issue, seven researchers/research teams from the fields of collaborative learning, computational thinking, educational psychology, and learning analytics presented contributions to self-regulated learning with the goal of stimulating cross-border discussion in the field.}
}
@article{GALLISTEL201266,
title = {Extinction from a rationalist perspective},
journal = {Behavioural Processes},
volume = {90},
number = {1},
pages = {66-80},
year = {2012},
note = {Society for the Quantitative Analyses of Behavior: Extinction},
issn = {0376-6357},
doi = {https://doi.org/10.1016/j.beproc.2012.02.008},
url = {https://www.sciencedirect.com/science/article/pii/S0376635712000447},
author = {C.R. Gallistel},
keywords = {Acquisition, Extinction, Partial reinforcement, Spontaneous recovery, Renewal, Reinstatement, Resurgence, Information theory, Bayesian inference},
abstract = {The merging of the computational theory of mind and evolutionary thinking leads to a kind of rationalism, in which enduring truths about the world have become implicit in the computations that enable the brain to cope with the experienced world. The dead reckoning computation, for example, is implemented within the brains of animals as one of the mechanisms that enables them to learn where they are (Gallistel, 1990, Gallistel, 1995). It integrates a velocity signal with respect to a time signal. Thus, the manner in which position and velocity relate to one another in the world is reflected in the manner in which signals representing those variables are processed in the brain. I use principles of information theory and Bayesian inference to derive from other simple principles explanations for: (1) the failure of partial reinforcement to increase reinforcements to acquisition; (2) the partial reinforcement extinction effect; (3) spontaneous recovery; (4) renewal; (5) reinstatement; (6) resurgence (aka facilitated reacquisition). Like the principle underlying dead-reckoning, these principles are grounded in analytic considerations. They are the kind of enduring truths about the world that are likely to have shaped the brain's computations.}
}
@article{BAKEEVA2022676,
title = {Increasing the student talking time parameter under the digitalization in transport engineering learning},
journal = {Transportation Research Procedia},
volume = {63},
pages = {676-685},
year = {2022},
note = {X International Scientific Siberian Transport Forum — TransSiberia 2022},
issn = {2352-1465},
doi = {https://doi.org/10.1016/j.trpro.2022.06.062},
url = {https://www.sciencedirect.com/science/article/pii/S2352146522003179},
author = {L Bakeeva and L Brylevskaya and L Gonchar and E Pastukhova and Y Romanova and O Skepko},
keywords = {Digitalization of education, transport engineering, student talking time, study-train-explain methodology},
abstract = {The most recent information technologies have become an integral part of modern life. As the study shows, along with the obvious benefits, their use can lead to negative consequences, namely the loss of communication and soft skills, changes in the ability to absorb information, decreased motivation to acquire new knowledge among the younger age group. The authors propose a new methodology for organizing the educational process Study-Train-Explain. The aim of the method is to increase the Student Talking Time parameter to develop the skills of mathematical data analysis, systematic and analytical thinking to master the methods of description and construction of mathematical model of the phenomenon or process. These competencies are extremely in demand in the professional field related to the organization of transportation and operation of transport-technological machines and complexes under the conditions of digitalization of global processes. The article presents an algorithm and the results of the experimental training process carried out by the authors according to the specified methodology.}
}
@article{NAJJAR19991907,
title = {Advances in the dataflow computational model},
journal = {Parallel Computing},
volume = {25},
number = {13},
pages = {1907-1929},
year = {1999},
issn = {0167-8191},
doi = {https://doi.org/10.1016/S0167-8191(99)00070-8},
url = {https://www.sciencedirect.com/science/article/pii/S0167819199000708},
author = {Walid A Najjar and Edward A Lee and Guang R Gao},
keywords = {Computational models, Dataflow, Multithreaded computer architecture, von Neumann computer, Dataflow history, Memory models},
abstract = {The dataflow program graph execution model, or dataflow for short, is an alternative to the stored-program (von Neumann) execution model. Because it relies on a graph representation of programs, the strengths of the dataflow model are very much the complements of those of the stored-program one. In the last thirty or so years since it was proposed, the dataflow model of computation has been used and developed in very many areas of computing research: from programming languages to processor design, and from signal processing to reconfigurable computing. This paper is a review of the current state-of-the-art in the applications of the dataflow model of computation. It focuses on three areas: multithreaded computing, signal processing and reconfigurable computing.}
}
@article{AGRE19951,
title = {Computational research on interaction and agency},
journal = {Artificial Intelligence},
volume = {72},
number = {1},
pages = {1-52},
year = {1995},
issn = {0004-3702},
doi = {https://doi.org/10.1016/0004-3702(94)00054-5},
url = {https://www.sciencedirect.com/science/article/pii/0004370294000545},
author = {Philip E. Agre},
abstract = {Recent research in artificial intelligence has developed computational theories of agents' involvements in their environments. Although inspired by a great diversity of formalisms and architectures, these research projects are unified by a common concern: using principled characterizations of agents' interactions with their environments to guide analysis of living agents and design of artificial ones. This article offers a conceptual framework for such theories, surveys several other fields of research that hold the potential for dialogue with these new computational projects, and summarizes the principal contributions of the articles in this special double volume. It also briefly describes a case study in these ideas—a computer program called Toast that acts as a short-order breakfast cook. Because its designers have discovered useful structures in the world it inhabits, Toast can employ an extremely simple mechanism to decide what to do next.}
}
@incollection{BACSKAY2025,
title = {Fifty-five years of quantum chemistry},
series = {Advances in Quantum Chemistry},
publisher = {Academic Press},
year = {2025},
issn = {0065-3276},
doi = {https://doi.org/10.1016/bs.aiq.2025.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0065327625000036},
author = {György B. Bácskay},
keywords = {Configuration interaction (CI), Direct CI, Coupled cluster theory, Quadratically convergent- self-consistent field (QC-SCF) method, Molecular properties, Thermal decomposition, Chemical bonding, Thermochemistry},
abstract = {Started doing Quantum Chemistry as a PhD student with Linnett in 1968 at Cambridge: configuration interaction (CI) calculations on small hydrogenic molecules with a mixed basis of Slater and Gaussian functions. Joined Hush at the University of Sydney in 1972 and worked with a number of graduate students on Coupled Cluster Theory, direct CI, the calculation of ionization energies, the computation of molecular properties and vibrational energies, studies of hydrogen bonded clusters and quadratically convergent Hartree-Fock theory. With Nordholm, worked on the implementation of Absorbing Boundary and Generalized Finite Element methods. As computational chemist, during the nineties and beyond, studied dihydrogen complexes of osmium and the catalytic properties of cis-platins, the potential energy surfaces relevant to thermal decomposition of small molecules, properties of halocarbenes, spectroscopy of C2, thermochemistry and covalent bonding. Retired in 2005. Research-active as Honorary Reader.}
}
@article{ONGUR2025220,
title = {Embracing complexity in psychiatry—from reductionistic to systems approaches},
journal = {The Lancet Psychiatry},
volume = {12},
number = {3},
pages = {220-227},
year = {2025},
issn = {2215-0366},
doi = {https://doi.org/10.1016/S2215-0366(24)00334-1},
url = {https://www.sciencedirect.com/science/article/pii/S2215036624003341},
author = {Dost Öngür and Martin P Paulus},
abstract = {Summary
The understanding and treatment of psychiatric disorders present unique challenges due to these conditions' multifaceted nature, comprising dynamic interactions between biological, psychological, social, and environmental factors. Traditional reductionistic approaches often simplify these conditions into linear cause-and-effect relationships, overlooking the complexity and interconnectedness inherent in psychiatric disorders. Advances in complex systems approaches provide a comprehensive framework to capture and quantify the non-linear and emergent properties of psychiatric disorders. This Personal View emphasises the importance of identifying rules for generative models that govern brain and behaviour over time, which might contribute to personalised assessments and interventions for psychiatric disorders. For instance, mood fluctuations in bipolar disorder can be understood through dynamical systems modelling, which identifies modifiable parameters, such as circadian disruption, that can be addressed through targeted therapies such as light therapy. Similarly, recognition of depression as an emergent property arising from complex interactions highlights the need for integrated treatment strategies that enhance adaptive reactions in the individual. A framework for quantifying multilevel interactions and network dynamics can help researchers and clinicians to understand the interplay between neural circuits, behaviours, and social contexts. Probabilistic models and self-organisation concepts contribute to building concrete dynamical systems models of mental disorders, facilitating early identification of risk states and promoting resilience through adaptive interventions delivered with optimal timing. Embracing these complex systems approaches in psychiatry could capture the true nature of psychiatric disorders as properties of a dynamic complex system and not the manifestation of any lesion or insult. This line of thinking might improve diagnosis and treatment, offering new hope for individuals affected by psychiatric conditions and paving the way for more effective, personalised mental health care.}
}
@incollection{BALAKRISHNAN202131,
title = {Chapter 2 - Computational intelligence in healthcare and biosignal processing},
editor = {Janmenjoy Nayak and Bighnaraj Naik and Danilo Pelusi and Asit Kumar Das},
booktitle = {Handbook of Computational Intelligence in Biomedical Engineering and Healthcare},
publisher = {Academic Press},
pages = {31-64},
year = {2021},
isbn = {978-0-12-822260-7},
doi = {https://doi.org/10.1016/B978-0-12-822260-7.00015-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128222607000157},
author = {Nagaraj Balakrishnan and Valentina E. Balas and Arunkumar Rajendran},
keywords = {ANN, Clustering, Data classification, Data mining, Deep clustering networks, Deep learning},
abstract = {In this new era, technological advancement toward the mission of a better tomorrow is reaching its limit because the exploration of the advanced possibilities of Artificial Intelligence is bounded with certain limitations. The application of analyzing various features of biosignal processing is key in the fields of medicine and healthcare. Biosignals such as Electroencephalogram (EEG), Electrocardiogram (ECG), Electromyography (EMG), Electrooculography (EOG), Galvanic Skin Response(GSR), and Magnetoencephalography (MEG) is already giving deep insight into the human body toward the identification of diverse nature and disorders. In recent years, the research toward analyzing biosignal gained interest among many researchers. The primary limitation for the algorithms to analyze these signals for more possibilities of insight is its uncertainty. Even though the algorithms of Artificial Intelligence have the capabilities to unravel the mysteries, it is bounded with specific difficulties. The machine learning algorithms designed to manage uncertain data but lacks accuracy due to many factors. Also, complete supervision is needed in a training process that involves the extraction and selection of adequate features for the training. The deep learning method (a subset of machine learning) comes into the picture due to one of these facts. This, indeed, as a supervised learning method, needs a massive volume of data to train to reach the accuracy goal. The deep learning algorithm plays a significant role in today's Artificial Intelligence–based applications. However, this platform needs many requirements, such as (a) high computational power like graphical processing units (GPU); (b) similar to machine learning methods, a massive labeled dataset for supervised learning; (c) adequate parameter selection to avoid overfitting or underfitting. To overcome the problems highlighted, the strategy of adopting the behaviors of unsupervised learning (performed by the clustering algorithm) in the deep learning methodology is needed. To achieve the goal, two-phase operations were processed, such as (1) transformation of the data elements into a latent feature space (Z) is processed through a nonlinear mapping of deep learning networks; (2) clustering the latent feature space to k-clusters, and simultaneously, the clustering loss is fed to the deep learning network for the next iteration of operation concerning the objective function convergence analyzed by the Kullback–Leibler divergence. Various strategies of enhancing the nature of deep learning methods and clustering methodologies for an unsupervised learning process are addressed in this chapter.}
}
@incollection{GARDNER202477,
title = {Chapter 4 - Smart design for urban activation and placemaking},
editor = {Nicole Gardner},
booktitle = {Scaling the Smart City},
publisher = {Elsevier},
pages = {77-102},
year = {2024},
series = {Smart Cities},
isbn = {978-0-443-18452-9},
doi = {https://doi.org/10.1016/B978-0-443-18452-9.00008-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780443184529000082},
author = {Nicole Gardner},
keywords = {Cyber-physical systems, Design, Digital placemaking, Interaction, Placemaking, Smart design, Urban activation, Urban amenitization},
abstract = {This chapter charts out the distinction between the concepts of digital placemaking and smart placemaking. It examines existing and speculative urban technology projects that combine spatial design thinking and physical computing to address placemaking design goals such as urban activation, amenitization, and safety and security. In ways different from conventional smart city initiatives that surveil urban dynamics en masse to imperceptibly calibrate large-scale urban services and infrastructural systems, the projects discussed in this chapter engage sensor-based technologies to create localized, responsive, and interactive urban environments to address the social, cultural, and aesthetic dimensions of urban livability.}
}
@article{BERGER2016337,
title = {Cognitive hierarchies in the minimizer game},
journal = {Journal of Economic Behavior & Organization},
volume = {130},
pages = {337-348},
year = {2016},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2016.08.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167268116301639},
author = {Ulrich Berger and Hannelore {De Silva} and Gerlinde Fellner-Röhling},
keywords = {Behavioral game theory, Experimental games, Poisson cognitive hierarchy, Level- model, Minimizer game},
abstract = {Experimental tests of choice predictions in one-shot games show only little support for Nash equilibrium (NE). Poisson Cognitive Hierarchy (PCH) and level-k (LK) are behavioral models of the thinking-steps variety where subjects differ in the number of levels of iterated reasoning they perform. Camerer et al. (2004) claim that substituting the Poisson parameter τ=1.5 yields a parameter-free PCH model (pfPCH) which predicts experimental data considerably better than NE. We design a new multi-person game, the Minimizer Game, as a testbed to compare initial choice predictions of NE, pfPCH and LK. Data obtained from two large-scale online experiments strongly reject NE and LK, but are well in line with the point-prediction of pfPCH.}
}
@article{FOLTZ2023127,
title = {Reflections on the nature of measurement in language-based automated assessments of patients' mental state and cognitive function},
journal = {Schizophrenia Research},
volume = {259},
pages = {127-139},
year = {2023},
note = {Language and Speech Analysis in Schizophrenia and Related Psychoses},
issn = {0920-9964},
doi = {https://doi.org/10.1016/j.schres.2022.07.011},
url = {https://www.sciencedirect.com/science/article/pii/S0920996422002833},
author = {Peter W. Foltz and Chelsea Chandler and Catherine Diaz-Asper and Alex S. Cohen and Zachary Rodriguez and Terje B. Holmlund and Brita Elvevåg},
keywords = {Natural language processing, Speech technologies, Artificial intelligence},
abstract = {Modern advances in computational language processing methods have enabled new approaches to the measurement of mental processes. However, the field has primarily focused on model accuracy in predicting performance on a task or a diagnostic category. Instead the field should be more focused on determining which computational analyses align best with the targeted neurocognitive/psychological functions that we want to assess. In this paper we reflect on two decades of experience with the application of language-based assessment to patients' mental state and cognitive function by addressing the questions of what we are measuring, how it should be measured and why we are measuring the phenomena. We address the questions by advocating for a principled framework for aligning computational models to the constructs being assessed and the tasks being used, as well as defining how those constructs relate to patient clinical states. We further examine the assumptions that go into the computational models and the effects that model design decisions may have on the accuracy, bias and generalizability of models for assessing clinical states. Finally, we describe how this principled approach can further the goal of transitioning language-based computational assessments to part of clinical practice while gaining the trust of critical stakeholders.}
}
@article{VEITAS201716,
title = {Living Cognitive Society: A ‘digital’ World of Views},
journal = {Technological Forecasting and Social Change},
volume = {114},
pages = {16-26},
year = {2017},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2016.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0040162516300610},
author = {Viktoras Veitas and David Weinbaum},
keywords = {Cognitive system, Living society, Information and communication technologies, Future social governance, Individuation, Cognitive development},
abstract = {The current social reality is characterized by all-encompassing change, which disrupts existing social structures at all levels. Yet the approach based on the ontological primacy of stable and often hierarchical structures is still prevalent in theoretical and, most importantly, practical thinking about social systems. We propose a conceptual framework for thinking about a dynamically changing social system: the Living Cognitive Society. Importantly, we show how it follows from a much broader philosophical framework, guided by the theory of individuation, which emphasizes the importance of relationships and interactive processes in the evolution of a system. The framework addresses society as a living cognitive system – an ecology of interacting social subsystems – each of which is also a living cognitive system. We argue that this approach can help us to conceive sustainable social systems that will thrive in the circumstances of accelerating change. The Living Cognitive Society is explained in terms of its fluid structure, dynamics and the mechanisms at work. We then discuss the disruptive effects of Information and Communication Technologies on the mechanisms at work. We conclude by delineating a major topic for future research – distributed social governance – which focuses on processes of coordination rather than on stable structures within global society.}
}
@article{ZHU2024115675,
title = {Identifying influential nodes in social networks via improved Laplacian centrality},
journal = {Chaos, Solitons & Fractals},
volume = {189},
pages = {115675},
year = {2024},
issn = {0960-0779},
doi = {https://doi.org/10.1016/j.chaos.2024.115675},
url = {https://www.sciencedirect.com/science/article/pii/S096007792401227X},
author = {Xiaoyu Zhu and Rongxia Hao},
keywords = {Social network, Influential nodes, Centrality measure, Improved Laplacian centrality},
abstract = {Identifying influential nodes in social networks has significant applications in terms of social analysis and information dissemination. How to capture the crucial features of influential nodes without increasing the computational complexity is an urgent issue to be solved in the context of big data. Laplacian centrality (LC) measures nodal influence by computing nodes' degree, making it extremely low complexity. However, there is still significant room for improvement. Consequently, we propose the improved Laplacian centrality (ILC) to identify influential nodes based on the concept of self-consistent. Identifying results on 9 real networks prove that ILC is superior to LC and other 6 classical measures in terms of ranking accuracy, top-k nodes identification and discrimination capability. Moreover, the computational complexity of ILC has not significantly increased compared to LC, and remains the linear order of magnitude O(m). Additionally, ILC has excellent robustness and universality such that there is no need to adjust parameters according to different network structures.}
}
@article{KUHN202428,
title = {A landscape of consciousness: Toward a taxonomy of explanations and implications},
journal = {Progress in Biophysics and Molecular Biology},
volume = {190},
pages = {28-169},
year = {2024},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2023.12.003},
url = {https://www.sciencedirect.com/science/article/pii/S0079610723001128},
author = {Robert Lawrence Kuhn},
keywords = {Consciousness, Mind-body problem, Materialism, Monism, Dualism, Idealism},
abstract = {Diverse explanations or theories of consciousness are arrayed on a roughly physicalist-to-nonphysicalist landscape of essences and mechanisms. Categories: Materialism Theories (philosophical, neurobiological, electromagnetic field, computational and informational, homeostatic and affective, embodied and enactive, relational, representational, language, phylogenetic evolution); Non-Reductive Physicalism; Quantum Theories; Integrated Information Theory; Panpsychisms; Monisms; Dualisms; Idealisms; Anomalous and Altered States Theories; Challenge Theories. There are many subcategories, especially for Materialism Theories. Each explanation is self-described by its adherents, critique is minimal and only for clarification, and there is no attempt to adjudicate among theories. The implications of consciousness explanations or theories are assessed with respect to four questions: meaning/purpose/value (if any); AI consciousness; virtual immortality; and survival beyond death. A Landscape of Consciousness, I suggest, offers perspective.}
}
@article{ALIABADI2000243,
title = {Stabilized-finite-element/interface-capturing technique for parallel computation of unsteady flows with interfaces},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {190},
number = {3},
pages = {243-261},
year = {2000},
issn = {0045-7825},
doi = {https://doi.org/10.1016/S0045-7825(00)00200-0},
url = {https://www.sciencedirect.com/science/article/pii/S0045782500002000},
author = {Shahrouz Aliabadi and Tayfun E. Tezduyar},
abstract = {We present the stabilized-finite-element/interface-capturing (SFE/IC) method developed for parallel computation of unsteady flow problems with two-fluid interfaces and free surfaces. The SFE/IC method involves stabilized formulations, an interface-sharpening technique, and the enforcement of global mass conservation for each fluid. The SFE/IC method has been efficiently implemented on the CRAY T3E parallel supercomputer. A number of 2D test problems are presented to demonstrate how the SFE/IC method works and the accuracy it attains. We also show how the SFE/IC method can be very effectively applied to 3D simulation of challenging flow problems, such as two-fluid interfaces in a centrifuge tube and operational stability of a partially filled tanker truck driving over a bump.}
}
@article{HASHIMZADE2025100310,
title = {Integrating Programming into the Modern Undergraduate Economics Curriculum},
journal = {International Review of Economics Education},
pages = {100310},
year = {2025},
issn = {1477-3880},
doi = {https://doi.org/10.1016/j.iree.2025.100310},
url = {https://www.sciencedirect.com/science/article/pii/S1477388025000027},
author = {Nigar Hashimzade and Oleg Kirsanov and Tatiana Kirsanova},
abstract = {The increasing demand for computational skills in economics necessitates the integration of programming into undergraduate economics curricula in the UK. This paper argues for a systematic incorporation of programming courses tailored to economics students, addressing the limitations of current approaches and highlighting the benefits of such integration. We propose a sequence of introductory and intermediate-level integrated courses, and argue that this curriculum change will enhance students’ understanding of economic concepts, improve their employment prospects, and better prepare them for postgraduate studies. This paper aims to initiate a discussion and exchange of ideas and experiences on this subject at the national level.}
}
@article{WANG2016357,
title = {Research on Application of Abduction to Fire Investigation},
journal = {Procedia Engineering},
volume = {135},
pages = {357-362},
year = {2016},
note = {2015 International Conference on Performance-based Fire and Fire Protection Engineering (ICPFFPE 2015)},
issn = {1877-7058},
doi = {https://doi.org/10.1016/j.proeng.2016.01.142},
url = {https://www.sciencedirect.com/science/article/pii/S1877705816001466},
author = {Shi Wang and Zhong-jun Shu},
keywords = {Fire investigation, Abduction, Logical thinking},
abstract = {To solve the problem of fire investigation caused by lack of exacting logical reasoning, it is of significance in helping that abduction, an important logical thinking should be introduced to the field of fire investigation. This paper first analyzes the fundamental reasoning forms of abduction as well as its general situation of application. Combined with practical work experience, the mode of application of abduction to fire investigation is put forward. The author shows it in detail by analyzing a real fire case. It is north noting that some matters needing attention in application are presented in the end. This paper will be conductive to constructing the right logical reasoning model in fire investigation.}
}
@article{DINU20242902,
title = {An integrated benchmark for verbal creativity testing of LLMs and humans},
journal = {Procedia Computer Science},
volume = {246},
pages = {2902-2911},
year = {2024},
note = {28th International Conference on Knowledge Based and Intelligent information and Engineering Systems (KES 2024)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.09.380},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924024141},
author = {Anca Dinu and Andra Maria Florescu},
keywords = {LLM creativity, AI creativity, verbal creativity tests, human-machine comparison},
abstract = {Until fairly recently, creativity was a human-specific characteristic. Computational creativity or artificial creativity was established as a domain in the late 90’s, with different fields such as verbal, musical, or graphical creativity. With the latest technological advances and the appearance of Large Language Models (LLMs), creativity as a feature of machines gained more and more interest in the scientific community. The scope of this study is twofold: to design a comprehensive benchmark for verbal creativity assessment of LLMs and then to run the same creativity tests on different LLMs as well as on humans, for a direct comparison. We aimed to raise the replicability and extensibility of the creativity assessment of LLMs. Hence, we adapted different types of creativity tests and different criteria from psychology to fit the LLMs profile. We also employed computer-assisted evaluation methods, by using the Open Creativity Scoring with Artificial Intelligence (OCSAI), as we wanted to focus exclusively on automated approaches to assessing creativity. We quantitatively and qualitatively analyzed the data set of both human and machine-generated answers and interpreted the results. Finally, we provide both the original verbal creativity test that we have designed, and the curated data comprising all the collected answers, from the LLMs and from the humans that participated in this research.}
}
@article{MARCHETTI20121517,
title = {Mindwandering heightens the accessibility of negative relative to positive thought},
journal = {Consciousness and Cognition},
volume = {21},
number = {3},
pages = {1517-1525},
year = {2012},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2012.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S1053810012001420},
author = {Igor Marchetti and Ernst H.W. Koster and Rudi {De Raedt}},
keywords = {Mindwandering, Negative cognitions, Mood, Depression, Individual differences},
abstract = {Mindwandering (MW) is associated with both positive and negative outcomes. Among the latter, negative mood and negative cognitions have been reported. However, the underlying mechanisms linking mindwandering to negative mood and cognition are still unclear. We hypothesized that MW could either directly enhance negative thinking or indirectly heighten the accessibility of negative thoughts. In an undergraduate sample (n=79) we measured emotional thoughts during the Sustained Attention on Response Task (SART) which induces MW, and accessibility of negative cognitions by means of the Scrambled Sentences Task (SST) after the task. We also measured depressive symptoms and rumination. Results show that in individuals with elevated levels of depressive symptoms MW during SART predicts higher accessibility of negative thoughts after the task, rather than negative thinking during the task. These findings contribute to our understanding of the underlying mechanisms of MW and provide insight into the relationship between task-involvement and affect.}
}
@article{HU2022116276,
title = {Multi granularity based label propagation with active learning for semi-supervised classification},
journal = {Expert Systems with Applications},
volume = {192},
pages = {116276},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.116276},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421015840},
author = {Shengdan Hu and Duoqian Miao and Witold Pedrycz},
keywords = {Semi-supervised learning, Granular computing, Multi granularity, Label propagation, Active learning, Three-way decision},
abstract = {Semi-supervised learning (SSL) methods, which exploit both the labeled and unlabeled data, have attracted a lot of attention. One of the major categories of SSL methods, graph-based semi-supervised learning (GBSSL) learns labels of unlabeled data on an adjacency graph, where neighborhood sparse graph is often used to reduce computational complexity. However, the neighborhood size is difficult to set. Instead of assigning a concrete value of neighborhood size, we propose a new label propagation algorithm called multi granularity based label propagation (MGLP) and developed from the view of granular computing. In MGLP, labels of unlabeled data are learned by two classic label propagation processes with diverse neighborhood size k, where granular computing delivers a guiding strategy to leverage multiple level neighborhood information granules, and three-way decision acts as an active learning strategy to select the unlabeled data for further annotating. Through the iterative procedures of label propagating, data annotating and data subset updating, the ultimate pseudo label accuracy of unlabeled data may be higher. Theoretically, the accuracy of pseudo labels is enhanced in some scenarios. Experimentally, the results of simulation studies on ten benchmark datasets, show that the proposed method MGLP can rise pseudo labels accuracy by 8.6% than LP (label propagation), 6.5% than LNP (linear neighborhood propagation), 6.4% than LPSN (label propagation through sparse neighborhood), 4.5% than Adaptive-NP (adaptive neighborhood propagation) and 4.6% than CRLP (consensus rate-based label propagation). It also provides a novel way to annotate data.}
}
@article{BEHR199399,
title = {Computation of incompressible flows with implicit finite element implementations on the Connection Machine},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {108},
number = {1},
pages = {99-118},
year = {1993},
issn = {0045-7825},
doi = {https://doi.org/10.1016/0045-7825(93)90155-Q},
url = {https://www.sciencedirect.com/science/article/pii/004578259390155Q},
author = {M. Behr and A. Johnson and J. Kennedy and S. Mittal and T. Tezduyar},
abstract = {Two implicit finite element formulations for incompressible flows have been implemented on the Connection Machine supercomputers and successfully applied to a set of time-dependent problems. The stabilized space-time formulation for moving boundaries and interfaces, and a new stabilized velocity-pressure-stress formulation are both described, and significant aspects of the implementation of these methods on massively parallel architectures are discussed. Several numerical results for flow problems involving moving as well as fixed cylinders and airfoils are reported. The parallel implementation, taking full advantage of the computational speed of the new generation of supercomputers, is found to be a significant asset in fluid dynamics research. Its current capability to solve large-scale problems, especially when coupled with the potential for growth enjoyed by massively parallel computers, make the implementation a worthwhile enterprise.}
}
@article{DEKKER2017554,
title = {Rasmussen's legacy and the long arm of rational choice},
journal = {Applied Ergonomics},
volume = {59},
pages = {554-557},
year = {2017},
note = {The Legacy of Jens Rasmussen},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2016.02.007},
url = {https://www.sciencedirect.com/science/article/pii/S0003687016300254},
author = {Sidney W.A. Dekker},
keywords = {Rasmussen, Rational choice, Human error, Second victim, Incidents},
abstract = {Rational choice theory says that operators and others make decisions by systematically and consciously weighing all possible outcomes along all relevant criteria. This paper first traces the long historical arm of rational choice thinking in the West to Judeo-Christian thinking, Calvin and Weber. It then presents a case study that illustrates the consequences of the ethic of rational choice and individual responsibility. It subsequently examines and contextualizes Rasmussen's legacy of pushing back against the long historical arm of rational choice, showing that bad outcomes are not the result of human immoral choice, but the product of normal interactions between people and systems. If we don't understand why people did what they did, Rasmussen suggested, it is not because people behaved inexplicably, but because we took the wrong perspective.}
}
@article{HONDA201718,
title = {The difference in foresight using the scanning method between experts and non-experts},
journal = {Technological Forecasting and Social Change},
volume = {119},
pages = {18-26},
year = {2017},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2017.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S004016251730313X},
author = {Hidehito Honda and Yuichi Washida and Akihito Sudo and Yuichiro Wajima and Keigo Awata and Kazuhiro Ueda},
keywords = {Foresight, Scanning method, Divergent thinking, Difference between experts and non-experts, Creativity},
abstract = {We examined the factors that produce differences in generating scenarios on the near future using the scanning method. Participants were asked to briefly read (scan) 151 articles about new technology, the latest customs, fashion, social change, value system transition, or emerging social problems, and then to generate three scenarios about the near future based on the articles. We compared the generated scenarios between scanning method experts and non-experts with no prior experience with the scanning method. We found that experts generated more unique scenarios than non-experts did, and that experts and non-experts differed in the diversity of articles referenced when generating scenarios. We discuss the relationship between the present findings and previous findings on divergent thinking.}
}
@article{BECKER2003164,
title = {A computational model of the role of dopamine and psychotropic drugs in modulating motivated action},
journal = {Schizophrenia Research},
volume = {60},
number = {1, Supplement },
pages = {164},
year = {2003},
note = {Abstracts of the IXth International Congress on Schizophrenia Research},
issn = {0920-9964},
doi = {https://doi.org/10.1016/S0920-9964(03)81019-8},
url = {https://www.sciencedirect.com/science/article/pii/S0920996403810198},
author = {S. Becker and A. Chan and P. Fletcher and A. Smith and S. Kapur}
}
@article{OBERKAMPF2002209,
title = {Verification and validation in computational fluid dynamics},
journal = {Progress in Aerospace Sciences},
volume = {38},
number = {3},
pages = {209-272},
year = {2002},
issn = {0376-0421},
doi = {https://doi.org/10.1016/S0376-0421(02)00005-2},
url = {https://www.sciencedirect.com/science/article/pii/S0376042102000052},
author = {William L. Oberkampf and Timothy G. Trucano},
abstract = {Verification and validation (V&V) are the primary means to assess accuracy and reliability in computational simulations. This paper presents an extensive review of the literature in V&V in computational fluid dynamics (CFD), discusses methods and procedures for assessing V&V, and develops a number of extensions to existing ideas. The review of the development of V&V terminology and methodology points out the contributions from members of the operations research, statistics, and CFD communities. Fundamental issues in V&V are addressed, such as code verification versus solution verification, model validation versus solution validation, the distinction between error and uncertainty, conceptual sources of error and uncertainty, and the relationship between validation and prediction. The fundamental strategy of verification is the identification and quantification of errors in the computational model and its solution. In verification activities, the accuracy of a computational solution is primarily measured relative to two types of highly accurate solutions: analytical solutions and highly accurate numerical solutions. Methods for determining the accuracy of numerical solutions are presented and the importance of software testing during verification activities is emphasized. The fundamental strategy of validation is to assess how accurately the computational results compare with the experimental data, with quantified error and uncertainty estimates for both. This strategy employs a hierarchical methodology that segregates and simplifies the physical and coupling phenomena involved in the complex engineering system of interest. A hypersonic cruise missile is used as an example of how this hierarchical structure is formulated. The discussion of validation assessment also encompasses a number of other important topics. A set of guidelines is proposed for designing and conducting validation experiments, supported by an explanation of how validation experiments are different from traditional experiments and testing. A description is given of a relatively new procedure for estimating experimental uncertainty that has proven more effective at estimating random and correlated bias errors in wind-tunnel experiments than traditional methods. Consistent with the authors’ contention that nondeterministic simulations are needed in many validation comparisons, a three-step statistical approach is offered for incorporating experimental uncertainties into the computational analysis. The discussion of validation assessment ends with the topic of validation metrics, where two sample problems are used to demonstrate how such metrics should be constructed. In the spirit of advancing the state of the art in V&V, the paper concludes with recommendations of topics for future research and with suggestions for needed changes in the implementation of V&V in production and commercial software.}
}
@article{REDKO2016105,
title = {Epistemological foundations of investigation of cognitive evolution},
journal = {Biologically Inspired Cognitive Architectures},
volume = {18},
pages = {105-115},
year = {2016},
issn = {2212-683X},
doi = {https://doi.org/10.1016/j.bica.2016.10.001},
url = {https://www.sciencedirect.com/science/article/pii/S2212683X16300597},
author = {Vladimir G. Red’ko},
keywords = {Modeling of cognitive evolution, Cognitive agents, Animal cognitive features, Epistemological foundations},
abstract = {Epistemological foundations for modeling of cognitive evolution are characterized. Cognitive evolution is the evolution of cognitive abilities of biological organisms. The important result of this evolution is the human thinking, which is used at scientific cognition of nature. The related epistemological viewpoints of David Hume, Immanuel Kant, Konrad Lorenz, and Eugene Wigner are outlined. The sketch program for future investigations of cognitive evolution is proposed; initial models of these studies are outlined. According to the presented analysis, it is possible to believe the following. Investigations of cognitive evolution are directed to analyze the fundamental problems: “Why is human thinking applicable to cognition of nature?”, “How did human thinking origin in the process of biological evolution?” There are powerful backgrounds for considered investigations: (1) models of autonomous cognitive agents, (2) biological investigations of animal cognitive features. Studies of cognitive evolution would have broad interdisciplinary relations. These studies should contribute significantly to the development of the scientific point of view.}
}
@incollection{TOPLAK202253,
title = {3 - Development of the ability to detect and override miserly information processing},
editor = {Maggie E. Toplak},
booktitle = {Cognitive Sophistication and the Development of Judgment and Decision-Making},
publisher = {Academic Press},
pages = {53-87},
year = {2022},
isbn = {978-0-12-816636-9},
doi = {https://doi.org/10.1016/B978-0-12-816636-9.00011-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128166369000116},
author = {Maggie E. Toplak},
keywords = {Miserly information processing, Dual process models, Children and youth, Development, Ratio bias, Belief bias syllogisms, Cognitive reflection},
abstract = {Several judgment and decision-making tasks require overriding an incorrect response that is signaled by miserly information processes. The successful detection and override of conflict between heuristic and analytic processes has been a focus of dual processes models, especially in adult samples. These miserly processing tendencies have also been described in developmental samples. The measurement of resistance to miserly information processing has been assessed using several tasks, including ratio bias, belief bias syllogisms, cognitive reflection, and disjunctive thinking tasks. Several of these tasks have been studied in developmental samples, including in the longitudinal study described in this volume. There is evidence to suggest that resistance to miserly information processing is measurable in children and youth. While judgment and decision-making tasks vary in the degree to which override of miserly processing is required, individuals also vary in their ability to resist miserly processing tendencies. Individual differences in resistance to miserly information processing serve as an additional foundation to support rational thinking performance.}
}
@article{ROBERTS2024100502,
title = {New approach methodologies (NAMs) in drug safety assessment: A vision of the future},
journal = {Current Opinion in Toxicology},
volume = {40},
pages = {100502},
year = {2024},
issn = {2468-2020},
doi = {https://doi.org/10.1016/j.cotox.2024.100502},
url = {https://www.sciencedirect.com/science/article/pii/S2468202024000445},
author = {Ruth A. Roberts},
keywords = {3Rs, NAMs, FTIH, Drug development, ICH, Nanobots},
abstract = {Much progress has been made in reducing and refining animal use in toxicology testing, but progress in the use of new approach methodologies (NAMs) to replace animals is disappointing. There are many highly sophisticated NAMs available, but societal, regulatory and political barriers to their implementation remain. Change requires vision, starting with imagining a future where we are successful. Specifically, this would comprise the registration of safe and effective medicines without animal tests. How do we achieve this vision? Thinking differently, in silico methods could be used to provide a detailed assessment of target- and modality-related toxicological risks, coupled with modelling of exposure. In vitro NAMs such as microphysiological systems, microelectrode array and ion channel panels could then be employed to address hypothetical risks. Finally, the safety of first time in human trials could be assessed and assured using circulating nanobots that measure conventional clinical pathology parameters alongside new biomarkers such as circulating tissue DNA. This may seem the stuff of fantasy, but imagination is key to shaping a better future and all change starts with a vision, however far-fetched it may seem today.}
}
@article{BAYNE201332,
title = {Thought},
journal = {New Scientist},
volume = {219},
number = {2935},
pages = {32-39},
year = {2013},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(13)62293-9},
url = {https://www.sciencedirect.com/science/article/pii/S0262407913622939},
author = {Tim Bayne},
abstract = {Conscious or unbidden, thoughts fill our heads from morning to night. But what are they, and what exactly is thinking? Join philosopher Tim Bayne on a journey into the fantastic, elusive and ceaseless world our minds create}
}
@article{FREYBERG20231,
title = {The morphological paradigm in robotics},
journal = {Studies in History and Philosophy of Science},
volume = {100},
pages = {1-11},
year = {2023},
issn = {0039-3681},
doi = {https://doi.org/10.1016/j.shpsa.2023.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0039368123000742},
author = {Sascha Freyberg and Helmut Hauser},
keywords = {Bionics, Embodied cognition, Morphology, Morphological computation, Soft robotics, Principles of orientation and control},
abstract = {In the paper, we are going to show how robotics is undergoing a shift in a bionic direction after a period of emphasis on artificial intelligence and increasing computational efficiency, which included isolation and extreme specialization. We assemble these new developments under the label of the morphological paradigm. The change in its paradigms and the development of alternatives to the principles that dominated robotics for a long time contains a more general epistemological significance. The role of body, material, environment, interaction and the paradigmatic status of biological and evolutionary systems for the principles of control are crucial here. Our focus will be on the introduction of the morphological paradigm in a new type of robotics and to contrast the interests behind this development with the interests shaping former models. The article aims to give a clear account of the changes in principles of orientation and control as well as concluding general observation in terms of historical epistemology, suggesting further political-epistemological analysis.}
}
@incollection{KRAAK2020141,
title = {Geovisualization},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {141-151},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10552-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955105529},
author = {Menno-Jan Kraak},
keywords = {Alternative visualization, Cartography, Cognition, Coordinated multiple views, Geocomputation, Geovisualization, Information visualization, Interfaces, Maps, Representation, Spatiotemporal data, Usability, Visual analytics, Visual exploration, Visual representation, Visual thinking},
abstract = {Due to technological developments and societal needs cartography, scientific visualization, image analysis and remote sensing, information visualization, exploratory data analysis, visual analytics, and GIScience have undergone fundamental changes in recent years. Interactivity and dynamics allow not only maps and diagrams to present known facts but also to analyze and explore unknown data. The environment in which the maps and diagrams are used has also changed and often includes coordinated multiple views display via the Internet. Such environments allow for simultaneous alternative views of the data and stimulate visual thinking, resulting in geovisualization.}
}
@article{DENNER2012240,
title = {Computer games created by middle school girls: Can they be used to measure understanding of computer science concepts?},
journal = {Computers & Education},
volume = {58},
number = {1},
pages = {240-249},
year = {2012},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2011.08.006},
url = {https://www.sciencedirect.com/science/article/pii/S0360131511001849},
author = {Jill Denner and Linda Werner and Eloy Ortiz},
keywords = {Construction of computer games, Secondary education, Programming, After-school},
abstract = {Computer game programming has been touted as a promising strategy for engaging children in the kinds of thinking that will prepare them to be producers, not just users of technology. But little is known about what they learn when programming a game. In this article, we present a strategy for coding student games, and summarize the results of an analysis of 108 games created by middle school girls using Stagecast Creator in an after school class. The findings show that students engaged in moderate levels of complex programming activity, created games with moderate levels of usability, and that the games were characterized by low levels of code organization and documentation. These results provide evidence that game construction involving both design and programming activities can support the learning of computer science concepts.}
}
@article{FLANIGAN2017179,
title = {Implicit intelligence beliefs of computer science students: Exploring change across the semester},
journal = {Contemporary Educational Psychology},
volume = {48},
pages = {179-196},
year = {2017},
issn = {0361-476X},
doi = {https://doi.org/10.1016/j.cedpsych.2016.10.003},
url = {https://www.sciencedirect.com/science/article/pii/S0361476X16300479},
author = {Abraham E. Flanigan and Markeya S. Peteranetz and Duane F. Shell and Leen-Kiat Soh},
keywords = {Motivation, Implicit intelligence beliefs, Computer science, Self-regulation, Engagement},
abstract = {This study investigated introductory computer science (CS1) students’ implicit beliefs of intelligence. Referencing Dweck and Leggett’s (1988) framework for implicit beliefs of intelligence, we examined how (1) students’ implicit beliefs changed over the course of a semester, (2) these changes differed as a function of course enrollment and students’ motivated self-regulated engagement profile, and (3) implicit beliefs predicted student learning based on standardized course grades and performance on a computational thinking knowledge test. For all students, there were significant increases in entity beliefs and significant decreases in incremental beliefs across the semester. However, examination of effect sizes suggests that significant findings for change across time were driven by changes in specific subpopulations of students. Moreover, results showed that students endorsed incremental belief more strongly than entity belief at both the beginning and end of the semester. Furthermore, the magnitude of changes differed based on students’ motivated self-regulated engagement profiles. Additionally, students’ achievement outcomes were weakly predicted by their implicit beliefs of intelligence. Finally, results showed that the relationship between changes in implicit intelligence beliefs and student achievement varied across different CS1 courses. Theoretical implications for implicit intelligence beliefs and recommendations for STEM educators are discussed.}
}
@article{RUSSWINKEL2011336,
title = {Predicting temporal errors in complex task environments: A computational and experimental approach},
journal = {Cognitive Systems Research},
volume = {12},
number = {3},
pages = {336-354},
year = {2011},
note = {Special Issue on Complex Cognition},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2010.09.003},
url = {https://www.sciencedirect.com/science/article/pii/S1389041711000143},
author = {Nele Russwinkel and Leon Urbas and Manfred Thüring},
keywords = {Cognitive modelling, Time perception, Working memory, Expectations, Surprise, ACT-R},
abstract = {Management in complex environments requires knowledge about temporal contingencies. Expectations about durations enable us to prepare for important events in good time, but also to detect irregularities. Unfortunately, time perception is not invariant. Situational aspects as well as features of the task at hand may dramatically change our sense of time. Particularly under varying workload conditions, temporal distortions may lead to performance errors. A valid and reliable model of time perception must account for these characteristics. Based on the cognitive architecture ACT-R (Anderson et al., 2004), we developed a computational model in line with this requirement. Specific emphasis was placed on mechanisms of coordinative working memory which seem to influence time encoding and perception. The model’s assumptions were tested in three steps. First, the model was applied to account for time distortions ‘a posteriori’. Effects of varying working memory demands reported by Dutke (2005) were replicated and explained by simulations of the model. Second, the model was used for predicting effects ‘a priori’. Augmenting Dutke’s (2005) approach by switching between different degrees of memory demands, predictions of time distortions were derived from the model. These predictions were compared with experimental data. Central assumptions of the model were supported, but there were also some deviations that the model had not captured. Based on the conclusions from the results of the experiment, a second a priori testing addressed temporal expectations in a complex task using a micro-world scenario. The results support the interpretation of the previous experiment and provide new insights for modelling time perception. In summary, our results indicate that coordinative working memory – in contrast to general attention – causes differences in timing performance. This characteristic is captured by our approach. The model we propose heavily relies on mechanisms of working memory and can be applied to explain effects for different time intervals, under a variety of experimental conditions and in different task environments.}
}
@article{FU20012567,
title = {Analytical and computational description of effect of grain size on yield stress of metals},
journal = {Acta Materialia},
volume = {49},
number = {13},
pages = {2567-2582},
year = {2001},
issn = {1359-6454},
doi = {https://doi.org/10.1016/S1359-6454(01)00062-3},
url = {https://www.sciencedirect.com/science/article/pii/S1359645401000623},
author = {H.-H. Fu and D.J. Benson and M.A. Meyers},
keywords = {Nanocrystalline materials, Grain size, Hall–Petch},
abstract = {Four principal factors contribute to grain-boundary strengthening: (a) the grain boundaries act as barriers to plastic flow; (b) the grain boundaries act as dislocation sources; (c) elastic anisotropy causes additional stresses in grain-boundary surroundings; (d) multislip is activated in the grain-boundary regions, whereas grain interiors are initially dominated by single slip, if properly oriented. As a result, the regions adjoining grain boundaries harden at a rate much higher than grain interiors. A phenomenological constitutive equation predicting the effect of grain size on the yield stress of metals is discussed and extended to the nanocrystalline regime. At large grain sizes, it has the Hall–Petch form, and in the nanocrystalline domain the slope gradually decreases until it asymptotically approaches the flow stress of the grain boundaries. The material is envisaged as a composite, comprised of the grain interior, with flow stress σfG, and grain boundary work-hardened layer, with flow stress σfGB. The predictions of this model are compared with experimental measurements over the mono, micro, and nanocrystalline domains. Computational predictions are made of plastic flow as a function of grain size incorporating differences of dislocation accumulation rate in grain-boundary regions and grain interiors. The material is modeled as a monocrystalline core surrounded by a mantle (grain-boundary region) with a high work hardening rate response. This is the first computational plasticity calculation that accounts for grain size effects in a physically-based manner. A discussion of statistically stored and geometrically necessary dislocations in the framework of strain-gradient plasticity is introduced to describe these effects. Grain-boundary sliding in the nanocrystalline regime is predicted from calculations using the Raj–Ashby model and incorporated into the computations; it is shown to predispose the material to shear localization.}
}
@article{ZHOU2025104052,
title = {Adaptive-solver framework for dynamic strategy selection in large language model reasoning},
journal = {Information Processing & Management},
volume = {62},
number = {3},
pages = {104052},
year = {2025},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2024.104052},
url = {https://www.sciencedirect.com/science/article/pii/S0306457324004114},
author = {Jianpeng Zhou and Wanjun Zhong and Yanlin Wang and Jiahai Wang},
keywords = {Large language models, Reasoning, Math word problems, Test-time computation allocation, Dynamic strategy selection},
abstract = {Large Language Models (LLMs) demonstrate impressive ability in handling reasoning tasks. However, unlike humans who can instinctively adapt their problem-solving strategies to the complexity of task, most LLM-based methods adopt a one-size-fits-all approach. These methods employ consistent models, sample sizes, prompting methods and levels of problem decomposition, regardless of the problem complexity. The inflexibility of these methods can bring unnecessary computational overhead or sub-optimal performance. To address this limitation, we introduce an Adaptive-Solver (AS) framework that dynamically adapts solving strategies to suit various problems, enabling the flexible allocation of test-time computational resources. The framework functions with two primary modules. The initial evaluation module assesses the reliability of the current solution using answer consistency. If the solution is deemed unreliable, the subsequent adaptation module comes into play. Within this module, various types of adaptation strategies are employed collaboratively. Through such dynamic and multi-faceted adaptations, our framework can help reduce computational consumption and improve performance. Experimental results from complex reasoning benchmarks reveal that our method can significantly reduce API costs (up to 85%) while maintaining original performance. Alternatively, it achieves up to 4.5% higher accuracy compared to the baselines at the same cost. The datasets and code are available at https://github.com/john1226966735/Adaptive-Solver.}
}
@article{ERBOZ202587,
title = {Electromagnetic radiation and biophoton emission in neuronal communication and neurodegenerative diseases},
journal = {Progress in Biophysics and Molecular Biology},
volume = {195},
pages = {87-99},
year = {2025},
issn = {0079-6107},
doi = {https://doi.org/10.1016/j.pbiomolbio.2024.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0079610724001159},
author = {Aysin Erboz and Elif Kesekler and Pier Luigi Gentili and Vladimir N. Uversky and Orkid Coskuner-Weber},
keywords = {Electromagnetic radiation, Biophotons, Neurodegenerative diseases, Neuron communication, Intrinsically disordered proteins},
abstract = {The intersection of electromagnetic radiation and neuronal communication, focusing on the potential role of biophoton emission in brain function and neurodegenerative diseases is an emerging research area. Traditionally, it is believed that neurons encode and communicate information via electrochemical impulses, generating electromagnetic fields detectable by EEG and MEG. Recent discoveries indicate that neurons may also emit biophotons, suggesting an additional communication channel alongside the regular synaptic interactions. This dual signaling system is analyzed for its potential in synchronizing neuronal activity and improving information transfer, with implications for brain-like computing systems. The clinical relevance is explored through the lens of neurodegenerative diseases and intrinsically disordered proteins, where oxidative stress may alter biophoton emission, offering clues for pathological conditions, such as Alzheimer's and Parkinson's diseases. The potential therapeutic use of Low-Level Laser Therapy (LLLT) is also examined for its ability to modulate biophoton activity and mitigate oxidative stress, presenting new opportunities for treatment. Here, we invite further exploration into the intricate roles the electromagnetic phenomena play in brain function, potentially leading to breakthroughs in computational neuroscience and medical therapies for neurodegenerative diseases.}
}
@article{PAZBARUCH2023101294,
title = {Cognitive abilities and creativity: The role of working memory and visual processing},
journal = {Thinking Skills and Creativity},
volume = {48},
pages = {101294},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101294},
url = {https://www.sciencedirect.com/science/article/pii/S1871187123000640},
author = {Nurit Paz-Baruch and Rotem Maor},
keywords = {Cognitive abilities, Creativity, Working memory, Visual processing},
abstract = {Recent studies have revealed the importance of cognitive abilities in creative thinking. However, most research addressed adults and only a few studies have examined the ways these correlations are manifested among young children. The present study explores the role of various cognitive abilities in creativity among school children. Measures of creativity, visual-spatial working memory (VSWM), verbal short-term memory (STM), working memory (WM), and visual processing (VP) were administered to 331 students in Grades 4 and 5. Cluster analysis was used to group students' creativity levels. A multivariate analysis of variance (MANOVA) was conducted to test differences in cognitive abilities across the three clusters. Group differences between high and moderate level creativity students and low creativity students were found regarding VP abilities in the following tests: VSWM, visual discrimination (VD), and Raven's Colored Progressive Matrices (RCPM). Group differences between high creativity students and low creativity students were also found on verbal STM and WM. Additionally, structural equation modeling (SEM) analysis revealed that VP can significantly account for unique variances associated with creativity, while verbal STM and WM are not significantly related to creativity. These findings enlighten the cognitive processes underlying creativity in young children.}
}
@article{KLATT2009536,
title = {Perspectives for process systems engineering—Personal views from academia and industry},
journal = {Computers & Chemical Engineering},
volume = {33},
number = {3},
pages = {536-550},
year = {2009},
note = {Selected Papers from the 17th European Symposium on Computer Aided Process Engineering held in Bucharest, Romania, May 2007},
issn = {0098-1354},
doi = {https://doi.org/10.1016/j.compchemeng.2008.09.002},
url = {https://www.sciencedirect.com/science/article/pii/S0098135408001737},
author = {Karsten-Ulrich Klatt and Wolfgang Marquardt},
keywords = {Review, Modeling, Design, Optimization, Control, Operations, Numerical algorithms, Software, Computer-aided process engineering (CAPE)},
abstract = {Process systems engineering (PSE) has been an active research field for almost 50 years. Its major achievements include methodologies and tools to support process modeling, simulation and optimization (MSO). Mature, commercially available technologies have been penetrating all fields of chemical engineering in academia as well as in industrial practice. MSO technologies have become a commodity, they are not a distinguishing feature of the PSE field any more. Consequently, PSE has to reassess and to reposition its future research agenda. Emphasis should be put on model-based applications in all PSE domains including product and process design, control and operations. Furthermore, systems thinking and systems problem solving have to be prioritized rather than the mere application of computational problem solving methods. This essay reflects on the past, present and future of PSE from an academic and industrial point of view. It redefines PSE as an active and future-proof research field which can play an active role in providing enabling technologies for product and process innovations in the chemical industries and beyond.}
}
@article{MORRIS2015,
title = {Efficacy of a Web-Based, Crowdsourced Peer-To-Peer Cognitive Reappraisal Platform for Depression: Randomized Controlled Trial},
journal = {Journal of Medical Internet Research},
volume = {17},
number = {3},
year = {2015},
issn = {1438-8871},
doi = {https://doi.org/10.2196/jmir.4167},
url = {https://www.sciencedirect.com/science/article/pii/S1438887115000783},
author = {Robert R Morris and Stephen M Schueller and Rosalind W Picard},
keywords = {Web-based intervention, crowdsourcing, randomized controlled trial, depression, cognitive behavioral therapy, mental health, social networks},
abstract = {Background
Self-guided, Web-based interventions for depression show promising results but suffer from high attrition and low user engagement. Online peer support networks can be highly engaging, but they show mixed results and lack evidence-based content.
Objective
Our aim was to introduce and evaluate a novel Web-based, peer-to-peer cognitive reappraisal platform designed to promote evidence-based techniques, with the hypotheses that (1) repeated use of the platform increases reappraisal and reduces depression and (2) that the social, crowdsourced interactions enhance engagement.
Methods
Participants aged 18-35 were recruited online and were randomly assigned to the treatment group, “Panoply” (n=84), or an active control group, online expressive writing (n=82). Both are fully automated Web-based platforms. Participants were asked to use their assigned platform for a minimum of 25 minutes per week for 3 weeks. Both platforms involved posting descriptions of stressful thoughts and situations. Participants on the Panoply platform additionally received crowdsourced reappraisal support immediately after submitting a post (median response time=9 minutes). Panoply participants could also practice reappraising stressful situations submitted by other users. Online questionnaires administered at baseline and 3 weeks assessed depression symptoms, reappraisal, and perseverative thinking. Engagement was assessed through self-report measures, session data, and activity levels.
Results
The Panoply platform produced significant improvements from pre to post for depression (P=.001), reappraisal (P<.001), and perseverative thinking (P<.001). The expressive writing platform yielded significant pre to post improvements for depression (P=.02) and perseverative thinking (P<.001), but not reappraisal (P=.45). The two groups did not diverge significantly at post-test on measures of depression or perseverative thinking, though Panoply users had significantly higher reappraisal scores (P=.02) than expressive writing. We also found significant group by treatment interactions. Individuals with elevated depression symptoms showed greater comparative benefit from Panoply for depression (P=.02) and perseverative thinking (P=.008). Individuals with baseline reappraisal deficits showed greater comparative benefit from Panoply for depression (P=.002) and perseverative thinking (P=.002). Changes in reappraisal mediated the effects of Panoply, but not the expressive writing platform, for both outcomes of depression (ab=-1.04, SE 0.58, 95% CI -2.67 to -.12) and perseverative thinking (ab=-1.02, SE 0.61, 95% CI -2.88 to -.20). Dropout rates were similar for the two platforms; however, Panoply yielded significantly more usage activity (P<.001) and significantly greater user experience scores (P<.001).
Conclusions
Panoply engaged its users and was especially helpful for depressed individuals and for those who might ordinarily underutilize reappraisal techniques. Further investigation is needed to examine the long-term effects of such a platform and whether the benefits generalize to a more diverse population of users.
Trial Registration
ClinicalTrials.gov NCT02302248; https://clinicaltrials.gov/ct2/show/NCT02302248 (Archived by WebCite at http://www.webcitation.org/6Wtkj6CXU).}
}
@article{KIMSEY1994113,
title = {Parallel computation of impact dynamics},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {119},
number = {1},
pages = {113-121},
year = {1994},
issn = {0045-7825},
doi = {https://doi.org/10.1016/0045-7825(94)00079-4},
url = {https://www.sciencedirect.com/science/article/pii/0045782594000794},
author = {K.D. Kimsey and M.A. Olson},
abstract = {This paper discusses a parallel algorithm and data structures for implementing multimaterial, two-step Eulerian finite difference solution schemes on hypercube architectures. Selected problems in impact dynamics have been modeled on the Connection Machine model CM5, and the results are compared with computational results reported in the literature, as well as direct comparison with experimental data.}
}
@incollection{ESFELD2015131,
title = {Atomism and Holism: Philosophical Aspects},
editor = {James D. Wright},
booktitle = {International Encyclopedia of the Social & Behavioral Sciences (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {131-135},
year = {2015},
isbn = {978-0-08-097087-5},
doi = {https://doi.org/10.1016/B978-0-08-097086-8.63003-9},
url = {https://www.sciencedirect.com/science/article/pii/B9780080970868630039},
author = {Michael Esfeld},
keywords = {Atomism, Collectivism, Confirmation, Externalism, Holism, Human nature, Individualism, Internalism, Meaning, Ontological dependence, Rationality, Rule-following, Thought},
abstract = {In the philosophy of the social sciences, atomism is the view that human beings can be thinking, rational beings independently of social relations. Holism, by contrast, is the view that social relations are essential to human beings insofar as they are thinking, rational beings. This article first provides an overview of different sorts of atomism and holism (see Section Types of Atomism and Holism). It then briefly sketches the historical background of these notions in modern philosophy (Section The Historical Background of Atomism and Holism). The main part is a systematic characterization of atomism and holism (see Section A Characterization of Atomism and Holism) and a summary of the most important arguments for both these positions (see Section Arguments for Atomism and Holism).}
}
@incollection{ZIMMERMAN2005255,
title = {VIII - Development of Theory with Computation},
editor = {M. Olivucci},
series = {Theoretical and Computational Chemistry},
publisher = {Elsevier},
volume = {16},
pages = {255-278},
year = {2005},
booktitle = {Computational Photochemistry},
issn = {1380-7323},
doi = {https://doi.org/10.1016/S1380-7323(05)80025-1},
url = {https://www.sciencedirect.com/science/article/pii/S1380732305800251},
author = {Howard E. Zimmerman}
}
@article{SUI2001529,
title = {Terrae Incognitae and Limits of Computation: Whither GIScience?},
journal = {Computers, Environment and Urban Systems},
volume = {25},
number = {6},
pages = {529-533},
year = {2001},
issn = {0198-9715},
doi = {https://doi.org/10.1016/S0198-9715(01)00027-8},
url = {https://www.sciencedirect.com/science/article/pii/S0198971501000278},
author = {Daniel Sui}
}
@article{HEYN2023111604,
title = {A compositional approach to creating architecture frameworks with an application to distributed AI systems},
journal = {Journal of Systems and Software},
volume = {198},
pages = {111604},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111604},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222002801},
author = {Hans-Martin Heyn and Eric Knauss and Patrizio Pelliccione},
keywords = {AI systems, Architectural frameworks, Compositional thinking, Requirements engineering, Systems engineering},
abstract = {Artificial intelligence (AI) in its various forms finds more and more its way into complex distributed systems. For instance, it is used locally, as part of a sensor system, on the edge for low-latency high-performance inference, or in the cloud, e.g. for data mining. Modern complex systems, such as connected vehicles, are often part of an Internet of Things (IoT). This poses additional architectural challenges. To manage complexity, architectures are described with architecture frameworks, which are composed of a number of architectural views connected through correspondence rules. Despite some attempts, the definition of a mathematical foundation for architecture frameworks that are suitable for the development of distributed AI systems still requires investigation and study. In this paper, we propose to extend the state of the art on architecture framework by providing a mathematical model for system architectures, which is scalable and supports co-evolution of different aspects for example of an AI system. Based on Design Science Research, this study starts by identifying the challenges with architectural frameworks in a use case of distributed AI systems. Then, we derive from the identified challenges four rules, and we formulate them by exploiting concepts from category theory. We show how compositional thinking can provide rules for the creation and management of architectural frameworks for complex systems, for example distributed systems with AI. The aim of the paper is not to provide viewpoints or architecture models specific to AI systems, but instead to provide guidelines based on a mathematical formulation on how a consistent framework can be built up with existing, or newly created, viewpoints. To put in practice and test the approach, the identified and formulated rules are applied to derive an architectural framework for the EU Horizon 2020 project “Very efficient deep learning in the IoT” (VEDLIoT) in the form of a case study.}
}
@article{WOLFE197853,
title = {The rise of network thinking in anthropology},
journal = {Social Networks},
volume = {1},
number = {1},
pages = {53-64},
year = {1978},
issn = {0378-8733},
doi = {https://doi.org/10.1016/0378-8733(78)90012-6},
url = {https://www.sciencedirect.com/science/article/pii/0378873378900126},
author = {Alvin W. Wolfe},
abstract = {The encyclopedic inventory of the first half of the twentieth century, “Anthropology Today”, published in 1953, gave little inkling that within a few decades developing trends in social theory, in field experience, in electronic data processing, and in mathematics would combine to bring to prominence a distinctive theoretical approach using a quite formal network model for social systems. Now, sophisticated mathematics and computer programming permit sophisticated network models — networks seen as sets of links, networks seen as generated structures, and networks seen as flow processes. Although network thinking has shown a dramatic rise from the “Anthropology Today” of 1953 to the current anthropology of 1978, it is predicted to soar in the next quarter century, much of the weighty burden of network analysis having been lifted from us by ever more rapid electronic data processing.}
}
@article{CHEN2014740,
title = {On the Systematic Method to Enhance the Epiphany Ability of Individuals},
journal = {Procedia Computer Science},
volume = {31},
pages = {740-746},
year = {2014},
note = {2nd International Conference on Information Technology and Quantitative Management, ITQM 2014},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.05.322},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914004992},
author = {Ailing Chen and Wei Liu and Zhihui Wu and Jun Zhang},
keywords = {Prototype heuristics, epiphany, extenics, Theory of Creativity, sytematic scheme ;},
abstract = {Epiphany is a crucial stage in the process of creative thinking. The prototype heuristic theory has proved that the individual epiphany ability depends on the individual's ability to get out of the fetter of mental fixation, activate the prototype and acquire the key heuristic information from the activated prototype. Based on this theory, this present research combines the findings of extenics, TRIZ and theory of creativity to have developed a systematic method on enhancing individual epiphany ability. Supported by information technology, the method takes theory of creativity as its methodology, extension strategy generation as its framework, element theory its database and knowledge management its feedback chain. The research aims to cultivate creative thinking and eventually enhance the creativity of individuals.}
}
@incollection{BICKHARD2025169,
title = {The mentality of Homo Sapiens},
editor = {Mark H. Bickhard},
booktitle = {The Whole Person},
publisher = {Academic Press},
pages = {169-260},
year = {2025},
isbn = {978-0-443-33050-6},
doi = {https://doi.org/10.1016/B978-0-443-33050-6.00009-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780443330506000094},
author = {Mark H. Bickhard},
keywords = {interactive knowing, situation knowledge, presupposition, higher order motivation and values, themes, central nervous system as a microgenetic dynamic system, thinking and concepts, contact-content, indexicality, attention, will, learning, topologies in heuristic learning, “enactive”, “semantic”, episodic, and autobiographical memory, knowing levels, development, emotions, evolution of emotion processes, organizations of microgenesis processes, reflective consciousness, rehearsal, purpose, emergence of reflection in the brain, experiencing, experiencing of experiencing, higher order theory, Brentano, Searle, folk psychology},
abstract = {The central macro-evolutionary ratchet supports myriad further emergences as specializations, further developments, and various kinds of interactions among the differing processes. This chapter, in the section on interactive knowing, addresses, for example, situation knowledge, presupposition, higher order motivation and values, the important process characteristic of themes, central nervous system as a microgenetic dynamic system, thinking and concepts, the contact-content distinction, indexicality, attention, and will. Concerning learning, the function of topologies in heuristic learning is explored, multiple kinds of memory are addressed, such as “enactive”, “semantic”, episodic, and autobiographical, and some of the enabling and constraining developmental consequences of knowing levels are examined. In the section on emotions, basic principles of developmental differentiations of emotions are examined, and the evolution of emotion processes within organizations of microgenesis processes is framed. The discussion of reflective consciousness addresses enablings such as rehearsal and purpose, and extends the evolutionary analysis to the emergence of the possibility of reflection in the brain. Issues concerning experiencing, such as unities and boundaries, are addressed. Reflective experiencing of experiencing is presented as a resolution of a fundamental problem in the literature. Some further literature is addressed, such as higher order theory, Brentano, Searle, and folk psychology.}
}
@article{FERNANDEZFONTECHA2022101067,
title = {Examining the relations between semantic memory structure and creativity in second language},
journal = {Thinking Skills and Creativity},
volume = {45},
pages = {101067},
year = {2022},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2022.101067},
url = {https://www.sciencedirect.com/science/article/pii/S1871187122000700},
author = {Almudena Fernández-Fontecha and Yoed N. Kenett},
keywords = {Creativity, Semantic network, L2, Bilingualism, Semantic fluency},
abstract = {Creativity is related to a higher flexible semantic memory structure, which could explain greater fluency of ideas. Extensive research has identified a positive connection between creativity and bi-/multilingualism mainly in contexts where two languages or more concur in daily communicative interactions. Yet, creativity has received scant attention in regard to L2 (second or foreign language) acquisition that mainly takes place in classroom situations. The scarce research points to a positive relationship between creativity and L2 fluency – understood as the number of words produced. We apply computational network science analysis and Forward Flow methods to examine lexical organization patterns of a low creativity (LC) and high creativity (HC) group of 12th grade Spanish English as a Foreign Language (EFL) learners. The participants completed two fluency tasks, where they generated animal names in their L2, and also L1 – used here as a control measure. EFL proficiency was controlled. Our analyses revealed that the HC individuals were more fluent in L1 and L2, generated more remote responses, and exhibited a more flexible and efficiently structured semantic memory in both languages, with a greater effect of creativity in L2. Contrary to previous research, the L2 semantic memory network exhibited a less random organization. Differences in the L2 learning conditions are adduced as likely causes of this result.}
}
@article{OXMAN2006229,
title = {Theory and design in the first digital age},
journal = {Design Studies},
volume = {27},
number = {3},
pages = {229-265},
year = {2006},
note = {Digital Design},
issn = {0142-694X},
doi = {https://doi.org/10.1016/j.destud.2005.11.002},
url = {https://www.sciencedirect.com/science/article/pii/S0142694X05000840},
author = {Rivka Oxman},
keywords = {digital design, design theory, design methodology, design thinking},
abstract = {Digital design and its growing impact on design and production practices have resulted in the need for a re-examination of current design theories and methodologies in order to explain and guide future research and development. The present research postulates the requirements for a conceptual framework and theoretical basis of digital design; reviews the recent theoretical and historical background; and defines a generic schema of design characteristics through which the paradigmatic classes of digital design are formulated. The implication of this research for the formulation of ‘digital design thinking’ is presented and discussed.}
}
@article{BROSSEAU2003373,
title = {Computational electromagnetics and the rational design of new dielectric heterostructures},
journal = {Progress in Materials Science},
volume = {48},
number = {5},
pages = {373-456},
year = {2003},
issn = {0079-6425},
doi = {https://doi.org/10.1016/S0079-6425(02)00013-0},
url = {https://www.sciencedirect.com/science/article/pii/S0079642502000130},
author = {C. Brosseau and A. Beroual},
abstract = {Dielectric properties of heterogeneous materials for various condensed-matter systems have been gaining world-wide attention over the past 50 or so years in the design (or engineering) of materials structures for desired properties and functional purposes. These applications range from cable and current limiters to sensors. These multiscale systems lead to challenging problems of connecting micro- or meso-structural features to macroscopic materials response, i.e. permittivity, conductivity. This article first reviews progress made at that time of the underlying physics of dielectric heterostructures and points out the missing elements that have led to a resurgence of interest in these and related materials. Recent advances in computational electromagnetics provide unparalleled control over morphology in this class of materials to produce a seemingly unlimited number of exquisitely structured materials endowed with tailored electromagnetic, and other physical properties. In the text to follow, we illustrate how an ab initio computational technique can be used to accurately characterize structure–dielectric property relationships of periodic heterostructures in the quasistatic limit. More specifically, we have carried out two-dimensional (2D) and three-dimensional (3D) numerical studies of two-component materials in which equal-sized inclusions, with shape and orientation and possibly fused together, are fixed in a periodic square (2D) or cubic (3D) array. Boundary-integral equations (BIE) are derived from Green's theorem and are solved for the local field with appropriate periodicity conditions on a unit cell of the structures using the field calculation package PHI3D. A number of illustrative examples shows how this computational technique can provide very accurate predictions for the complex effective permittivity of translationally-invariant heterostructures. The performance of the method is also compared with those of other computational and analytical techniques. We comment on how this computational method helps identify some important characteristics for rationalizing and predicting the structure of composite materials in terms of the nature, size, shape and orientation of their constituents.}
}
@incollection{GARDNER202427,
title = {Chapter 2 - Ethics and the smart city},
editor = {Nicole Gardner},
booktitle = {Scaling the Smart City},
publisher = {Elsevier},
pages = {27-49},
year = {2024},
series = {Smart Cities},
isbn = {978-0-443-18452-9},
doi = {https://doi.org/10.1016/B978-0-443-18452-9.00004-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780443184529000045},
author = {Nicole Gardner},
keywords = {Design, process, Ethics, Ethics by design, Material ethics, Philosophy of technology, Postphenomenology, Practical ethics, Smart city, Urban technology, Value-sensitive design},
abstract = {Examining who (and what) stands to gain and lose in the smart city, and what kind of urban life smart cities create, are questions of a philosophical and ethical import. Since the early 2010s scholars and journalists have critiqued the smart city paradigm and examined its various projects and experimental initiatives to surface its ethical dimensions and significance. The smart city's rhetorical swing from tech-centric to human-centric and from smart city to smart citizen is construed here as a further effort to create the image of a more ethical smart city. Consequently, the topic of ethics has also intersected with the smart city in particular ways, and predominantly through the lens of data governance and the protection of privacy rights. This chapter argues for an expanded approach to smart city ethics. It proposes a focus on urban technology design to bridge the gap between a micro-ethical focus on data ethics and macro-level political-ethical critique. Bringing philosophical thinking on technology together with a design-led approach to urban technology is argued to provide a further way to draw out a potentially different set of ethical concerns and to explore how urban life can be lived with technology.}
}
@article{JOEL2002535,
title = {Actor–critic models of the basal ganglia: new anatomical and computational perspectives},
journal = {Neural Networks},
volume = {15},
number = {4},
pages = {535-547},
year = {2002},
issn = {0893-6080},
doi = {https://doi.org/10.1016/S0893-6080(02)00047-3},
url = {https://www.sciencedirect.com/science/article/pii/S0893608002000473},
author = {Daphna Joel and Yael Niv and Eytan Ruppin},
keywords = {Basal ganglia, Dopamine, Reinforcement learning, Actor–critic, Dimensionality reduction, Evolutionary computation, Behavioral switching, Striosomes/patches},
abstract = {A large number of computational models of information processing in the basal ganglia have been developed in recent years. Prominent in these are actor–critic models of basal ganglia functioning, which build on the strong resemblance between dopamine neuron activity and the temporal difference prediction error signal in the critic, and between dopamine-dependent long-term synaptic plasticity in the striatum and learning guided by a prediction error signal in the actor. We selectively review several actor–critic models of the basal ganglia with an emphasis on two important aspects: the way in which models of the critic reproduce the temporal dynamics of dopamine firing, and the extent to which models of the actor take into account known basal ganglia anatomy and physiology. To complement the efforts to relate basal ganglia mechanisms to reinforcement learning (RL), we introduce an alternative approach to modeling a critic network, which uses Evolutionary Computation techniques to ‘evolve’ an optimal RL mechanism, and relate the evolved mechanism to the basic model of the critic. We conclude our discussion of models of the critic by a critical discussion of the anatomical plausibility of implementations of a critic in basal ganglia circuitry, and conclude that such implementations build on assumptions that are inconsistent with the known anatomy of the basal ganglia. We return to the actor component of the actor–critic model, which is usually modeled at the striatal level with very little detail. We describe an alternative model of the basal ganglia which takes into account several important, and previously neglected, anatomical and physiological characteristics of basal ganglia–thalamocortical connectivity and suggests that the basal ganglia performs reinforcement-biased dimensionality reduction of cortical inputs. We further suggest that since such selective encoding may bias the representation at the level of the frontal cortex towards the selection of rewarded plans and actions, the reinforcement-driven dimensionality reduction framework may serve as a basis for basal ganglia actor models. We conclude with a short discussion of the dual role of the dopamine signal in RL and in behavioral switching.}
}
@incollection{CAMERON200789,
title = {Designing Computational Clusters for Performance and Power},
editor = {Marvin V. Zelkowitz},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {69},
pages = {89-153},
year = {2007},
booktitle = {Architectural Issues},
issn = {0065-2458},
doi = {https://doi.org/10.1016/S0065-2458(06)69002-5},
url = {https://www.sciencedirect.com/science/article/pii/S0065245806690025},
author = {Kirk W. Cameron and Rong Ge and Xizhou Feng},
abstract = {Power consumption in computational clusters has reached critical levels. High-end cluster performance improves exponentially while the power consumed and heat dissipated increase operational costs and failure rates. Yet, the demand for more powerful machines continues to grow. In this chapter, we motivate the need to reconsider the traditional performance-at-any-cost cluster design approach. We propose designs where power and performance are considered critical constraints. We describe power-aware and low power techniques to reduce the power profiles of parallel applications and mitigate the impact on performance.}
}
@article{DRAGGIOTIS1998157,
title = {On the computation of multigluon amplitudes},
journal = {Physics Letters B},
volume = {439},
number = {1},
pages = {157-164},
year = {1998},
issn = {0370-2693},
doi = {https://doi.org/10.1016/S0370-2693(98)01015-6},
url = {https://www.sciencedirect.com/science/article/pii/S0370269398010156},
author = {Petros Draggiotis and Ronald H.P. Kleiss and Costas G. Papadopoulos},
abstract = {A computational algorithm based on recursive equations is developed in order to estimate multigluon production processes at high energy hadron colliders. The partonic reactions gg→(n−2)g with n≤9 are studied and comparisons with known approximations are presented.}
}
@incollection{YANG20133,
title = {1 - Swarm Intelligence and Bio-Inspired Computation: An Overview},
editor = {Xin-She Yang and Zhihua Cui and Renbin Xiao and Amir Hossein Gandomi and Mehmet Karamanoglu},
booktitle = {Swarm Intelligence and Bio-Inspired Computation},
publisher = {Elsevier},
address = {Oxford},
pages = {3-23},
year = {2013},
isbn = {978-0-12-405163-8},
doi = {https://doi.org/10.1016/B978-0-12-405163-8.00001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780124051638000016},
author = {Xin-She Yang and Mehmet Karamanoglu},
keywords = {Algorithm, ant algorithm, bee algorithm, bat algorithm, bio-inspired, cuckoo search, firefly algorithm, harmony search, particle swarm optimization, swarm intelligence, metaheuristics},
abstract = {Swarm intelligence (SI) and bio-inspired computing in general have attracted great interest in almost every area of science, engineering, and industry over the last two decades. In this chapter, we provide an overview of some of the most widely used bio-inspired algorithms, especially those based on SI such as cuckoo search, firefly algorithm, and particle swarm optimization. We also analyze the essence of algorithms and their connections to self-organization. Furthermore, we highlight the main challenging issues associated with these metaheuristic algorithms with in-depth discussions. Finally, we provide some key, open problems that need to be addressed in the next decade.}
}