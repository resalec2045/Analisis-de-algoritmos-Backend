@article{FEDORENKO2014120,
title = {Reworking the language network},
journal = {Trends in Cognitive Sciences},
volume = {18},
number = {3},
pages = {120-126},
year = {2014},
issn = {1364-6613},
doi = {https://doi.org/10.1016/j.tics.2013.12.006},
url = {https://www.sciencedirect.com/science/article/pii/S136466131300288X},
author = {Evelina Fedorenko and Sharon L. Thompson-Schill},
keywords = {domain specificity, domain generality, language network, cognitive control, fMRI},
abstract = {Prior investigations of functional specialization have focused on the response profiles of particular brain regions. Given the growing emphasis on regional covariation, we propose to reframe these questions in terms of brain ‘networks’ (collections of regions jointly engaged by some mental process). Despite the challenges that investigations of the language network face, a network approach may prove useful in understanding the cognitive architecture of language. We propose that a language network plausibly includes a functionally specialized ‘core’ (brain regions that coactivate with each other during language processing) and a domain-general ‘periphery’ (a set of brain regions that may coactivate with the language core regions at some times but with other specialized systems at other times, depending on task demands). Framing the debate around network properties such as this may prove to be a more fruitful way to advance our understanding of the neurobiology of language.}
}
@article{NG2025100807,
title = {Proteome-wide assessment of differential missense variant clustering in neurodevelopmental disorders and cancer},
journal = {Cell Genomics},
pages = {100807},
year = {2025},
issn = {2666-979X},
doi = {https://doi.org/10.1016/j.xgen.2025.100807},
url = {https://www.sciencedirect.com/science/article/pii/S2666979X25000631},
author = {Jeffrey K. Ng and Yilin Chen and Titilope M. Akinwe and Hillary B. Heins and Elvisa Mehinovic and Yoonhoo Chang and David H. Gutmann and Christina A. Gurnett and Zachary L. Payne and Juana G. Manuel and Rachel Karchin and Tychele N. Turner},
keywords = {neurodevelopmental disorders, cancer, clustering algorithm, 3D protein structure models, missense, , somatic, variant interpretation, protein},
abstract = {Summary
Prior studies examining genomic variants suggest that some proteins contribute to both neurodevelopmental disorders (NDDs) and cancer. While there are several potential etiologies, here, we hypothesize that missense variation in proteins occurs in different clustering patterns, resulting in distinct phenotypic outcomes. This concept was first explored in 1D protein space and expanded using 3D protein structure models. Missense de novo variants were examined from 39,883 families with NDDs and missense somatic variants from 10,543 sequenced tumors covering five The Cancer Genome Atlas (TCGA) cancer types and two Catalog of Somatic Mutations in Cancer (COSMIC) pan-cancer aggregates of tissue types. We find 18 proteins with differential missense variation clustering in NDDs compared to cancers and 19 in cancers relative to NDDs. These proteins may be important for detailed assessments in thinking of future prognostic and therapeutic applications. We establish a framework for interpreting missense patterns in NDDs and cancer, using advances in 3D protein structure prediction.}
}
@article{ZAHRAH2024100481,
title = {Unmasking hate in the pandemic: A cross-platform study of the COVID-19 infodemic},
journal = {Big Data Research},
volume = {37},
pages = {100481},
year = {2024},
issn = {2214-5796},
doi = {https://doi.org/10.1016/j.bdr.2024.100481},
url = {https://www.sciencedirect.com/science/article/pii/S2214579624000558},
author = {Fatima Zahrah and Jason R.C. Nurse and Michael Goldsmith},
keywords = {Social media analysis, Cross-platform analysis, Online hate, COVID-19},
abstract = {The past few decades have established how digital technologies and platforms have provided an effective medium for spreading hateful content, which has been linked to several catastrophic consequences. Recent academic studies have also highlighted how online hate is a phenomenon that strategically makes use of multiple online platforms. In this article, we seek to advance the current research landscape by harnessing a cross-platform approach to computationally analyse content relating to the 2020 COVID-19 pandemic. More specifically, we analyse content on hate-specific environments from Twitter, Reddit, 4chan and Stormfront. Our findings show how content and posting activity can change across platforms, and how the psychological components of online content can differ depending on the platform being used. Through this, we provide unique insight into the cross-platform behaviours of online hate. We further define several avenues for future research within this field so as to gain a more comprehensive understanding of the global hate ecosystem.}
}
@article{SUN2025129677,
title = {StereoSqueezeNet: With fewer parameters but higher accuracy than SqueezeNet},
journal = {Neurocomputing},
volume = {627},
pages = {129677},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2025.129677},
url = {https://www.sciencedirect.com/science/article/pii/S0925231225003492},
author = {Qiaoyan Sun and Jianfei Chen},
keywords = {SSNet, Stereo, SqueezeNet, Network compression, Network parameters},
abstract = {Convolutional neural networks (CNNs) have evolved from the initial LeNet to date, and network models have become increasingly deep and comprehensive. It has been proven that deeper networks have better fitting effects, but the corresponding parameter size and computational complexity increase rapidly. With the continuous development of mobile Internet technology, portable devices have been rapidly popularized, and users have put forward more and more demands. Thus, how to design efficient and high-performance lightweight convolutional neural networks (CNNs) is the key to solve this challenging problem. Recently, this type of convolutional neural networks (CNNs)--lightweight convolutional neural networks (CNNs), which adopt the design concept of compression networks and maintain high accuracy with fewer parameters, has attracted increasing attention. SqueezeNet is a lightweight CNN adapting to edge device deployment. Its number of parameters is only 1/50 of AlexNet, but it achieves the same accuracy as AlexNet. In order to make the network more lightweight, inspired by SqueezeNet, MobileNet, SENet, SKNet, AlexNet, etc., in this paper we propose StereoSqueezeNet, using much fewer parameters but achieving even better accuracy than SqueezeNet.}
}
@article{ROHLFS2025128701,
title = {Generalization in neural networks: A broad survey},
journal = {Neurocomputing},
volume = {611},
pages = {128701},
year = {2025},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2024.128701},
url = {https://www.sciencedirect.com/science/article/pii/S0925231224014723},
author = {Chris Rohlfs},
keywords = {Literature review, Deep learning, Overfitting, Causality, Domain generalization, Transfer learning, Foundation models, Multimodal, Semantic knowledge, Abstraction, Biologically-inspired},
abstract = {This paper reviews concepts, modeling approaches, and recent findings along a spectrum of different levels of abstraction of neural network models including generalization across (1) Samples, (2) Distributions, (3) Domains, (4) Tasks, (5) Modalities, and (6) Scopes. Strategies for (1) sample generalization from training to test data are discussed, with suggestive evidence presented that, at least for the ImageNet dataset, popular classification models show substantial overfitting. An empirical example and perspectives from statistics highlight how models’ (2) distribution generalization can benefit from consideration of causal relationships and counterfactual scenarios. Transfer learning approaches and results for (3) domain generalization are summarized, as is the wealth of domain generalization benchmark datasets available. Recent breakthroughs surveyed in (4) task generalization include few-shot meta-learning approaches and the emergence of transformer-based foundation models such as those used for language processing. Studies performing (5) modality generalization are reviewed, including those that integrate image and text data and that apply a biologically-inspired network across olfactory, visual, and auditory modalities. Higher-level (6) scope generalization results are surveyed, including graph-based approaches to represent symbolic knowledge in networks and attribution strategies for improving networks’ explainability. Additionally, concepts from neuroscience are discussed on the modular architecture of brains and the steps by which dopamine-driven conditioning leads to abstract thinking.}
}
@incollection{OBRIEN2014141,
title = {7 - Reasoning with graphs},
editor = {Jamie O’Brien},
booktitle = {Shaping Knowledge},
publisher = {Chandos Publishing},
pages = {141-174},
year = {2014},
series = {Chandos Information Professional Series},
isbn = {978-1-84334-751-4},
doi = {https://doi.org/10.1533/9781780634326.141},
url = {https://www.sciencedirect.com/science/article/pii/B9781843347514500078},
author = {Jamie O’Brien},
keywords = {graph databases, logic and computing, reasoning, spatial data structures, visualization},
abstract = {Abstract:
Knowledge complexity poses a problem to the modeller of representation and, in turn, of reasoning. We seek to overcome this problem by using our ‘privileged’ sense of vision. This means that we render dynamic, multi-modal phenomena as graphic depictions, be they technical visualizations, thought experiments, logic constructions or network graphs. The ‘graphic act’ has been described a being a fundamental activity in human perception, and both scientists and artists have undertaken advanced analyses of the human perception of nature based on visual experiments. Analysis based on reasoning is similarly a graphic act, in the sense that it seeks out patterns of connectivity among socio-spatial agents and entities. Reasoning also depends upon symbolism, which serves to overcome the problem of infinity in nature (a matter that lies at the heart of machine computation). Hence, this chapter introduces some elements in logical reasoning, set theory and computation, and outlines the particular importance of working with symbols. It also provides an introduction to data modelling with graphs, including current advances in graph databases. It provides some ‘tools for thinking’ about knowledge complexity and suggests the potential power in adapting these technologies to organize knowledge of dynamic, complex domains. It also introduces some standard methods for spatial data modelling, including powerful surface network models, which borrow from physical landscape analysis, to support reasoning about knowledge-driven domains.}
}
@article{CARBONARO20101098,
title = {Computer-game construction: A gender-neutral attractor to Computing Science},
journal = {Computers & Education},
volume = {55},
number = {3},
pages = {1098-1111},
year = {2010},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2010.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S0360131510001399},
author = {Mike Carbonaro and Duane Szafron and Maria Cutumisu and Jonathan Schaeffer},
keywords = {Computing Science, Females in Science, Computer game construction},
abstract = {Enrollment in Computing Science university programs is at a dangerously low level. A major reason for this is the general lack of interest in Computing Science by females. In this paper, we discuss our experience with using a computer game construction environment as a vehicle to encourage female participation in Computing Science. Experiments with game construction in grade 10 English classes showed that females enjoyed this activity as much as males and were just as successful. In this paper, we argue that: a) computer game construction is a viable activity for teaching higher-order thinking skills that are essential for Science; b) computer game construction that involves scripting teaches valuable Computing Science abstraction skills; c) this activity is an enjoyable introduction to Computing Science; and d) outcome measures for this activity are not male-dominated in any of the three aspects (higher-order thinking, Computing Science abstraction skills, activity enjoyment). Therefore, we claim that this approach is a viable gender-neutral approach to teaching Computing Science in particular and Science in general that may increase female participation in the discipline.}
}
@incollection{WARE2021425,
title = {Chapter Twelve - Designing Cognitively Efficient Visualizations},
editor = {Colin Ware},
booktitle = {Information Visualization (Fourth Edition)},
publisher = {Morgan Kaufmann},
edition = {Fourth Edition},
pages = {425-456},
year = {2021},
series = {Interactive Technologies},
isbn = {978-0-12-812875-6},
doi = {https://doi.org/10.1016/B978-0-12-812875-6.00012-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128128756000128},
author = {Colin Ware},
keywords = {Interactive visualization design, Visual thinking, Visual thinking design patterns, Visual working memory, Visualization design, Visualization types},
abstract = {A design methodology for producing cognitively efficient visualizations is introduced. The method involves seven steps (1) a high-level cognitive task description; (2) a data inventory; (3) cognitive task refinement; (4) identification of appropriate visualization types; (5) applying visual thinking design patterns (VTDPs); (6) prototype development; and (7) evaluation and design refinement. Most of the chapter is devoted to a set of VTDPs. These are descriptions of interactive visualization methods that have demonstrated value, together with a description of perceptual and cognitive issues relating to their use and guidelines for applicability. VDTPs provide a method for taking into account perceptual and cognitive issues in designing interaction especially with respect to key bottlenecks in the visual thinking process, such as limited visual working memory capacity. They also provide a way of reasoning about semiotic issues in perceptual terms via the concept of the visual query.}
}
@article{NEWMAN20031668,
title = {Frontal and parietal participation in problem solving in the Tower of London: fMRI and computational modeling of planning and high-level perception},
journal = {Neuropsychologia},
volume = {41},
number = {12},
pages = {1668-1682},
year = {2003},
issn = {0028-3932},
doi = {https://doi.org/10.1016/S0028-3932(03)00091-5},
url = {https://www.sciencedirect.com/science/article/pii/S0028393203000915},
author = {Sharlene D Newman and Patricia A Carpenter and Sashank Varma and Marcel Adam Just},
keywords = {Planning, fMRI, Spatial working memory, Problem solving, Tower of London, Computational modeling, 4CAPS},
abstract = {This study triangulates executive planning and visuo-spatial reasoning in the context of the Tower of London (TOL) task by using a variety of methodological approaches. These approaches include functional magnetic resonance imaging (fMRI), functional connectivity analysis, individual difference analysis, and computational modeling. A graded fMRI paradigm compared the brain activation during the solution of problems with varying path lengths: easy (1 and 2 moves), moderate (3 and 4 moves) and difficult (5 and 6 moves). There were three central findings regarding the prefrontal cortex: (1) while both the left and right prefrontal cortices were equally involved during the solution of moderate and difficult problems, the activation on the right was differentially attenuated during the solution of the easy problems; (2) the activation observed in the right prefrontal cortex was highly correlated with individual differences in working memory (measured independently by the reading span task); and (3) different patterns of functional connectivity were observed in the left and right prefrontal cortices. Results obtained from the superior parietal region also revealed left/right differences; only the left superior parietal region revealed an effect of difficulty. These fMRI results converged upon two hypotheses: (1) the right prefrontal area may be more involved in the generation of a plan, whereas the left prefrontal area may be more involved in plan execution; and (2) the right superior parietal region is more involved in attention processes while the left homologue is more of a visuo-spatial workspace. A 4CAPS computational model of the cognitive processes and brain activation in the TOL task integrated these hypothesized mechanisms, and provided a reasonably good fit to the observed behavioral and brain activation data. The multiple research approaches presented here converge on a deepening understanding of the combination of perceptual and conceptual processes in this type of visual problem solving.}
}
@incollection{MAYER2010273,
title = {Problem Solving and Reasoning},
editor = {Penelope Peterson and Eva Baker and Barry McGaw},
booktitle = {International Encyclopedia of Education (Third Edition)},
publisher = {Elsevier},
edition = {Third Edition},
address = {Oxford},
pages = {273-278},
year = {2010},
isbn = {978-0-08-044894-7},
doi = {https://doi.org/10.1016/B978-0-08-044894-7.00487-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780080448947004875},
author = {R.E. Mayer},
keywords = {Convergent thinking, Creativity, Deductive reasoning, Directed thinking, Divergent thinking, Einstellung, Everyday thinking, Expert problem solving, Functional fixedness, Ill-defined problem, Inductive reasoning, Insight, Means-ends analysis, Nonroutine problem, Problem solving, Problem space, Productive thinking, Reasoning, Reproductive thinking, Routine problem, Thinking, Transfer, Well-defined problem},
abstract = {A major goal of education is to help students become effective problem solvers, that is, people who can generate useful and original solutions when they are confronted with problems they have never seen before. This article covers definitions of problem solving and reasoning, types of problems, cognitive processes and types of knowledge in problem solving, rigidity in thinking, problem-solving transfer, the distinction between productive and reproductive thinking, the nature of insight, problem space and search processes, and problem solving in realistic situations.}
}
@article{WEYDMANN2025111173,
title = {Disentangling negative reinforcement, working memory, and deductive reasoning deficits in elevated BMI},
journal = {Progress in Neuro-Psychopharmacology and Biological Psychiatry},
volume = {136},
pages = {111173},
year = {2025},
issn = {0278-5846},
doi = {https://doi.org/10.1016/j.pnpbp.2024.111173},
url = {https://www.sciencedirect.com/science/article/pii/S0278584624002410},
author = {Gibson Weydmann and Igor Palmieri and Reinaldo A.G. Simões and Samara Buchmann and Eduardo Schmidt and Paulina Alves and Lisiane Bizarro},
keywords = {Overweight, Reinforcement Learning, Working Memory, Computational Modelling},
abstract = {Neuropsychological data suggest that being overweight or obese is associated with a tendency to perseverate behavior despite negative feedback. This deficit might be observed due to other cognitive factors, such as working memory (WM) deficits or decreased ability to deduce model-based strategies when learning by trial-and-error. In the present study, a group of subjects with overweight or obesity (Ow/Ob, n = 30) was compared to normal-weight individuals (n = 42) in a modified Reinforcement Learning (RL) task. The task was designed to control WM effects on learning by manipulating cognitive load and to foster model-based learning via deductive reasoning. Computational modelling and analysis were conducted to isolate parameters related to RL mechanisms, WM use, and model-based learning (deduction parameter). Results showed that subjects with Ow/Ob had a higher number of perseverative errors and used a weaker deduction mechanism in their performance than control individuals, indicating impairments in negative reinforcement and model-based learning, whereas WM impairments were not responsible for deficits in RL. The present data suggests that obesity is associated with impairments in negative reinforcement and model-based learning.}
}
@article{ROSS2021100069,
title = {Kinenoetic analysis: Unveiling the material traces of insight},
journal = {Methods in Psychology},
volume = {5},
pages = {100069},
year = {2021},
issn = {2590-2601},
doi = {https://doi.org/10.1016/j.metip.2021.100069},
url = {https://www.sciencedirect.com/science/article/pii/S2590260121000266},
author = {Wendy Ross and Frédéric Vallée-Tourangeau},
keywords = {Insight, Case study, Observation},
abstract = {Research on insight problem solving sets itself a challenging goal: How to explain the origin of a new idea. It compounds the difficulty of this challenge by traditionally seeking to explain the phenomenon in strictly mental terms. Rather, we suggest that thoughts and actions are bound to objects, inviting a granular description of the world within which thinking proceeds. As the reasoner transforms the world, the physical traces of these changes can be mapped in space and time. Not only can the reasoner see these changes, and act upon them, the researcher can develop new inscription devices that captures the trajectory of the creative arc along spatial and temporal coordinates. Kinenoetic is a term we employ to capture the idea that knowledge comes from the movement of objects and that this knowledge is both at the level of the problem-solver and at the level of the researcher. This form of knowledge can only be constructed in problem solving environments where reasoners can manipulate physical elements. A kinenoetic analysis tracks and maps the changes to the object-qua-models of proto solutions, and in the process unveils the physical genesis of new ideas and creativity. Our aim here is to lay out a method for using the objects commonly employed in interactive problem-solving research, tracing the process of thought to elucidate underlying cognitive mechanisms. Thus, the focus turns from the effects of objects on thoughts, to tracing object-thought mutualities as they are enacted and made visible.}
}
@article{KITTAS2010401,
title = {Evolution of the rate of biological aging using a phenotype based computational model},
journal = {Journal of Theoretical Biology},
volume = {266},
number = {3},
pages = {401-407},
year = {2010},
issn = {0022-5193},
doi = {https://doi.org/10.1016/j.jtbi.2010.07.012},
url = {https://www.sciencedirect.com/science/article/pii/S0022519310003619},
author = {Aristotelis Kittas},
keywords = {Evolution, Aging, Computer simulations, Age-structured populations, Modelling},
abstract = {In this work I introduce a simple model to study how natural selection acts upon aging, which focuses on the viability of each individual. It is able to reproduce the Gompertz law of mortality and can make predictions about the relation between the level of mutation rates (beneficial/deleterious/neutral), age at reproductive maturity and the degree of biological aging. With no mutations, a population with low age at reproductive maturity R stabilizes at higher density values, while with mutations it reaches its maximum density, because even for large pre-reproductive periods each individual evolves to survive to maturity. Species with very short pre-reproductive periods can only tolerate a small number of detrimental mutations. The probabilities of detrimental (Pd) or beneficial (Pb) mutations are demonstrated to greatly affect the process. High absolute values produce peaks in the viability of the population over time. Mutations combined with low selection pressure move the system towards weaker phenotypes. For low values in the ratio Pd/Pb, the speed at which aging occurs is almost independent of R, while higher values favor significantly species with high R. The value of R is critical to whether the population survives or dies out. The aging rate is controlled by Pd and Pb and the amount of the viability of each individual is modified, with neutral mutations allowing the system more “room” to evolve. The process of aging in this simple model is revealed to be fairly complex, yielding a rich variety of results.}
}
@article{LI201985,
title = {Government accounting optimization based on computational linguistics},
journal = {Cognitive Systems Research},
volume = {57},
pages = {85-91},
year = {2019},
issn = {1389-0417},
doi = {https://doi.org/10.1016/j.cogsys.2018.10.017},
url = {https://www.sciencedirect.com/science/article/pii/S1389041718304650},
author = {Jiyou Li},
keywords = {Computational linguistics, Accounting optimization, Research},
abstract = {The level of moral development and moral intensity in cognitive psychology will not only affect the ethical behavior of accountants, but also have a direct impact on the quality and level of accounting work. Therefore, in this paper, the ethical behavior of accountants was analyzed from the perspective of cognitive psychology. Computer-aided data mining techniques were introduced, and government accounting risk assessment management of financial accountants was studied. In this paper, the principle of cognitive psychology to measure the ethical level of accountants was first described. The predicament of moral judgments was analyzed and an optimization plan to improve the ethical intention of accountants was proposed. Support Vector Machine classification technology in data mining was studied to explore how to conduct effective and reliable evaluation, so as to provide a scientific basis for decision-making in improving accounting management. After the simulation experiment, it is proved that continuously improving the ethical standards of accountants and strengthening the forecast of accounting risks can continue to optimize the accounting office management.}
}
@article{ROSSITER20083713,
title = {Compromises between feasibility and performance within linear MPC},
journal = {IFAC Proceedings Volumes},
volume = {41},
number = {2},
pages = {3713-3718},
year = {2008},
note = {17th IFAC World Congress},
issn = {1474-6670},
doi = {https://doi.org/10.3182/20080706-5-KR-1001.00627},
url = {https://www.sciencedirect.com/science/article/pii/S147466701639526X},
author = {J.A. Rossiter and Yihang Ding},
keywords = {Constraints, Feasibility, Performance, Computational Efficiency, Contours},
abstract = {This paper explores the issues of feasibility and performance within predictive control. Conventional thinking is that there is typically a trade off between performance and the volume of the feasible region. However, this paper seeks to show that the trade off is often not as stark as might be expected and in fact one can sometimes gain huge amounts in feasibility with an almost negligible loss in performance while using a simple and conventional MPC algorithm.}
}
@article{BELVEDERE201218,
title = {A computational index derived from whole-genome copy number analysis is a novel tool for prognosis in early stage lung squamous cell carcinoma},
journal = {Genomics},
volume = {99},
number = {1},
pages = {18-24},
year = {2012},
issn = {0888-7543},
doi = {https://doi.org/10.1016/j.ygeno.2011.10.006},
url = {https://www.sciencedirect.com/science/article/pii/S0888754311002424},
author = {Ornella Belvedere and Stefano Berri and Rebecca Chalkley and Caroline Conway and Fabio Barbone and Federica Pisa and Kenneth MacLennan and Catherine Daly and Melissa Alsop and Joanne Morgan and Jessica Menis and Peter Tcherveniakov and Kostas Papagiannopoulos and Pamela Rabbitts and Henry M. Wood},
keywords = {Lung cancer, Copy number, Survival, Next-generation sequencing},
abstract = {Squamous cell carcinoma of the lung is remarkable for the extent to which the same chromosomal abnormalities are detected in individual tumours. We have used next generation sequencing at low coverage to produce high resolution copy number karyograms of a series of 89 non-small cell lung tumours specifically of the squamous cell subtype. Because this methodology is able to create karyograms from formalin-fixed paraffin-embedded material, we were able to use archival stored samples for which survival data were available and correlate frequently occurring copy number changes with disease outcome. No single region of genomic change showed significant correlation with survival. However, adopting a whole-genome approach, we devised an algorithm that relates to total genomic damage, specifically the relative ratios of copy number states across the genome. This algorithm generated a novel index, which is an independent prognostic indicator in early stage squamous cell carcinoma of the lung.}
}
@article{YANG202075,
title = {Local temporal-spatial multi-granularity learning for sequential three-way granular computing},
journal = {Information Sciences},
volume = {541},
pages = {75-97},
year = {2020},
issn = {0020-0255},
doi = {https://doi.org/10.1016/j.ins.2020.06.020},
url = {https://www.sciencedirect.com/science/article/pii/S0020025520306009},
author = {Xin Yang and Yingying Zhang and Hamido Fujita and Dun Liu and Tianrui Li},
keywords = {Three-way granular computing, Sequential three-way decision, Local neighborhood, Temporal-spatial, Multi-granularity},
abstract = {Based on multiple levels of granularity, the notion of sequential three-way granular computing focuses on a multiple stages of thinking, problem-solving, and information processing in threes. This paper interprets, represents, and implements sequential three-way granular computing by a framework of temporal-spatial multi-granularity learning, which is described with the temporality of data and the spatiality of parameters. In real-world decision-making, such a sequential approach is useful to make faster decisions for some objects with the lower cost of decision process and the acceptable accuracy when information is insufficient or unavailable. However, the cost of time-consuming computation for hierarchical multilevel granularity is our concern. To address this issue, we utilize a local strategy to accelerate a sequence of neighborhood-based granulation induced by Gaussian kernel function. Subsequently, local three-way decision rules are investigated based on the Bayesian minimum risk criterion. Moreover, by the construction of a novel local trisection model, we propose a local sequential approach of three-way granular computing under a temporal-spatial multilevel granular structure. Finally, a series of comparative experiments between global and local perspectives is carried out to verify the effectiveness of our proposed models.}
}
@article{ARCHAMBAULT2024102865,
title = {Ethical dimensions of algorithmic literacy for college students: Case studies and cross-disciplinary connections},
journal = {The Journal of Academic Librarianship},
volume = {50},
number = {3},
pages = {102865},
year = {2024},
issn = {0099-1333},
doi = {https://doi.org/10.1016/j.acalib.2024.102865},
url = {https://www.sciencedirect.com/science/article/pii/S0099133324000260},
author = {Susan Gardner Archambault and Shalini Ramachandran and Elisa Acosta and Sheree Fu},
keywords = {Algorithmic literacy, Information literacy, Algorithmic bias, AI ethics, Algorithmic fairness, Computer science education},
abstract = {This article addresses three key questions related to the ethical facets of algorithmic literacy. First, it synthesizes existing literature to identify six core ethical components, including bias, privacy, transparency, accountability, accuracy, and non-maleficence. Second, a crosswalk maps the intersections of these principles across the Association of College and Research Libraries' Framework for Information Literacy for Higher Education and the Association of Computing Machinery's Code of Ethics and Professional Conduct and Joint Statement on Principles for Responsible Algorithmic Systems. This analysis reveals significant overlap on issues like unfairness and transparency, helping prioritize topics for instruction. Finally, case studies showcase pedagogical strategies for teaching ethical considerations, informed by the crosswalk. Workshops for diverse undergraduates and computer science students employed reallife instances of algorithmic bias to prompt reflection on unintended harm, contestability, and responsible development. Pre-post surveys indicated expanded critical perspectives after the interventions. By systematically examining shared values and testing instructional approaches, this study provides practical tools to shape ethical thinking on algorithms. It also demonstrates promising practices for responsibly advancing algorithmic literacy across disciplines. Ultimately, fostering interdisciplinary awareness and multipronged educational initiatives can empower students to question algorithmic authority and biases.}
}
@article{RONAYNE2021318,
title = {Evaluating the sunk cost effect},
journal = {Journal of Economic Behavior & Organization},
volume = {186},
pages = {318-327},
year = {2021},
issn = {0167-2681},
doi = {https://doi.org/10.1016/j.jebo.2021.03.029},
url = {https://www.sciencedirect.com/science/article/pii/S0167268121001293},
author = {David Ronayne and Daniel Sgroi and Anthony Tuckwell},
keywords = {Sunk cost effect, Sunk cost fallacy, Endowment effect, Cognitive ability, Psychological scales, Scale validation},
abstract = {We provide experimental evidence of behavior consistent with the sunk cost effect. Subjects who earned a lottery via a real-effort task were given an opportunity to switch to a dominant lottery; 23% chose to stick with their dominated lottery. The endowment effect accounts for roughly only one third of the effect. Subjects’ capacity for cognitive reflection is a significant determinant of sunk cost behavior. We also find stocks of knowledge or experience (crystallized intelligence) predict sunk cost behavior, rather than algorithmic thinking (fluid intelligence) or the personality trait of openness. We construct and validate a scale, the “SCE-8”, which encompasses many resources individuals can spend, and offers researchers an efficient way to measure susceptibility to the sunk cost effect.}
}
@article{OMHOLT2003107,
title = {Eberhard O. Voit, Computational Analysis of Biochemical Systems. A Practical Guide for Biochemists and Molecular Biologists, Cambridge University Press, 2000, 531 pages (ISBN 0-521-78579-0; paperback)},
journal = {Mathematical Biosciences},
volume = {181},
number = {1},
pages = {107-109},
year = {2003},
issn = {0025-5564},
doi = {https://doi.org/10.1016/S0025-5564(02)00153-0},
url = {https://www.sciencedirect.com/science/article/pii/S0025556402001530},
author = {Stig W Omholt}
}
@article{MUSSO2015267,
title = {A single dual-stream framework for syntactic computations in music and language},
journal = {NeuroImage},
volume = {117},
pages = {267-283},
year = {2015},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2015.05.020},
url = {https://www.sciencedirect.com/science/article/pii/S1053811915004000},
author = {Mariacristina Musso and Cornelius Weiller and Andreas Horn and Volkmer Glauche and Roza Umarova and Jürgen Hennig and Albrecht Schneider and Michel Rijntjes},
abstract = {This study is the first to compare in the same subjects the specific spatial distribution and the functional and anatomical connectivity of the neuronal resources that activate and integrate syntactic representations during music and language processing. Combining functional magnetic resonance imaging with functional connectivity and diffusion tensor imaging-based probabilistic tractography, we examined the brain network involved in the recognition and integration of words and chords that were not hierarchically related to the preceding syntax; that is, those deviating from the universal principles of grammar and tonal relatedness. This kind of syntactic processing in both domains was found to rely on a shared network in the left hemisphere centered on the inferior part of the inferior frontal gyrus (IFG), including pars opercularis and pars triangularis, and on dorsal and ventral long association tracts connecting this brain area with temporo-parietal regions. Language processing utilized some adjacent left hemispheric IFG and middle temporal regions more than music processing, and music processing also involved right hemisphere regions not activated in language processing. Our data indicate that a dual-stream system with dorsal and ventral long association tracts centered on a functionally and structurally highly differentiated left IFG is pivotal for domain–general syntactic competence over a broad range of elements including words and chords.}
}
@article{LOWE20219898316,
title = {In-Depth Computational Analysis of Natural and Artificial Carbon Fixation Pathways},
journal = {BioDesign Research},
volume = {2021},
pages = {9898316},
year = {2021},
issn = {2693-1257},
doi = {https://doi.org/10.34133/2021/9898316},
url = {https://www.sciencedirect.com/science/article/pii/S2693125724000566},
author = {Hannes Löwe and Andreas Kremling},
abstract = {In the recent years, engineering new-to-nature CO2- and C1-fixing metabolic pathways made a leap forward. New, artificial pathways promise higher yields and activity than natural ones like the Calvin-Benson-Bassham (CBB) cycle. The question remains how to best predict their in vivo performance and what actually makes one pathway “better” than another. In this context, we explore aerobic carbon fixation pathways by a computational approach and compare them based on their specific activity and yield on methanol, formate, and CO2/H2 considering the kinetics and thermodynamics of the reactions. Besides pathways found in nature or implemented in the laboratory, this included two completely new cycles with favorable features: the reductive citramalyl-CoA cycle and the 2-hydroxyglutarate-reverse tricarboxylic acid cycle. A comprehensive kinetic data set was collected for all enzymes of all pathways, and missing kinetic data were sampled with the Parameter Balancing algorithm. Kinetic and thermodynamic data were fed to the Enzyme Cost Minimization algorithm to check for respective inconsistencies and calculate pathway-specific activities. The specific activities of the reductive glycine pathway, the CETCH cycle, and the new reductive citramalyl-CoA cycle were predicted to match the best natural cycles with superior product-substrate yield. However, the CBB cycle performed better in terms of activity compared to the alternative pathways than previously thought. We make an argument that stoichiometric yield is likely not the most important design criterion of the CBB cycle. Still, alternative carbon fixation pathways were paretooptimal for specific activity and product-substrate yield in simulations with C1 substrates and CO2/H2 and therefore hold great potential for future applications in Industrial Biotechnology and Synthetic Biology.}
}
@article{ARUN2009S1116,
title = {P03-117 A bedside schizophrenia thought disorder scale},
journal = {European Psychiatry},
volume = {24},
pages = {S1116},
year = {2009},
note = {17th EPA Congress - Lisbon, Portugal, January 2009, Abstract book},
issn = {0924-9338},
doi = {https://doi.org/10.1016/S0924-9338(09)71349-5},
url = {https://www.sciencedirect.com/science/article/pii/S0924933809713495},
author = {C.P. Arun},
abstract = {Present classification systems for thought disorder lack consistency and require one to remember long-winded definitions limiting their use to research settings. As an extension of recent work in this area (World Congress, 2008), we classify the characteristic thought disorder patterns seen in schizophrenia according to the location of the lesion in notional "threads" of mental computational processes that string speech together. These threads must take both semantics and syntax into consideration in performing their function. When we speak - just as when we write - there is a natural hierarchy topic thread (the topic of the ‘essay’) and multiples of paragraph threads, sentence threads, clause threads, word threads and phoneme threads. Intuitively, we grade the severity of thought disorder depending upon whether a particular thread gets stuck (S), reconnects abnormally (R) or is absent altogether: I.paragraph thread R: Disjointed sentences S: Circumstantiality;II.topic threadR: Tangentiality S: Preoccupatory thinking;III.sentence threads R: Knight's move thinking S: Clause perseveration;IV.clause threads R: Word salad S: Word perseveration, fusion;V.word threads R: Incoherent sounds/ neologisms/ paraphasias S: Phoneme/syllable perseveration;VI.phoneme threads - Failure of production: Mutism.Of course, one must record all the lesions that are present at any given time. This scale incorporates a intuitive progression from mild to severe thought disorder in Schizophrenia. Using the STDS would allow the straightforward ‘bedside’ quantification of the severity of thought disorder and enforce discipline into the thought assessment section of the Mental State Examination.}
}
@article{BLACK202010653,
title = {A revolution in biochemistry and molecular biology education informed by basic research to meet the demands of 21st century career paths},
journal = {Journal of Biological Chemistry},
volume = {295},
number = {31},
pages = {10653-10661},
year = {2020},
issn = {0021-9258},
doi = {https://doi.org/10.1074/jbc.AW120.011104},
url = {https://www.sciencedirect.com/science/article/pii/S0021925817501040},
author = {Paul N. Black},
keywords = {biochemistry, molecular biology, teaching, learning, primary research, leadership, environment, inclusive excellence, STEM education, biochemistry and molecular biology teaching and learning},
abstract = {The National Science Foundation estimates that 80% of the jobs available during the next decade will require math and science skills, dictating that programs in biochemistry and molecular biology must be transformative and use new pedagogical approaches and experiential learning for careers in industry, research, education, engineering, health-care professions, and other interdisciplinary fields. These efforts require an environment that values the individual student and integrates recent advances from the primary literature in the discipline, experimentally directed research, data collection and analysis, and scientific writing. Current trends shaping these efforts must include critical thinking, experimental testing, computational modeling, and inferential logic. In essence, modern biochemistry and molecular biology education must be informed by, and integrated with, cutting-edge research. This environment relies on sustained research support, commitment to providing the requisite mentoring, access to instrumentation, and state-of-the-art facilities. The academic environment must establish a culture of excellence and faculty engagement, leading to innovation in the classroom and laboratory. These efforts must not lose sight of the importance of multidimensional programs that enrich science literacy in all facets of the population, students and teachers in K-12 schools, nonbiochemistry and molecular biology students, and other stakeholders. As biochemistry and molecular biology educators, we have an obligation to provide students with the skills that allow them to be innovative and self-reliant. The next generation of biochemistry and molecular biology students must be taught proficiencies in scientific and technological literacy, the importance of the scientific discourse, and skills required for problem solvers of the 21st century.}
}
@article{LIU2006207,
title = {Evolutionary design in a multi-agent design environment},
journal = {Applied Soft Computing},
volume = {6},
number = {2},
pages = {207-220},
year = {2006},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2005.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S156849460500013X},
author = {Hong Liu and Mingxi Tang},
keywords = {Multi-agent system, Evolutionary computing, Generic algorithm, Computer-aided design, Creative design},
abstract = {This paper presents a novel evolutionary design approach in a multi-agent design environment. Multi-agent system architecture offers a promising framework for dynamically managing cooperative agents in a distributed environment while the tree structure based generic algorithm provides a foundation for supporting evolutionary and innovative design abilities. Design is a complex knowledge discovery process. Creative design is a human trait that is not easily converted into a computational tool. Rather than to implement the innovative design by computers, this environment is used to stimulate the imagination of designers and extend their thinking space. It wants to explore a feasible and useful evolutionary approach in a distributed environment that will give the designers concrete help for the creative designs. This approach is illustrated by a mobile phone design example, which used binary algebraic expression tree to form sketch shapes and a feature based product tree to produce component combination choices. Because evolution is guided by human selectors, the evolutionary algorithm is not complex. It shows that approach is able to generate some creative solutions, demonstrating the power of explorative evolution.}
}
@article{WILLIAMS2023145,
title = {Stabilizing expectations when shifting from analytical to intuitive reasoning: The role of prediction errors in reasoning},
journal = {Cortex},
volume = {161},
pages = {145-153},
year = {2023},
issn = {0010-9452},
doi = {https://doi.org/10.1016/j.cortex.2023.02.005},
url = {https://www.sciencedirect.com/science/article/pii/S0010945223000412},
author = {Chad C. Williams and Cameron D. Hassall and Olave E. Krigolson},
keywords = {Reasoning, Prediction errors, Cognitive control, Theta, EEG},
abstract = {As humans, we rely on intuitive reasoning for most of our decisions. However, when there is a novel or atypical decision to be made, we must rely on a slower and more deliberative thought process—analytical reasoning. As we gain experience with these novel or atypical decisions, our reasoning shifts from analytical to intuitive, which parallels a reduction in the need for cognitive control. Here, we sought to confirm this claim by employing electroencephalographic (EEG) measures of cognitive control as participants performed a simple perceptual decision-making task. Specifically, we had participants categorize “blobs” into families based on their visual attributes so we could examine how their reasoning changed with learning. In a key manipulation, halfway through the experiment we introduced novel blob families to categorize, thus temporarily increasing the need for analytical reasoning (i.e., cognitive control). Congruent with past research, we focused our EEG analyses on frontal theta activity as it has been linked to cognitive control and analytical thinking. As hypothesized, we found a transition from analytical to intuitive decision-making systems with learning as indexed by a decrease in frontal theta power. Further, when the novel blobs were introduced at the midpoint of the experiment, we found that decisions about these stimuli recruited analytical reasoning as indicated by increased theta power in comparison to decisions about well-practiced stimuli. We propose our findings to reflect prediction errors to decision demands—a monitoring process that determines whether our expectations of demands are met. Shifting from analytical to intuitive reasoning thus reflects the stabilization of our expectations of decision demands, which can be violated with unexpected demands when encountering novel stimuli.}
}
@article{TAY20211,
title = {Modelability across time as a signature of identity construction on YouTube},
journal = {Journal of Pragmatics},
volume = {182},
pages = {1-15},
year = {2021},
issn = {0378-2166},
doi = {https://doi.org/10.1016/j.pragma.2021.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S0378216621002307},
author = {Dennis Tay},
keywords = {Identity construction, Social media, LIWC, Modelability, ARIMA, Time series analysis},
abstract = {Linguistic self-representation and identity construction on social media have attracted much scholarly attention. However, relevant studies tend to overlook the temporal dimension of social media, potentially systematic patterning of linguistic behavior across time, and the attendant implications of such a temporal perspective on identity. Combining an automated lexical tool (LIWC) and the Box–Jenkins method of statistical time series analysis, this paper shows how the ‘modelability’ of linguistic choices across time can be interpreted as signatures of identity construction and complement existing frameworks for identity analysis. Two levels of modelability are discussed—the availability of a well-fitting time series model as evidence of temporal patterning, and specific parameters of that model interpreted in context. These are demonstrated with a case study of the construction of ‘amateur expertise’ over 109 consecutive makeup tutorial videos on the popular YouTube channel ‘Nikkiestutorials’. Results show that the linguistic display of ‘analytical thinking’ reflects a strategy of ‘short term momentum’, the display of ‘clout’ and ‘authenticity’ a strategy of ‘short term restoration’, while the display of ‘emotional tone’ fluctuates randomly across time. The approach is further discussed in terms of its general principles and potential applications in other contexts of identity and related research.}
}
@article{LORE2024105149,
title = {Using multiple, dynamically linked representations to develop representational competency and conceptual understanding of the earthquake cycle},
journal = {Computers & Education},
volume = {222},
pages = {105149},
year = {2024},
issn = {0360-1315},
doi = {https://doi.org/10.1016/j.compedu.2024.105149},
url = {https://www.sciencedirect.com/science/article/pii/S0360131524001635},
author = {Christopher Lore and Hee-Sun Lee and Amy Pallant and Jie Chao},
keywords = {Teaching/learning strategies, Simulations, Pedagogical issues, Applications in subject areas},
abstract = {Using computational methods to produce and interpret multiple scientific representations is now a common practice in many science disciplines. Research has shown students have difficulty in moving across, connecting, and sensemaking from multiple representations. There is a need to develop task-specific representational competencies for students to reason and conduct scientific investigations using multiple representations. In this study, we focus on three representational competencies: 1) linking between representations, 2) disciplinary sensemaking from multiple representations, and 3) conceptualizing domain-relevant content derived from multiple representations. We developed a block code-based computational modeling environment with three different representations and embedded it within an online activity for students to carry out investigations around the earthquake cycle. The three representations include a procedural representation of block codes, a geometric representation of land deformation build-up, and a graphical representation of deformation build-up over time. We examined the extent of students' representational competencies and which competencies are most correlated with students’ future performance in a computationally supported geoscience investigation. Results indicate that a majority of the 431 students showed at least some form of representational competence. However, a relatively small number of students showed sophisticated levels of linking, sensemaking, and conceptualizing from the representations. Five of seven representational competencies, the most prominent being code sensemaking (η2 = 0.053, p < 0.001), were significantly correlated to student performance on a summative geoscience investigation.}
}
@article{JIANG201814,
title = {Computational intelligence techniques for maximum power point tracking in PV systems: A review},
journal = {Renewable and Sustainable Energy Reviews},
volume = {85},
pages = {14-45},
year = {2018},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2018.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S1364032118300054},
author = {Lian L. Jiang and R. Srivatsan and Douglas L. Maskell},
keywords = {Maximum power point tracking, PV system, Computational intelligence algorithm, Heuristic algorithm, Global tracking, Partial shading},
abstract = {Maximum power point (MPP) tracking (MPPT) is an important technique for maximizing the power extraction from photovoltaic (PV) systems under varying climatic conditions. In an array of PV modules it is possible to observe multiple peaks in the power versus voltage (P-V) curve due to the current versus voltage (I–V) PV cell mismatch caused by differences in the received irradiance, such as occurs during partial shading. In these circumstances, the ability of the MPPT devices to track the global MPP of the PV array directly influences the system efficiency. In the literature, various MPPT techniques have been proposed. Among them, computational intelligence (CI) algorithm based MPPT methods have demonstrated the ability to find the global MPP. This paper presents a detailed and specific review of CI- based MPPT techniques. Each method type is classified into one of several subcategories according to its application strategy. The various ways of applying CI into MPPTs are analyzed in detail. The advantages and disadvantages of each method are discussed and compared. The purpose of this study is to provide a compendium on CI-based MPPT techniques for users to understand and select an appropriate method based on application requirements and system constraints.}
}
@article{ECONOMOU1994131,
title = {Activities, issues and perspectives in computational physics: a view from Greece},
journal = {Computational Materials Science},
volume = {2},
number = {1},
pages = {131-136},
year = {1994},
issn = {0927-0256},
doi = {https://doi.org/10.1016/0927-0256(94)90055-8},
url = {https://www.sciencedirect.com/science/article/pii/0927025694900558},
author = {E.N. Economou}
}
@article{GOLDBERG2012261,
title = {An efficient tree-based computation of a metric comparable to a natural diffusion distance},
journal = {Applied and Computational Harmonic Analysis},
volume = {33},
number = {2},
pages = {261-281},
year = {2012},
issn = {1063-5203},
doi = {https://doi.org/10.1016/j.acha.2011.12.001},
url = {https://www.sciencedirect.com/science/article/pii/S1063520311001266},
author = {Maxim J. Goldberg and Seonja Kim},
keywords = {Tree, Diffusion, Distance, Metric},
abstract = {Using diffusion to define distances between points on a manifold (or a sampled data set) has been successfully employed in various applications such as data organization and approximately isometric embedding of high dimensional data in low dimensional Euclidean space. Recently, P. Jones has proposed a diffusion distance which is both intuitively appealing and scales appropriately with increasing time. In the first part of our paper, we present an efficient tree-based approach to computing an approximation to Jonesʼs diffusion distance. We also show our approximation is comparable to Jonesʼs distance. Neither Jonesʼs distance, nor our approximation, satisfies the triangle inequality; in particular, in the case of heat flow on Rn, Jonesʼs separation distance gives a scaled square of the Euclidean distance. In the second part of our paper, we present a general construction to obtain an “almost” metric from a general distance. We also discuss a numerical procedure to implement our construction. Additionally, we show that in the case of heat flow on Rn, we recover (scaled) Euclidean distance from Jonesʼs distance.}
}
@article{THAKIRABED20232293,
title = {The computation intelligent system of role of parental leadership in organizational familiarity in Iraqi Airways employees},
journal = {Materials Today: Proceedings},
volume = {80},
pages = {2293-2301},
year = {2023},
note = {SI:5 NANO 2021},
issn = {2214-7853},
doi = {https://doi.org/10.1016/j.matpr.2021.06.318},
url = {https://www.sciencedirect.com/science/article/pii/S221478532104709X},
author = {Marwan {Thakir Abed}},
keywords = {Parental leadership, Organizational familiarity, Empowerment, Iraqi Airways},
abstract = {The research aimed to know the effect of parental leadership represented by (benevolent leadership, moral leadership, and authoritarian leadership) found in the research sample, in the organizational familiarity (employee morale, empowerment, and objective merit), the research relied on the questionnaire as a key instrument to collect the necessary data to meet its goal. As (60) forms were distributed to find the level of availability of parental leadership and organizational harmony, while (56) forms were retrieved. A set of statistical methods were used, represented by normal distribution, stability factor (Alpha Kronbach), reliability, arithmetic mean, standard deviation, and coefficient Simple correlation Pearson, multiple regression coefficient. The results showed that there is a positive correlation and effect relationship with statistically significant between parental leadership with its dimensions (benevolent leadership, moral leadership, authoritarian leadership) and organizational affiliation with its dimensions (employee morale, empowerment, and merit's Objectivity), and the research showed a direct impact relationship between parental leadership and the organizational affiliation of the studied sample. Accordingly, the research concluded that the study sample should pay attention to the nature and type of empowering workers in order to give them freedom and independence in making decisions regarding the tasks assigned to them.}
}
@incollection{BUNGE1980155,
title = {CHAPTER 7 - Thinking and Knowing},
editor = {MARIO BUNGE},
booktitle = {The Mind–Body Problem},
publisher = {Pergamon},
pages = {155-173},
year = {1980},
isbn = {978-0-08-024720-5},
doi = {https://doi.org/10.1016/B978-0-08-024720-5.50012-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780080247205500128},
author = {MARIO BUNGE},
abstract = {Publisher Summary
This chapter discusses the process of thinking and knowing. Forming concepts, propositions, problems, and directions are examples of thinking. Thinking can be visual, verbal, or abstract. It can be chaotic or orderly, creative, or routine. Thinking of any kind is an activity of some plastic neural systems. Of all mental activities, thinking is probably the one most affected by chemical changes and changes in basic properties of neurons. For example, humans unable to oxidize the amino acid phenylalanine cannot think, thyroid hypofunction produces cretinism, and normal subjects cannot think straight when in states of extreme stress, which are often states of hormonal imbalance, or when under the action of psychotropic drugs. The chapter also discusses the concept of cognition. All cognition is learned but not every learned item is of a cognitive nature. All cognition is cognition of some object, concrete or conceptual, and it consists in some information about its object—complete or partial, true or false. Cognition can be behavioral, perceptual, or conceptual.}
}
@incollection{MARON1965118,
title = {On Cybernetics, Information Processing, and Thinking},
editor = {Norbert Wiener and J.P. Schadé},
series = {Progress in Brain Research},
publisher = {Elsevier},
volume = {17},
pages = {118-138},
year = {1965},
issn = {0079-6123},
doi = {https://doi.org/10.1016/S0079-6123(08)60158-2},
url = {https://www.sciencedirect.com/science/article/pii/S0079612308601582},
author = {M.E. Maron},
abstract = {Publisher Summary
It is the purpose of this chapter to examine the origins, development, and present status of those key cybernetic notions that provide an information-flow framework within which to attack one aspect of the question of how a person thinks— that is,.the question of the information mechanisms and processes that underlie and are correlated with thinking. After an introductory survey of the scope and ramifications of the information sciences, the cybernetic way of looking at the information processing in the nervous system is examined, so as to see in what sense it provides new and sharp tools of analysis for the neurophysiologist. With this as background, the problem of artificial intelligence is considered and with that the logical and linguistic difficulties in talking about the relationship between thinking and brain activity. An information-flow model of an artificial brain mechanism is described whose activity; it is argued is the correlate to activity, such as perceiving, learning, thinking, knowing, etc. This leads finally to a consideration of the impact of these notions on theoretical neurophysiology and its attempt to frame suitable hypotheses and on epistemology that is concerned with the logical analysis of measures, methods, and techniques, which can justify the activity of knowing.}
}
@article{MACHADO2023101290,
title = {A multiple criteria framework to assess learning methodologies},
journal = {Thinking Skills and Creativity},
volume = {48},
pages = {101290},
year = {2023},
issn = {1871-1871},
doi = {https://doi.org/10.1016/j.tsc.2023.101290},
url = {https://www.sciencedirect.com/science/article/pii/S1871187123000603},
author = {Rafaela Heloisa Carvalho Machado and Samuel Vieira Conceição and Renata Pelissari and Sarah Ben Amor and Thiago Lombardi Resende},
keywords = {Active learning methodologies, Skills, Multiple criteria decision making, MCDA, MCDM},
abstract = {New job skills required by the professional market have been causing significant changes in the learning process of undergraduate students. Different learning methodologies can be adopted to assist in the development of those skills, and the process of choosing the most suitable learning methodology for each situation may be complex, involving multiple and conflicting criteria. In order to support the choice of learning methodologies for the development of the “4C skills”, i.e, collaboration, communication, creativity and critical thinking, we propose a new framework based on the multiple criteria decision-making approach PROMETHEE II (Preference Ranking Organization Method for Enrichment of Evaluations), considering as criteria the “4C skills”, student motivation, level of learning, student comfort, decision-making capacity and time required for class preparation. Passive methods and active learning methodologies such as Guided Reciprocal Peer Questioning (GRPQ), Think-Pair-Share (TPS), and Problem Based Learning (PBL) are compared. Each methodology was applied to three groups of students of Industrial Engineering of a Brazilian University, totaling 138 students. As a result, PBL obtained the best assessment in the three groups, followed by GRPQ. The proposed framework validates the assessment of learning methodologies, providing a structure and guideline for its replication in other educational institutions.}
}
@article{WANG20231225,
title = {Parameterization Design of 3D Fractal Images in Packaging Design Based on Genetic Algorithm},
journal = {Procedia Computer Science},
volume = {228},
pages = {1225-1232},
year = {2023},
note = {3rd International Conference on Machine Learning and Big Data Analytics for IoT Security and Privacy},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.11.104},
url = {https://www.sciencedirect.com/science/article/pii/S187705092301935X},
author = {Jinxia Wang},
keywords = {Genetic Algorithm, Packaging Design, 3D Fractal Image, NSGA - II},
abstract = {Packaging design, as an important element in product appearance, can directly affect customers' sensory perception of the product. Many universities even offer packaging design majors, which mainly use natural science and aesthetic knowledge to promote product sales. However, many old brands remain complacent and their packaging design still adopts traditional thinking, which to some extent affects their sales. Therefore, this article decided to use genetic algorithms as a tool to parameterize the 3D fractal images in packaging design, aiming to create more creative and eye-catching packaging designs. At the end of this article, an experiment was conducted on two branches of a certain brand. Branch 1 tried out the new design provided in this article, while Branch 2 continued to use the original design. After Branch 1 fully adopted the design, sales skyrocketed, from the original daily sales of 50-60 units to 70-85 units. Branch 2 remained unchanged, with a sharp contrast.}
}
@article{CHINTA20248181,
title = {Cascade reactions of HDDA-benzynes with tethered cyclohexadienones: strain-driven events originating from ortho-annulated benzocyclobutenes††Electronic supplementary information (ESI) available. CCDC 2302618–2302621. For ESI and crystallographic data in CIF or other electronic format see DOI: https://doi.org/10.1039/d4sc00571f},
journal = {Chemical Science},
volume = {15},
number = {21},
pages = {8181-8189},
year = {2024},
issn = {2041-6520},
doi = {https://doi.org/10.1039/d4sc00571f},
url = {https://www.sciencedirect.com/science/article/pii/S2041652024006813},
author = {Bhavani Shankar Chinta and Dorian S. Sneddon and Thomas R. Hoye},
abstract = {Intramolecular net [2 + 2] cycloadditions between benzyne intermediates and an electron-deficient alkene to give benzocyclobutene intermediates are relatively rare. Benzynes are electrophilic and generally engage nucleophiles or electron-rich π-systems. We describe here reactions in which an alkene of a tethered enone traps thermally generated benzynes in a variety of interesting ways. The number of atoms that link the benzyne to C4 of a cyclohexa-2,5-dienone induces varying amounts of strain in the intermediates and products. This leads to a variety of different reaction outcomes by way of various strain-releasing events that are mechanistically intriguing. This work demonstrates an underappreciated class of strain that originates from the adjacent fusion of two rings to both C1–C2 and C2–C3 of a benzenoid ring – i.e. ‘ortho-annulation strain’. DFT computations shed considerable light on the mechanistic diversions among various reaction pathways as well as allow more fundamental evaluation of the strain in a homologous series of ortho-annulated carbocycles.}
}
@article{MARRET2025,
title = {Turning the kaleidoscope: Innovations shaping the future of clinical trial design},
journal = {Cancer Cell},
year = {2025},
issn = {1535-6108},
doi = {https://doi.org/10.1016/j.ccell.2025.02.019},
url = {https://www.sciencedirect.com/science/article/pii/S1535610825000716},
author = {Grégoire Marret and Mercedes Herrera and Lillian L. Siu},
abstract = {Current clinical trials are based on rigid designs and drug-centric approaches that can stifle flexibility and innovation. With advances in molecular biology and technology, there is an urgent call to revitalize trial designs to meet these evolving demands. We propose a reshaped, prismatic vision of clinical trials combining different knowledge layers, synergized with modern computational approaches. This paradigm based on iterative learning will enable a more adaptive and precise framework for oncology drug development.}
}
@article{BERRUTO2024858,
title = {Engineering agricultural soil microbiomes and predicting plant phenotypes},
journal = {Trends in Microbiology},
volume = {32},
number = {9},
pages = {858-873},
year = {2024},
issn = {0966-842X},
doi = {https://doi.org/10.1016/j.tim.2024.02.003},
url = {https://www.sciencedirect.com/science/article/pii/S0966842X2400043X},
author = {Chiara A. Berruto and Gozde S. Demirer},
keywords = {rhizosphere engineering, plant microbiome, machine learning, community modeling, host–microbe interactions, microbiome-associated phenotype},
abstract = {Plant growth-promoting rhizobacteria (PGPR) can improve crop yields, nutrient use efficiency, plant tolerance to stressors, and confer benefits to future generations of crops grown in the same soil. Unlocking the potential of microbial communities in the rhizosphere and endosphere is therefore of great interest for sustainable agriculture advancements. Before plant microbiomes can be engineered to confer desirable phenotypic effects on their plant hosts, a deeper understanding of the interacting factors influencing rhizosphere community structure and function is needed. Dealing with this complexity is becoming more feasible using computational approaches. In this review, we discuss recent advances at the intersection of experimental and computational strategies for the investigation of plant–microbiome interactions and the engineering of desirable soil microbiomes.}
}
@article{ALTARABICHI2023118528,
title = {Fast Genetic Algorithm for feature selection — A qualitative approximation approach},
journal = {Expert Systems with Applications},
volume = {211},
pages = {118528},
year = {2023},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2022.118528},
url = {https://www.sciencedirect.com/science/article/pii/S0957417422016049},
author = {Mohammed Ghaith Altarabichi and Sławomir Nowaczyk and Sepideh Pashami and Peyman Sheikholharam Mashhadi},
keywords = {Feature selection, Evolutionary computation, Genetic Algorithm, Particle Swarm Intelligence, Fitness approximation, Meta-model, Optimization},
abstract = {Evolutionary Algorithms (EAs) are often challenging to apply in real-world settings since evolutionary computations involve a large number of evaluations of a typically expensive fitness function. For example, an evaluation could involve training a new machine learning model. An approximation (also known as meta-model or a surrogate) of the true function can be used in such applications to alleviate the computation cost. In this paper, we propose a two-stage surrogate-assisted evolutionary approach to address the computational issues arising from using Genetic Algorithm (GA) for feature selection in a wrapper setting for large datasets. We define “Approximation Usefulness” to capture the necessary conditions to ensure correctness of the EA computations when an approximation is used. Based on this definition, we propose a procedure to construct a lightweight qualitative meta-model by the active selection of data instances. We then use a meta-model to carry out the feature selection task. We apply this procedure to the GA-based algorithm CHC (Cross generational elitist selection, Heterogeneous recombination and Cataclysmic mutation) to create a Qualitative approXimations variant, CHCQX. We show that CHCQX converges faster to feature subset solutions of significantly higher accuracy (as compared to CHC), particularly for large datasets with over 100K instances. We also demonstrate the applicability of the thinking behind our approach more broadly to Swarm Intelligence (SI), another branch of the Evolutionary Computation (EC) paradigm with results of PSOQX, a qualitative approximation adaptation of the Particle Swarm Optimization (PSO) method. A GitHub repository with the complete implementation is available.22https://github.com/Ghaith81/Fast-Genetic-Algorithm-For-Feature-Selection.}
}
@article{ZHOU2025101900,
title = {Parking Vehicle-Assisted Task Offloading in Edge Computing: A dynamic multi-objective evolutionary algorithm with multi-strategy fusion response},
journal = {Swarm and Evolutionary Computation},
volume = {94},
pages = {101900},
year = {2025},
issn = {2210-6502},
doi = {https://doi.org/10.1016/j.swevo.2025.101900},
url = {https://www.sciencedirect.com/science/article/pii/S2210650225000586},
author = {Yingbo Zhou and Zheng-Yi Chai and Ya-Lun Li and Jun-Jie Li},
keywords = {Vehicle edge computing, Dynamic multi-objective optimization, Evolutionary algorithms, Computational offloading, Vehicle collaboration},
abstract = {Vehicle-edge computing, as a promising paradigm, is employed to support applications that require low latency and high computational capability. In this study, we consider the idle resources of the surrounding parked vehicles (PVs) and roadside units (RSUs) as service providers to enhance the performance of User Equipment (UE). We propose a joint offloading architecture that uses parked vehicles. Additionally, owing to the dynamic and uncertain nature of the environment, we model computation offloading as a dynamic multi-objective optimization problem to simultaneously optimize the latency and energy consumption of UE applications. In this study, we propose a dynamic multi-objective evolutionary algorithm with a multi-strategy fusion response (DMOEA/D-MSFR). Specifically, we introduce a population center positioning strategy and a learnable prediction mechanism using Long Short-Term Memory (LSTM) in DMOEA-MSFR, which divides the prediction optimization process into two stages and exhibits a rapid response to environmental changes. In the static optimization phase, an adaptive weight vector adjustment strategy is employed, which significantly aids in the distribution and diversity of the solutions. Comprehensive experiments demonstrate that our proposed framework balances the trade-off between latency and energy consumption, and the convergence, feasibility, and diversity of the non-dominated solutions obtained.}
}
@article{TEO2024102655,
title = {Age-appropriate adaptation of creativity tasks for infants aged 12–24 months},
journal = {MethodsX},
volume = {12},
pages = {102655},
year = {2024},
issn = {2215-0161},
doi = {https://doi.org/10.1016/j.mex.2024.102655},
url = {https://www.sciencedirect.com/science/article/pii/S2215016124001092},
author = {Ling Zheng Teo and Victoria Leong},
keywords = {Precursors of creativity, Infancy, Measurements of creativity},
abstract = {Creativity is an important skill that relates to innovation, problem-solving and artistic achievement. However, relatively little is known about the early development of creative potential in very young children, in part due to a paucity of tasks suitable for use during infancy. Current measures of creativity in early childhood include the Unusual Box Test, Torrance's Thinking Creatively in Action and Movement (TCAM) task and the Toca Kitchen Monsters task. These tasks are designed for children aged above 12, 36 and 18 months respectively, but very few measures of creativity can be used for infants aged below 2. Accordingly, here we report age-appropriate adaptations of TCAM and Toca Kitchen Monsters tasks for infants as young as 12 to 24 months. Considerations taken into account include (1) infants’ cognitive capacities (i.e., attention span, language comprehension skills, motor skills, and approach to play), and (2) practicality of the stimuli, including suitability for use amid the COVID-19 pandemic. The modified creativity battery for infants includes three tasks: Music Play, Object Play and Exploratory Play tasks. The task protocols elaborated in this paper are intended to facilitate studies on the early development of creativity in infants aged between 12 and 24 months. Primary highlights include:•Age-appropriate adaptation of creativity tasks for use with infants aged between 12 and 24 months.•Consideration of infants’ cognitive capacities and stimulus practicality.•Innovative use of movement as expression of infants’ creative behaviour.}
}
@incollection{GALLICCHIO201127,
title = {Recent theoretical and computational advances for modeling protein–ligand binding affinities},
editor = {Christo Christov},
series = {Advances in Protein Chemistry and Structural Biology},
publisher = {Academic Press},
volume = {85},
pages = {27-80},
year = {2011},
booktitle = {Computational chemistry methods in structural biology},
issn = {1876-1623},
doi = {https://doi.org/10.1016/B978-0-12-386485-7.00002-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780123864857000028},
author = {Emilio Gallicchio and Ronald M. Levy},
keywords = {Quasi-chemical description, Statistical mechanics, Potential of mean force, PDT, MM/PBSA, free energy perturbation, BEDAM, double decoupling},
abstract = {We review recent theoretical and algorithmic advances for the modeling of protein ligand binding free energies. We first describe a statistical mechanics theory of noncovalent association, with particular focus on deriving the fundamental formulas on which computational methods are based. The second part reviews the main computational models and algorithms in current use or development, pointing out the relations with each other and with the theory developed in the first part. Particular emphasis is given to the modeling of conformational reorganization and entropic effect. The methods reviewed are free energy perturbation, double decoupling, the Binding Energy Distribution Analysis Method, the potential of mean force method, mining minima and MM/PBSA. These models have different features and limitations, and their ranges of applicability vary correspondingly. Yet their origins can all be traced back to a single fundamental theory.}
}
@article{IYER2024e32546,
title = {Inspiring a convergent engineering approach to measure and model the tissue microenvironment},
journal = {Heliyon},
volume = {10},
number = {12},
pages = {e32546},
year = {2024},
issn = {2405-8440},
doi = {https://doi.org/10.1016/j.heliyon.2024.e32546},
url = {https://www.sciencedirect.com/science/article/pii/S2405844024085773},
author = {Rishyashring R. Iyer and Catherine C. Applegate and Opeyemi H. Arogundade and Sushant Bangru and Ian C. Berg and Bashar Emon and Marilyn Porras-Gomez and Pei-Hsuan Hsieh and Yoon Jeong and Yongdeok Kim and Hailey J. Knox and Amir Ostadi Moghaddam and Carlos A. Renteria and Craig Richard and Ashlie Santaliz-Casiano and Sourya Sengupta and Jason Wang and Samantha G. Zambuto and Maria A. Zeballos and Marcia Pool and Rohit Bhargava and H. Rex Gaskins},
keywords = {Bioengineering, Interdisciplinary research, Bioimaging, Biomaterials, Biosensing, Computational biology, Biomedical devices, Biotechnology},
abstract = {Understanding the molecular and physical complexity of the tissue microenvironment (TiME) in the context of its spatiotemporal organization has remained an enduring challenge. Recent advances in engineering and data science are now promising the ability to study the structure, functions, and dynamics of the TiME in unprecedented detail; however, many advances still occur in silos that rarely integrate information to study the TiME in its full detail. This review provides an integrative overview of the engineering principles underlying chemical, optical, electrical, mechanical, and computational science to probe, sense, model, and fabricate the TiME. In individual sections, we first summarize the underlying principles, capabilities, and scope of emerging technologies, the breakthrough discoveries enabled by each technology and recent, promising innovations. We provide perspectives on the potential of these advances in answering critical questions about the TiME and its role in various disease and developmental processes. Finally, we present an integrative view that appreciates the major scientific and educational aspects in the study of the TiME.}
}
@article{PAN2025125506,
title = {CISL-PD: A deep learning framework of clinical intervention strategies for Parkinson’s disease based on directional counterfactual Dual GANs},
journal = {Expert Systems with Applications},
volume = {261},
pages = {125506},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2024.125506},
url = {https://www.sciencedirect.com/science/article/pii/S095741742402373X},
author = {Changrong Pan and Yu Tian and Lingyan Ma and Tianshu Zhou and Shuyu Ouyang and Jingsong Li},
keywords = {Parkinson’s disease, Intervention strategies, Counterfactual generation, Generative Adversarial Network},
abstract = {Parkinson’s disease (PD) is a prevalent chronic neurodegenerative disorder characterized by both motor and non-motor symptoms. The significant heterogeneity among PD patients poses a major challenge for treatment interventions. Current clinical interventions for PD primarily target motor symptoms, often neglecting non-motor symptoms, which can lead to unnecessary complications in non-motor symptoms while treating motor symptoms. Therefore, it is crucial to provide comprehensive and precise intervention strategies that encompass both symptom types. To address this issue, we develop a deep learning framework of clinical intervention strategies for PD (CISL-PD) based on counterfactual thinking. This framework introduces Directional Counterfactual Dual Generative Adversarial Networks (DCD-GANs), which apply various counterfactual constraints to longitudinal data to generate practical and plausible counterfactual instances aligned with clinical reality. By analyzing these counterfactual instances and their differences from the original instances, we explore PD intervention strategies with duration-specific fine regulation of multidimensional features. Experiments conducted on 374 PD patients from the Parkinson’s Progression Markers Initiative (PPMI) demonstrate that the counterfactual instances generated by DCD-GANs surpass other state-of-the-art models in terms of similarity (0.307 ± 0.246), sparsity (0.513 ± 0.161), smoothness (0.238 ± 0.135), and trend consistency (0.100 ± 0.089). From these generated counterfactual instances, we develop three clinically feasible intervention strategies that address both motor and non-motor symptoms and identify corresponding patterns of PD with distinct progression differences. Validation on an independent cohort of 351 patients from the National Institute of Neurological Disorders and Stroke Parkinson’s Disease Biomarkers Program (PDBP) confirmed the framework’s robustness and generalizability. By offering precise, multidimensional intervention strategies that can address both motor and non-motor symptoms, the CISL-PD framework has the potential to enhance patient outcomes, reduce complications, improve overall quality of life, and guide clinical decision-making.}
}
@article{ZHANG2024100667,
title = {Research on the application value of Multimedia-Based virtual reality technology in drama education activities},
journal = {Entertainment Computing},
volume = {50},
pages = {100667},
year = {2024},
issn = {1875-9521},
doi = {https://doi.org/10.1016/j.entcom.2024.100667},
url = {https://www.sciencedirect.com/science/article/pii/S1875952124000351},
author = {Bingyu Zhang and Wenwen Jiang},
keywords = {Multimedia, Virtual reality, Academic technology, Drama education, and ANOVA},
abstract = {The research and moral use of academic technology focuses on developing, implementing, and overseeing the use of suitable technical resources and procedures to enhance learning and achievement. Multimedia has found its position in some form as an educational technology platform in the contemporary environment of academic universities. The use of virtual reality software as an intellectual tool and learning provider allows students to perform cognitive rehabilitation of preexisting information frameworks. People are paying more and more attention to how preschoolers' holistic skills develop as education reform progresses. Drama education is incorporated into the school curriculum to enhance young children's artistic, intellectual, and linguistic skills. Therefore, this study aims to examine the potential of multimedia-based virtual reality technology (MVRT) in drama education. The participants in this study were students from different universities in China. Students were exposed to multimedia-based virtual reality technology, and its efficacy was assessed using a statistical analytic approach called Analysis of variance (ANOVA). Drama understanding rate, educational improvement ratio, teaching quality rate, student achievement ratio, computation time, and parental support rate are among the performance metrics used to assess performance. Multimedia-based virtual reality technology (MVRT) for drama education showed outstanding success, with a 98% improvement ratio in educational outcomes and higher teaching quality. Students exhibited improved performance, supported by solid parental approval, demonstrating the effectiveness of MVRT in enhancing educational experiences.}
}
@article{CANIZARES2024100020,
title = {Taming the Rhinoceros: A brief history of a ubiquitous tool},
journal = {Perspectives in Architecture and Urbanism},
volume = {1},
number = {2},
pages = {100020},
year = {2024},
issn = {2950-2675},
doi = {https://doi.org/10.1016/j.pau.2024.100020},
url = {https://www.sciencedirect.com/science/article/pii/S2950267524000241},
author = {Galo Canizares},
keywords = {Software, History, Parametric design, Digital fabrication, Theory},
abstract = {At the turn of the millennium, architects and educators, propelled by the demise of critical theory, found a space to speculate about technology’s role in the future of architecture. As Michael Speaks wrote in 2002, “if philosophy was the intellectual dominant of early twentieth century vanguards and theory the intellectual dominant of late twentieth century vanguards, then intelligence has become the intellectual dominant of twenty-first century post-vanguards” (Speaks, 2010, p. 211). This emphasis on intelligence fostered a progressive narrative around the increasing reliance on software in design processes. This paper examines architectural practice during this period, with a specific focus on the rise of a new set of values, priorities, and factors that transformed architectural thinking and making. In contrast to existing accounts of digital design’s history, this paper places less importance on outputs and more on the shifts in modes of working and enacting design labor. More specifically, it narrows in on a software application that, as will be argued, drastically changed both cultural values and design knowledge: Rhinoceros. Beyond simply facilitating the production of geometrically intricate and complex architectural assemblies, Rhinoceros helped shape the discourse on parametric, computational, and algorithmic design, redefining the role of the designer as a creative technologist. In doing so, it also engendered a specific community of practice, which in turn produced its own culture and folklore. The spread of this software greatly contributed to the rise of two new kinds of architectural technologists: the “parametric designer” and the “digital fabricator,” two actors who would significantly impact how architecture was imagined and produced from the mid-2000s through the 2010s.}
}
@article{LOPEZBRAU2023105524,
title = {People can use the placement of objects to infer communicative goals},
journal = {Cognition},
volume = {239},
pages = {105524},
year = {2023},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2023.105524},
url = {https://www.sciencedirect.com/science/article/pii/S0010027723001580},
author = {Michael Lopez-Brau and Julian Jara-Ettinger},
keywords = {Computational modeling, Social objects, Theory of Mind},
abstract = {Beyond words and gestures, people have a remarkable capacity to communicate indirectly through everyday objects: A hat on a chair can mean it is occupied, rope hanging across an entrance can mean we should not cross, and objects placed in a closed box can imply they are not ours to take. How do people generate and interpret the communicative meaning of objects? We hypothesized that this capacity is supported by social goal inference, where observers recover what social goal explains an object being placed in a particular location. To test this idea, we study a category of common ad-hoc communicative objects where a small cost is used to signal avoidance. Using computational modeling, we first show that goal inference from indirect physical evidence can give rise to the ability to use object placement to communicate. We then show that people from the U.S. and the Tsimane’—a farming-foraging group native to the Bolivian Amazon—can infer the communicative meaning of object placement in the absence of a pre-existing convention, and that people’s inferences are quantitatively predicted by our model. Finally, we show evidence that people can store and retrieve this meaning for use in subsequent encounters, revealing a potential mechanism for how ad-hoc communicative objects become quickly conventionalized. Our model helps shed light on how humans use their ability to interpret other people’s behavior to embed social meaning into the physical world.}
}
@article{FRITSCH2024100297,
title = {Teaching advanced topics in econometrics using introductory textbooks: The case of dynamic panel data methods},
journal = {International Review of Economics Education},
volume = {47},
pages = {100297},
year = {2024},
issn = {1477-3880},
doi = {https://doi.org/10.1016/j.iree.2024.100297},
url = {https://www.sciencedirect.com/science/article/pii/S147738802400015X},
author = {Markus Fritsch and Andrew Adrian Yu Pua and Joachim Schnurbus},
keywords = {Teaching econometrics, instrumental variables, linear dynamic panel data methods, cigarette demand, lagged variables},
abstract = {We show how to use the introductory econometrics textbook by Stock and Watson (2019) as a starting point for teaching and studying dynamic panel data methods. The materials are intended for undergraduate students taking their second econometrics course, undergraduate students in seminar-type courses, independent study courses, capstone, or thesis projects, and beginning graduate students in a research methods course. First, we distill the methodological core necessary to understand dynamic panel data methods. Second, we design an empirical and a theoretical case study to highlight the capabilities, downsides, and hazards of the method. The empirical case study is based on the cigarette demand example in Stock and Watson (2019) and illustrates that economic and methodological issues are interrelated. The theoretical case study shows how to evaluate current empirical practices from a theoretical standpoint. We designed both case studies to boost students’ confidence in working with technical material and to provide instructors with more opportunities to let students develop econometric thinking and to actively communicate with applied economists. Although we focus on Stock and Watson (2019) and the statistical software R, we also show how to modify the material for use with another introductory textbook by Wooldridge (2020) and Stata, and highlight some possible further pathways for instructors and students to reuse and extend our materials.}
}
@article{EBERBACH2007200,
title = {The $-calculus process algebra for problem solving: A paradigmatic shift in handling hard computational problems},
journal = {Theoretical Computer Science},
volume = {383},
number = {2},
pages = {200-243},
year = {2007},
note = {Complexity of Algorithms and Computations},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2007.04.012},
url = {https://www.sciencedirect.com/science/article/pii/S0304397507003192},
author = {Eugene Eberbach},
keywords = {Problem solving, Process algebras, Anytime algorithms, SuperTuring models of computation, Bounded rational agents, $-calculus, Intractability, Undecidability, Completeness, Optimality, Search optimality, Total optimality},
abstract = {The $-calculus is the extension of the π-calculus, built around the central notion of cost and allowing infinity in its operators. We propose the $-calculus as a more complete model for problem solving to provide a support to handle intractability and undecidability. It goes beyond the Turing Machine model. We define the semantics of the $-calculus using a novel optimization method (the kΩ-optimization), which approximates a nonexisting universal search algorithm and allows the simulation of many other search methods. In particular, the notion of total optimality has been utilized to provide an automatic way to deal with intractability of problem solving by optimizing together the quality of solutions and search costs. The sufficient conditions needed for completeness, optimality and total optimality of problem solving search are defined. A very flexible classification scheme of problem solving methods into easy, hard and solvable in the limit classes has been proposed. In particular, the third class deals with non-recursive solutions of undecidable problems. The approach is illustrated by solutions of some intractable and undecidable problems. We also briefly overview two possible implementations of the $-calculus.}
}
@article{BIBRI2018758,
title = {A foundational framework for smart sustainable city development: Theoretical, disciplinary, and discursive dimensions and their synergies},
journal = {Sustainable Cities and Society},
volume = {38},
pages = {758-794},
year = {2018},
issn = {2210-6707},
doi = {https://doi.org/10.1016/j.scs.2017.12.032},
url = {https://www.sciencedirect.com/science/article/pii/S2210670717313069},
author = {Simon Elias Bibri},
keywords = {Smart sustainable cities, Theories, Academic disciplines, Academic discourses, Multidimensional framework, Interdisciplinarity and transdisciplinarity, Systems thinking, Complexity science, Sustainability, Computing and ICT},
abstract = {In the subject of smart sustainable cities, the underlying theories are a foundation for practice. Moreover, scholarly research in the field of smart sustainable cities operates out of the understanding that advances in the underlying knowledge necessitate pursuing multifaceted questions that can only be resolved from the vantage point of interdisciplinarity or transdisciplinarity. Indeed, research problems in this field are inherently too complex to be addressed by single disciplines. The PhD study addressing the topic of smart sustainable city development falls within the broad research field of sustainability transition and sustainability science where ICT is seen as a salient factor given its transformational, disruptive, and synergetic effects as an enabling, integrative, and constitutive technology. In light of this, the approach to the PhD study is of an applied theoretical kind, and its aim is to investigate and analyze how to advance and sustain the contribution of sustainable urban forms to the goals of sustainable development with support of ICT of pervasive computing. This is to primarily create a framework for strategic smart sustainable city development based on scientific principles, theories, and academic disciplines and discourses used to guide urban actors in their practice towards sustainability and analyze its impact. This involves the application of a set of integrative foundational elements drawn from urban planning, urban design, sustainability, sustainable development, sustainability science, data science, computer science, complexity science, systems theory, systems thinking, and ICT. Accordingly, it is deemed of high significance to devise a multidimensional framework consisting of relevant theories and academic disciplines and discourses that underpin the development of smart sustainable cities as a set of future practices. This framework in turn emphasizes the interdisciplinary and transdisciplinary nature and orientation of the topic of smart sustainable cities and thus the relevance of pursuing an interdisciplinary and transdisciplinary approach into studying this topic. Therefore, this paper endeavors to systematize the very complex and dense scientific area of smart sustainable cities in terms of identifying, distilling, and structuring the core dimensions of a foundational framework for smart sustainable city development as a set of future practices. In doing so, it focuses on a number of fundamental theories along with academic disciplines and discourses, with the aim of setting a framework that analytically relates city development, sustainability, and ICT, while emphasizing how and to what extent sustainability and ICT have particularly become influential in city development in modern society. In addition, this paper offers an in–depth interdisciplinary and transdisciplinary discussion covering topics of high relevance to the PhD study and at the heart of the very synergic relationship between the theoretical, disciplinary, and discursive dimensions of the foundational framework underpinning smart sustainable city development. These dimensions thus form the basis for the framework for strategic smart sustainable city development that is under investigation and will be developed based on a backcasting approach to strategic planning. This study provides an important lens through which to understand a set of influential theories and established academic disciplines and discourses with high potential for integration, fusion, and practicality in relation to the practice of smart sustainable city development.}
}
@article{VEITAS201716,
title = {Living Cognitive Society: A ‘digital’ World of Views},
journal = {Technological Forecasting and Social Change},
volume = {114},
pages = {16-26},
year = {2017},
issn = {0040-1625},
doi = {https://doi.org/10.1016/j.techfore.2016.05.002},
url = {https://www.sciencedirect.com/science/article/pii/S0040162516300610},
author = {Viktoras Veitas and David Weinbaum},
keywords = {Cognitive system, Living society, Information and communication technologies, Future social governance, Individuation, Cognitive development},
abstract = {The current social reality is characterized by all-encompassing change, which disrupts existing social structures at all levels. Yet the approach based on the ontological primacy of stable and often hierarchical structures is still prevalent in theoretical and, most importantly, practical thinking about social systems. We propose a conceptual framework for thinking about a dynamically changing social system: the Living Cognitive Society. Importantly, we show how it follows from a much broader philosophical framework, guided by the theory of individuation, which emphasizes the importance of relationships and interactive processes in the evolution of a system. The framework addresses society as a living cognitive system – an ecology of interacting social subsystems – each of which is also a living cognitive system. We argue that this approach can help us to conceive sustainable social systems that will thrive in the circumstances of accelerating change. The Living Cognitive Society is explained in terms of its fluid structure, dynamics and the mechanisms at work. We then discuss the disruptive effects of Information and Communication Technologies on the mechanisms at work. We conclude by delineating a major topic for future research – distributed social governance – which focuses on processes of coordination rather than on stable structures within global society.}
}
@incollection{RZESZEWSKI2024219,
title = {Chapter 10 - Augmented reality content and relations of power in smart spaces},
editor = {Zhihan Lyu},
booktitle = {Smart Spaces},
publisher = {Academic Press},
pages = {219-234},
year = {2024},
series = {Intelligent Data-Centric Systems},
isbn = {978-0-443-13462-3},
doi = {https://doi.org/10.1016/B978-0-443-13462-3.00008-X},
url = {https://www.sciencedirect.com/science/article/pii/B978044313462300008X},
author = {Michal Rzeszewski and Leighton Evans},
keywords = {Augmented reality, Smart space, Agency of place, Power relations},
abstract = {This chapter’s main aim is to interrogate how augmented reality (AR) content can change the relations of power within a place and transform the perception of place from being a material background for social interactions into a living agent that can have its own agency. We use the concept of sociotechnical imaginary and thematic analysis of AR-related media content to explore main narrations in the current discourse on AR and smart urban spaces. We identify two dominant themes: “smart information in place” and “subversion of meaning” that combine two ways of thinking about relation between place and AR. In the first one, AR acts not as an augmentation of place, but as a reducer of the experience of place down to seeing the world as information and data for the achievement of efficiencies in late capitalism. In the second one, AR can be seen as transcending the traditional constellations of power relations that shape places and spaces of modern cities, skewing them toward nonhuman actors, of which AR may be the most visible and influential. Through the visible differentiation of users, alteration of perception and the forcing of presence and behavior, AR is a technology that makes visible the systems and processes of control and mediation of space and place. As such, the smart space itself will become visible through the presence, use, and functioning of AR in a manner that has not been the case previously. We posit, therefore, that AR can be seen as a physical manifestation of the agency of place. This position can have consequences for the practical development of smart spaces and for theoretical consideration of the human-technology interaction in urban space in its material and digital dimensions.}
}
@article{LOCK2023102310,
title = {Conserving complexity: A complex systems paradigm and framework to study public relations’ contribution to grand challenges},
journal = {Public Relations Review},
volume = {49},
number = {2},
pages = {102310},
year = {2023},
issn = {0363-8111},
doi = {https://doi.org/10.1016/j.pubrev.2023.102310},
url = {https://www.sciencedirect.com/science/article/pii/S0363811123000255},
author = {Irina Lock},
keywords = {Grand challenges, Public issues, Public relations, Strategic communication, Complex adaptive systems, Digital communication, Complexity},
abstract = {Sustainable development poses a grand challenge for society, addressed by organisations through their public relations activities. Grand challenges are complex by nature and call for nontrivial solutions whose effects show at the level of society. That is why studying public relations’ contribution to grand challenges requires a macro perspective that accounts for the dynamic interaction between individual, organisational, and system levels in a digital communication environment. This paper offers a new paradigm to analyse organisations’ significant and at times undue impact on grand challenges through public relations. It develops a framework inspired by complex adaptive systems thinking and adopts its ten properties for public relations: emergence, adaptivity, heterogeneous actors, nonlinear effects, feedback mechanisms, self-organisation, phase transitions, networks, scaling, and cooperation. The paper applies the framework to the example of sustainable development. It shows why research on grand challenges requires a holistic perspective and how it can help study digitally born communication phenomena. The proposed complex systems paradigm provides space for critical, social scientific, and interpretative research lines in public relations. Inquiries start from the grand challenge and study the communicative interactions between organisations and other actors from existing theory while accounting for the ten properties of complex adaptive systems. The paper outlines how future research can enrich the study of public relations and discusses its limits.}
}
@article{PRINA2024132735,
title = {Machine learning as a surrogate model for EnergyPLAN: Speeding up energy system optimization at the country level},
journal = {Energy},
volume = {307},
pages = {132735},
year = {2024},
issn = {0360-5442},
doi = {https://doi.org/10.1016/j.energy.2024.132735},
url = {https://www.sciencedirect.com/science/article/pii/S036054422402509X},
author = {Matteo Giacomo Prina and Mattia Dallapiccola and David Moser and Wolfram Sparber},
keywords = {Energy system modelling, Energy scenarios, Energy planning, Machine learning},
abstract = {In the field of energy system modelling, increasing complexity and optimization analysis are essential for understanding the most effective decarbonization options. However, the growing need for intricate models leads to increased computational time, which can hinder progress in research and policy-making. This study aims to address this issue by integrating machine learning algorithms with EnergyPLAN and EPLANopt, a coupling of EnergyPLAN software and a multi-objective evolutionary algorithm, to expedite the optimization process while maintaining accuracy. By saving computational time, we can increase the number of evaluations, thereby enabling deeper exploration of uncertainty in energy system modelling. Although machine learning models have been widely employed as surrogate models to accelerate optimization problems, their application in energy system modeling at the national scale, while preserving high temporal resolution and extensive sector-coupling, remains scarce. Several machine learning models were evaluated, and an artificial neural network was selected as the most effective surrogate model. The findings demonstrate that incorporating this surrogate model within the optimization process reduces computational time by 64 % compared to the conventional EPLANopt approach, while maintaining an accuracy level close to that obtained by running EPLANopt without the surrogate model.}
}
@article{HASHIMZADE2025100310,
title = {Integrating programming into the modern undergraduate economics curriculum},
journal = {International Review of Economics Education},
volume = {49},
pages = {100310},
year = {2025},
issn = {1477-3880},
doi = {https://doi.org/10.1016/j.iree.2025.100310},
url = {https://www.sciencedirect.com/science/article/pii/S1477388025000027},
author = {Nigar Hashimzade and Oleg Kirsanov and Tatiana Kirsanova},
abstract = {The increasing demand for computational skills in economics necessitates the integration of programming into undergraduate economics curricula in the UK. This paper argues for a systematic incorporation of programming courses tailored to economics students, addressing the limitations of current approaches and highlighting the benefits of such integration. We propose a sequence of introductory and intermediate-level integrated courses, and argue that this curriculum change will enhance students’ understanding of economic concepts, improve their employment prospects, and better prepare them for postgraduate studies. This paper aims to initiate a discussion and exchange of ideas and experiences on this subject at the national level.}
}
@article{PAPADOPOULOS2019210,
title = {Using mobile puzzles to exhibit certain algebraic habits of mind and demonstrate symbol-sense in primary school students},
journal = {The Journal of Mathematical Behavior},
volume = {53},
pages = {210-227},
year = {2019},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2018.07.001},
url = {https://www.sciencedirect.com/science/article/pii/S073231231730189X},
author = {Ioannis Papadopoulos},
keywords = {Algebraic habits of mind, mobile puzzles, Symbol sense},
abstract = {Given the growing concern for developing students’ algebraic ideas and thinking in earlier grades (NCTM, 2000) it is important for students to have experiences that better prepare them for their formal introduction to algebra. Mobile puzzles seem to be an opportunity for exhibiting certain algebraic habits of mind as well as for demonstrating symbol-sense which might support students in their transition from arithmetic to algebra. These puzzles include multiple balanced collections of objects whose weights must be determined by the solver. The arms/beams must be perfectly balanced for it to hang properly. Therefore, they represent, in a pictorial way, systems of equations. Each arm/beam that balances two sets of objects (representing variables as unknown “weights”) represents an equation. The data derived from Grade-6 students who were asked to solve a collection of tasks reflect the presence of the “Puzzling and Persevering” and “Seeking and Using Structure” habits of mind. At the same time these data incorporate instances of some main components of symbol-sense such as “friendliness with symbols”, “manipulating and ‘reading through’ symbolic expressions”, and “choice of symbols”. Also discussed is the way this experience contributes to an intuitive application of the conventional rules for solving equations that will be later introduced to the students as the standard algebraic “moves”.}
}
@article{MARCHETTI20121517,
title = {Mindwandering heightens the accessibility of negative relative to positive thought},
journal = {Consciousness and Cognition},
volume = {21},
number = {3},
pages = {1517-1525},
year = {2012},
issn = {1053-8100},
doi = {https://doi.org/10.1016/j.concog.2012.05.013},
url = {https://www.sciencedirect.com/science/article/pii/S1053810012001420},
author = {Igor Marchetti and Ernst H.W. Koster and Rudi {De Raedt}},
keywords = {Mindwandering, Negative cognitions, Mood, Depression, Individual differences},
abstract = {Mindwandering (MW) is associated with both positive and negative outcomes. Among the latter, negative mood and negative cognitions have been reported. However, the underlying mechanisms linking mindwandering to negative mood and cognition are still unclear. We hypothesized that MW could either directly enhance negative thinking or indirectly heighten the accessibility of negative thoughts. In an undergraduate sample (n=79) we measured emotional thoughts during the Sustained Attention on Response Task (SART) which induces MW, and accessibility of negative cognitions by means of the Scrambled Sentences Task (SST) after the task. We also measured depressive symptoms and rumination. Results show that in individuals with elevated levels of depressive symptoms MW during SART predicts higher accessibility of negative thoughts after the task, rather than negative thinking during the task. These findings contribute to our understanding of the underlying mechanisms of MW and provide insight into the relationship between task-involvement and affect.}
}
@article{HU2022116276,
title = {Multi granularity based label propagation with active learning for semi-supervised classification},
journal = {Expert Systems with Applications},
volume = {192},
pages = {116276},
year = {2022},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2021.116276},
url = {https://www.sciencedirect.com/science/article/pii/S0957417421015840},
author = {Shengdan Hu and Duoqian Miao and Witold Pedrycz},
keywords = {Semi-supervised learning, Granular computing, Multi granularity, Label propagation, Active learning, Three-way decision},
abstract = {Semi-supervised learning (SSL) methods, which exploit both the labeled and unlabeled data, have attracted a lot of attention. One of the major categories of SSL methods, graph-based semi-supervised learning (GBSSL) learns labels of unlabeled data on an adjacency graph, where neighborhood sparse graph is often used to reduce computational complexity. However, the neighborhood size is difficult to set. Instead of assigning a concrete value of neighborhood size, we propose a new label propagation algorithm called multi granularity based label propagation (MGLP) and developed from the view of granular computing. In MGLP, labels of unlabeled data are learned by two classic label propagation processes with diverse neighborhood size k, where granular computing delivers a guiding strategy to leverage multiple level neighborhood information granules, and three-way decision acts as an active learning strategy to select the unlabeled data for further annotating. Through the iterative procedures of label propagating, data annotating and data subset updating, the ultimate pseudo label accuracy of unlabeled data may be higher. Theoretically, the accuracy of pseudo labels is enhanced in some scenarios. Experimentally, the results of simulation studies on ten benchmark datasets, show that the proposed method MGLP can rise pseudo labels accuracy by 8.6% than LP (label propagation), 6.5% than LNP (linear neighborhood propagation), 6.4% than LPSN (label propagation through sparse neighborhood), 4.5% than Adaptive-NP (adaptive neighborhood propagation) and 4.6% than CRLP (consensus rate-based label propagation). It also provides a novel way to annotate data.}
}
@article{GLASSMAN20101412,
title = {Pragmatism, connectionism and the internet: A mind’s perfect storm},
journal = {Computers in Human Behavior},
volume = {26},
number = {6},
pages = {1412-1418},
year = {2010},
note = {Online Interactivity: Role of Technology in Behavior Change},
issn = {0747-5632},
doi = {https://doi.org/10.1016/j.chb.2010.04.019},
url = {https://www.sciencedirect.com/science/article/pii/S0747563210000956},
author = {Michael Glassman and Min Ju Kang},
keywords = {Internet, Dewey, Connectionism, Democracy},
abstract = {This paper explores that natural relationships between Pragmatic theory of knowing, the dynamic structuring of the mind and thinking suggested by connectionist theory, and the way information is distributed and organized through the world wide web (www). We suggest that these three “innovations” can be brought together to offer a better understanding of the way the human mind works. The internet and the information revolution may finally offer the opportunity to use and develop inductive learning practices and information based social inquiry in ways Pragmatic philosophers envisioned a hundred years ago, while the recent rise of connectionist and cognitive architecture works provides a concrete context for such developments. This confluence of process represents the type of synergy that only history can offer. The information revolution – exemplified by both the rise of connectionism and the internet – is the apotheosis of the Pragmatic revolution – bringing together radical empiricism and democratization of information in community practice. We offer three important realizations in our understanding of how information is organized and thinking progresses made possible by burgeoning virtual communities on the internet – open source thinking, scale-free networks, and interrelationships in the development of blogs to illustrate our thesis.}
}
@article{20213190,
title = {Eunji Cheong},
journal = {Neuron},
volume = {109},
number = {20},
pages = {3190-3192},
year = {2021},
issn = {0896-6273},
doi = {https://doi.org/10.1016/j.neuron.2021.09.027},
url = {https://www.sciencedirect.com/science/article/pii/S0896627321007005},
abstract = {In Korea, the pandemic has elevated scientists as trusted sources for both policy decisions and dinner table conversation. In an interview with Neuron, Eunji Cheong discusses how we need to support future generations by fostering scientific thinking, patience, and flexibility.}
}
@incollection{KAMAREDDINE2012801,
title = {Russell's Orders in Kripke's Theory of Truth and Computational Type Theory},
editor = {Dov M. Gabbay and Akihiro Kanamori and John Woods},
series = {Handbook of the History of Logic},
publisher = {North-Holland},
volume = {6},
pages = {801-845},
year = {2012},
booktitle = {Sets and Extensions in the Twentieth Century},
issn = {1874-5857},
doi = {https://doi.org/10.1016/B978-0-444-51621-3.50011-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780444516213500116},
author = {Fairouz Kamareddine and Twan Laan and Robert Constable}
}
@incollection{CORMACK2005325,
title = {4.1 - Computational Models of Early Human Vision},
editor = {AL BOVIK},
booktitle = {Handbook of Image and Video Processing (Second Edition)},
publisher = {Academic Press},
edition = {Second Edition},
address = {Burlington},
pages = {325-IX},
year = {2005},
series = {Communications, Networking and Multimedia},
isbn = {978-0-12-119792-6},
doi = {https://doi.org/10.1016/B978-012119792-6/50083-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780121197926500838},
author = {Lawrence K. Cormack}
}
@article{WANG201854,
title = {Studying cognitive development in cultural context: A multi-level analysis approach},
journal = {Developmental Review},
volume = {50},
pages = {54-64},
year = {2018},
note = {Towards a Cultural Developmental Science},
issn = {0273-2297},
doi = {https://doi.org/10.1016/j.dr.2018.03.002},
url = {https://www.sciencedirect.com/science/article/pii/S0273229717301041},
author = {Qi Wang},
keywords = {Culture, Cognition, Episodic thinking, Memory, Multiple levels of analysis},
abstract = {I discuss a multi-level analysis approach in the study of cognitive development in cultural context. In this approach, culture is conceived of as a system and a process of symbolic mediation, where values, norms, and beliefs manifest in and through customs, rituals, and practices in directing and regulating both intrapersonal and interpersonal psychological functions. To capture the dynamic process in which cognitive development unfolds in cultural context, this approach examines the influence of culture on the developing cognitive skills between groups – group level analysis, between the child and socialization agents – dyadic level analysis, at the level of the child – individual level analysis, and within the child – situation level analysis. The temporal level analysis further situates cognitive development in historical time. By utilizing different analytical and methodological strategies, the multi-level analysis approach provides converging evidence for the development of cognitive skills in cultural context. Important challenges in this approach include the development of a “big picture” to guide the investigation, methodological training for research assistants, and difficulties in collecting, managing, and analyzing diverse datasets. I discuss the conceptual and methodological issues by using our work on the development of episodic thinking as an example.}
}
@article{PEARSON1994203,
title = {Report on University of Wales Institute of non-Newtonian Fluid Mechanics Mini-Symposium on “Continuum and Microstructural Modelling in Computational Rheology” Seiont Manor, Gwynedd, 11–12 April 1994},
journal = {Journal of Non-Newtonian Fluid Mechanics},
volume = {55},
number = {2},
pages = {203-205},
year = {1994},
issn = {0377-0257},
doi = {https://doi.org/10.1016/0377-0257(94)80006-5},
url = {https://www.sciencedirect.com/science/article/pii/0377025794800065},
author = {J.R.A. Pearson}
}
@incollection{TOPLAK202253,
title = {3 - Development of the ability to detect and override miserly information processing},
editor = {Maggie E. Toplak},
booktitle = {Cognitive Sophistication and the Development of Judgment and Decision-Making},
publisher = {Academic Press},
pages = {53-87},
year = {2022},
isbn = {978-0-12-816636-9},
doi = {https://doi.org/10.1016/B978-0-12-816636-9.00011-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128166369000116},
author = {Maggie E. Toplak},
keywords = {Miserly information processing, Dual process models, Children and youth, Development, Ratio bias, Belief bias syllogisms, Cognitive reflection},
abstract = {Several judgment and decision-making tasks require overriding an incorrect response that is signaled by miserly information processes. The successful detection and override of conflict between heuristic and analytic processes has been a focus of dual processes models, especially in adult samples. These miserly processing tendencies have also been described in developmental samples. The measurement of resistance to miserly information processing has been assessed using several tasks, including ratio bias, belief bias syllogisms, cognitive reflection, and disjunctive thinking tasks. Several of these tasks have been studied in developmental samples, including in the longitudinal study described in this volume. There is evidence to suggest that resistance to miserly information processing is measurable in children and youth. While judgment and decision-making tasks vary in the degree to which override of miserly processing is required, individuals also vary in their ability to resist miserly processing tendencies. Individual differences in resistance to miserly information processing serve as an additional foundation to support rational thinking performance.}
}
@article{NOH2006554,
title = {Computational tools for isotopically instationary 13C labeling experiments under metabolic steady state conditions},
journal = {Metabolic Engineering},
volume = {8},
number = {6},
pages = {554-577},
year = {2006},
issn = {1096-7176},
doi = {https://doi.org/10.1016/j.ymben.2006.05.006},
url = {https://www.sciencedirect.com/science/article/pii/S1096717606000449},
author = {Katharina Nöh and Aljoscha Wahl and Wolfgang Wiechert},
keywords = {Instationary C metabolic flux analysis, C labeling experiment, C labeling dynamics, Parameter identifiability, Optimal experimental design, },
abstract = {13C metabolic flux analysis (MFA) has become an important and powerful tool for the quantitative analysis of metabolic networks in the framework of metabolic engineering. Isotopically instationary 13C MFA under metabolic stationary conditions is a promising refinement of classical stationary MFA. It accounts for the experimental requirements of non-steady-state cultures as well as for the shortening of the experimental duration. This contribution extends all computational methods developed for classical stationary 13C MFA to the instationary situation by using high-performance computing methods. The developed tools allow for the simulation of instationary carbon labeling experiments (CLEs), sensitivity calculation with respect to unknown parameters, fitting of the model to the measured data, statistical identifiability analysis and an optimal experimental design facility. To explore the potential of the new approach all these tools are applied to the central metabolism of Escherichia coli. The achieved results are compared to the outcome of the stationary counterpart, especially focusing on statistical properties. This demonstrates the specific strengths of the instationary method. A new ranking method is proposed making both an a priori and an a posteriori design of the sampling times available. It will be shown that although still not all fluxes are identifiable, the quality of flux estimates can be strongly improved in the instationary case. Moreover, statements about the size of some immeasurable pool sizes can be made.}
}
@article{FUXJAGER2023105340,
title = {Systems biology as a framework to understand the physiological and endocrine bases of behavior and its evolution—From concepts to a case study in birds},
journal = {Hormones and Behavior},
volume = {151},
pages = {105340},
year = {2023},
issn = {0018-506X},
doi = {https://doi.org/10.1016/j.yhbeh.2023.105340},
url = {https://www.sciencedirect.com/science/article/pii/S0018506X23000387},
author = {Matthew J. Fuxjager and T. Brandt Ryder and Nicole M. Moody and Camilo Alfonso and Christopher N. Balakrishnan and Julia Barske and Mariane Bosholn and W. Alice Boyle and Edward L. Braun and Ioana Chiver and Roslyn Dakin and Lainy B. Day and Robert Driver and Leonida Fusani and Brent M. Horton and Rebecca T. Kimball and Sara Lipshutz and Claudio V. Mello and Eliot T. Miller and Michael S. Webster and Morgan Wirthlin and Roy Wollman and Ignacio T. Moore and Barney A. Schlinger},
keywords = {Systems biology, Animal behavior, Organismal physiology, Adaptive evolution, Manakin birds, Androgenic hormones, Robustness},
abstract = {Organismal behavior, with its tremendous complexity and diversity, is generated by numerous physiological systems acting in coordination. Understanding how these systems evolve to support differences in behavior within and among species is a longstanding goal in biology that has captured the imagination of researchers who work on a multitude of taxa, including humans. Of particular importance are the physiological determinants of behavioral evolution, which are sometimes overlooked because we lack a robust conceptual framework to study mechanisms underlying adaptation and diversification of behavior. Here, we discuss a framework for such an analysis that applies a “systems view” to our understanding of behavioral control. This approach involves linking separate models that consider behavior and physiology as their own networks into a singular vertically integrated behavioral control system. In doing so, hormones commonly stand out as the links, or edges, among nodes within this system. To ground our discussion, we focus on studies of manakins (Pipridae), a family of Neotropical birds. These species have numerous physiological and endocrine specializations that support their elaborate reproductive displays. As a result, manakins provide a useful example to help imagine and visualize the way systems concepts can inform our appreciation of behavioral evolution. In particular, manakins help clarify how connectedness among physiological systems—which is maintained through endocrine signaling—potentiate and/or constrain the evolution of complex behavior to yield behavioral differences across taxa. Ultimately, we hope this review will continue to stimulate thought, discussion, and the emergence of research focused on integrated phenotypes in behavioral ecology and endocrinology.}
}
@article{CAO2024102160,
title = {Self-assembly of peptides: The acceleration by molecular dynamics simulations and machine learning},
journal = {Nano Today},
volume = {55},
pages = {102160},
year = {2024},
issn = {1748-0132},
doi = {https://doi.org/10.1016/j.nantod.2024.102160},
url = {https://www.sciencedirect.com/science/article/pii/S174801322400015X},
author = {Nana Cao and Kang Huang and Jianjun Xie and Hui Wang and Xinghua Shi},
keywords = {Peptides, Self-assembly, Molecular dynamics, Machine learning},
abstract = {Peptides, biopolymeric compounds connected by peptide bonds, have garnered significant attention in recent years as their potential wide applications in fields such as drug delivery, tissue engineering, and antibiotics. Peptides exhibit excellent biocompatibility and stability due to their structural similarities to many bioactive substances found in human bodies. The self-assembly of peptides has piqued considerable interest with groundbreaking advancements achieved in experimental research. However, it is still a big challenge to establish comprehensive theoretical model to accurately describe the behavior of peptide self-assembly. Current peptide self-assembly designs primarily rely on experimental outcomes and general rules, which is inefficient and susceptible to human errors. In recent years, thanks to rapid advancements in computer techniques and theoretical methods, computational research has become a vital tool in complementing experimental research with rapid development witted in this field. This review delves into the description of peptide self-assembly, covering relevant sequences, structures, morphologies, rules, and application areas. It places particular emphasis on the recent progress in computational methods such as molecular dynamics (MD) simulations and machine learning (ML) techniques in the study. Finally, we provide a perspective on the application of computational methods to expedite exploration in the realm of multi-peptide self-assembly.}
}
@article{BAYNE201332,
title = {Thought},
journal = {New Scientist},
volume = {219},
number = {2935},
pages = {32-39},
year = {2013},
issn = {0262-4079},
doi = {https://doi.org/10.1016/S0262-4079(13)62293-9},
url = {https://www.sciencedirect.com/science/article/pii/S0262407913622939},
author = {Tim Bayne},
abstract = {Conscious or unbidden, thoughts fill our heads from morning to night. But what are they, and what exactly is thinking? Join philosopher Tim Bayne on a journey into the fantastic, elusive and ceaseless world our minds create}
}
@article{MASOUD201293,
title = {Synthesis, computational, spectroscopic, thermal and antimicrobial activity studies on some metal–urate complexes},
journal = {Spectrochimica Acta Part A: Molecular and Biomolecular Spectroscopy},
volume = {90},
pages = {93-108},
year = {2012},
issn = {1386-1425},
doi = {https://doi.org/10.1016/j.saa.2012.01.028},
url = {https://www.sciencedirect.com/science/article/pii/S1386142512000418},
author = {Mamdouh S. Masoud and Alaa E. Ali and Medhat A. Shaker and Gehan S. Elasala},
keywords = {Uric, Complexes, Synthesis, Spectroscopy, Thermal analysis, Computational},
abstract = {New sixteen uric acid metal complexes of different stoichiometry, stereo-chemistries and modes of interactions were synthesized using different metals Cr, Mn, Fe, Co, Ni, Cu, Cd, UO2, Na and K. The synthesized complexes were characterized by elemental analysis, spectral (IR, UV–Vis and ESR) methods, thermal analysis (TG, DTA and DSC) and magnetic susceptibility studies. Molecular modeling calculations were used to characterize the ligation sites of the free ligand. Furthermore, quantum chemical parameters of uric acid such as the energies of highest occupied molecular orbital (EHOMO), energies of lowest unoccupied molecular orbital (ELUMO), the separation energy (ΔE=ELUMO−EHOMO), the absolute electronegativity, χ, the chemical potential, Pi, the absolute hardness, η and the softness (σ) were obtained for uric acid. Eight different microbial categories were used to study the antimicrobial activity of the free ligand and ten of its complexes. The results indicate that the ligand and its metal complexes possess antimicrobial properties. The stoichiometry of iron–uric acid complex was studied by using different spectrophotometric methods.}
}
@article{HAMMIA2024517,
title = {Enhancing Real-time Simultaneous Localization and Mapping with FPGA-based EKF-SLAM's Hardware Architecture},
journal = {Procedia Computer Science},
volume = {236},
pages = {517-526},
year = {2024},
note = {International Symposium on Green Technologies and Applications (ISGTA’2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.05.061},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924010779},
author = {Slama Hammia and Anas Hatim and Abdelilah Haijoub},
keywords = {EKF-SLAM, Simultaneous Localization and Mapping, Green Technologies, FPGA},
abstract = {Simultaneous Localization and Mapping enable a mobile robot that is exploring an uncharted environment to localize itself and calculate its path within the map. In the context of green technologies and applications, there is a growing need for efficient SLAM solutions that not only provide accurate localization and mapping but also minimize power consumption. EKF-SLAM is a SLAM solution based on the Extended Kalman Filter, it is well known In the domain of robotics for its ability to handle non-linear models, its ability to handle noise related to the sensors, and its extremely high degree of precision. To guarantee real-time performance, the EKF-SLAM implementation requires a high-performance hardware architecture. In light of this challenge, researchers are thinking about using parallel processing platforms like FPGAs, which can provide the required level of performance and meet strict constraints on physics, computing capacity, and electrical power. This study describes a hardware architecture's implementation design for EKF-SLAM on an FPGA platform. The entire design is built on the Cyclone 2 FPGA, which has a maximum speed of 114 MHz and 18577 LUTs, creating a highly efficient hardware architecture.}
}
@article{VANDENHURK2023106030,
title = {Consideration of compound drivers and impacts in the disaster risk reduction cycle},
journal = {iScience},
volume = {26},
number = {3},
pages = {106030},
year = {2023},
issn = {2589-0042},
doi = {https://doi.org/10.1016/j.isci.2023.106030},
url = {https://www.sciencedirect.com/science/article/pii/S2589004223001074},
author = {Bart J.J.M. {van den Hurk} and Christopher J. White and Alexandre M. Ramos and Philip J. Ward and Olivia Martius and Indiana Olbert and Kathryn Roscoe and Henrique M.D. Goulart and Jakob Zscheischler},
keywords = {Earth sciences, Social sciences, Decision science},
abstract = {Summary
Consideration of compound drivers and impacts are often missing from applications within the Disaster Risk Reduction (DRR) cycle, leading to poorer understanding of risk and benefits of actions. The need to include compound considerations is known, but lack of guidance is prohibiting practitioners from including these considerations. This article makes a step toward practitioner guidance by providing examples where consideration of compound drivers, hazards, and impacts may affect different application domains within disaster risk management. We discern five DRR categories and provide illustrative examples of studies that highlight the role of “compound thinking” in early warning, emergency response, infrastructure management, long-term planning, and capacity building. We conclude with a number of common elements that may contribute to the development of practical guidelines to develop appropriate applications for risk management.}
}
@article{NISHI2022314,
title = {Health and landscape approaches: A comparative review of integrated approaches to health and landscape management},
journal = {Environmental Science & Policy},
volume = {136},
pages = {314-325},
year = {2022},
issn = {1462-9011},
doi = {https://doi.org/10.1016/j.envsci.2022.06.015},
url = {https://www.sciencedirect.com/science/article/pii/S1462901122002027},
author = {Maiko Nishi and Shizuka Hashimoto},
keywords = {Landscape approaches, One Health, Ecohealth, Planetary Health, Social-ecological systems, Sustainability transformation},
abstract = {Landscape approaches are integrated place-based approaches and provide cross-sectoral opportunities to facilitate sustainability transformations. The COVID-19 outbreak has profound ramifications for multiple dimensions of landscapes, ranging from mobility and lifestyle to value to environment and society. Therefore, integrated approaches to “health” have been more vigorously promoted in the policy arena dealing with human–nature interactions. The ecosystem principles of the Convention on Biological Diversity, which resonate with landscape approaches, are generally aligned with integrated approaches to health. However, commonalities and distinctions between these integrated approaches in both political and scientific domains have not been clarified. Drawing on a narrative review of the literature on “One Health,” “Ecohealth,” and “Planetary Health” as major health-oriented approaches in comparison with landscape approaches, the aspects of landscape approaches to be complemented in addressing health-related challenges were examined in this study. In addition to the review on the intellectual roots and evolutionary pathways, a comparative analysis of these relevant approaches was conducted in terms of three realms including theoretical assumptions, knowledge bases, and research paradigms. The results of the comparative review show that all approaches share systems thinking, interdisciplinarity, cross-sectoral collaboration, and holistic paradigm but differ with respect to their focused management problems, disciplines, and sectors as well as ontological and epistemological underpinnings. Pointing to the recent theoretical and methodological development in integrating health in placemaking, the results of this study suggest that pragmatic landscape approaches could be strengthened by using health-related research paradigms to achieve better constructivism–positivism meeting grounds regarding health–landscape intersections.}
}
@article{OLIVER2014289,
title = {Crack-path field and strain-injection techniques in computational modeling of propagating material failure},
journal = {Computer Methods in Applied Mechanics and Engineering},
volume = {274},
pages = {289-348},
year = {2014},
issn = {0045-7825},
doi = {https://doi.org/10.1016/j.cma.2014.01.008},
url = {https://www.sciencedirect.com/science/article/pii/S0045782514000139},
author = {J. Oliver and I.F. Dias and A.E. Huespe},
keywords = {Fracture, Computational material failure, Strong discontinuities, Crack-path field, Strain injection, Finite elements with embedded discontinuities},
abstract = {The work presents two new numerical techniques devised for modeling propagating material failure, i.e. cracks in fracture mechanics or slip-lines in soil mechanics. The first one is termed crack-path-field technique and is conceived for the identification of the path of those cracks, or slip-lines, represented by strain-localization based solutions of the material failure problem. The second one is termed strain-injection, and consists of a procedure to insert, during specific stages of the simulation and in selected areas of the domain of analysis, goal oriented specific strain fields via mixed finite element formulations. In the approach, a first injection, of elemental constant strain modes (CSM) in quadrilaterals, is used, in combination of the crack-path-field technique, for obtaining reliable information that anticipates the position of the crack-path. Based on this information, in a subsequent stage, a discontinuous displacement mode (DDM) is efficiently injected, ensuring the required continuity of the crack-path across sides of contiguous elements. Combination of both techniques results in an efficient and robust procedure based on the staggered resolution of the crack-path-field and the mechanical failure problems. It provides the classical advantages of the “intra-elemental” methods for capturing complex propagating displacement discontinuities in coarse meshes, as E-FEM or X-FEM methods, with the non-code-invasive character of the crack-path-field technique. Numerical representative simulations of a wide range of benchmarks, in terms of the type of material and the failure problem, show the broad applicability, accuracy and robustness of the proposed methodology. The finite element code used for the simulations is open-source and available at http://www.cimne.com/compdesmat/.}
}
@article{LI2023104935,
title = {Development of a distributed MR-IoT method for operations and maintenance of underground pipeline network},
journal = {Tunnelling and Underground Space Technology},
volume = {133},
pages = {104935},
year = {2023},
issn = {0886-7798},
doi = {https://doi.org/10.1016/j.tust.2022.104935},
url = {https://www.sciencedirect.com/science/article/pii/S0886779822005764},
author = {Wei Li and Zhoujing Ye and Yajian Wang and Hailu Yang and Songli Yang and Zhenlong Gong and Linbing Wang},
keywords = {Underground Pipeline Network, Mixed Reality, IoT Cloud Platform, Data Communication, Operation and Maintenance},
abstract = {The underground pipeline network (UPN) is an essential infrastructure and plays an irreplaceable role in national defense and urban activities. The complexity of structural environment and management makes its operation and maintenance difficult. To solve this problem, a distributed mixed reality (MR) and internet of things (IoT) system is developed through game thinking. Firstly, digital models are created based on design drawings and real-world environments, and then an MR system for the UPN is built by the game engine and the OpenXR platform. Secondly, an IoT cloud platform is built to connect with the MR system based on the API sets and cloud services; the data communication between sensors and MR devices is linked with the Socket method, and the data filtering model is constructed by the Kalman algorithm to realize the information exchange between the field workers and the backend managers. Finally, the National Center for Materials Service Safety at the University of Science and Technology Beijing (NCMS_USTB) is used as the experimental site to test this system, and its underground sewage and rainwater pipeline network are used to simulate the key problems in the operation and maintenance. The effect of the application shows that there is potential technical complementarity between the MR and IoT, and the distributed MR-IoT approach can be used as a new technical reference for the operation and maintenance of the UPN.}
}
@article{VALLESPERIS2024102448,
title = {Digital citizenship at school: Democracy, pragmatism and RRI},
journal = {Technology in Society},
volume = {76},
pages = {102448},
year = {2024},
issn = {0160-791X},
doi = {https://doi.org/10.1016/j.techsoc.2023.102448},
url = {https://www.sciencedirect.com/science/article/pii/S0160791X23002531},
author = {Núria Vallès-Peris and Miquel Domènech},
keywords = {Science and technology studies, Digital citizenship, Responsible research and innovation, Democracy, Pragmatism},
abstract = {This paper presents a strategy for fostering digital citizenship at school that transcends the mere use of digital devices or instructional methods focused solely on their use. The core premise of this proposal rests on the need for an ethical-political debate concerning digitization in education. In addition, it emphasizes the need to cultivate a form of digital literacy that blends science and technology with the humanities, and erases the traditional boundaries between making and thinking. The proposed approach encapsulates two primary concerns: firstly, it asserts that digital literacy serves as a foundation for meaningful participation in digital societies; secondly, it underscores the importance of democratizing digital technologies by incorporating the perspectives, needs, and concerns of children. Drawing inspiration from the theories of pragmatism and responsible research and innovation (RRI), we present a conceptual framework for digital citizenship. To operationalize this approach, we adapt John Dewey's pragmatic model of inquiry as a method that can be applied within the school setting. This pragmatic methodology serves as a conduit for developing hands-on experience geared towards developing digital citizenship. The practical implementation of this methodology is illustrated through an actualized experience with 10- and 11-year-old children in a public primary school, regarding the issue of care robots. This paper advocates for a symbiotic relationship between theoretical understanding and practical application, and puts forward a concrete proposal for the integration of digital citizenship in schools in the form of a four-phase procedural model, based on the creation of what we term ‘the encounter’ between the educational community and the research and development community.}
}
@article{FRANTZ2003265,
title = {Herbert Simon. Artificial intelligence as a framework for understanding intuition},
journal = {Journal of Economic Psychology},
volume = {24},
number = {2},
pages = {265-277},
year = {2003},
note = {The Economic Psychology of Herbert A. Simon},
issn = {0167-4870},
doi = {https://doi.org/10.1016/S0167-4870(02)00207-6},
url = {https://www.sciencedirect.com/science/article/pii/S0167487002002076},
author = {Roger Frantz},
keywords = {Herbert Simon, Intuition, Artificial intelligence, Bounded rationality, Economics and psychology},
abstract = {Herbert Simon made overlapping substantive contributions to the fields of economics, psychology, cognitive science, artificial intelligence, decision theory, and organization theory. Simon’s work was motivated by the belief that neither the human mind, human thinking and decision making, nor human creativity need be mysterious. It was after he helped create “thinking” machines that Simon came to understand human intuition as subconscious pattern recognition. In doing so he showed that intuition need not be associated with magic and mysticism, and that it is complementary with analytical thinking. This paper will show how the overlaps in his work and especially his work on AI affected his view towards intuition.}
}
@article{SUJAN2023105994,
title = {Operationalising FRAM in Healthcare: A critical reflection on practice},
journal = {Safety Science},
volume = {158},
pages = {105994},
year = {2023},
issn = {0925-7535},
doi = {https://doi.org/10.1016/j.ssci.2022.105994},
url = {https://www.sciencedirect.com/science/article/pii/S0925753522003332},
author = {M. Sujan and L. Pickup and M.S. {de Vos} and R. Patriarca and L. Konwinski and A. Ross and P. McCulloch},
keywords = {Patient Safety, FRAM, Resilience Engineering, System Safety},
abstract = {Resilience Engineering principles are becoming increasingly popular in healthcare to improve patient safety. FRAM is the best-known Resilience Engineering method with several examples of its application in healthcare available. However, the guidance on how to apply FRAM leaves gaps, and this can be a potential barrier to its adoption and potentially lead to misuse and disappointing results. The article provides a self-reflective analysis of FRAM use cases to provide further methodological guidance for successful application of FRAM to improve patient safety. Five FRAM use cases in a range of healthcare settings are described in a structured way including critical reflection by the original authors of those studies. Individual reflections are synthesised through group discussion to identify lessons for the operationalisation of FRAM in healthcare. Four themes are developed: (1) core characteristics of a FRAM study, (2) flexibility regarding the underlying epistemological paradigm, (3) diversity with respect to the development of interventions, and (4) model complexity. FRAM is a systems analysis method that offers considerable flexibility to accommodate different epistemological positions, ranging from realism to phenomenology. We refer to these as computational FRAM and reflexive FRAM, respectively. Practitioners need to be clear about their analysis aims and their analysis position. Further guidance is needed to support practitioners to tell a convincing and meaningful “system story” through the lens of FRAM.}
}
@article{CAMPAGNOLO2007387,
title = {The Methods of Approximation and Lifting in Real Computation},
journal = {Electronic Notes in Theoretical Computer Science},
volume = {167},
pages = {387-423},
year = {2007},
note = {Proceedings of the Third International Conference on Computability and Complexity in Analysis (CCA 2006)},
issn = {1571-0661},
doi = {https://doi.org/10.1016/j.entcs.2006.09.013},
url = {https://www.sciencedirect.com/science/article/pii/S1571066107000229},
author = {Manuel L. Campagnolo and Kerry Ojakian},
keywords = {Computable Analysis, Real Recursive Functions, Elementary Computable},
abstract = {The basic motivation behind this work is to tie together various computational complexity classes, whether over different domains such as the naturals or the reals, or whether defined in different manners, via function algebras (Real Recursive Functions) or via Turing Machines (Computable Analysis). We provide general tools for investigating these issues, using a technique we call the method of approximation. We give the general development of this method, and apply it to obtain 2 theorems. First we connect the discrete operation of linear recursion (basically equivalent to the combination of bounded sums and bounded products) to linear differential equations, thus providing an alternative proof of the result from Campagnolo, Moore and Costa [M.L. Campagnolo, C. Moore and J. F. Costa, An analog characterization of the Grzegorczyk hierarchy, Journal of Complexity 18 (2002) 977–100]. Secondly, we extend this to prove a result similar to that of Bournez and Hainry [O. Bournez and E. Hainry, Elementarily computable functions over the real numbers and R-sub-recursive functions, Theoretical Computer Science 348 (2005) 130–147], providing a function algebra for the real functions computable in elementary time. Their proof involves simulating the operation of a Turing Machine using a function algebra. We avoid this simulation, using a technique we call “lifting,” which allows us to lift the classic result regarding the Kalmar elementary computable functions to a result on the reals. While we do not claim that our result is necessarily an improvement (perhaps just different), we do want to make the point that our two techniques appear readily applicable to other problems of this sort.}
}
@article{YANG2001167,
title = {Computational verb systems: computing with perceptions of dynamics},
journal = {Information Sciences},
volume = {134},
number = {1},
pages = {167-248},
year = {2001},
note = {Computing with Words},
issn = {0020-0255},
doi = {https://doi.org/10.1016/S0020-0255(01)00096-2},
url = {https://www.sciencedirect.com/science/article/pii/S0020025501000962},
author = {Tao Yang},
abstract = {The concepts and principles of (computational) verb logic, (computational) verb set, (computational) verb number, (computational) verb prediction and (computational) verb control are presented.}
}
@article{DEBRIGARD2021104574,
title = {Perceived similarity of imagined possible worlds affects judgments of counterfactual plausibility},
journal = {Cognition},
volume = {209},
pages = {104574},
year = {2021},
issn = {0010-0277},
doi = {https://doi.org/10.1016/j.cognition.2020.104574},
url = {https://www.sciencedirect.com/science/article/pii/S0010027720303930},
author = {Felipe {De Brigard} and Paul Henne and Matthew L. Stanley},
keywords = {Imagination, Counterfactual thinking, Plausibility, Similarity, Possible worlds},
abstract = {People frequently entertain counterfactual thoughts, or mental simulations about alternative ways the world could have been. But the perceived plausibility of those counterfactual thoughts varies widely. The current article interfaces research in the philosophy and semantics of counterfactual statements with the psychology of mental simulations, and it explores the role of perceived similarity in judgments of counterfactual plausibility. We report results from seven studies (N = 6405) jointly supporting three interconnected claims. First, the perceived plausibility of a counterfactual event is predicted by the perceived similarity between the possible world in which the imagined situation is thought to occur and the actual world. Second, when people attend to differences between imagined possible worlds and the actual world, they think of the imagined possible worlds as less similar to the actual world and tend to judge counterfactuals in such worlds as less plausible. Lastly, when people attend to what is identical between imagined possible worlds and the actual world, they think of the imagined possible worlds as more similar to the actual world and tend to judge counterfactuals in such worlds as more plausible. We discuss these results in light of philosophical, semantic, and psychological theories of counterfactual thinking.}
}
@article{OGBUNU2024R913,
title = {C. Brandon Ogbunu},
journal = {Current Biology},
volume = {34},
number = {20},
pages = {R913-R915},
year = {2024},
issn = {0960-9822},
doi = {https://doi.org/10.1016/j.cub.2024.09.022},
url = {https://www.sciencedirect.com/science/article/pii/S0960982224012338},
author = {C. Brandon Ogbunu}
}
@article{LEE2005261,
title = {Insights into nucleic acid reactivity through gas-phase experimental and computational studies},
journal = {International Journal of Mass Spectrometry},
volume = {240},
number = {3},
pages = {261-272},
year = {2005},
note = {Mass Spectrometry of Biopolymers: From Model Systems to Ribosomes},
issn = {1387-3806},
doi = {https://doi.org/10.1016/j.ijms.2004.09.020},
url = {https://www.sciencedirect.com/science/article/pii/S1387380604003975},
author = {Jeehiun K. Lee},
keywords = {Nucleic acids, RNA, DNA, Enzyme nucleobase, Acidity, Proton affinity},
abstract = {Accurate measurements of the acidities and basicities of nucleic bases and nucleic base derivatives is essential for understanding issues of fundamental importance in biological systems. Hydrogen bonding modulates recognition of DNA and RNA bases, and the interaction energy between two bonded complementary nucleobases is dependent on the intrinsic basicity and acidity of the acceptor and donor groups. In addition, understanding the intrinsic reactivity of nucleic bases can shed light on key biosynthetic mechanisms for which nucleobases are substrates. In this review, we highlight advances in our lab toward understanding the fundamental reactivity of DNA and RNA. In particular, we focus on our investigation of the gas phase acidities and basicities of natural and unnatural nucleobases, and the implications of our results for the mechanisms of nucleotide biosynthetic and repair enzymes.}
}
@article{MATHESON2020116697,
title = {The role of the motor system in generating creative thoughts},
journal = {NeuroImage},
volume = {213},
pages = {116697},
year = {2020},
issn = {1053-8119},
doi = {https://doi.org/10.1016/j.neuroimage.2020.116697},
url = {https://www.sciencedirect.com/science/article/pii/S1053811920301841},
author = {Heath E. Matheson and Yoed N. Kenett},
keywords = {Motor system, Creativity, Simulations, Embodied cognition, Grounded cognition, Divergent thinking, Improvisation},
abstract = {Neurocognitive research is pertinent to developing mechanistic models of how humans generate creative thoughts. Such models usually overlook the role of the motor cortex in creative thinking. The framework of embodied or grounded cognition suggests that creative thoughts (e.g. using a shoe as a hammer, improvising a piano solo) are partially served by simulations of motor activity associated with tools and their use. The major hypothesis stemming from the embodied or grounded account is that, while the motor system is used to execute actions, simulations within this system also support higher-order cognition, creativity included. That is, the cognitive process of generating creative output, not just executing it, is deeply embedded in motor processes. Here, we highlight a collection of neuroimaging research that implicates the motor system in generating creative thoughts, including some evidence for its functionally necessary role in generating creative output. Specifically, the grounded or embodied framework suggests that generating creative output may, in part, rely on motor simulations of possible actions, and that these simulations may by partially implemented in the motor regions themselves. In such cases, action simulations (i.e. reactivating or re-using the motor system), do not result in overt action but instead are used to support higher-order cognitive goals like generating creative uses or improvising.}
}
@article{HUBERMAN19981169,
title = {Computation as economics},
journal = {Journal of Economic Dynamics and Control},
volume = {22},
number = {8},
pages = {1169-1186},
year = {1998},
issn = {0165-1889},
doi = {https://doi.org/10.1016/S0165-1889(98)00008-6},
url = {https://www.sciencedirect.com/science/article/pii/S0165188998000086},
author = {Bernardo A. Huberman},
abstract = {We use computers to study economics, but few people realize that we can use economics to study and design computational systems. The reason is that computer networks can be regarded as a community of processes that in their interactions, strategies and lack of perfect knowledge face the same issues as people in markets. This paper describes how computers have evolved to a point where economics approaches are useful for designing them and understanding their dynamics. Examples are given of existing computer systems that use market mechanisms and of novel phenomena, such as clustered volatility, that we uncovered when studying their evolution.}
}
@article{JIANG2024102530,
title = {Product innovation design approach driven by implicit relationship completion via patent knowledge graph},
journal = {Advanced Engineering Informatics},
volume = {61},
pages = {102530},
year = {2024},
issn = {1474-0346},
doi = {https://doi.org/10.1016/j.aei.2024.102530},
url = {https://www.sciencedirect.com/science/article/pii/S1474034624001782},
author = {Shaofei Jiang and Jingwei Yang and Jing Xie and Xuesong Xu and Yubo Dou and Liting Jing},
keywords = {Product innovation design, Patent text, Knowledge graph, RFSB ontology model, Implicit relationship completion},
abstract = {Product innovation design process involves a great deal of discrete engineering knowledge, limiting the ability of designers to quickly utilize this knowledge to support design innovation. Nowadays, innovation design based on knowledge graphs has enhanced the ability to explore design knowledge, improving the efficiency of knowledge retrieval. Previous studies have focused on mining more design knowledge to enrich the knowledge graph overlooks the implicit relationships with potential value among design knowledge, wasting design resources. To address these issues, an approach for product innovation design based on implicit knowledge relationship completion in the patent knowledge graph is proposed, which explores the implicit relationships between design knowledge to provide new knowledge satisfying design preferences and enhance the innovativeness of solutions. First, a requirements-function-structure-benefit (RFSB) knowledge ontology is constructed and extracted from the benefit knowledge of patents to build the knowledge graph. Second, an implicit relationship completion model based on the similarity of function or benefit entities explores the implicit relationships, replacing structure entities directly connected to similar function or benefit entities to generate new relationships and outputs novel ideas. Third, a scheme improvement process based on the co-occurrence frequency of requirement and structure knowledge supplements neglected design preferences. Final, a pipeline inspection robot case study is further employed to verify the proposed approach, and a patent knowledge graph assisted design solution prototype system is developed to assist in the utilization of innovative design knowledge. Evaluation results show the significant design potential of the proposed approach in inspiring innovative thinking and knowledge reuse.}
}
@article{ATREIDES202135,
title = {E-governance with ethical living democracy},
journal = {Procedia Computer Science},
volume = {190},
pages = {35-39},
year = {2021},
note = {2020 Annual International Conference on Brain-Inspired Cognitive Architectures for Artificial Intelligence: Eleventh Annual Meeting of the BICA Society},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2021.06.004},
url = {https://www.sciencedirect.com/science/article/pii/S1877050921012461},
author = {Kyrtin Atreides},
keywords = {mASI, AGI, e-governance, mediated artificial superintelligence, collective superintelligence, direct digital democracy, liquid democracy, EAP, Effective Altruistic Principles, Ethical Living Democracy, ELD},
abstract = {A new form of e-governance is proposed based on systems seen in biological life at all scales. This model of e-governance offers the performance of collective superintelligence, equally high ethical quality, and a substantial reduction in resource requirements for government functions. In addition, the problems seen in modern forms of government such as misrepresentation, corruption, lack of expertise, short-term thinking, political squabbling, and popularity contests may be rendered virtually obsolete by this approach. Lastly, this model of government generates a digital ecosystem of intelligent life which mirrors physical citizens, serving to bridge the emotional divide between physical and digital life, while also producing the first form of government able to keep pace with accelerating technological progress.}
}
@article{MORGAN20052564,
title = {The visual computation of 2-D area by human observers},
journal = {Vision Research},
volume = {45},
number = {19},
pages = {2564-2570},
year = {2005},
issn = {0042-6989},
doi = {https://doi.org/10.1016/j.visres.2005.04.004},
url = {https://www.sciencedirect.com/science/article/pii/S0042698905002075},
author = {M.J. Morgan},
keywords = {Psychophysics, Shape, Weber fraction},
abstract = {Normal human observers compared either the width, height or area of two simultaneously-presented shapes (the standard and the test), with a cue to indicate which decision had to be made. On ‘area’ trials, test width was a random variable, ensuring that neither shape (aspect ratio), width nor height by themselves was a reliable signal. Weber fractions for width and height of both ellipses and rectangles were in the range 5–10%, but for area they were higher (10–20%) than predicted from the combination of noisy width and height decisions. With ellipses, observers were more likely to overestimate width or height when the other dimension differed from the standard in the same direction (e.g. both greater). We conclude that observers have no access to high-precision codes for 2-D area, and that they base their decisions on a variety of heuristics derived from 1-D codes. A second experiment measured acuity for changes in aspect ratio. For ellipses, accuracy for aspect ratio was higher than predicted by the combination of noisy width and height signals; for rectangles it was worse, suggesting that 2-D curvature is a potent cue to shape.}
}
@article{TAN2025127211,
title = {Neural architecture search with integrated template-modules for efficient defect detection},
journal = {Expert Systems with Applications},
pages = {127211},
year = {2025},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2025.127211},
url = {https://www.sciencedirect.com/science/article/pii/S0957417425008334},
author = {Wanrong Tan and Lingling Huang and Hong Li and Menghao Tan and Jin Xie and Weifeng Gao},
keywords = {Neural architecture search, Defect detection, Bi-level optimization, Genetic algorithm, Penalty term},
abstract = {Surface defect detection in industrial production is critical for quality control. Traditional manual design of detection models is time-consuming, inefficient, and lacks adaptability to diverse defect scenarios. To address these limitations, we propose TMNAS (Template-Module Neural Architecture Search), a bi-level optimization framework that automates the design of high-performance defect detection models. TMNAS uniquely integrates predefined template-modules into a flexible search space, enabling simultaneous exploration of architectural components and parameters. By incorporating a single-objective genetic algorithm with a computational complexity penalty term, our approach effectively avoids local optima and significantly reduces search resource consumption. Extensive experiments on industrial defect datasets demonstrate that TMNAS surpasses state-of-the-art models, while on the COCO benchmark, it achieves a competitive mean average precision (mAP) of 58.4%, all with lower computational overhead.}
}
@article{OLIVEIRA2022102347,
title = {Beyond energy services: A multidimensional and cross-disciplinary agenda for home energy management research},
journal = {Energy Research & Social Science},
volume = {85},
pages = {102347},
year = {2022},
issn = {2214-6296},
doi = {https://doi.org/10.1016/j.erss.2021.102347},
url = {https://www.sciencedirect.com/science/article/pii/S2214629621004382},
author = {Sonja Oliveira and Lidia Badarnah and Merate Barakat and Anna Chatzimichali and Ed Atkins},
keywords = {Architecture, Biomimetics, Computational design, Cross-disciplinary methods, Home energy management},
abstract = {Home Energy Management (HEM) has a significantly growing impact on strategic energy policy, digital equity, as well as housing development and transport issues. With the proliferation of home working, reliance on electricity for heating and cooling and the increasing needs for electric charging for transportation, there is an urgent need to develop novel ways for efficient management of home energy use. Current efforts focus on HEM technologies at individual household levels, without considering the social or spatial context or their collective community-wide interrelated dependencies. We propose a multifaceted agenda at the intersection of disciplinary domains to tackle this problem by using a multidimensional lens that draws on energy behaviour, architectural research, biomimetics, and computational design, simultaneously. Optimal and effective behavioural patterns can be extracted and abstracted from nature, informing a more collective and interrelated behavioural dependencies approach that considers the complex multidimensional energy use patterns of different housing typologies. This paper discusses the analytical benefits of this new research approach through a study of home energy management behaviour. The approach though could be expanded to consider other similar empirical contexts whereby sustainable multidimensional resource management is sought such as water use, food distribution as well as transport and mobility.}
}
@article{CHEN2014740,
title = {On the Systematic Method to Enhance the Epiphany Ability of Individuals},
journal = {Procedia Computer Science},
volume = {31},
pages = {740-746},
year = {2014},
note = {2nd International Conference on Information Technology and Quantitative Management, ITQM 2014},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.05.322},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914004992},
author = {Ailing Chen and Wei Liu and Zhihui Wu and Jun Zhang},
keywords = {Prototype heuristics, epiphany, extenics, Theory of Creativity, sytematic scheme ;},
abstract = {Epiphany is a crucial stage in the process of creative thinking. The prototype heuristic theory has proved that the individual epiphany ability depends on the individual's ability to get out of the fetter of mental fixation, activate the prototype and acquire the key heuristic information from the activated prototype. Based on this theory, this present research combines the findings of extenics, TRIZ and theory of creativity to have developed a systematic method on enhancing individual epiphany ability. Supported by information technology, the method takes theory of creativity as its methodology, extension strategy generation as its framework, element theory its database and knowledge management its feedback chain. The research aims to cultivate creative thinking and eventually enhance the creativity of individuals.}
}
@article{CAGLAYAN2015131,
title = {Making sense of eigenvalue–eigenvector relationships: Math majors’ linear algebra – Geometry connections in a dynamic environment},
journal = {The Journal of Mathematical Behavior},
volume = {40},
pages = {131-153},
year = {2015},
issn = {0732-3123},
doi = {https://doi.org/10.1016/j.jmathb.2015.08.003},
url = {https://www.sciencedirect.com/science/article/pii/S0732312315000504},
author = {Günhan Caglayan},
keywords = {Undergraduate mathematics education, Dynamic geometry software, Visualization, Representation, Connection, Linear algebra, Eigenvectors, Eigenvalues, Matrices},
abstract = {The present qualitative case study on mathematics majors’ visualization of eigenvector–eigenvalue concepts in a dynamic environment highlights the significance of student-generated representations in a theoretical framework drawn from Sierpinska's (2000) modes of thinking in linear algebra. Such an approach seemed to provide the research participants with mathematical freedom, which resulted in an awareness of the multiple ways that eigenvalue–eigenvector relationships could be visualized in a manner that widened students’ repertoire of meta-representational competences (diSessa, 2004) in coordination with their preferred modes of visualization. Students’ expression of visual fluency in the course of making sense of the eigenvalue problem Au=λu associated with a variety of matrices occurred in different, yet not necessarily hierarchical modes of visualizations that differed from matrix to matrix: (i) synthetic/analytic mode manifested in the process of detecting eigenvectors when the sought eigenvector and the matrix-applied product vector were aligned in the same/opposite directions; (ii) analytic arithmetic mode manifested in the case of singular matrices (in the determination of the zero eigenvalue) and invertible matrices with nonreal eigenvalues; (iii) analytic structural mode, though rarely occurred, manifested in making sense of the trajectory (circle, ellipse, line segment) of the matrix-applied product vector and relating trajectory behavior to matrix type. While the connection between the thinking modes (Sierpinska, 2000) and the concreteness–necessity–generalizability triad (Harel, 2000) was not sharp, math majors still frequently implemented the CNG principles, which proved facilitatory tools in the evolution of students’ thinking about the eigenvalue–eigenvector relationships.}
}
@incollection{GRIFFIN2020303,
title = {Information Graphics},
editor = {Audrey Kobayashi},
booktitle = {International Encyclopedia of Human Geography (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {303-314},
year = {2020},
isbn = {978-0-08-102296-2},
doi = {https://doi.org/10.1016/B978-0-08-102295-5.10563-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780081022955105633},
author = {Amy L. Griffin},
keywords = {Big data, Cognition, Exploratory data analysis, Geovisualization, Graphics, Information dashboard, Information visualization, Interactivity, Perception, Semiotics, Storytelling, Visual analytics},
abstract = {Information graphics include a wide variety of static and dynamic visual representations of information such as diagrams, statistical graphics, and maps. These displays take different forms depending on the purpose for which the graphic is being constructed (e.g., for thinking or communication), characteristics of the data, potential visual display forms, and whether or not the information graphic is interactive. Choosing an appropriate method for representing data requires a basic understanding of the perceptual and cognitive processing that occurs in the human visual system. Information graphics can be constructed using a wide array of tools, but are increasingly constructed using computer code and distributed through the internet.}
}
@article{ASSIOURAS2025104063,
title = {The evolution of artificial empathy in the hospitality metaverse era},
journal = {International Journal of Hospitality Management},
volume = {126},
pages = {104063},
year = {2025},
issn = {0278-4319},
doi = {https://doi.org/10.1016/j.ijhm.2024.104063},
url = {https://www.sciencedirect.com/science/article/pii/S027843192400375X},
author = {Ioannis Assiouras and Cornelia Laserer and Dimitrios Buhalis},
keywords = {Empathy, Artificial empathy, Artificial intelligence, Metaverse, Hospitality, Artificial intelligence agents},
abstract = {As hospitality enters the metaverse era, artificial empathy becomes essential for developing of artificial intelligence (AI) agents. Using the empathy cycle model, computational empathy frameworks and interdisciplinary research, this conceptual paper proposes a model explaining how artificial empathy will evolve in the hospitality metaverse era. The paper also addresses customer empathy and responses towards AI agents and other human actors with in the hospitality context. It explores how metaverse characteristics such as immersiveness, sociability, experiential nature, interoperability, blended virtual and physical environments as well as environmental fidelity will shape computational models and evolution of artificial empathy. Findings suggests that metaverse enables AI agents to form a seamless cycle of detection, resonation, and response to consumers’ affective states, facilitating the evolution of artificial empathy. Additionally, the paper outlines conditions under which the artificial empathy cycle may be disrupted and proposes future research questions that can advance our understanding of artificial empathy.}
}
@incollection{YETURU20233,
title = {Chapter 1 - Object-oriented basis of artificial intelligence methodologies},
editor = {Steven G. Krantz and Arni S.R. {Srinivasa Rao} and C.R. Rao},
series = {Handbook of Statistics},
publisher = {Elsevier},
volume = {49},
pages = {3-46},
year = {2023},
booktitle = {Artificial Intelligence},
issn = {0169-7161},
doi = {https://doi.org/10.1016/bs.host.2023.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169716123000251},
author = {Kalidas Yeturu},
keywords = {Object oriented, Vector representation, Induction, Deduction, Machine learning, Artificial intelligence},
abstract = {If O2 is for I, then what is O2 for AI? What is oxygen for humans is what is object-oriented thinking for artificial intelligent systems. Much the same way as humans need O2 knowingly or unknowingly, the first step in designing an AI system requires the application of object-oriented principles either explicitly or implicitly. The basis of the definition of state in AI is a description of the concept of interest as an object with properties. The idea of an object extends beyond typical noun forms that describe elements of the real world. The verb forms are included as well with -able suffixes such as runnable, serializable, and executable. In the software world, the first step in modeling a business requirement is the identification of objects of interest and defining their properties and interactions. For instance, in the case of web services, a service is an object; in the case of database systems, a table or a transaction is an object; or in the case of large-scale integration, electronic components are objects. The concept of an object extends beyond the software realm and into the mathematical world in an implicit form. Functional analysis is an old topic in mathematics where each function is an object indeed. The concept of space in mathematics relates closely to the possible value ranges of all attributes of an object. Mathematical operators are the same as methods of objects. It is day-to-day practical life in any modern operating system software dealing with process objects and applications such as Python scripts involving function objects. In this chapter, the application of object-oriented thinking to convert a business requirement to a machine learning (ML) formulation is presented with examples. The five steps of supervised ML formulation based on vector representations of input and output, mapping function, loss function, and data set are clarified. The scope and limitation of ML formulation as against general AI methodology are discussed to demystify popular myths. This chapter also reveals the secret behind the success of deep learning methodology as automatic differentiation involving function objects.}
}
@article{LAKAMSANI1995993,
title = {Mapping molecular dynamics computations on to hypercubes},
journal = {Parallel Computing},
volume = {21},
number = {6},
pages = {993-1013},
year = {1995},
issn = {0167-8191},
doi = {https://doi.org/10.1016/0167-8191(95)00006-A},
url = {https://www.sciencedirect.com/science/article/pii/016781919500006A},
author = {Vamsee Lakamsani and Laxmi N. Bhuyan and D.Scott Linthicum},
keywords = {Mapping problem, Recursive mincut, Molecular dynamics, Compact MD graph, Hypercube},
abstract = {We propose an approach for partitioning an irregular application problem in computational biology called Molecular Dynamics (MD) of Macromolecules. We model the application as a task graph which we call a compact MD graph. Such a modeling allows existing mapping heuristics to be applied to this problem. We then provide a parallel algorithm for this application, by using an efficient mapping heuristic called Allocation By Recursive Mincut (ARM) to map the compact MD graph to a hypercube connected parallel computer, the nCUBE 2S. A canonical model for executing parallel computations modeled as graphs is described. Thus, we attempt to provide the missing link between the mapping research and application implementation research, and demonstrate that the execution time can be sufficiently reduced by considering formal mapping techniques, while designing parallel programs for important applications.}
}
@article{ALOUPIS2015135,
title = {Classic Nintendo games are (computationally) hard},
journal = {Theoretical Computer Science},
volume = {586},
pages = {135-160},
year = {2015},
note = {Fun with Algorithms},
issn = {0304-3975},
doi = {https://doi.org/10.1016/j.tcs.2015.02.037},
url = {https://www.sciencedirect.com/science/article/pii/S0304397515001735},
author = {Greg Aloupis and Erik D. Demaine and Alan Guo and Giovanni Viglietta},
keywords = {Nintendo games, Video games, Computational complexity, NP-hardness, PSPACE-hardness},
abstract = {We prove NP-hardness results for five of Nintendo's largest video game franchises: Mario, Donkey Kong, Legend of Zelda, Metroid, and Pokémon. Our results apply to generalized versions of Super Mario Bros. 1–3, The Lost Levels, and Super Mario World; Donkey Kong Country 1–3; all Legend of Zelda games; all Metroid games; and all Pokémon role-playing games. In addition, we prove PSPACE-completeness of the Donkey Kong Country games and several Legend of Zelda games.}
}
@incollection{WU2012223,
title = {10 - Computational modeling and ab initio calculations in MAX phases – II},
editor = {I.M. Low},
booktitle = {Advances in Science and Technology of Mn+1AXn Phases},
publisher = {Woodhead Publishing},
pages = {223-270},
year = {2012},
isbn = {978-1-84569-991-8},
doi = {https://doi.org/10.1533/9780857096012.223},
url = {https://www.sciencedirect.com/science/article/pii/B9781845699918500102},
author = {E. Wu},
keywords = {computational modeling,  calculations, density function theory, energy band, electronic properties, density of states},
abstract = {Abstract:
This chapter reviews the latest researches and advances in the uses of the computational modeling and ab initio calculations on the study of the MAX phases and their properties. The fundamentals and approaches of the density functional theory in the ab initio quantum mechanical calculations and the importance of the theory in the study of the MAX phases are introduced. The studies of the electronic structures and properties, in particular, the energy band structures and total and/or partial density of states of the MAX phases, by using the means of the density function theory are illustrated and discussed. The stability and occurrence of the MAX phases predicted and confirmed by the density functional theory based energetic calculations are addressed. The ab initio calculated elastic and other physical properties of the MAX phases, and the effects of pressure, defects and impurities on the various structural and physical properties are also discussed.}
}
@article{BENTON20001135,
title = {Computational modelling of interleaved first- and second-order motion sequences and translating 3f+4f beat patterns},
journal = {Vision Research},
volume = {40},
number = {9},
pages = {1135-1142},
year = {2000},
issn = {0042-6989},
doi = {https://doi.org/10.1016/S0042-6989(00)00026-2},
url = {https://www.sciencedirect.com/science/article/pii/S0042698900000262},
author = {Christopher P. Benton and Alan Johnston and Peter W. McOwan},
keywords = {Motion perception, Computational modelling, Second-order, Feature tracking},
abstract = {Despite detailed psychophysical, neurophysiological and electrophysiological investigation, the number and nature of independent and parallel motion processing mechanisms in the visual cortex remains controversial. Here we use computational modelling to evaluate evidence from two psychophysical studies collectively thought to demonstrate the existence of three separate and independent motion processing channels. We show that the pattern of psychophysical results can largely be accounted for by a single mechanism. The results demonstrate that a low-level luminance based approach can potentially provide a wider account of human motion processing than generally thought possible.}
}